{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import utils\n",
    "from datagenerators import VARGenerator\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, random_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "((432, 7840, 2), (432, 2, 2), (432, 2, 2), (432, 7840, 2), (432, 2, 2))"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_data():\n",
    "    # Create a config file\n",
    "    config = utils.parse_config()\n",
    "    config.update(utils.generate_parameters(config)[0])\n",
    "    config.c11 = [0.1, 0.5, 0.8, 0.9]\n",
    "    config.c22 = [0.1, 0.8, 0.9]\n",
    "    config.c12 = [0, 0.1, 0.3, 0.9]\n",
    "    config.sigma_eta_diag = [0.3, 0.1, 0.05]\n",
    "    config.sigma_eta_off_diag = [0.05, 0.02, 0.01]\n",
    "    mini_configs = utils.generate_parameters(config)\n",
    "\n",
    "    # Get data from configs\n",
    "    X, coef_mats, edges, noises, noises_cov = [], [], [], [], []\n",
    "    for exp_params in mini_configs:\n",
    "        # Parse config\n",
    "        experiment_config = utils.deepcopy_lvl1(config)\n",
    "        experiment_config.update(exp_params)\n",
    "        experiment_config = utils.get_nested_config(experiment_config)\n",
    "        noise_cov = np.array([[experiment_config.sigma_eta_diag, experiment_config.sigma_eta_off_diag], [experiment_config.sigma_eta_off_diag, experiment_config.sigma_eta_diag]])\n",
    "\n",
    "        # Get all of the necessary data\n",
    "        generator = VARGenerator(experiment_config)\n",
    "        data, coef_mat, edge = generator.generate()\n",
    "        noise = generator.get_noise()\n",
    "\n",
    "        # Save the data\n",
    "        X.append(data)\n",
    "        coef_mats.append(coef_mat)\n",
    "        noises_cov.append(noise_cov)\n",
    "        edges.append(edge)\n",
    "        noises.append(noise)\n",
    "\n",
    "    return np.array(X), np.array(coef_mats), np.array(edges), np.array(noises), np.array(noises_cov)\n",
    "\n",
    "X, coef_mats, edges, noises, noises_cov = generate_data()\n",
    "X.shape, coef_mats.shape, edges.shape, noises.shape, noises_cov.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float\n",
    "\n",
    "tvt_split = (0.8, 0.1, 0.1)\n",
    "N = X.shape[0]\n",
    "train_size, valid_size = int(tvt_split[0] * N), int(tvt_split[1] * N)\n",
    "test_size = N - train_size - valid_size\n",
    "\n",
    "# Create datasets\n",
    "X_ = torch.from_numpy(X-noises).to(device).to(dtype)\n",
    "noise_ = torch.from_numpy(noises).to(device).to(dtype)\n",
    "noises_cov_ = torch.from_numpy(noises_cov).to(device).to(dtype)\n",
    "dataset = TensorDataset(X_, noise_, noises_cov_)\n",
    "train_set, valid_set, test_set = random_split(dataset, [train_size, valid_size, test_size])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden1, hidden2):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "\n",
    "        p = input_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(p+p*(p+1)//2, hidden1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden1, hidden2, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(hidden2, hidden1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden1, p, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cov_mat):\n",
    "        # Reshape inputs in the desired shape\n",
    "        x = x.transpose(1, 0)\n",
    "        cov_mat = cov_mat[np.tril_indices(cov_mat.shape[0])].flatten()\n",
    "        cov_mat = cov_mat.repeat(x.shape[-1], 1).transpose(1,0)\n",
    "\n",
    "        # Perform the forward pass\n",
    "        x = torch.concat([x, cov_mat], 0)\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x.transpose(0,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "input_size = X.shape[-1]\n",
    "hidden1, hidden2 = 100, 50\n",
    "\n",
    "model = DenoisingAutoencoder(input_size, hidden1, hidden2).to(device)\n",
    "loss_mse = nn.MSELoss()\n",
    "optim = torch.optim.Adam(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500 | Train Loss: 0.17294585581301994 | Valid loss: 0.12713542901152788\n",
      "Epoch 1/500 | Train Loss: 0.10497896515264891 | Valid loss: 0.13129984734709874\n",
      "Epoch 2/500 | Train Loss: 0.10433038844038611 | Valid loss: 0.13338434410303138\n",
      "Epoch 3/500 | Train Loss: 0.10405305197759383 | Valid loss: 0.1277689740581568\n",
      "Epoch 4/500 | Train Loss: 0.1029552616448938 | Valid loss: 0.1276602966840877\n",
      "Epoch 5/500 | Train Loss: 0.10362780946104423 | Valid loss: 0.12493061768107636\n",
      "Epoch 6/500 | Train Loss: 0.10280899283462676 | Valid loss: 0.12445568605217823\n",
      "Epoch 7/500 | Train Loss: 0.10287499624587919 | Valid loss: 0.1244877117664315\n",
      "Epoch 8/500 | Train Loss: 0.10242005997755821 | Valid loss: 0.12407267041677653\n",
      "Epoch 9/500 | Train Loss: 0.10257096674995146 | Valid loss: 0.1242353859855685\n",
      "Epoch 10/500 | Train Loss: 0.10249043327850708 | Valid loss: 0.12426334030406419\n",
      "Epoch 11/500 | Train Loss: 0.10261397785034733 | Valid loss: 0.12246935325133246\n",
      "Epoch 12/500 | Train Loss: 0.10278521800224764 | Valid loss: 0.12285741318970225\n",
      "Epoch 13/500 | Train Loss: 0.10138220626968836 | Valid loss: 0.12370464851170085\n",
      "Epoch 14/500 | Train Loss: 0.10186700045090655 | Valid loss: 0.12271802816106829\n",
      "Epoch 15/500 | Train Loss: 0.10192317982892628 | Valid loss: 0.12064816113994566\n",
      "Epoch 16/500 | Train Loss: 0.10168346067269643 | Valid loss: 0.12137286828527617\n",
      "Epoch 17/500 | Train Loss: 0.10139384967261467 | Valid loss: 0.12033899999115356\n",
      "Epoch 18/500 | Train Loss: 0.10149415854796551 | Valid loss: 0.12014601271339627\n",
      "Epoch 19/500 | Train Loss: 0.10134280799901572 | Valid loss: 0.11977878005005592\n",
      "Epoch 20/500 | Train Loss: 0.10125631595543329 | Valid loss: 0.11962042350408643\n",
      "Epoch 21/500 | Train Loss: 0.10116670431326265 | Valid loss: 0.11954485932581646\n",
      "Epoch 22/500 | Train Loss: 0.1010930053025916 | Valid loss: 0.11922009077009767\n",
      "Epoch 23/500 | Train Loss: 0.1010137248703319 | Valid loss: 0.11904248729521452\n",
      "Epoch 24/500 | Train Loss: 0.10092361378281013 | Valid loss: 0.1188382274622834\n",
      "Epoch 25/500 | Train Loss: 0.10087940496379051 | Valid loss: 0.12084680696039699\n",
      "Epoch 26/500 | Train Loss: 0.1009111285911522 | Valid loss: 0.1179319812028214\n",
      "Epoch 27/500 | Train Loss: 0.10057991560332585 | Valid loss: 0.11848727951562682\n",
      "Epoch 28/500 | Train Loss: 0.10059438851540503 | Valid loss: 0.11843111630269261\n",
      "Epoch 29/500 | Train Loss: 0.10054046226044495 | Valid loss: 0.11823949330421381\n",
      "Epoch 30/500 | Train Loss: 0.10046913035861824 | Valid loss: 0.1181707764694164\n",
      "Epoch 31/500 | Train Loss: 0.10041079426157301 | Valid loss: 0.11805995276501012\n",
      "Epoch 32/500 | Train Loss: 0.10035949790769297 | Valid loss: 0.11807734930757867\n",
      "Epoch 33/500 | Train Loss: 0.10031243523555822 | Valid loss: 0.11795938591104607\n",
      "Epoch 34/500 | Train Loss: 0.10026624825769577 | Valid loss: 0.11791994465991508\n",
      "Epoch 35/500 | Train Loss: 0.1002192439943336 | Valid loss: 0.11785668485559696\n",
      "Epoch 36/500 | Train Loss: 0.10018054794358171 | Valid loss: 0.11786857230025669\n",
      "Epoch 37/500 | Train Loss: 0.10017070897679398 | Valid loss: 0.11813684691523396\n",
      "Epoch 38/500 | Train Loss: 0.10018614044385976 | Valid loss: 0.11835082416790862\n",
      "Epoch 39/500 | Train Loss: 0.10023128852194202 | Valid loss: 0.11900117174657278\n",
      "Epoch 40/500 | Train Loss: 0.10036679851566103 | Valid loss: 0.12002334895348826\n",
      "Epoch 41/500 | Train Loss: 0.1005617426249428 | Valid loss: 0.12064014240926088\n",
      "Epoch 42/500 | Train Loss: 0.10070132120795872 | Valid loss: 0.12069916058071824\n",
      "Epoch 43/500 | Train Loss: 0.10068766907451378 | Valid loss: 0.12008787955828877\n",
      "Epoch 44/500 | Train Loss: 0.10059368468389131 | Valid loss: 0.11966868866841461\n",
      "Epoch 45/500 | Train Loss: 0.10041215909725947 | Valid loss: 0.11918025788699471\n",
      "Epoch 46/500 | Train Loss: 0.10030644052540479 | Valid loss: 0.11899454298234263\n",
      "Epoch 47/500 | Train Loss: 0.10023703687162935 | Valid loss: 0.11879268693629393\n",
      "Epoch 48/500 | Train Loss: 0.10018495560178291 | Valid loss: 0.11858583137739537\n",
      "Epoch 49/500 | Train Loss: 0.1003629512059084 | Valid loss: 0.1193979244332674\n",
      "Epoch 50/500 | Train Loss: 0.10008781260230402 | Valid loss: 0.11802854282824798\n",
      "Epoch 51/500 | Train Loss: 0.10009622319560985 | Valid loss: 0.11964698031891224\n",
      "Epoch 52/500 | Train Loss: 0.09991993149337561 | Valid loss: 0.11721386507066876\n",
      "Epoch 53/500 | Train Loss: 0.09998538783494977 | Valid loss: 0.11732469743854085\n",
      "Epoch 54/500 | Train Loss: 0.09990910831298949 | Valid loss: 0.11745318625295578\n",
      "Epoch 55/500 | Train Loss: 0.09991738201807375 | Valid loss: 0.11736692338748726\n",
      "Epoch 56/500 | Train Loss: 0.09988534740995669 | Valid loss: 0.11729451877519835\n",
      "Epoch 57/500 | Train Loss: 0.09983630631173003 | Valid loss: 0.11713024289455524\n",
      "Epoch 58/500 | Train Loss: 0.09980191252855719 | Valid loss: 0.1170926034060675\n",
      "Epoch 59/500 | Train Loss: 0.09976295063515073 | Valid loss: 0.11693722609603821\n",
      "Epoch 60/500 | Train Loss: 0.09978370221628659 | Valid loss: 0.1174464057264633\n",
      "Epoch 61/500 | Train Loss: 0.09968313888201247 | Valid loss: 0.11677361665250258\n",
      "Epoch 62/500 | Train Loss: 0.09969680839744599 | Valid loss: 0.11668564112813667\n",
      "Epoch 63/500 | Train Loss: 0.09966987035358729 | Valid loss: 0.11664586141705513\n",
      "Epoch 64/500 | Train Loss: 0.09963319581001998 | Valid loss: 0.11661363046529681\n",
      "Epoch 65/500 | Train Loss: 0.09960878583184186 | Valid loss: 0.11656194481305605\n",
      "Epoch 66/500 | Train Loss: 0.09958983570931182 | Valid loss: 0.1165673567501958\n",
      "Epoch 67/500 | Train Loss: 0.09956316479846188 | Valid loss: 0.1165345432020204\n",
      "Epoch 68/500 | Train Loss: 0.09953589939336846 | Valid loss: 0.11649437331009743\n",
      "Epoch 69/500 | Train Loss: 0.09951639055287924 | Valid loss: 0.11646694852429074\n",
      "Epoch 70/500 | Train Loss: 0.09949945030005082 | Valid loss: 0.11643857443921787\n",
      "Epoch 71/500 | Train Loss: 0.09947431104230708 | Valid loss: 0.11642065065977879\n",
      "Epoch 72/500 | Train Loss: 0.09945127018659874 | Valid loss: 0.11640665816619646\n",
      "Epoch 73/500 | Train Loss: 0.09943629183361063 | Valid loss: 0.11639820970594883\n",
      "Epoch 74/500 | Train Loss: 0.09942193400330733 | Valid loss: 0.11640467548872842\n",
      "Epoch 75/500 | Train Loss: 0.09957319093679172 | Valid loss: 0.11654186090671045\n",
      "Epoch 76/500 | Train Loss: 0.09941159741602082 | Valid loss: 0.11647192120205524\n",
      "Epoch 77/500 | Train Loss: 0.09932531183720499 | Valid loss: 0.11646377895200669\n",
      "Epoch 78/500 | Train Loss: 0.09933501727123191 | Valid loss: 0.11648650051549424\n",
      "Epoch 79/500 | Train Loss: 0.09933994331707557 | Valid loss: 0.11654615068678247\n",
      "Epoch 80/500 | Train Loss: 0.09934480518534564 | Valid loss: 0.11663590910909481\n",
      "Epoch 81/500 | Train Loss: 0.09934731077780758 | Valid loss: 0.11674282550378594\n",
      "Epoch 82/500 | Train Loss: 0.09934812559867683 | Valid loss: 0.11688039497320735\n",
      "Epoch 83/500 | Train Loss: 0.09935312991071006 | Valid loss: 0.1170556039960925\n",
      "Epoch 84/500 | Train Loss: 0.0993642748940898 | Valid loss: 0.11720107528272757\n",
      "Epoch 85/500 | Train Loss: 0.09936839178528474 | Valid loss: 0.1174626130567387\n",
      "Epoch 86/500 | Train Loss: 0.09939304333504127 | Valid loss: 0.11769194294546925\n",
      "Epoch 87/500 | Train Loss: 0.09941292508788731 | Valid loss: 0.11792893900514342\n",
      "Epoch 88/500 | Train Loss: 0.0994274607738075 | Valid loss: 0.11789428579166185\n",
      "Epoch 89/500 | Train Loss: 0.09943579524161592 | Valid loss: 0.11811614370103492\n",
      "Epoch 90/500 | Train Loss: 0.09943504906866861 | Valid loss: 0.11820591940696157\n",
      "Epoch 91/500 | Train Loss: 0.0994370588681836 | Valid loss: 0.11819050207647473\n",
      "Epoch 92/500 | Train Loss: 0.09942254207866347 | Valid loss: 0.11814017104374808\n",
      "Epoch 93/500 | Train Loss: 0.09939547503717999 | Valid loss: 0.11818138094142426\n",
      "Epoch 94/500 | Train Loss: 0.09937805211360472 | Valid loss: 0.11798811307566803\n",
      "Epoch 95/500 | Train Loss: 0.09938289786594501 | Valid loss: 0.11806150179269702\n",
      "Epoch 96/500 | Train Loss: 0.09935528686459082 | Valid loss: 0.1180218573616341\n",
      "Epoch 97/500 | Train Loss: 0.09934153120586837 | Valid loss: 0.11796691162555023\n",
      "Epoch 98/500 | Train Loss: 0.09932437325571326 | Valid loss: 0.1179367194905184\n",
      "Epoch 99/500 | Train Loss: 0.09930310359467631 | Valid loss: 0.11776685942137657\n",
      "Epoch 100/500 | Train Loss: 0.09927140028364416 | Valid loss: 0.11791130215969196\n",
      "Epoch 101/500 | Train Loss: 0.0992875175928508 | Valid loss: 0.11780698248726684\n",
      "Epoch 102/500 | Train Loss: 0.09926832867381366 | Valid loss: 0.11785474512726068\n",
      "Epoch 103/500 | Train Loss: 0.09924943541595037 | Valid loss: 0.11777547554134629\n",
      "Epoch 104/500 | Train Loss: 0.09923911478532398 | Valid loss: 0.11780097890038822\n",
      "Epoch 105/500 | Train Loss: 0.09923833130818346 | Valid loss: 0.11787656948057025\n",
      "Epoch 106/500 | Train Loss: 0.09923222294698159 | Valid loss: 0.11771264893197736\n",
      "Epoch 107/500 | Train Loss: 0.09921442120457473 | Valid loss: 0.1177954382458052\n",
      "Epoch 108/500 | Train Loss: 0.09920489517566951 | Valid loss: 0.11776025127619505\n",
      "Epoch 109/500 | Train Loss: 0.09919454229968613 | Valid loss: 0.1177548373247995\n",
      "Epoch 110/500 | Train Loss: 0.09918623289930216 | Valid loss: 0.11773263907796422\n",
      "Epoch 111/500 | Train Loss: 0.09917814791526483 | Valid loss: 0.11778675734476987\n",
      "Epoch 112/500 | Train Loss: 0.09916616895276567 | Valid loss: 0.1176715701125389\n",
      "Epoch 113/500 | Train Loss: 0.09915527803958327 | Valid loss: 0.11776932017054668\n",
      "Epoch 114/500 | Train Loss: 0.09914906447132428 | Valid loss: 0.11773111432964026\n",
      "Epoch 115/500 | Train Loss: 0.09913622303123491 | Valid loss: 0.11771728884602009\n",
      "Epoch 116/500 | Train Loss: 0.09912559702128604 | Valid loss: 0.11773889949328678\n",
      "Epoch 117/500 | Train Loss: 0.09911747050296137 | Valid loss: 0.1176802575544909\n",
      "Epoch 118/500 | Train Loss: 0.09910367800705674 | Valid loss: 0.11780211292640415\n",
      "Epoch 119/500 | Train Loss: 0.09909688934305871 | Valid loss: 0.11763479097118211\n",
      "Epoch 120/500 | Train Loss: 0.09908531343537397 | Valid loss: 0.11771723223027102\n",
      "Epoch 121/500 | Train Loss: 0.09908017475103986 | Valid loss: 0.11768924072384834\n",
      "Epoch 122/500 | Train Loss: 0.0990701060121258 | Valid loss: 0.11765503263924011\n",
      "Epoch 123/500 | Train Loss: 0.09905939398576384 | Valid loss: 0.11767305516053078\n",
      "Epoch 124/500 | Train Loss: 0.09905422476484724 | Valid loss: 0.11757098339757947\n",
      "Epoch 125/500 | Train Loss: 0.09904819874065941 | Valid loss: 0.1175923203659612\n",
      "Epoch 126/500 | Train Loss: 0.09906154496028372 | Valid loss: 0.11756800747541495\n",
      "Epoch 127/500 | Train Loss: 0.09902351322597351 | Valid loss: 0.11756498117519673\n",
      "Epoch 128/500 | Train Loss: 0.09901902457780164 | Valid loss: 0.11754189362359602\n",
      "Epoch 129/500 | Train Loss: 0.09900870141505763 | Valid loss: 0.11754906959398541\n",
      "Epoch 130/500 | Train Loss: 0.09900852300334667 | Valid loss: 0.11750268882010566\n",
      "Epoch 131/500 | Train Loss: 0.0989912557942064 | Valid loss: 0.11755845236570336\n",
      "Epoch 132/500 | Train Loss: 0.09898488967800918 | Valid loss: 0.1174456556002761\n",
      "Epoch 133/500 | Train Loss: 0.09897299275288116 | Valid loss: 0.11749334087552027\n",
      "Epoch 134/500 | Train Loss: 0.0989635932839651 | Valid loss: 0.1174555360664462\n",
      "Epoch 135/500 | Train Loss: 0.09895449436769105 | Valid loss: 0.11745041610976291\n",
      "Epoch 136/500 | Train Loss: 0.09896117733332557 | Valid loss: 0.11744893799254368\n",
      "Epoch 137/500 | Train Loss: 0.0989465680891189 | Valid loss: 0.11744259235037621\n",
      "Epoch 138/500 | Train Loss: 0.09891163600689691 | Valid loss: 0.11731308181012093\n",
      "Epoch 139/500 | Train Loss: 0.0989065371264798 | Valid loss: 0.1174881077739735\n",
      "Epoch 140/500 | Train Loss: 0.09892973278959592 | Valid loss: 0.11748484775510638\n",
      "Epoch 141/500 | Train Loss: 0.09892557782755382 | Valid loss: 0.11747650254171255\n",
      "Epoch 142/500 | Train Loss: 0.09891780905857467 | Valid loss: 0.11747200605134632\n",
      "Epoch 143/500 | Train Loss: 0.09890969508476015 | Valid loss: 0.11749559279184701\n",
      "Epoch 144/500 | Train Loss: 0.09890681248752103 | Valid loss: 0.11744526398996281\n",
      "Epoch 145/500 | Train Loss: 0.09889571599500335 | Valid loss: 0.11750403408307669\n",
      "Epoch 146/500 | Train Loss: 0.0988914684459999 | Valid loss: 0.11743892883058897\n",
      "Epoch 147/500 | Train Loss: 0.09887992999367956 | Valid loss: 0.11748053661959115\n",
      "Epoch 148/500 | Train Loss: 0.09887852744887704 | Valid loss: 0.11745201269987711\n",
      "Epoch 149/500 | Train Loss: 0.09887020407595497 | Valid loss: 0.11743561598623908\n",
      "Epoch 150/500 | Train Loss: 0.09886680684605802 | Valid loss: 0.11744199015286773\n",
      "Epoch 151/500 | Train Loss: 0.09885879696171353 | Valid loss: 0.11742431869687037\n",
      "Epoch 152/500 | Train Loss: 0.09884790566520414 | Valid loss: 0.117422683251112\n",
      "Epoch 153/500 | Train Loss: 0.09884417283524206 | Valid loss: 0.11741384473997493\n",
      "Epoch 154/500 | Train Loss: 0.09883682164970947 | Valid loss: 0.11741363878773395\n",
      "Epoch 155/500 | Train Loss: 0.09882916342737018 | Valid loss: 0.11739787047858848\n",
      "Epoch 156/500 | Train Loss: 0.09882401265203952 | Valid loss: 0.11740979233886613\n",
      "Epoch 157/500 | Train Loss: 0.09881826478556015 | Valid loss: 0.1173920938095381\n",
      "Epoch 158/500 | Train Loss: 0.09881187756748303 | Valid loss: 0.1174130335146951\n",
      "Epoch 159/500 | Train Loss: 0.09881203967969919 | Valid loss: 0.1173822264252014\n",
      "Epoch 160/500 | Train Loss: 0.09879979141110527 | Valid loss: 0.11742550017702025\n",
      "Epoch 161/500 | Train Loss: 0.09879926629634439 | Valid loss: 0.11738352098523877\n",
      "Epoch 162/500 | Train Loss: 0.0987906196627064 | Valid loss: 0.11738540682681771\n",
      "Epoch 163/500 | Train Loss: 0.09878789436989936 | Valid loss: 0.11736817114315061\n",
      "Epoch 164/500 | Train Loss: 0.09879319114421589 | Valid loss: 0.11736748774730882\n",
      "Epoch 165/500 | Train Loss: 0.098786085544397 | Valid loss: 0.1173622410008034\n",
      "Epoch 166/500 | Train Loss: 0.09877725830976514 | Valid loss: 0.11734681479026411\n",
      "Epoch 167/500 | Train Loss: 0.09877439817494672 | Valid loss: 0.11734413847240598\n",
      "Epoch 168/500 | Train Loss: 0.09876900934546753 | Valid loss: 0.11733421832756248\n",
      "Epoch 169/500 | Train Loss: 0.09876584958393073 | Valid loss: 0.11731219270028347\n",
      "Epoch 170/500 | Train Loss: 0.09875543827487938 | Valid loss: 0.11732908394620863\n",
      "Epoch 171/500 | Train Loss: 0.09864386329562336 | Valid loss: 0.116983468319441\n",
      "Epoch 172/500 | Train Loss: 0.09858350042199743 | Valid loss: 0.1175452723536034\n",
      "Epoch 173/500 | Train Loss: 0.09883577908402767 | Valid loss: 0.11747353086464626\n",
      "Epoch 174/500 | Train Loss: 0.0987916380436956 | Valid loss: 0.11734827302482932\n",
      "Epoch 175/500 | Train Loss: 0.098758445294115 | Valid loss: 0.11731141809980537\n",
      "Epoch 176/500 | Train Loss: 0.09875057486790245 | Valid loss: 0.11728455119787953\n",
      "Epoch 177/500 | Train Loss: 0.09874384051755719 | Valid loss: 0.1172648167324274\n",
      "Epoch 178/500 | Train Loss: 0.09873482557418553 | Valid loss: 0.11725175400199585\n",
      "Epoch 179/500 | Train Loss: 0.09872553067254847 | Valid loss: 0.11724175954627436\n",
      "Epoch 180/500 | Train Loss: 0.09871798790721358 | Valid loss: 0.1172319118496637\n",
      "Epoch 181/500 | Train Loss: 0.09871034438357405 | Valid loss: 0.11723235024269237\n",
      "Epoch 182/500 | Train Loss: 0.09870871085457612 | Valid loss: 0.11721892997102681\n",
      "Epoch 183/500 | Train Loss: 0.09869767166944085 | Valid loss: 0.1172158969566226\n",
      "Epoch 184/500 | Train Loss: 0.09869820356261039 | Valid loss: 0.11718626905146033\n",
      "Epoch 185/500 | Train Loss: 0.09868653227399657 | Valid loss: 0.11722220289845799\n",
      "Epoch 186/500 | Train Loss: 0.09868968691730845 | Valid loss: 0.11719212477463622\n",
      "Epoch 187/500 | Train Loss: 0.09867862123672082 | Valid loss: 0.11716466106820939\n",
      "Epoch 188/500 | Train Loss: 0.09867360625631999 | Valid loss: 0.11714113542680131\n",
      "Epoch 189/500 | Train Loss: 0.09866978034809015 | Valid loss: 0.11715208104443411\n",
      "Epoch 190/500 | Train Loss: 0.0986674900315162 | Valid loss: 0.1171599775985923\n",
      "Epoch 191/500 | Train Loss: 0.09866579018965148 | Valid loss: 0.11716447922206202\n",
      "Epoch 192/500 | Train Loss: 0.0986619642139345 | Valid loss: 0.1171663136243127\n",
      "Epoch 193/500 | Train Loss: 0.09865449858910363 | Valid loss: 0.11716889702650003\n",
      "Epoch 194/500 | Train Loss: 0.09865474343785773 | Valid loss: 0.11714946017275717\n",
      "Epoch 195/500 | Train Loss: 0.09864939036306695 | Valid loss: 0.11717165694680325\n",
      "Epoch 196/500 | Train Loss: 0.09864726514434037 | Valid loss: 0.11715336187287818\n",
      "Epoch 197/500 | Train Loss: 0.09863904987502357 | Valid loss: 0.11717774255504441\n",
      "Epoch 198/500 | Train Loss: 0.09864779722107493 | Valid loss: 0.11718554134199093\n",
      "Epoch 199/500 | Train Loss: 0.09864270671446254 | Valid loss: 0.1171706447507753\n",
      "Epoch 200/500 | Train Loss: 0.09863587762836529 | Valid loss: 0.11715384958268599\n",
      "Epoch 201/500 | Train Loss: 0.09863054110412149 | Valid loss: 0.11716472130095543\n",
      "Epoch 202/500 | Train Loss: 0.09862907800445522 | Valid loss: 0.11716349360121545\n",
      "Epoch 203/500 | Train Loss: 0.09862513422858024 | Valid loss: 0.11717158666443686\n",
      "Epoch 204/500 | Train Loss: 0.09862195311141186 | Valid loss: 0.11716856244345043\n",
      "Epoch 205/500 | Train Loss: 0.0986199670376769 | Valid loss: 0.11717300472217937\n",
      "Epoch 206/500 | Train Loss: 0.09861656166613102 | Valid loss: 0.11717073058406281\n",
      "Epoch 207/500 | Train Loss: 0.09861233781321325 | Valid loss: 0.11717551989957344\n",
      "Epoch 208/500 | Train Loss: 0.0986110912716907 | Valid loss: 0.11717543148890484\n",
      "Epoch 209/500 | Train Loss: 0.09860780326568562 | Valid loss: 0.11719375612690698\n",
      "Epoch 210/500 | Train Loss: 0.0986049687165929 | Valid loss: 0.11718102412428273\n",
      "Epoch 211/500 | Train Loss: 0.09860041521057702 | Valid loss: 0.11718671719088804\n",
      "Epoch 212/500 | Train Loss: 0.0984726687785292 | Valid loss: 0.11671713741799426\n",
      "Epoch 213/500 | Train Loss: 0.09859048966858266 | Valid loss: 0.1176057793849776\n",
      "Epoch 214/500 | Train Loss: 0.09866537575650475 | Valid loss: 0.11722153472865737\n",
      "Epoch 215/500 | Train Loss: 0.0985948507471577 | Valid loss: 0.11718504635487186\n",
      "Epoch 216/500 | Train Loss: 0.09858726525976175 | Valid loss: 0.11717592820871708\n",
      "Epoch 217/500 | Train Loss: 0.09858472920630289 | Valid loss: 0.11717273617639791\n",
      "Epoch 218/500 | Train Loss: 0.09857794496946144 | Valid loss: 0.11718281287006861\n",
      "Epoch 219/500 | Train Loss: 0.09857436173959919 | Valid loss: 0.11718640623744143\n",
      "Epoch 220/500 | Train Loss: 0.09857110134663358 | Valid loss: 0.11719312739753446\n",
      "Epoch 221/500 | Train Loss: 0.09856937253118857 | Valid loss: 0.11719098340633304\n",
      "Epoch 222/500 | Train Loss: 0.09856585557909979 | Valid loss: 0.11721383340570123\n",
      "Epoch 223/500 | Train Loss: 0.098565186838201 | Valid loss: 0.11720207721254854\n",
      "Epoch 224/500 | Train Loss: 0.0985584792478577 | Valid loss: 0.11721697371712951\n",
      "Epoch 225/500 | Train Loss: 0.0985559875442498 | Valid loss: 0.11721045630009369\n",
      "Epoch 226/500 | Train Loss: 0.09855176501408003 | Valid loss: 0.11724461311864298\n",
      "Epoch 227/500 | Train Loss: 0.09855401263018881 | Valid loss: 0.11721478190359681\n",
      "Epoch 228/500 | Train Loss: 0.09854555598905552 | Valid loss: 0.11724121101893657\n",
      "Epoch 229/500 | Train Loss: 0.09854455324888661 | Valid loss: 0.11722872430068809\n",
      "Epoch 230/500 | Train Loss: 0.09854276759307022 | Valid loss: 0.11725749356975389\n",
      "Epoch 231/500 | Train Loss: 0.09855635677720757 | Valid loss: 0.11721766072996827\n",
      "Epoch 232/500 | Train Loss: 0.09852254231446896 | Valid loss: 0.11722520468106797\n",
      "Epoch 233/500 | Train Loss: 0.09852430054890937 | Valid loss: 0.11722137358819329\n",
      "Epoch 234/500 | Train Loss: 0.0985233371483459 | Valid loss: 0.1172637329545132\n",
      "Epoch 235/500 | Train Loss: 0.09853256983763498 | Valid loss: 0.11723407507375923\n",
      "Epoch 236/500 | Train Loss: 0.09852714520487664 | Valid loss: 0.11724858276199462\n",
      "Epoch 237/500 | Train Loss: 0.09851997048742529 | Valid loss: 0.1172408111566721\n",
      "Epoch 238/500 | Train Loss: 0.09851174932243167 | Valid loss: 0.11722281806950652\n",
      "Epoch 239/500 | Train Loss: 0.09850905531666417 | Valid loss: 0.11726206627696059\n",
      "Epoch 240/500 | Train Loss: 0.09851230532740769 | Valid loss: 0.11723147337000038\n",
      "Epoch 241/500 | Train Loss: 0.09849347570182189 | Valid loss: 0.11739663996322211\n",
      "Epoch 242/500 | Train Loss: 0.09855356995502244 | Valid loss: 0.11733789954247863\n",
      "Epoch 243/500 | Train Loss: 0.09852057903519143 | Valid loss: 0.1172499176288067\n",
      "Epoch 244/500 | Train Loss: 0.09849738081026337 | Valid loss: 0.11722993865869073\n",
      "Epoch 245/500 | Train Loss: 0.09848823147299497 | Valid loss: 0.11724315061732087\n",
      "Epoch 246/500 | Train Loss: 0.0984846214534364 | Valid loss: 0.1172332999967905\n",
      "Epoch 247/500 | Train Loss: 0.09848152450623288 | Valid loss: 0.11725031794590313\n",
      "Epoch 248/500 | Train Loss: 0.09848730272303025 | Valid loss: 0.11727114140900761\n",
      "Epoch 249/500 | Train Loss: 0.09848833335698515 | Valid loss: 0.11728050267367168\n",
      "Epoch 250/500 | Train Loss: 0.09848475429782833 | Valid loss: 0.11722587820055873\n",
      "Epoch 251/500 | Train Loss: 0.09846951802787574 | Valid loss: 0.11722140165782252\n",
      "Epoch 252/500 | Train Loss: 0.09847333342160867 | Valid loss: 0.11725169052045013\n",
      "Epoch 253/500 | Train Loss: 0.09846988670203997 | Valid loss: 0.11722005466215832\n",
      "Epoch 254/500 | Train Loss: 0.0984689670925339 | Valid loss: 0.1172497515501671\n",
      "Epoch 255/500 | Train Loss: 0.0984633223976994 | Valid loss: 0.11722106144352015\n",
      "Epoch 256/500 | Train Loss: 0.09846000995055057 | Valid loss: 0.11723350356657837\n",
      "Epoch 257/500 | Train Loss: 0.09845559353791718 | Valid loss: 0.11719172419763582\n",
      "Epoch 258/500 | Train Loss: 0.09844672435867613 | Valid loss: 0.11722580510256596\n",
      "Epoch 259/500 | Train Loss: 0.09845830120308244 | Valid loss: 0.11723357306923284\n",
      "Epoch 260/500 | Train Loss: 0.09845466084130432 | Valid loss: 0.1172015812291309\n",
      "Epoch 261/500 | Train Loss: 0.09844345242001008 | Valid loss: 0.11720776661883953\n",
      "Epoch 262/500 | Train Loss: 0.09844500932140626 | Valid loss: 0.11720917874210796\n",
      "Epoch 263/500 | Train Loss: 0.09843896433873021 | Valid loss: 0.11720631600812424\n",
      "Epoch 264/500 | Train Loss: 0.09843587692611028 | Valid loss: 0.11722278246266205\n",
      "Epoch 265/500 | Train Loss: 0.0984334721027509 | Valid loss: 0.11721886677104373\n",
      "Epoch 266/500 | Train Loss: 0.09848455340210079 | Valid loss: 0.11734274107702942\n",
      "Epoch 267/500 | Train Loss: 0.09843065750825664 | Valid loss: 0.11719982122439285\n",
      "Epoch 268/500 | Train Loss: 0.09841207700201135 | Valid loss: 0.11721895169466734\n",
      "Epoch 269/500 | Train Loss: 0.09841676903483661 | Valid loss: 0.11723223426054384\n",
      "Epoch 270/500 | Train Loss: 0.09841671280994796 | Valid loss: 0.11720063452890446\n",
      "Epoch 271/500 | Train Loss: 0.09841603264482557 | Valid loss: 0.11726505151235087\n",
      "Epoch 272/500 | Train Loss: 0.09842097837801861 | Valid loss: 0.11721932318408129\n",
      "Epoch 273/500 | Train Loss: 0.09840974331549976 | Valid loss: 0.11722941694477963\n",
      "Epoch 274/500 | Train Loss: 0.09840677342229132 | Valid loss: 0.11719735451908998\n",
      "Epoch 275/500 | Train Loss: 0.09839458073880794 | Valid loss: 0.11722844475230505\n",
      "Epoch 276/500 | Train Loss: 0.09840904630828595 | Valid loss: 0.11718588664607946\n",
      "Epoch 277/500 | Train Loss: 0.0984058361986409 | Valid loss: 0.11724223254984895\n",
      "Epoch 278/500 | Train Loss: 0.09839660473491835 | Valid loss: 0.11721701122993647\n",
      "Epoch 279/500 | Train Loss: 0.0983910477058827 | Valid loss: 0.11722219739715721\n",
      "Epoch 280/500 | Train Loss: 0.09838814440628757 | Valid loss: 0.11718627806146477\n",
      "Epoch 281/500 | Train Loss: 0.0983738004675378 | Valid loss: 0.11718878026531879\n",
      "Epoch 282/500 | Train Loss: 0.09837564480207536 | Valid loss: 0.11718672626586847\n",
      "Epoch 283/500 | Train Loss: 0.0983706036798548 | Valid loss: 0.1171461628143524\n",
      "Epoch 284/500 | Train Loss: 0.09836574969138356 | Valid loss: 0.11712060442064391\n",
      "Epoch 285/500 | Train Loss: 0.09835359818045644 | Valid loss: 0.11713757149355356\n",
      "Epoch 286/500 | Train Loss: 0.09835422308170709 | Valid loss: 0.11715083125285632\n",
      "Epoch 287/500 | Train Loss: 0.09835161254673765 | Valid loss: 0.11711625204703142\n",
      "Epoch 288/500 | Train Loss: 0.09835028406502544 | Valid loss: 0.11715309220084617\n",
      "Epoch 289/500 | Train Loss: 0.09834721949275421 | Valid loss: 0.11713265523661015\n",
      "Epoch 290/500 | Train Loss: 0.09834300792249648 | Valid loss: 0.11712136182414237\n",
      "Epoch 291/500 | Train Loss: 0.09833691026205602 | Valid loss: 0.11711897984753515\n",
      "Epoch 292/500 | Train Loss: 0.09833628566854674 | Valid loss: 0.11713994056159674\n",
      "Epoch 293/500 | Train Loss: 0.09833875743645257 | Valid loss: 0.11710494531448497\n",
      "Epoch 294/500 | Train Loss: 0.09833136414434167 | Valid loss: 0.11710743623417477\n",
      "Epoch 295/500 | Train Loss: 0.0983273073679943 | Valid loss: 0.11712151876282553\n",
      "Epoch 296/500 | Train Loss: 0.09832798146467278 | Valid loss: 0.11711338864162911\n",
      "Epoch 297/500 | Train Loss: 0.09832856331452511 | Valid loss: 0.11710752629090188\n",
      "Epoch 298/500 | Train Loss: 0.09832125837928143 | Valid loss: 0.11712047269264626\n",
      "Epoch 299/500 | Train Loss: 0.09832238759845495 | Valid loss: 0.11710709881384013\n",
      "Epoch 300/500 | Train Loss: 0.09831879032586796 | Valid loss: 0.11710792652136365\n",
      "Epoch 301/500 | Train Loss: 0.09831929340472688 | Valid loss: 0.1171109538395391\n",
      "Epoch 302/500 | Train Loss: 0.0983192293945214 | Valid loss: 0.11710859371652436\n",
      "Epoch 303/500 | Train Loss: 0.0983139601279644 | Valid loss: 0.11712493899083415\n",
      "Epoch 304/500 | Train Loss: 0.09831048892675967 | Valid loss: 0.11709419899988313\n",
      "Epoch 305/500 | Train Loss: 0.09830643749539403 | Valid loss: 0.11711126719709745\n",
      "Epoch 306/500 | Train Loss: 0.09830513803855233 | Valid loss: 0.11711094255537488\n",
      "Epoch 307/500 | Train Loss: 0.09830452102789844 | Valid loss: 0.11709584354228059\n",
      "Epoch 308/500 | Train Loss: 0.09829923629058876 | Valid loss: 0.11711213434504908\n",
      "Epoch 309/500 | Train Loss: 0.09830432409231646 | Valid loss: 0.11717741858474044\n",
      "Epoch 310/500 | Train Loss: 0.09832848080204448 | Valid loss: 0.11710838828409134\n",
      "Epoch 311/500 | Train Loss: 0.09829683465262254 | Valid loss: 0.11711143877703784\n",
      "Epoch 312/500 | Train Loss: 0.09829709124683902 | Valid loss: 0.11708315637309191\n",
      "Epoch 313/500 | Train Loss: 0.09828566736069279 | Valid loss: 0.11709084084560704\n",
      "Epoch 314/500 | Train Loss: 0.09828434242610483 | Valid loss: 0.1170800085677657\n",
      "Epoch 315/500 | Train Loss: 0.09828837563054285 | Valid loss: 0.11709844511608745\n",
      "Epoch 316/500 | Train Loss: 0.09828263405656469 | Valid loss: 0.1170994341156857\n",
      "Epoch 317/500 | Train Loss: 0.0982906258435569 | Valid loss: 0.11710792693287828\n",
      "Epoch 318/500 | Train Loss: 0.09828702446505211 | Valid loss: 0.11707034663752068\n",
      "Epoch 319/500 | Train Loss: 0.0982775672595354 | Valid loss: 0.11708884159926065\n",
      "Epoch 320/500 | Train Loss: 0.09827764637876248 | Valid loss: 0.11709159959194272\n",
      "Epoch 321/500 | Train Loss: 0.09827774951531404 | Valid loss: 0.11707439089497161\n",
      "Epoch 322/500 | Train Loss: 0.09826817029844159 | Valid loss: 0.11706582164435192\n",
      "Epoch 323/500 | Train Loss: 0.09826885553859714 | Valid loss: 0.117053447161303\n",
      "Epoch 324/500 | Train Loss: 0.098264223612521 | Valid loss: 0.11705845964769292\n",
      "Epoch 325/500 | Train Loss: 0.09826834310871968 | Valid loss: 0.11706125601952852\n",
      "Epoch 326/500 | Train Loss: 0.09826740155509417 | Valid loss: 0.11705013537822767\n",
      "Epoch 327/500 | Train Loss: 0.09826124149064223 | Valid loss: 0.11705452043476493\n",
      "Epoch 328/500 | Train Loss: 0.09825897119615389 | Valid loss: 0.11703610480871311\n",
      "Epoch 329/500 | Train Loss: 0.09825476732212995 | Valid loss: 0.11705315920935813\n",
      "Epoch 330/500 | Train Loss: 0.09825889587780272 | Valid loss: 0.1170509169311371\n",
      "Epoch 331/500 | Train Loss: 0.09825622638336558 | Valid loss: 0.11703928306612164\n",
      "Epoch 332/500 | Train Loss: 0.09824848777034144 | Valid loss: 0.1170639298682989\n",
      "Epoch 333/500 | Train Loss: 0.0982534923820176 | Valid loss: 0.1170703107274549\n",
      "Epoch 334/500 | Train Loss: 0.09825038329792628 | Valid loss: 0.11703949274365293\n",
      "Epoch 335/500 | Train Loss: 0.09824051252309828 | Valid loss: 0.1170397266572298\n",
      "Epoch 336/500 | Train Loss: 0.09823913579617721 | Valid loss: 0.11703971535140692\n",
      "Epoch 337/500 | Train Loss: 0.09823617653084406 | Valid loss: 0.11707374252119036\n",
      "Epoch 338/500 | Train Loss: 0.09824123189015233 | Valid loss: 0.11707043756059436\n",
      "Epoch 339/500 | Train Loss: 0.09823997522015934 | Valid loss: 0.11705266131997802\n",
      "Epoch 340/500 | Train Loss: 0.09823024573101513 | Valid loss: 0.11706557399918173\n",
      "Epoch 341/500 | Train Loss: 0.09823201005279586 | Valid loss: 0.11704648526515378\n",
      "Epoch 342/500 | Train Loss: 0.09821695812802383 | Valid loss: 0.11693120953561954\n",
      "Epoch 343/500 | Train Loss: 0.09824466414047756 | Valid loss: 0.11702315723740084\n",
      "Epoch 344/500 | Train Loss: 0.09822813433635494 | Valid loss: 0.1170423302236338\n",
      "Epoch 345/500 | Train Loss: 0.09822550963448441 | Valid loss: 0.11703732705046964\n",
      "Epoch 346/500 | Train Loss: 0.09822105063746372 | Valid loss: 0.1170431335868184\n",
      "Epoch 347/500 | Train Loss: 0.09822436045531345 | Valid loss: 0.11703121270118065\n",
      "Epoch 348/500 | Train Loss: 0.09820688607954028 | Valid loss: 0.11706582920322585\n",
      "Epoch 349/500 | Train Loss: 0.09825497757427502 | Valid loss: 0.11711630693008733\n",
      "Epoch 350/500 | Train Loss: 0.09823807470932387 | Valid loss: 0.117038746667636\n",
      "Epoch 351/500 | Train Loss: 0.09822436725801748 | Valid loss: 0.11704796933850577\n",
      "Epoch 352/500 | Train Loss: 0.09821109754181859 | Valid loss: 0.11703800591965054\n",
      "Epoch 353/500 | Train Loss: 0.09821002466091211 | Valid loss: 0.11702355822591587\n",
      "Epoch 354/500 | Train Loss: 0.09820376770111962 | Valid loss: 0.11700508715368288\n",
      "Epoch 355/500 | Train Loss: 0.09819589238613843 | Valid loss: 0.11702387117195961\n",
      "Epoch 356/500 | Train Loss: 0.09820106857442769 | Valid loss: 0.11700187944049059\n",
      "Epoch 357/500 | Train Loss: 0.0982070474628953 | Valid loss: 0.11700446994671988\n",
      "Epoch 358/500 | Train Loss: 0.09819703390146943 | Valid loss: 0.11702137594219557\n",
      "Epoch 359/500 | Train Loss: 0.09819981047370728 | Valid loss: 0.11702855624431788\n",
      "Epoch 360/500 | Train Loss: 0.09818650916002798 | Valid loss: 0.11698313793817232\n",
      "Epoch 361/500 | Train Loss: 0.09818764945249195 | Valid loss: 0.11701782290325609\n",
      "Epoch 362/500 | Train Loss: 0.09818882007097852 | Valid loss: 0.11699515552974718\n",
      "Epoch 363/500 | Train Loss: 0.09819251257602288 | Valid loss: 0.11704516735707604\n",
      "Epoch 364/500 | Train Loss: 0.0981867461208848 | Valid loss: 0.11698306501344886\n",
      "Epoch 365/500 | Train Loss: 0.09818027857421101 | Valid loss: 0.1170098566298568\n",
      "Epoch 366/500 | Train Loss: 0.09818276247900465 | Valid loss: 0.11696872262414111\n",
      "Epoch 367/500 | Train Loss: 0.09816029866536459 | Valid loss: 0.11689233251435813\n",
      "Epoch 368/500 | Train Loss: 0.09816970684822054 | Valid loss: 0.11696882732212543\n",
      "Epoch 369/500 | Train Loss: 0.09817245478431384 | Valid loss: 0.11697782532766808\n",
      "Epoch 370/500 | Train Loss: 0.09817092728031718 | Valid loss: 0.11700109234296305\n",
      "Epoch 371/500 | Train Loss: 0.09817438008992568 | Valid loss: 0.11699017797878315\n",
      "Epoch 372/500 | Train Loss: 0.09816815067028654 | Valid loss: 0.11699573821279892\n",
      "Epoch 373/500 | Train Loss: 0.0981638096731858 | Valid loss: 0.11696788839735957\n",
      "Epoch 374/500 | Train Loss: 0.09816128460663384 | Valid loss: 0.11697886749928774\n",
      "Epoch 375/500 | Train Loss: 0.09816099337531604 | Valid loss: 0.11697655173321796\n",
      "Epoch 376/500 | Train Loss: 0.09815414360845866 | Valid loss: 0.11699393025577762\n",
      "Epoch 377/500 | Train Loss: 0.0981629993631572 | Valid loss: 0.11699652353431596\n",
      "Epoch 378/500 | Train Loss: 0.098153274031221 | Valid loss: 0.11696302407789369\n",
      "Epoch 379/500 | Train Loss: 0.09815467674069214 | Valid loss: 0.11696580938215173\n",
      "Epoch 380/500 | Train Loss: 0.09814709720836169 | Valid loss: 0.11696360193106324\n",
      "Epoch 381/500 | Train Loss: 0.09815165537638941 | Valid loss: 0.11698408251584963\n",
      "Epoch 382/500 | Train Loss: 0.09814214506119058 | Valid loss: 0.11695666197514118\n",
      "Epoch 383/500 | Train Loss: 0.09818233671881582 | Valid loss: 0.11699176291653583\n",
      "Epoch 384/500 | Train Loss: 0.09814618682029887 | Valid loss: 0.11698260541658762\n",
      "Epoch 385/500 | Train Loss: 0.09814333100735709 | Valid loss: 0.11697923628136862\n",
      "Epoch 386/500 | Train Loss: 0.0981331521915137 | Valid loss: 0.11694177885561488\n",
      "Epoch 387/500 | Train Loss: 0.09814026201332825 | Valid loss: 0.11698951734619778\n",
      "Epoch 388/500 | Train Loss: 0.09813177070000033 | Valid loss: 0.11694948539830917\n",
      "Epoch 389/500 | Train Loss: 0.09813071811652702 | Valid loss: 0.116959041396026\n",
      "Epoch 390/500 | Train Loss: 0.09812966049581334 | Valid loss: 0.1169509662661788\n",
      "Epoch 391/500 | Train Loss: 0.098137087015894 | Valid loss: 0.11696762418331103\n",
      "Epoch 392/500 | Train Loss: 0.09812142665403477 | Valid loss: 0.11694456751696593\n",
      "Epoch 393/500 | Train Loss: 0.09811991479625737 | Valid loss: 0.11695473890231792\n",
      "Epoch 394/500 | Train Loss: 0.098139262266889 | Valid loss: 0.11700208673556876\n",
      "Epoch 395/500 | Train Loss: 0.09812504864581253 | Valid loss: 0.11697380740715321\n",
      "Epoch 396/500 | Train Loss: 0.09811187057445446 | Valid loss: 0.11696503079647935\n",
      "Epoch 397/500 | Train Loss: 0.09811561858632427 | Valid loss: 0.11696049735643142\n",
      "Epoch 398/500 | Train Loss: 0.09811304336872653 | Valid loss: 0.1169531081348311\n",
      "Epoch 399/500 | Train Loss: 0.0981071980211182 | Valid loss: 0.116938314162368\n",
      "Epoch 400/500 | Train Loss: 0.09811581333262333 | Valid loss: 0.11699274472545745\n",
      "Epoch 401/500 | Train Loss: 0.0981136494923545 | Valid loss: 0.11699711819460919\n",
      "Epoch 402/500 | Train Loss: 0.09810889545234217 | Valid loss: 0.11695042986769316\n",
      "Epoch 403/500 | Train Loss: 0.09810576255068831 | Valid loss: 0.11696499397674966\n",
      "Epoch 404/500 | Train Loss: 0.09810330180478269 | Valid loss: 0.1169557579858012\n",
      "Epoch 405/500 | Train Loss: 0.09810555753014658 | Valid loss: 0.11697220345309307\n",
      "Epoch 406/500 | Train Loss: 0.09811881160552519 | Valid loss: 0.11692551737867815\n",
      "Epoch 407/500 | Train Loss: 0.09812262849207373 | Valid loss: 0.11691402184755303\n",
      "Epoch 408/500 | Train Loss: 0.09811131470715222 | Valid loss: 0.1169256277945499\n",
      "Epoch 409/500 | Train Loss: 0.09810993860651186 | Valid loss: 0.1169441157172239\n",
      "Epoch 410/500 | Train Loss: 0.09810520810331555 | Valid loss: 0.11694616971667422\n",
      "Epoch 411/500 | Train Loss: 0.09810405864339808 | Valid loss: 0.11694531201190034\n",
      "Epoch 412/500 | Train Loss: 0.09809842068378044 | Valid loss: 0.116946505902465\n",
      "Epoch 413/500 | Train Loss: 0.09809613736967246 | Valid loss: 0.11692925167898106\n",
      "Epoch 414/500 | Train Loss: 0.0980982742987681 | Valid loss: 0.11693226115041694\n",
      "Epoch 415/500 | Train Loss: 0.09808916474382083 | Valid loss: 0.11686993988100873\n",
      "Epoch 416/500 | Train Loss: 0.09808485267980807 | Valid loss: 0.11691311309330685\n",
      "Epoch 417/500 | Train Loss: 0.09807986241589854 | Valid loss: 0.11691400469389072\n",
      "Epoch 418/500 | Train Loss: 0.09835509036708137 | Valid loss: 0.11766266766502413\n",
      "Epoch 419/500 | Train Loss: 0.09808716454637656 | Valid loss: 0.1171019995844988\n",
      "Epoch 420/500 | Train Loss: 0.09804430256287257 | Valid loss: 0.11706252361452857\n",
      "Epoch 421/500 | Train Loss: 0.09807025866340036 | Valid loss: 0.11701658279310133\n",
      "Epoch 422/500 | Train Loss: 0.0980779615001402 | Valid loss: 0.11697512585669756\n",
      "Epoch 423/500 | Train Loss: 0.09808006645652695 | Valid loss: 0.11696159694517075\n",
      "Epoch 424/500 | Train Loss: 0.09807878948100235 | Valid loss: 0.11698147379468347\n",
      "Epoch 425/500 | Train Loss: 0.09807561264251885 | Valid loss: 0.11695448257202326\n",
      "Epoch 426/500 | Train Loss: 0.09807448479738357 | Valid loss: 0.11694505113328613\n",
      "Epoch 427/500 | Train Loss: 0.09806910181768995 | Valid loss: 0.11694209790940202\n",
      "Epoch 428/500 | Train Loss: 0.09806603399253842 | Valid loss: 0.11692560838838649\n",
      "Epoch 429/500 | Train Loss: 0.09806485420389884 | Valid loss: 0.11692459918125424\n",
      "Epoch 430/500 | Train Loss: 0.09805923148827708 | Valid loss: 0.11688604983392843\n",
      "Epoch 431/500 | Train Loss: 0.09806208826763474 | Valid loss: 0.11691640510202148\n",
      "Epoch 432/500 | Train Loss: 0.09806199146947567 | Valid loss: 0.11690544235238502\n",
      "Epoch 433/500 | Train Loss: 0.0980640723788436 | Valid loss: 0.11692040279334368\n",
      "Epoch 434/500 | Train Loss: 0.09806215801964635 | Valid loss: 0.11692626343303641\n",
      "Epoch 435/500 | Train Loss: 0.09805890097134355 | Valid loss: 0.1169691344419884\n",
      "Epoch 436/500 | Train Loss: 0.09805854978174836 | Valid loss: 0.11691768870277461\n",
      "Epoch 437/500 | Train Loss: 0.0980504051849678 | Valid loss: 0.11689691832505686\n",
      "Epoch 438/500 | Train Loss: 0.09805315985897745 | Valid loss: 0.1169118213272372\n",
      "Epoch 439/500 | Train Loss: 0.09805571851253078 | Valid loss: 0.11688460650052442\n",
      "Epoch 440/500 | Train Loss: 0.09808157725340647 | Valid loss: 0.11695330641990484\n",
      "Epoch 441/500 | Train Loss: 0.09804249511680742 | Valid loss: 0.11692945403588373\n",
      "Epoch 442/500 | Train Loss: 0.09803966567514168 | Valid loss: 0.11691253779585971\n",
      "Epoch 443/500 | Train Loss: 0.0980383111325943 | Valid loss: 0.11687822743903759\n",
      "Epoch 444/500 | Train Loss: 0.09803913096910802 | Valid loss: 0.11685848134288261\n",
      "Epoch 445/500 | Train Loss: 0.0980414342135191 | Valid loss: 0.11687183035754187\n",
      "Epoch 446/500 | Train Loss: 0.09803974248630845 | Valid loss: 0.11687616308674563\n",
      "Epoch 447/500 | Train Loss: 0.09804101099624582 | Valid loss: 0.11687190248089474\n",
      "Epoch 448/500 | Train Loss: 0.09803074910372928 | Valid loss: 0.11685321020872094\n",
      "Epoch 449/500 | Train Loss: 0.09803360244664161 | Valid loss: 0.11687490762059771\n",
      "Epoch 450/500 | Train Loss: 0.09803496885612822 | Valid loss: 0.11686367652010779\n",
      "Epoch 451/500 | Train Loss: 0.09802715190672788 | Valid loss: 0.11684828496343175\n",
      "Epoch 452/500 | Train Loss: 0.09802919427553812 | Valid loss: 0.11686581992652527\n",
      "Epoch 453/500 | Train Loss: 0.09805129251132409 | Valid loss: 0.11686701260420472\n",
      "Epoch 454/500 | Train Loss: 0.09804185752527438 | Valid loss: 0.11694598689588696\n",
      "Epoch 455/500 | Train Loss: 0.09803149687477213 | Valid loss: 0.1168651927565766\n",
      "Epoch 456/500 | Train Loss: 0.09801171097526516 | Valid loss: 0.11684566541293333\n",
      "Epoch 457/500 | Train Loss: 0.09801980623354514 | Valid loss: 0.11684809464874656\n",
      "Epoch 458/500 | Train Loss: 0.09801486621786287 | Valid loss: 0.1168213123239057\n",
      "Epoch 459/500 | Train Loss: 0.09801849560353203 | Valid loss: 0.11686665556112001\n",
      "Epoch 460/500 | Train Loss: 0.09801760582064373 | Valid loss: 0.11683133142718742\n",
      "Epoch 461/500 | Train Loss: 0.09801056583182535 | Valid loss: 0.11683791802199774\n",
      "Epoch 462/500 | Train Loss: 0.09801616062666627 | Valid loss: 0.11685395634971386\n",
      "Epoch 463/500 | Train Loss: 0.09801427330011907 | Valid loss: 0.11682773301334576\n",
      "Epoch 464/500 | Train Loss: 0.0980106679101785 | Valid loss: 0.11683884230550638\n",
      "Epoch 465/500 | Train Loss: 0.09801330271892357 | Valid loss: 0.1168393193159339\n",
      "Epoch 466/500 | Train Loss: 0.09801263729029376 | Valid loss: 0.11685528575854245\n",
      "Epoch 467/500 | Train Loss: 0.09801093118832163 | Valid loss: 0.11686127758476623\n",
      "Epoch 468/500 | Train Loss: 0.09801125928651595 | Valid loss: 0.11685731983202141\n",
      "Epoch 469/500 | Train Loss: 0.09800987464200327 | Valid loss: 0.11686022960862448\n",
      "Epoch 470/500 | Train Loss: 0.09800809124835591 | Valid loss: 0.11687982515540234\n",
      "Epoch 471/500 | Train Loss: 0.09800312177588542 | Valid loss: 0.11686073273940142\n",
      "Epoch 472/500 | Train Loss: 0.09800627166000397 | Valid loss: 0.11687684503145689\n",
      "Epoch 473/500 | Train Loss: 0.09799960909708255 | Valid loss: 0.11686031793265841\n",
      "Epoch 474/500 | Train Loss: 0.09800100584338972 | Valid loss: 0.11689436745418366\n",
      "Epoch 475/500 | Train Loss: 0.09799823073157365 | Valid loss: 0.11688158717439619\n",
      "Epoch 476/500 | Train Loss: 0.09799780150254568 | Valid loss: 0.11688869264583256\n",
      "Epoch 477/500 | Train Loss: 0.09800143574728915 | Valid loss: 0.11690900435801162\n",
      "Epoch 478/500 | Train Loss: 0.09799495003577592 | Valid loss: 0.11690387848851293\n",
      "Epoch 479/500 | Train Loss: 0.09802587802481393 | Valid loss: 0.11700731362107882\n",
      "Epoch 480/500 | Train Loss: 0.09800839277011329 | Valid loss: 0.11691708312651446\n",
      "Epoch 481/500 | Train Loss: 0.09797974931265133 | Valid loss: 0.11691483144843301\n",
      "Epoch 482/500 | Train Loss: 0.0979835598797038 | Valid loss: 0.11691601639396923\n",
      "Epoch 483/500 | Train Loss: 0.09798155279857093 | Valid loss: 0.11691832804489274\n",
      "Epoch 484/500 | Train Loss: 0.09798878826794849 | Valid loss: 0.11695199563752773\n",
      "Epoch 485/500 | Train Loss: 0.09798174021576626 | Valid loss: 0.11692515949090553\n",
      "Epoch 486/500 | Train Loss: 0.09798171035945416 | Valid loss: 0.1169424977066905\n",
      "Epoch 487/500 | Train Loss: 0.09797821798888237 | Valid loss: 0.11692261150063471\n",
      "Epoch 488/500 | Train Loss: 0.09798071469733681 | Valid loss: 0.11694243199430233\n",
      "Epoch 489/500 | Train Loss: 0.09797938535718814 | Valid loss: 0.11694323301835116\n",
      "Epoch 490/500 | Train Loss: 0.09798065393191317 | Valid loss: 0.11690901579378649\n",
      "Epoch 491/500 | Train Loss: 0.0979725084119085 | Valid loss: 0.11692834662836651\n",
      "Epoch 492/500 | Train Loss: 0.09797734521005465 | Valid loss: 0.11695082487841678\n",
      "Epoch 493/500 | Train Loss: 0.09797089753969423 | Valid loss: 0.11692250277413878\n",
      "Epoch 494/500 | Train Loss: 0.0979708899082481 | Valid loss: 0.11694209589514622\n",
      "Epoch 495/500 | Train Loss: 0.09797183914191049 | Valid loss: 0.11695146365740965\n",
      "Epoch 496/500 | Train Loss: 0.0979717317590679 | Valid loss: 0.11697691200344368\n",
      "Epoch 497/500 | Train Loss: 0.09796896104201458 | Valid loss: 0.1169454415307142\n",
      "Epoch 498/500 | Train Loss: 0.09796700029215519 | Valid loss: 0.11694751887820488\n",
      "Epoch 499/500 | Train Loss: 0.0979687397516724 | Valid loss: 0.11696787754636864\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    losses = []\n",
    "    for i, (x, noise, noise_cov) in enumerate(train_set):\n",
    "        # Forward pass\n",
    "        x_noisy = x + noise\n",
    "        x_est = model(x_noisy, noise_cov)\n",
    "        loss = loss_mse(x, x_est)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "    train_losses.append(sum(losses) / len(losses))\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, noise, noise_cov) in enumerate(valid_set):\n",
    "            # Forward pass\n",
    "            x_noisy = x + noise\n",
    "            x_est = model(x_noisy, noise_cov)\n",
    "            loss = loss_mse(x, x_est)\n",
    "\n",
    "            # Add validation loss\n",
    "            losses.append(loss.item())\n",
    "    valid_losses.append(sum(losses) / len(losses))\n",
    "\n",
    "    print(f\"Epoch {epoch}/{epochs} | Train Loss: {train_losses[-1]} | Valid loss: {valid_losses[-1]}\")\n",
    "\n",
    "# Test phase\n",
    "losses = []\n",
    "model.eval()\n",
    "for i, (x, noise, noise_cov) in enumerate(test_set):\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        x_noisy = x + noise\n",
    "        x_est = model(x_noisy, noise_cov)\n",
    "        loss = loss_mse(x, x_est)\n",
    "\n",
    "        # Add validation loss\n",
    "        losses.append(loss.item())\n",
    "test_loss = sum(losses) / len(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(valid_losses, label='valid_loss')\n",
    "plt.scatter(len(valid_losses), test_loss, label='test_loss', color='r')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "#plt.ylim(0, 1)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Usage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([7840, 2]), torch.Size([2, 2]))"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Real data\n",
    "X_np = np.load('X.npy')\n",
    "noise_np = np.load('noise.npy')\n",
    "\n",
    "X_noisy = torch.from_numpy(X_np).to(device).to(dtype)\n",
    "noise_cov = torch.from_numpy(np.cov(noise_np.T)).to(device).to(dtype)\n",
    "\n",
    "X_noisy.shape, noise_cov.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.09819469, -0.12679765],\n       [-0.04360104, -0.10288863],\n       [-0.12310693, -0.1398263 ],\n       [-0.21098924, -0.2025033 ],\n       [-0.13089958, -0.14385152]], dtype=float32)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model(X_noisy, noise_cov).detach().cpu().numpy()\n",
    "y[200:205]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.29063462, -0.29063463],\n       [-0.18083572, -0.18083572],\n       [-0.13751319, -0.13751319],\n       [-0.2062843 , -0.20628429],\n       [-0.29935245, -0.29935245]])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_np - noise_np)[200:205]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "np.save('Y.npy', y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
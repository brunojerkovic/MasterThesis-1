/home/bjerkovic/thesis/models/nri/sourcecode/modules.py:27: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.
  nn.init.xavier_normal(m.weight.data)
/home/bjerkovic/thesis/models/nri/sourcecode/modules.py:110: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.
  nn.init.xavier_normal(m.weight.data)
/home/bjerkovic/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/bjerkovic/thesis/models/nri/sourcecode/utils.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  soft_max_1d = F.softmax(trans_input) # dim=1
/home/bjerkovic/thesis/models/nri/nri_train_test.py:117: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data, relations = Variable(data, volatile=True), Variable(
/home/bjerkovic/thesis/models/nri/nri_train_test.py:191: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data, relations = Variable(data, volatile=True), Variable(
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 17940.9987386068 kl_train: -0.1661141201 mse_train: 0.0944263097 acc_train: 0.4791666667 nll_val: 16392.7552083333 kl_val: -0.1720539629 mse_val: 0.0862776538 acc_val: 0.5000000000 time: 6.6140s
Best model so far, saving...
Epoch: 0001 nll_train: 13997.8870442708 kl_train: -0.1397659397 mse_train: 0.0736730887 acc_train: 0.5000000000 nll_val: 15245.3642578125 kl_val: -0.1777706345 mse_val: 0.0802387546 acc_val: 0.5000000000 time: 0.5065s
Best model so far, saving...
Epoch: 0002 nll_train: 13397.7867431641 kl_train: -0.1965624783 mse_train: 0.0705146687 acc_train: 0.5000000000 nll_val: 14636.7242838542 kl_val: -0.2042613973 mse_val: 0.0770353898 acc_val: 0.5000000000 time: 0.5212s
Best model so far, saving...
Epoch: 0003 nll_train: 13221.3880208333 kl_train: -0.1855949194 mse_train: 0.0695862534 acc_train: 0.5000000000 nll_val: 14113.1373697917 kl_val: -0.1610757609 mse_val: 0.0742796734 acc_val: 0.5000000000 time: 0.5005s
Best model so far, saving...
Epoch: 0004 nll_train: 12970.0039062500 kl_train: -0.1428257280 mse_train: 0.0682631790 acc_train: 0.4375000000 nll_val: 13842.2701822917 kl_val: -0.1127843137 mse_val: 0.0728540520 acc_val: 0.5000000000 time: 0.5028s
Best model so far, saving...
Epoch: 0005 nll_train: 12880.8468831380 kl_train: -0.1210322188 mse_train: 0.0677939310 acc_train: 0.4791666667 nll_val: 13674.4807942708 kl_val: -0.1045928473 mse_val: 0.0719709471 acc_val: 0.5000000000 time: 0.5019s
Best model so far, saving...
Epoch: 0006 nll_train: 12867.9297688802 kl_train: -0.1024212645 mse_train: 0.0677259451 acc_train: 0.4375000000 nll_val: 13564.6901041667 kl_val: -0.0893012683 mse_val: 0.0713931049 acc_val: 0.3333333333 time: 0.4998s
Best model so far, saving...
Epoch: 0007 nll_train: 12859.4673258464 kl_train: -0.0914729980 mse_train: 0.0676814069 acc_train: 0.5000000000 nll_val: 13560.8688151042 kl_val: -0.0892450064 mse_val: 0.0713729883 acc_val: 0.5000000000 time: 0.4998s
Best model so far, saving...
Epoch: 0008 nll_train: 12852.6872558594 kl_train: -0.0948757635 mse_train: 0.0676457238 acc_train: 0.5000000000 nll_val: 13545.0511067708 kl_val: -0.0899802347 mse_val: 0.0712897405 acc_val: 0.5000000000 time: 0.5016s
Best model so far, saving...
Epoch: 0009 nll_train: 12862.9923502604 kl_train: -0.0977116541 mse_train: 0.0676999595 acc_train: 0.4791666667 nll_val: 13534.9248046875 kl_val: -0.1061353485 mse_val: 0.0712364465 acc_val: 0.5000000000 time: 0.5014s
Best model so far, saving...
Epoch: 0010 nll_train: 12871.3668619792 kl_train: -0.0980817539 mse_train: 0.0677440365 acc_train: 0.5000000000 nll_val: 13496.9501953125 kl_val: -0.0941305483 mse_val: 0.0710365772 acc_val: 0.3333333333 time: 0.5026s
Best model so far, saving...
Epoch: 0011 nll_train: 12893.5220540365 kl_train: -0.1079745066 mse_train: 0.0678606423 acc_train: 0.5208333333 nll_val: 13558.9420572917 kl_val: -0.1060881739 mse_val: 0.0713628506 acc_val: 0.3333333333 time: 0.5122s
Epoch: 0012 nll_train: 12870.3999837240 kl_train: -0.1156010870 mse_train: 0.0677389499 acc_train: 0.5625000000 nll_val: 13662.2226562500 kl_val: -0.1166983421 mse_val: 0.0719064325 acc_val: 0.5000000000 time: 0.5008s
Epoch: 0013 nll_train: 12855.7266438802 kl_train: -0.1240857129 mse_train: 0.0676617178 acc_train: 0.5208333333 nll_val: 13524.8860677083 kl_val: -0.1100150471 mse_val: 0.0711836119 acc_val: 0.3333333333 time: 0.4963s
Epoch: 0014 nll_train: 12812.5797119141 kl_train: -0.1075532623 mse_train: 0.0674346294 acc_train: 0.5625000000 nll_val: 13494.7262369792 kl_val: -0.1080275526 mse_val: 0.0710248773 acc_val: 0.5000000000 time: 0.4980s
Best model so far, saving...
Epoch: 0015 nll_train: 12802.9158121745 kl_train: -0.1081565712 mse_train: 0.0673837673 acc_train: 0.5416666667 nll_val: 13462.7272135417 kl_val: -0.1049963733 mse_val: 0.0708564594 acc_val: 0.5000000000 time: 0.4994s
Best model so far, saving...
Epoch: 0016 nll_train: 12788.4762776693 kl_train: -0.1211379000 mse_train: 0.0673077704 acc_train: 0.5416666667 nll_val: 13490.0784505208 kl_val: -0.1199592600 mse_val: 0.0710004096 acc_val: 0.3333333333 time: 0.5014s
Epoch: 0017 nll_train: 12785.9821777344 kl_train: -0.1268709097 mse_train: 0.0672946440 acc_train: 0.5625000000 nll_val: 13499.2604166667 kl_val: -0.1100207443 mse_val: 0.0710487366 acc_val: 0.3333333333 time: 0.5034s
Epoch: 0018 nll_train: 12788.5562337240 kl_train: -0.1221130695 mse_train: 0.0673081906 acc_train: 0.5416666667 nll_val: 13467.2698567708 kl_val: -0.1106210252 mse_val: 0.0708803659 acc_val: 0.3333333333 time: 0.5009s
Epoch: 0019 nll_train: 12770.1778564453 kl_train: -0.1198534255 mse_train: 0.0672114637 acc_train: 0.5416666667 nll_val: 13456.2548828125 kl_val: -0.1055491269 mse_val: 0.0708223904 acc_val: 0.3333333333 time: 0.4969s
Best model so far, saving...
Epoch: 0020 nll_train: 12764.6712239583 kl_train: -0.1188204906 mse_train: 0.0671824810 acc_train: 0.5416666667 nll_val: 13417.4833984375 kl_val: -0.1013355951 mse_val: 0.0706183289 acc_val: 0.5000000000 time: 0.5000s
Best model so far, saving...
Epoch: 0021 nll_train: 12761.6749267578 kl_train: -0.1172432418 mse_train: 0.0671667092 acc_train: 0.5625000000 nll_val: 13435.7552083333 kl_val: -0.1153024087 mse_val: 0.0707144986 acc_val: 0.5000000000 time: 0.5020s
Epoch: 0022 nll_train: 12757.4645996094 kl_train: -0.1313679634 mse_train: 0.0671445502 acc_train: 0.5833333333 nll_val: 13452.6432291667 kl_val: -0.1305761238 mse_val: 0.0708033840 acc_val: 0.3333333333 time: 0.4972s
Epoch: 0023 nll_train: 12751.8247070312 kl_train: -0.1439879568 mse_train: 0.0671148674 acc_train: 0.5000000000 nll_val: 13445.5712890625 kl_val: -0.1280547281 mse_val: 0.0707661609 acc_val: 0.3333333333 time: 0.4982s
Epoch: 0024 nll_train: 12752.6674804688 kl_train: -0.1327919184 mse_train: 0.0671193010 acc_train: 0.5625000000 nll_val: 13417.2819010417 kl_val: -0.1228425627 mse_val: 0.0706172660 acc_val: 0.3333333333 time: 0.4982s
Best model so far, saving...
Epoch: 0025 nll_train: 12745.1724853516 kl_train: -0.1394186296 mse_train: 0.0670798539 acc_train: 0.6041666667 nll_val: 13424.3489583333 kl_val: -0.1333758757 mse_val: 0.0706544667 acc_val: 0.5000000000 time: 0.5028s
Epoch: 0026 nll_train: 12736.4874674479 kl_train: -0.1354702991 mse_train: 0.0670341449 acc_train: 0.5625000000 nll_val: 13439.2962239583 kl_val: -0.1131877204 mse_val: 0.0707331424 acc_val: 0.5000000000 time: 0.4964s
Epoch: 0027 nll_train: 12734.0754394531 kl_train: -0.1209631724 mse_train: 0.0670214510 acc_train: 0.5625000000 nll_val: 13435.0901692708 kl_val: -0.1165680215 mse_val: 0.0707109993 acc_val: 0.3333333333 time: 0.4993s
Epoch: 0028 nll_train: 12730.3104654948 kl_train: -0.1291881573 mse_train: 0.0670016340 acc_train: 0.5833333333 nll_val: 13456.1354166667 kl_val: -0.1222696155 mse_val: 0.0708217621 acc_val: 0.6666666667 time: 0.5014s
Epoch: 0029 nll_train: 12734.5622965495 kl_train: -0.1392636218 mse_train: 0.0670240126 acc_train: 0.5416666667 nll_val: 13422.8095703125 kl_val: -0.1283636615 mse_val: 0.0706463630 acc_val: 0.6666666667 time: 0.4982s
Epoch: 0030 nll_train: 12727.6585286458 kl_train: -0.1447510710 mse_train: 0.0669876765 acc_train: 0.5833333333 nll_val: 13445.5260416667 kl_val: -0.1448450536 mse_val: 0.0707659225 acc_val: 0.5000000000 time: 0.4993s
Epoch: 0031 nll_train: 12722.2681884766 kl_train: -0.1586596922 mse_train: 0.0669593074 acc_train: 0.5416666667 nll_val: 13393.2861328125 kl_val: -0.1397725642 mse_val: 0.0704909811 acc_val: 0.3333333333 time: 0.4961s
Best model so far, saving...
Epoch: 0032 nll_train: 12722.7652180990 kl_train: -0.1598817216 mse_train: 0.0669619218 acc_train: 0.5833333333 nll_val: 13453.5634765625 kl_val: -0.1330935607 mse_val: 0.0708082318 acc_val: 0.3333333333 time: 0.4983s
Epoch: 0033 nll_train: 12725.0198160807 kl_train: -0.1428928065 mse_train: 0.0669737888 acc_train: 0.5625000000 nll_val: 13423.8352864583 kl_val: -0.1350031346 mse_val: 0.0706517672 acc_val: 0.3333333333 time: 0.4976s
Epoch: 0034 nll_train: 12719.3941243490 kl_train: -0.1452101152 mse_train: 0.0669441802 acc_train: 0.5833333333 nll_val: 13387.2786458333 kl_val: -0.1373089751 mse_val: 0.0704593634 acc_val: 0.6666666667 time: 0.4989s
Best model so far, saving...
Epoch: 0035 nll_train: 12722.7004394531 kl_train: -0.1451686705 mse_train: 0.0669615804 acc_train: 0.5833333333 nll_val: 13395.1097005208 kl_val: -0.1223851666 mse_val: 0.0705005775 acc_val: 0.5000000000 time: 0.5126s
Epoch: 0036 nll_train: 12723.8614908854 kl_train: -0.1364069091 mse_train: 0.0669676928 acc_train: 0.5833333333 nll_val: 13391.7630208333 kl_val: -0.1254829367 mse_val: 0.0704829643 acc_val: 0.5000000000 time: 0.4978s
Epoch: 0037 nll_train: 12710.8973388672 kl_train: -0.1348092609 mse_train: 0.0668994593 acc_train: 0.6250000000 nll_val: 13395.9300130208 kl_val: -0.1203946422 mse_val: 0.0705048939 acc_val: 0.5000000000 time: 0.4973s
Epoch: 0038 nll_train: 12712.7707519531 kl_train: -0.1318063298 mse_train: 0.0669093191 acc_train: 0.5416666667 nll_val: 13380.5719401042 kl_val: -0.1171521495 mse_val: 0.0704240625 acc_val: 0.5000000000 time: 0.4991s
Best model so far, saving...
Epoch: 0039 nll_train: 12718.0229899089 kl_train: -0.1338208877 mse_train: 0.0669369643 acc_train: 0.5833333333 nll_val: 13427.5644531250 kl_val: -0.1209177673 mse_val: 0.0706713945 acc_val: 0.3333333333 time: 0.4962s
Epoch: 0040 nll_train: 12709.7084960938 kl_train: -0.1370352414 mse_train: 0.0668932035 acc_train: 0.5208333333 nll_val: 13406.4456380208 kl_val: -0.1364556899 mse_val: 0.0705602393 acc_val: 0.3333333333 time: 0.5001s
Epoch: 0041 nll_train: 12717.8510742188 kl_train: -0.1409329685 mse_train: 0.0669360586 acc_train: 0.6041666667 nll_val: 13436.6194661458 kl_val: -0.1284862931 mse_val: 0.0707190484 acc_val: 0.3333333333 time: 0.5007s
Epoch: 0042 nll_train: 12712.9193929036 kl_train: -0.1266777844 mse_train: 0.0669101023 acc_train: 0.5625000000 nll_val: 13475.6722005208 kl_val: -0.1207932308 mse_val: 0.0709245925 acc_val: 0.3333333333 time: 0.4993s
Epoch: 0043 nll_train: 12708.5288085938 kl_train: -0.1502199266 mse_train: 0.0668869922 acc_train: 0.5416666667 nll_val: 13407.3404947917 kl_val: -0.1352347930 mse_val: 0.0705649455 acc_val: 0.3333333333 time: 0.4979s
Epoch: 0044 nll_train: 12700.2038574219 kl_train: -0.1459212831 mse_train: 0.0668431784 acc_train: 0.5208333333 nll_val: 13409.8636067708 kl_val: -0.1304691136 mse_val: 0.0705782299 acc_val: 0.3333333333 time: 0.4963s
Epoch: 0045 nll_train: 12707.0656331380 kl_train: -0.1363667985 mse_train: 0.0668792931 acc_train: 0.5833333333 nll_val: 13483.4716796875 kl_val: -0.1269620260 mse_val: 0.0709656353 acc_val: 0.3333333333 time: 0.4953s
Epoch: 0046 nll_train: 12703.7054850260 kl_train: -0.1490280541 mse_train: 0.0668616074 acc_train: 0.5625000000 nll_val: 13424.6959635417 kl_val: -0.1448893100 mse_val: 0.0706562971 acc_val: 0.5000000000 time: 0.5008s
Epoch: 0047 nll_train: 12691.7685139974 kl_train: -0.1366159255 mse_train: 0.0667987807 acc_train: 0.5625000000 nll_val: 13384.3984375000 kl_val: -0.1311828593 mse_val: 0.0704442014 acc_val: 0.5000000000 time: 0.5008s
Epoch: 0048 nll_train: 12695.2860107422 kl_train: -0.1380576389 mse_train: 0.0668172951 acc_train: 0.5833333333 nll_val: 13393.3255208333 kl_val: -0.1374327193 mse_val: 0.0704911848 acc_val: 0.6666666667 time: 0.4983s
Epoch: 0049 nll_train: 12695.7045491536 kl_train: -0.1516309083 mse_train: 0.0668194989 acc_train: 0.5833333333 nll_val: 13367.5898437500 kl_val: -0.1481827994 mse_val: 0.0703557357 acc_val: 0.6666666667 time: 0.4979s
Best model so far, saving...
Epoch: 0050 nll_train: 12690.6846516927 kl_train: -0.1544738878 mse_train: 0.0667930765 acc_train: 0.5833333333 nll_val: 13422.1178385417 kl_val: -0.1298829665 mse_val: 0.0706427271 acc_val: 0.3333333333 time: 0.4983s
Epoch: 0051 nll_train: 12691.3554280599 kl_train: -0.1497005718 mse_train: 0.0667966078 acc_train: 0.5416666667 nll_val: 13398.8134765625 kl_val: -0.1330682437 mse_val: 0.0705200682 acc_val: 0.5000000000 time: 0.4942s
Epoch: 0052 nll_train: 12701.5543212891 kl_train: -0.1598522613 mse_train: 0.0668502854 acc_train: 0.5833333333 nll_val: 13368.9674479167 kl_val: -0.1397723407 mse_val: 0.0703629901 acc_val: 0.6666666667 time: 0.4985s
Epoch: 0053 nll_train: 12691.4694417318 kl_train: -0.1467126890 mse_train: 0.0667972085 acc_train: 0.5416666667 nll_val: 13474.1015625000 kl_val: -0.1403415501 mse_val: 0.0709163249 acc_val: 0.5000000000 time: 0.4983s
Epoch: 0054 nll_train: 12692.1100667318 kl_train: -0.1550176352 mse_train: 0.0668005793 acc_train: 0.6041666667 nll_val: 13395.5253906250 kl_val: -0.1451082230 mse_val: 0.0705027605 acc_val: 0.3333333333 time: 0.4999s
Epoch: 0055 nll_train: 12683.0925292969 kl_train: -0.1454468158 mse_train: 0.0667531202 acc_train: 0.5833333333 nll_val: 13417.1416015625 kl_val: -0.1346340378 mse_val: 0.0706165383 acc_val: 0.3333333333 time: 0.5040s
Epoch: 0056 nll_train: 12685.8264160156 kl_train: -0.1434142304 mse_train: 0.0667675069 acc_train: 0.5625000000 nll_val: 13466.3587239583 kl_val: -0.1391532620 mse_val: 0.0708755727 acc_val: 0.3333333333 time: 0.4994s
Epoch: 0057 nll_train: 12688.8481852214 kl_train: -0.1521451532 mse_train: 0.0667834108 acc_train: 0.5625000000 nll_val: 13547.5664062500 kl_val: -0.1336818586 mse_val: 0.0713029802 acc_val: 0.5000000000 time: 0.5115s
Epoch: 0058 nll_train: 12689.3735758464 kl_train: -0.1482465708 mse_train: 0.0667861756 acc_train: 0.5833333333 nll_val: 13442.5348307292 kl_val: -0.1384188334 mse_val: 0.0707501844 acc_val: 0.5000000000 time: 0.4998s
Epoch: 0059 nll_train: 12677.2500000000 kl_train: -0.1375118395 mse_train: 0.0667223677 acc_train: 0.6250000000 nll_val: 13432.5325520833 kl_val: -0.1318364988 mse_val: 0.0706975361 acc_val: 0.5000000000 time: 0.4961s
Epoch: 0060 nll_train: 12697.7439778646 kl_train: -0.1314792090 mse_train: 0.0668302321 acc_train: 0.5833333333 nll_val: 13496.5904947917 kl_val: -0.1306752364 mse_val: 0.0710346848 acc_val: 0.3333333333 time: 0.4985s
Epoch: 0061 nll_train: 12681.5078938802 kl_train: -0.1347487709 mse_train: 0.0667447794 acc_train: 0.6041666667 nll_val: 13486.7701822917 kl_val: -0.1416310966 mse_val: 0.0709829976 acc_val: 0.5000000000 time: 0.4958s
Epoch: 0062 nll_train: 12690.1713460286 kl_train: -0.1339982534 mse_train: 0.0667903763 acc_train: 0.6041666667 nll_val: 13523.2623697917 kl_val: -0.1258304492 mse_val: 0.0711750636 acc_val: 0.3333333333 time: 0.4964s
Epoch: 0063 nll_train: 12690.1863199870 kl_train: -0.1443942164 mse_train: 0.0667904558 acc_train: 0.5833333333 nll_val: 13478.2093098958 kl_val: -0.1430700074 mse_val: 0.0709379415 acc_val: 0.3333333333 time: 0.5003s
Epoch: 0064 nll_train: 12686.9904378255 kl_train: -0.1483377842 mse_train: 0.0667736350 acc_train: 0.5833333333 nll_val: 13541.6888020833 kl_val: -0.1420512547 mse_val: 0.0712720429 acc_val: 0.6666666667 time: 0.5025s
Epoch: 0065 nll_train: 12686.2549641927 kl_train: -0.1431068471 mse_train: 0.0667697637 acc_train: 0.5833333333 nll_val: 13428.3294270833 kl_val: -0.1337851187 mse_val: 0.0706754203 acc_val: 0.6666666667 time: 0.4961s
Epoch: 0066 nll_train: 12679.1870524089 kl_train: -0.1383833320 mse_train: 0.0667325642 acc_train: 0.6041666667 nll_val: 13506.7903645833 kl_val: -0.1393273224 mse_val: 0.0710883712 acc_val: 0.6666666667 time: 0.4995s
Epoch: 0067 nll_train: 12676.4464111328 kl_train: -0.1385931754 mse_train: 0.0667181408 acc_train: 0.6250000000 nll_val: 13425.5559895833 kl_val: -0.1318066716 mse_val: 0.0706608221 acc_val: 0.5000000000 time: 0.4948s
Epoch: 0068 nll_train: 12674.4042154948 kl_train: -0.1501808064 mse_train: 0.0667073908 acc_train: 0.6041666667 nll_val: 13463.4912109375 kl_val: -0.1610280027 mse_val: 0.0708604778 acc_val: 0.6666666667 time: 0.4958s
Epoch: 0069 nll_train: 12665.7634277344 kl_train: -0.1607469053 mse_train: 0.0666619116 acc_train: 0.6041666667 nll_val: 13449.0856119792 kl_val: -0.1523255060 mse_val: 0.0707846632 acc_val: 0.6666666667 time: 0.4962s
Epoch: 0070 nll_train: 12671.4545084635 kl_train: -0.1367753809 mse_train: 0.0666918677 acc_train: 0.5833333333 nll_val: 13511.3668619792 kl_val: -0.1370657583 mse_val: 0.0711124589 acc_val: 0.5000000000 time: 0.4993s
Epoch: 0071 nll_train: 12672.1835123698 kl_train: -0.1491038312 mse_train: 0.0666957018 acc_train: 0.6041666667 nll_val: 13573.5179036458 kl_val: -0.1348717213 mse_val: 0.0714395667 acc_val: 0.6666666667 time: 0.5383s
Epoch: 0072 nll_train: 12678.2255045573 kl_train: -0.1459020060 mse_train: 0.0667275021 acc_train: 0.6250000000 nll_val: 13425.3548177083 kl_val: -0.1396543384 mse_val: 0.0706597616 acc_val: 0.5000000000 time: 0.5519s
Epoch: 0073 nll_train: 12666.2367350260 kl_train: -0.1423313761 mse_train: 0.0666644039 acc_train: 0.5625000000 nll_val: 13558.5009765625 kl_val: -0.1500201871 mse_val: 0.0713605334 acc_val: 0.6666666667 time: 0.4984s
Epoch: 0074 nll_train: 12673.8057454427 kl_train: -0.1506426757 mse_train: 0.0667042412 acc_train: 0.6041666667 nll_val: 13551.7386067708 kl_val: -0.1498900652 mse_val: 0.0713249346 acc_val: 0.5000000000 time: 0.4979s
Epoch: 0075 nll_train: 12669.1750488281 kl_train: -0.1580572780 mse_train: 0.0666798688 acc_train: 0.6041666667 nll_val: 13547.6054687500 kl_val: -0.1492410004 mse_val: 0.0713031863 acc_val: 0.6666666667 time: 0.4953s
Epoch: 0076 nll_train: 12676.7361246745 kl_train: -0.1442157322 mse_train: 0.0667196650 acc_train: 0.5416666667 nll_val: 13463.8935546875 kl_val: -0.1364987095 mse_val: 0.0708625938 acc_val: 0.6666666667 time: 0.4938s
Epoch: 0077 nll_train: 12665.4025065104 kl_train: -0.1471277823 mse_train: 0.0666600135 acc_train: 0.5625000000 nll_val: 13503.7246093750 kl_val: -0.1478052884 mse_val: 0.0710722357 acc_val: 0.6666666667 time: 0.4979s
Epoch: 0078 nll_train: 12671.6158447266 kl_train: -0.1472623258 mse_train: 0.0666927152 acc_train: 0.5000000000 nll_val: 13400.6988932292 kl_val: -0.1238497595 mse_val: 0.0705299998 acc_val: 0.8333333333 time: 0.4998s
Epoch: 0079 nll_train: 12667.0505777995 kl_train: -0.1402787178 mse_train: 0.0666686872 acc_train: 0.5416666667 nll_val: 13514.0957031250 kl_val: -0.1317993353 mse_val: 0.0711268211 acc_val: 0.6666666667 time: 0.4968s
Epoch: 0080 nll_train: 12674.2294514974 kl_train: -0.1437555694 mse_train: 0.0667064710 acc_train: 0.6041666667 nll_val: 13403.6731770833 kl_val: -0.1396172941 mse_val: 0.0705456461 acc_val: 0.5000000000 time: 0.4976s
Epoch: 0081 nll_train: 12662.6672363281 kl_train: -0.1424029041 mse_train: 0.0666456179 acc_train: 0.6041666667 nll_val: 13501.9589843750 kl_val: -0.1347762644 mse_val: 0.0710629399 acc_val: 0.5000000000 time: 0.5010s
Epoch: 0082 nll_train: 12669.5339762370 kl_train: -0.1338301146 mse_train: 0.0666817582 acc_train: 0.5625000000 nll_val: 13544.7955729167 kl_val: -0.1349265575 mse_val: 0.0712883969 acc_val: 0.6666666667 time: 0.4999s
Epoch: 0083 nll_train: 12673.5708007812 kl_train: -0.1424861656 mse_train: 0.0667030026 acc_train: 0.5625000000 nll_val: 13492.1513671875 kl_val: -0.1269203201 mse_val: 0.0710113222 acc_val: 0.6666666667 time: 0.4981s
Epoch: 0084 nll_train: 12667.2718505859 kl_train: -0.1423632586 mse_train: 0.0666698523 acc_train: 0.5416666667 nll_val: 13490.0143229167 kl_val: -0.1523660421 mse_val: 0.0710000743 acc_val: 0.6666666667 time: 0.4980s
Epoch: 0085 nll_train: 12655.7225341797 kl_train: -0.1575429641 mse_train: 0.0666090672 acc_train: 0.5208333333 nll_val: 13484.4788411458 kl_val: -0.1593284011 mse_val: 0.0709709451 acc_val: 0.5000000000 time: 0.5126s
Epoch: 0086 nll_train: 12648.0702311198 kl_train: -0.1608028940 mse_train: 0.0665687892 acc_train: 0.5416666667 nll_val: 13545.2141927083 kl_val: -0.1491644035 mse_val: 0.0712905998 acc_val: 0.6666666667 time: 0.4974s
Epoch: 0087 nll_train: 12664.3253173828 kl_train: -0.1490700633 mse_train: 0.0666543439 acc_train: 0.5416666667 nll_val: 13532.3750000000 kl_val: -0.1541418384 mse_val: 0.0712230255 acc_val: 0.6666666667 time: 0.5016s
Epoch: 0088 nll_train: 12669.4354248047 kl_train: -0.1520300175 mse_train: 0.0666812394 acc_train: 0.5416666667 nll_val: 13437.5514322917 kl_val: -0.1419894099 mse_val: 0.0707239509 acc_val: 0.3333333333 time: 0.4973s
Epoch: 0089 nll_train: 12655.8636067708 kl_train: -0.1553324520 mse_train: 0.0666098096 acc_train: 0.6250000000 nll_val: 13502.1077473958 kl_val: -0.1444239865 mse_val: 0.0710637247 acc_val: 0.5000000000 time: 0.4949s
Epoch: 0090 nll_train: 12659.1601562500 kl_train: -0.1497772845 mse_train: 0.0666271589 acc_train: 0.5833333333 nll_val: 13452.1777343750 kl_val: -0.1347670406 mse_val: 0.0708009402 acc_val: 0.3333333333 time: 0.4948s
Epoch: 0091 nll_train: 12672.1951090495 kl_train: -0.1524769841 mse_train: 0.0666957648 acc_train: 0.5833333333 nll_val: 13552.8772786458 kl_val: -0.1541196009 mse_val: 0.0713309298 acc_val: 0.6666666667 time: 0.4955s
Epoch: 0092 nll_train: 12644.1602376302 kl_train: -0.1503173249 mse_train: 0.0665482131 acc_train: 0.5208333333 nll_val: 13520.0914713542 kl_val: -0.1424984882 mse_val: 0.0711583743 acc_val: 0.6666666667 time: 0.4956s
Epoch: 0093 nll_train: 12644.7808024089 kl_train: -0.1475144792 mse_train: 0.0665514787 acc_train: 0.5625000000 nll_val: 13484.6728515625 kl_val: -0.1589232932 mse_val: 0.0709719633 acc_val: 0.5000000000 time: 0.4954s
Epoch: 0094 nll_train: 12642.0163981120 kl_train: -0.1483783685 mse_train: 0.0665369290 acc_train: 0.5625000000 nll_val: 13513.4570312500 kl_val: -0.1464032729 mse_val: 0.0711234560 acc_val: 0.6666666667 time: 0.4975s
Epoch: 0095 nll_train: 12645.5181884766 kl_train: -0.1593788483 mse_train: 0.0665553606 acc_train: 0.5208333333 nll_val: 13598.4895833333 kl_val: -0.1568397433 mse_val: 0.0715709974 acc_val: 0.6666666667 time: 0.4985s
Epoch: 0096 nll_train: 12637.2395019531 kl_train: -0.1576580790 mse_train: 0.0665117869 acc_train: 0.5625000000 nll_val: 13493.0501302083 kl_val: -0.1518726299 mse_val: 0.0710160484 acc_val: 0.5000000000 time: 0.4951s
Epoch: 0097 nll_train: 12636.0225016276 kl_train: -0.1441572122 mse_train: 0.0665053817 acc_train: 0.5625000000 nll_val: 13431.6585286458 kl_val: -0.1434399933 mse_val: 0.0706929415 acc_val: 0.5000000000 time: 0.4969s
Epoch: 0098 nll_train: 12649.6193847656 kl_train: -0.1531485931 mse_train: 0.0665769448 acc_train: 0.5208333333 nll_val: 13513.1318359375 kl_val: -0.1511248449 mse_val: 0.0711217473 acc_val: 0.6666666667 time: 0.4960s
Epoch: 0099 nll_train: 12633.7673746745 kl_train: -0.1434898650 mse_train: 0.0664935131 acc_train: 0.5416666667 nll_val: 13520.7757161458 kl_val: -0.1537846675 mse_val: 0.0711619804 acc_val: 0.6666666667 time: 0.4960s
Epoch: 0100 nll_train: 12639.4335937500 kl_train: -0.1535666327 mse_train: 0.0665233342 acc_train: 0.5416666667 nll_val: 13498.6263020833 kl_val: -0.1423688879 mse_val: 0.0710454012 acc_val: 0.6666666667 time: 0.4979s
Epoch: 0101 nll_train: 12651.5518391927 kl_train: -0.1492186664 mse_train: 0.0665871149 acc_train: 0.6041666667 nll_val: 13521.8818359375 kl_val: -0.1421547805 mse_val: 0.0711677944 acc_val: 0.6666666667 time: 0.4963s
Epoch: 0102 nll_train: 12635.9107259115 kl_train: -0.1511929377 mse_train: 0.0665047942 acc_train: 0.5000000000 nll_val: 13498.3551432292 kl_val: -0.1409774025 mse_val: 0.0710439757 acc_val: 0.6666666667 time: 0.4963s
Epoch: 0103 nll_train: 12634.6120198568 kl_train: -0.1441798570 mse_train: 0.0664979573 acc_train: 0.5000000000 nll_val: 13499.3990885417 kl_val: -0.1399575224 mse_val: 0.0710494667 acc_val: 0.8333333333 time: 0.4965s
Epoch: 0104 nll_train: 12641.1140543620 kl_train: -0.1597898171 mse_train: 0.0665321789 acc_train: 0.5625000000 nll_val: 13507.0921223958 kl_val: -0.1629003634 mse_val: 0.0710899557 acc_val: 0.6666666667 time: 0.4993s
Epoch: 0105 nll_train: 12639.8531494141 kl_train: -0.1554391033 mse_train: 0.0665255426 acc_train: 0.5416666667 nll_val: 13514.3531901042 kl_val: -0.1488826027 mse_val: 0.0711281697 acc_val: 0.3333333333 time: 0.4953s
Epoch: 0106 nll_train: 12644.9969889323 kl_train: -0.1380847804 mse_train: 0.0665526162 acc_train: 0.4791666667 nll_val: 13536.9560546875 kl_val: -0.1388815095 mse_val: 0.0712471331 acc_val: 0.6666666667 time: 0.5024s
Epoch: 0107 nll_train: 12649.8264160156 kl_train: -0.1399020127 mse_train: 0.0665780342 acc_train: 0.5833333333 nll_val: 13537.9309895833 kl_val: -0.1586830169 mse_val: 0.0712522715 acc_val: 0.6666666667 time: 0.4952s
Epoch: 0108 nll_train: 12637.5362955729 kl_train: -0.1525516802 mse_train: 0.0665133495 acc_train: 0.5416666667 nll_val: 13518.0703125000 kl_val: -0.1505067299 mse_val: 0.0711477399 acc_val: 0.6666666667 time: 0.4973s
Epoch: 0109 nll_train: 12639.0255126953 kl_train: -0.1595467937 mse_train: 0.0665211870 acc_train: 0.5208333333 nll_val: 13689.8186848958 kl_val: -0.1538603306 mse_val: 0.0720516766 acc_val: 0.6666666667 time: 0.4939s
Epoch: 0110 nll_train: 12637.3425292969 kl_train: -0.1408477894 mse_train: 0.0665123287 acc_train: 0.5208333333 nll_val: 13562.4091796875 kl_val: -0.1420496106 mse_val: 0.0713810995 acc_val: 0.6666666667 time: 0.5027s
Epoch: 0111 nll_train: 12641.6783040365 kl_train: -0.1521019544 mse_train: 0.0665351495 acc_train: 0.4583333333 nll_val: 13620.4550781250 kl_val: -0.1698627869 mse_val: 0.0716866056 acc_val: 0.6666666667 time: 0.4904s
Epoch: 0112 nll_train: 12636.6395670573 kl_train: -0.1545558022 mse_train: 0.0665086286 acc_train: 0.5000000000 nll_val: 13539.4944661458 kl_val: -0.1430701812 mse_val: 0.0712604970 acc_val: 0.6666666667 time: 0.5043s
Epoch: 0113 nll_train: 12639.2776285807 kl_train: -0.1306453589 mse_train: 0.0665225160 acc_train: 0.5416666667 nll_val: 13525.9365234375 kl_val: -0.1452432722 mse_val: 0.0711891428 acc_val: 0.6666666667 time: 0.4932s
Epoch: 0114 nll_train: 12645.7746175130 kl_train: -0.1604702007 mse_train: 0.0665567087 acc_train: 0.6041666667 nll_val: 13682.6796875000 kl_val: -0.1783430179 mse_val: 0.0720141058 acc_val: 0.6666666667 time: 0.4948s
Epoch: 0115 nll_train: 12631.6472167969 kl_train: -0.1667010598 mse_train: 0.0664823549 acc_train: 0.4791666667 nll_val: 13648.2180989583 kl_val: -0.1651593198 mse_val: 0.0718327264 acc_val: 0.5000000000 time: 0.4935s
Epoch: 0116 nll_train: 12637.5355224609 kl_train: -0.1571696692 mse_train: 0.0665133450 acc_train: 0.5000000000 nll_val: 13637.7750651042 kl_val: -0.1723121802 mse_val: 0.0717777610 acc_val: 0.6666666667 time: 0.4926s
Epoch: 0117 nll_train: 12652.0273437500 kl_train: -0.1650325159 mse_train: 0.0665896178 acc_train: 0.5416666667 nll_val: 13502.2340494792 kl_val: -0.1631944378 mse_val: 0.0710643853 acc_val: 0.5000000000 time: 0.4935s
Epoch: 0118 nll_train: 12651.8958740234 kl_train: -0.1425647043 mse_train: 0.0665889275 acc_train: 0.5416666667 nll_val: 13426.3004557292 kl_val: -0.1380538543 mse_val: 0.0706647336 acc_val: 0.6666666667 time: 0.4979s
Epoch: 0119 nll_train: 12640.7055664062 kl_train: -0.1545834374 mse_train: 0.0665300299 acc_train: 0.5416666667 nll_val: 13590.5211588542 kl_val: -0.1546425323 mse_val: 0.0715290581 acc_val: 0.6666666667 time: 0.4962s
Epoch: 0120 nll_train: 12643.1864420573 kl_train: -0.1432213324 mse_train: 0.0665430871 acc_train: 0.5208333333 nll_val: 13604.2304687500 kl_val: -0.1393803731 mse_val: 0.0716012120 acc_val: 0.6666666667 time: 0.4977s
Epoch: 0121 nll_train: 12645.0334879557 kl_train: -0.1516314577 mse_train: 0.0665528082 acc_train: 0.5833333333 nll_val: 13548.3359375000 kl_val: -0.1488454590 mse_val: 0.0713070333 acc_val: 0.6666666667 time: 0.4944s
Epoch: 0122 nll_train: 12639.8393147786 kl_train: -0.1528968945 mse_train: 0.0665254692 acc_train: 0.4375000000 nll_val: 13739.7555338542 kl_val: -0.1728384346 mse_val: 0.0723144983 acc_val: 0.6666666667 time: 0.4970s
Epoch: 0123 nll_train: 12628.7431233724 kl_train: -0.1725374339 mse_train: 0.0664670696 acc_train: 0.5833333333 nll_val: 13611.9091796875 kl_val: -0.1587340285 mse_val: 0.0716416215 acc_val: 0.6666666667 time: 0.4993s
Epoch: 0124 nll_train: 12631.9845784505 kl_train: -0.1676120693 mse_train: 0.0664841295 acc_train: 0.4791666667 nll_val: 13606.5989583333 kl_val: -0.1723401050 mse_val: 0.0716136818 acc_val: 0.6666666667 time: 0.5010s
Epoch: 0125 nll_train: 12642.3399658203 kl_train: -0.1490497980 mse_train: 0.0665386313 acc_train: 0.5208333333 nll_val: 13541.1988932292 kl_val: -0.1604119241 mse_val: 0.0712694675 acc_val: 0.6666666667 time: 0.4939s
Epoch: 0126 nll_train: 12625.9899902344 kl_train: -0.1479925066 mse_train: 0.0664525785 acc_train: 0.5625000000 nll_val: 13617.2760416667 kl_val: -0.1516344796 mse_val: 0.0716698716 acc_val: 0.6666666667 time: 0.5184s
Epoch: 0127 nll_train: 12629.9505208333 kl_train: -0.1443057743 mse_train: 0.0664734238 acc_train: 0.5000000000 nll_val: 13589.2369791667 kl_val: -0.1588789076 mse_val: 0.0715223004 acc_val: 0.6666666667 time: 0.5002s
Epoch: 0128 nll_train: 12621.2420247396 kl_train: -0.1634900703 mse_train: 0.0664275897 acc_train: 0.5416666667 nll_val: 13636.5332031250 kl_val: -0.1571232627 mse_val: 0.0717712219 acc_val: 0.6666666667 time: 0.4964s
Epoch: 0129 nll_train: 12650.7131754557 kl_train: -0.1605900135 mse_train: 0.0665827016 acc_train: 0.5000000000 nll_val: 13626.2669270833 kl_val: -0.1409001648 mse_val: 0.0717171952 acc_val: 0.8333333333 time: 0.4991s
Epoch: 0130 nll_train: 12634.5590413411 kl_train: -0.1606334361 mse_train: 0.0664976796 acc_train: 0.4583333333 nll_val: 13620.2382812500 kl_val: -0.1737173150 mse_val: 0.0716854657 acc_val: 0.6666666667 time: 0.4989s
Epoch: 0131 nll_train: 12628.5779622396 kl_train: -0.1612400596 mse_train: 0.0664661980 acc_train: 0.5208333333 nll_val: 13668.6455078125 kl_val: -0.1831708600 mse_val: 0.0719402333 acc_val: 0.8333333333 time: 0.4966s
Epoch: 0132 nll_train: 12622.2426350911 kl_train: -0.1513311168 mse_train: 0.0664328566 acc_train: 0.4791666667 nll_val: 13556.0598958333 kl_val: -0.1547269622 mse_val: 0.0713476812 acc_val: 0.6666666667 time: 0.4986s
Epoch: 0133 nll_train: 12622.4325764974 kl_train: -0.1456699378 mse_train: 0.0664338550 acc_train: 0.5000000000 nll_val: 13575.5218098958 kl_val: -0.1707373311 mse_val: 0.0714501118 acc_val: 0.6666666667 time: 0.4966s
Epoch: 0134 nll_train: 12628.5831298828 kl_train: -0.1499629042 mse_train: 0.0664662286 acc_train: 0.5208333333 nll_val: 13621.9560546875 kl_val: -0.1736182322 mse_val: 0.0716945032 acc_val: 0.6666666667 time: 0.4975s
Epoch: 0135 nll_train: 12630.5064290365 kl_train: -0.1583437417 mse_train: 0.0664763507 acc_train: 0.5000000000 nll_val: 13657.2373046875 kl_val: -0.1541141222 mse_val: 0.0718801965 acc_val: 0.6666666667 time: 0.4942s
Epoch: 0136 nll_train: 12626.2810872396 kl_train: -0.1638155576 mse_train: 0.0664541114 acc_train: 0.5000000000 nll_val: 13658.9082031250 kl_val: -0.1935220162 mse_val: 0.0718889907 acc_val: 0.6666666667 time: 0.4979s
Epoch: 0137 nll_train: 12622.4226888021 kl_train: -0.1609235356 mse_train: 0.0664338041 acc_train: 0.4583333333 nll_val: 13672.5123697917 kl_val: -0.1578544378 mse_val: 0.0719605883 acc_val: 0.6666666667 time: 0.4978s
Epoch: 0138 nll_train: 12619.6465250651 kl_train: -0.1473564679 mse_train: 0.0664191926 acc_train: 0.4791666667 nll_val: 13513.3873697917 kl_val: -0.1539557725 mse_val: 0.0711230834 acc_val: 0.6666666667 time: 0.4989s
Epoch: 0139 nll_train: 12631.3391113281 kl_train: -0.1446271669 mse_train: 0.0664807325 acc_train: 0.5416666667 nll_val: 13614.5439453125 kl_val: -0.1461526453 mse_val: 0.0716554870 acc_val: 0.6666666667 time: 0.4957s
Epoch: 0140 nll_train: 12639.5652669271 kl_train: -0.1486327378 mse_train: 0.0665240271 acc_train: 0.5000000000 nll_val: 13804.0843098958 kl_val: -0.1604368985 mse_val: 0.0726530726 acc_val: 0.6666666667 time: 0.4962s
Epoch: 0141 nll_train: 12644.8688151042 kl_train: -0.1444867536 mse_train: 0.0665519418 acc_train: 0.5000000000 nll_val: 13619.6507161458 kl_val: -0.1550812125 mse_val: 0.0716823687 acc_val: 0.6666666667 time: 0.4960s
Epoch: 0142 nll_train: 12638.1432291667 kl_train: -0.1623126656 mse_train: 0.0665165440 acc_train: 0.4583333333 nll_val: 13661.2262369792 kl_val: -0.1802937388 mse_val: 0.0719011848 acc_val: 0.6666666667 time: 0.4997s
Epoch: 0143 nll_train: 12641.4912516276 kl_train: -0.1625837668 mse_train: 0.0665341651 acc_train: 0.4166666667 nll_val: 13741.0690104167 kl_val: -0.1842928032 mse_val: 0.0723214149 acc_val: 0.6666666667 time: 0.4965s
Epoch: 0144 nll_train: 12632.5203043620 kl_train: -0.1557489416 mse_train: 0.0664869499 acc_train: 0.4791666667 nll_val: 13647.5117187500 kl_val: -0.1687708795 mse_val: 0.0718290061 acc_val: 0.8333333333 time: 0.4983s
Epoch: 0145 nll_train: 12645.9215494792 kl_train: -0.1573320888 mse_train: 0.0665574817 acc_train: 0.4791666667 nll_val: 13556.5104166667 kl_val: -0.1505995890 mse_val: 0.0713500505 acc_val: 0.6666666667 time: 0.4958s
Epoch: 0146 nll_train: 12642.8766682943 kl_train: -0.1544206537 mse_train: 0.0665414562 acc_train: 0.4791666667 nll_val: 13653.2714843750 kl_val: -0.1920235157 mse_val: 0.0718593175 acc_val: 0.6666666667 time: 0.4960s
Epoch: 0147 nll_train: 12650.6701253255 kl_train: -0.1562716557 mse_train: 0.0665824753 acc_train: 0.4583333333 nll_val: 13787.7705078125 kl_val: -0.1724901249 mse_val: 0.0725672146 acc_val: 0.6666666667 time: 0.4960s
Epoch: 0148 nll_train: 12646.4985351562 kl_train: -0.1550937245 mse_train: 0.0665605194 acc_train: 0.4791666667 nll_val: 13706.2151692708 kl_val: -0.2050678482 mse_val: 0.0721379742 acc_val: 0.6666666667 time: 0.5006s
Epoch: 0149 nll_train: 12664.5073242188 kl_train: -0.1722517510 mse_train: 0.0666553012 acc_train: 0.5208333333 nll_val: 13804.5634765625 kl_val: -0.1724363267 mse_val: 0.0726555983 acc_val: 0.5000000000 time: 0.4987s
Epoch: 0150 nll_train: 12645.2310384115 kl_train: -0.1477550495 mse_train: 0.0665538486 acc_train: 0.5416666667 nll_val: 13763.0813802083 kl_val: -0.1628931910 mse_val: 0.0724372665 acc_val: 0.6666666667 time: 0.4948s
Epoch: 0151 nll_train: 12653.9468994141 kl_train: -0.1552249696 mse_train: 0.0665997216 acc_train: 0.4375000000 nll_val: 13685.5973307292 kl_val: -0.1627907902 mse_val: 0.0720294540 acc_val: 0.6666666667 time: 0.4946s
Epoch: 0152 nll_train: 12642.4814046224 kl_train: -0.1466035234 mse_train: 0.0665393764 acc_train: 0.4375000000 nll_val: 13609.9505208333 kl_val: -0.1709522704 mse_val: 0.0716313149 acc_val: 0.8333333333 time: 0.4990s
Epoch: 0153 nll_train: 12659.7008463542 kl_train: -0.1609528055 mse_train: 0.0666300061 acc_train: 0.4375000000 nll_val: 13763.9186197917 kl_val: -0.1763888448 mse_val: 0.0724416773 acc_val: 0.6666666667 time: 0.4958s
Epoch: 0154 nll_train: 12660.9484049479 kl_train: -0.1565427172 mse_train: 0.0666365712 acc_train: 0.4375000000 nll_val: 13650.7076822917 kl_val: -0.1876506408 mse_val: 0.0718458270 acc_val: 0.8333333333 time: 0.4961s
Epoch: 0155 nll_train: 12673.4705810547 kl_train: -0.1674185048 mse_train: 0.0667024773 acc_train: 0.4791666667 nll_val: 13614.5192057292 kl_val: -0.1610003014 mse_val: 0.0716553579 acc_val: 0.8333333333 time: 0.4962s
Epoch: 0156 nll_train: 12695.8522135417 kl_train: -0.1625197210 mse_train: 0.0668202755 acc_train: 0.4583333333 nll_val: 13778.7490234375 kl_val: -0.1928472718 mse_val: 0.0725197295 acc_val: 0.6666666667 time: 0.4979s
Epoch: 0157 nll_train: 12659.2736409505 kl_train: -0.1658637840 mse_train: 0.0666277555 acc_train: 0.5625000000 nll_val: 13729.9674479167 kl_val: -0.1672793676 mse_val: 0.0722629850 acc_val: 0.6666666667 time: 0.5018s
Epoch: 0158 nll_train: 12705.8365071615 kl_train: -0.1547567751 mse_train: 0.0668728241 acc_train: 0.5000000000 nll_val: 13680.2932942708 kl_val: -0.1666888595 mse_val: 0.0720015417 acc_val: 0.6666666667 time: 0.4958s
Epoch: 0159 nll_train: 12674.5093587240 kl_train: -0.1558096384 mse_train: 0.0667079447 acc_train: 0.4791666667 nll_val: 13710.8902994792 kl_val: -0.1618786802 mse_val: 0.0721625835 acc_val: 0.6666666667 time: 0.4953s
Epoch: 0160 nll_train: 12681.0114746094 kl_train: -0.1709420700 mse_train: 0.0667421664 acc_train: 0.5000000000 nll_val: 13624.8180338542 kl_val: -0.2077836742 mse_val: 0.0717095658 acc_val: 0.8333333333 time: 0.4983s
Epoch: 0161 nll_train: 12655.8373616536 kl_train: -0.1458936020 mse_train: 0.0666096718 acc_train: 0.4375000000 nll_val: 13530.2174479167 kl_val: -0.1583349705 mse_val: 0.0712116684 acc_val: 0.6666666667 time: 0.4949s
Epoch: 0162 nll_train: 12678.4686686198 kl_train: -0.1600034439 mse_train: 0.0667287819 acc_train: 0.5208333333 nll_val: 13608.6334635417 kl_val: -0.1870398571 mse_val: 0.0716243858 acc_val: 0.6666666667 time: 0.4969s
Epoch: 0163 nll_train: 12657.2517089844 kl_train: -0.1685413333 mse_train: 0.0666171143 acc_train: 0.4583333333 nll_val: 13570.5598958333 kl_val: -0.1638777802 mse_val: 0.0714240000 acc_val: 0.6666666667 time: 0.5227s
Epoch: 0164 nll_train: 12692.2308756510 kl_train: -0.1551526977 mse_train: 0.0668012144 acc_train: 0.4583333333 nll_val: 13433.3893229167 kl_val: -0.1679002692 mse_val: 0.0707020437 acc_val: 0.5000000000 time: 0.4971s
Epoch: 0165 nll_train: 12697.1131998698 kl_train: -0.1562604581 mse_train: 0.0668269121 acc_train: 0.4375000000 nll_val: 13437.7343750000 kl_val: -0.1751692593 mse_val: 0.0707249145 acc_val: 0.8333333333 time: 0.4962s
Epoch: 0166 nll_train: 12696.0697021484 kl_train: -0.1729913040 mse_train: 0.0668214195 acc_train: 0.4375000000 nll_val: 13382.9199218750 kl_val: -0.1831049770 mse_val: 0.0704364230 acc_val: 0.6666666667 time: 0.4975s
Epoch: 0167 nll_train: 12672.9726155599 kl_train: -0.1556102205 mse_train: 0.0666998566 acc_train: 0.4791666667 nll_val: 13448.6416015625 kl_val: -0.1564940959 mse_val: 0.0707823212 acc_val: 0.8333333333 time: 0.4957s
Epoch: 0168 nll_train: 12655.5173339844 kl_train: -0.1785771375 mse_train: 0.0666079853 acc_train: 0.5208333333 nll_val: 13536.8883463542 kl_val: -0.1866449515 mse_val: 0.0712467805 acc_val: 0.6666666667 time: 0.4966s
Epoch: 0169 nll_train: 12699.9267171224 kl_train: -0.1700498232 mse_train: 0.0668417186 acc_train: 0.4791666667 nll_val: 13451.9160156250 kl_val: -0.1738466173 mse_val: 0.0707995569 acc_val: 0.8333333333 time: 0.4968s
Epoch: 0170 nll_train: 12710.6271972656 kl_train: -0.1572340109 mse_train: 0.0668980372 acc_train: 0.4166666667 nll_val: 13441.4033203125 kl_val: -0.1572612723 mse_val: 0.0707442289 acc_val: 0.6666666667 time: 0.4966s
Epoch: 0171 nll_train: 12711.0781250000 kl_train: -0.1428966044 mse_train: 0.0669004122 acc_train: 0.3958333333 nll_val: 13382.8333333333 kl_val: -0.1305938214 mse_val: 0.0704359586 acc_val: 0.6666666667 time: 0.4940s
Epoch: 0172 nll_train: 12705.9901936849 kl_train: -0.1469780421 mse_train: 0.0668736331 acc_train: 0.4166666667 nll_val: 13606.9703776042 kl_val: -0.1829548081 mse_val: 0.0716156314 acc_val: 0.6666666667 time: 0.4983s
Epoch: 0173 nll_train: 12680.0332438151 kl_train: -0.1875737626 mse_train: 0.0667370182 acc_train: 0.4791666667 nll_val: 13487.7037760417 kl_val: -0.1957066556 mse_val: 0.0709879125 acc_val: 0.6666666667 time: 0.4986s
Epoch: 0174 nll_train: 12693.4061279297 kl_train: -0.1612699705 mse_train: 0.0668074007 acc_train: 0.4583333333 nll_val: 13560.8369140625 kl_val: -0.1645707587 mse_val: 0.0713728244 acc_val: 0.6666666667 time: 0.5338s
Epoch: 0175 nll_train: 12674.3024902344 kl_train: -0.1682841880 mse_train: 0.0667068564 acc_train: 0.3958333333 nll_val: 13417.9785156250 kl_val: -0.1790144742 mse_val: 0.0706209391 acc_val: 0.6666666667 time: 0.5518s
Epoch: 0176 nll_train: 12648.8840738932 kl_train: -0.1657609353 mse_train: 0.0665730735 acc_train: 0.4375000000 nll_val: 13501.1757812500 kl_val: -0.1557252159 mse_val: 0.0710588197 acc_val: 0.6666666667 time: 0.5024s
Epoch: 0177 nll_train: 12666.1769205729 kl_train: -0.1568833664 mse_train: 0.0666640890 acc_train: 0.5000000000 nll_val: 13474.4527994792 kl_val: -0.1746741086 mse_val: 0.0709181726 acc_val: 0.6666666667 time: 0.5014s
Epoch: 0178 nll_train: 12648.3723958333 kl_train: -0.1694156888 mse_train: 0.0665703805 acc_train: 0.5000000000 nll_val: 13502.9563802083 kl_val: -0.1752405365 mse_val: 0.0710681925 acc_val: 0.6666666667 time: 0.5000s
Epoch: 0179 nll_train: 12640.5942382812 kl_train: -0.1618320780 mse_train: 0.0665294441 acc_train: 0.5000000000 nll_val: 13439.6575520833 kl_val: -0.1728064716 mse_val: 0.0707350348 acc_val: 0.6666666667 time: 0.4963s
Epoch: 0180 nll_train: 12615.7571614583 kl_train: -0.1718276870 mse_train: 0.0663987215 acc_train: 0.5000000000 nll_val: 13508.0745442708 kl_val: -0.2068597227 mse_val: 0.0710951289 acc_val: 0.6666666667 time: 0.4962s
Epoch: 0181 nll_train: 12644.2443033854 kl_train: -0.1757430465 mse_train: 0.0665486567 acc_train: 0.4791666667 nll_val: 13595.0263671875 kl_val: -0.1585597843 mse_val: 0.0715527683 acc_val: 0.6666666667 time: 0.4949s
Epoch: 0182 nll_train: 12641.6594645182 kl_train: -0.1374058848 mse_train: 0.0665350505 acc_train: 0.4791666667 nll_val: 13538.6162109375 kl_val: -0.1607157836 mse_val: 0.0712558726 acc_val: 0.6666666667 time: 0.4957s
Epoch: 0183 nll_train: 12622.6729736328 kl_train: -0.1542992741 mse_train: 0.0664351202 acc_train: 0.5416666667 nll_val: 13534.1236979167 kl_val: -0.1752842317 mse_val: 0.0712322295 acc_val: 0.6666666667 time: 0.4956s
Epoch: 0184 nll_train: 12626.1072591146 kl_train: -0.1618476802 mse_train: 0.0664531960 acc_train: 0.5000000000 nll_val: 13510.2607421875 kl_val: -0.1676254968 mse_val: 0.0711066400 acc_val: 0.6666666667 time: 0.4986s
Epoch: 0185 nll_train: 12610.3917236328 kl_train: -0.1631938874 mse_train: 0.0663704826 acc_train: 0.5416666667 nll_val: 13395.5947265625 kl_val: -0.1847412338 mse_val: 0.0705031306 acc_val: 0.6666666667 time: 0.4961s
Epoch: 0186 nll_train: 12596.0510253906 kl_train: -0.1679397213 mse_train: 0.0662950082 acc_train: 0.5208333333 nll_val: 13470.0029296875 kl_val: -0.1829948574 mse_val: 0.0708947505 acc_val: 0.6666666667 time: 0.5190s
Epoch: 0187 nll_train: 12601.7991943359 kl_train: -0.1699046961 mse_train: 0.0663252582 acc_train: 0.5000000000 nll_val: 13442.6481119792 kl_val: -0.1883680920 mse_val: 0.0707507779 acc_val: 0.6666666667 time: 0.4977s
Epoch: 0188 nll_train: 12583.4665527344 kl_train: -0.1696127116 mse_train: 0.0662287731 acc_train: 0.5000000000 nll_val: 13379.4280598958 kl_val: -0.1713623106 mse_val: 0.0704180375 acc_val: 0.6666666667 time: 0.5007s
Epoch: 0189 nll_train: 12585.5394694010 kl_train: -0.1587973082 mse_train: 0.0662396813 acc_train: 0.5833333333 nll_val: 13393.1087239583 kl_val: -0.1707924555 mse_val: 0.0704900449 acc_val: 0.6666666667 time: 0.4948s
Epoch: 0190 nll_train: 12583.6307373047 kl_train: -0.1604734727 mse_train: 0.0662296369 acc_train: 0.5625000000 nll_val: 13422.8710937500 kl_val: -0.1440427552 mse_val: 0.0706466908 acc_val: 0.5000000000 time: 0.4952s
Epoch: 0191 nll_train: 12577.3854573568 kl_train: -0.1589375154 mse_train: 0.0661967651 acc_train: 0.5208333333 nll_val: 13425.6347656250 kl_val: -0.1710207413 mse_val: 0.0706612344 acc_val: 0.6666666667 time: 0.4983s
Epoch: 0192 nll_train: 12599.2293294271 kl_train: -0.1544379117 mse_train: 0.0663117343 acc_train: 0.5208333333 nll_val: 13452.6100260417 kl_val: -0.1542387456 mse_val: 0.0708032126 acc_val: 0.6666666667 time: 0.4967s
Epoch: 0193 nll_train: 12590.7771402995 kl_train: -0.1317302013 mse_train: 0.0662672481 acc_train: 0.5625000000 nll_val: 13418.4589843750 kl_val: -0.1677139401 mse_val: 0.0706234698 acc_val: 0.6666666667 time: 0.4949s
Epoch: 0194 nll_train: 12595.7236328125 kl_train: -0.1561083520 mse_train: 0.0662932837 acc_train: 0.5833333333 nll_val: 13396.0413411458 kl_val: -0.1723535409 mse_val: 0.0705054800 acc_val: 0.6666666667 time: 0.4971s
Epoch: 0195 nll_train: 12584.9244384766 kl_train: -0.1543634239 mse_train: 0.0662364443 acc_train: 0.5625000000 nll_val: 13421.7083333333 kl_val: -0.1551108708 mse_val: 0.0706405664 acc_val: 0.6666666667 time: 0.4947s
Epoch: 0196 nll_train: 12584.5485432943 kl_train: -0.1489081290 mse_train: 0.0662344665 acc_train: 0.5208333333 nll_val: 13386.9820963542 kl_val: -0.1694282939 mse_val: 0.0704577938 acc_val: 0.6666666667 time: 0.4983s
Epoch: 0197 nll_train: 12599.5217285156 kl_train: -0.1538684210 mse_train: 0.0663132723 acc_train: 0.5000000000 nll_val: 13363.9029947917 kl_val: -0.1768820037 mse_val: 0.0703363294 acc_val: 0.6666666667 time: 0.4955s
Best model so far, saving...
Epoch: 0198 nll_train: 12581.9620361328 kl_train: -0.1537816220 mse_train: 0.0662208536 acc_train: 0.5416666667 nll_val: 13452.1044921875 kl_val: -0.2000508706 mse_val: 0.0708005478 acc_val: 0.5000000000 time: 0.5002s
Epoch: 0199 nll_train: 12571.6993815104 kl_train: -0.1749413274 mse_train: 0.0661668379 acc_train: 0.4791666667 nll_val: 13477.0468750000 kl_val: -0.1870911767 mse_val: 0.0709318270 acc_val: 0.6666666667 time: 0.4969s
Optimization finished
Best epoch 197
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 15522.2539062500 kl_test: -0.1634678791 mse_test: 0.0816960707 acc_test: 0.1666666667
MSE: [ 0.072202555835 , 0.071829326451 , 0.071142278612 , 0.077420324087 , 0.075618393719 , 0.074612349272 , 0.075567595661 , 0.075729139149 , 0.076340191066 , 0.078616678715 , 0.079228721559 , 0.076942443848 , 0.084788285196 , 0.084558784962 , 0.085975565016 , 0.085935585201 , 0.085828192532 , 0.088015355170 , 0.088429041207 ]
Accuracy for experiment id 0 is 0.25
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 15998.4723714193 kl_train: -0.1571161486 mse_train: 0.0842024866 acc_train: 0.5000000000 nll_val: 13059.2607421875 kl_val: -0.1842624793 mse_val: 0.0687329521 acc_val: 0.5000000000 time: 0.5036s
Best model so far, saving...
Epoch: 0001 nll_train: 12298.0837809245 kl_train: -0.1781088493 mse_train: 0.0647267575 acc_train: 0.5000000000 nll_val: 11960.3131510417 kl_val: -0.2624276131 mse_val: 0.0629490130 acc_val: 0.5000000000 time: 0.5029s
Best model so far, saving...
Epoch: 0002 nll_train: 11638.2824300130 kl_train: -0.1675668452 mse_train: 0.0612541197 acc_train: 0.5000000000 nll_val: 11041.3336588542 kl_val: -0.1147040601 mse_val: 0.0581122798 acc_val: 0.5000000000 time: 0.5007s
Best model so far, saving...
Epoch: 0003 nll_train: 11401.8459472656 kl_train: -0.1735701704 mse_train: 0.0600097161 acc_train: 0.5000000000 nll_val: 10763.6673177083 kl_val: -0.1182495976 mse_val: 0.0566508820 acc_val: 0.6666666667 time: 0.4979s
Best model so far, saving...
Epoch: 0004 nll_train: 11223.5470377604 kl_train: -0.1347361989 mse_train: 0.0590713009 acc_train: 0.5000000000 nll_val: 10540.2167968750 kl_val: -0.0997090514 mse_val: 0.0554748252 acc_val: 0.6666666667 time: 0.4969s
Best model so far, saving...
Epoch: 0005 nll_train: 11136.1627197266 kl_train: -0.1201089534 mse_train: 0.0586113830 acc_train: 0.5000000000 nll_val: 10435.3876953125 kl_val: -0.0925471534 mse_val: 0.0549230886 acc_val: 0.5000000000 time: 0.4985s
Best model so far, saving...
Epoch: 0006 nll_train: 11110.5829671224 kl_train: -0.1032769252 mse_train: 0.0584767518 acc_train: 0.5000000000 nll_val: 10355.6142578125 kl_val: -0.0783501851 mse_val: 0.0545032322 acc_val: 0.5000000000 time: 0.5090s
Best model so far, saving...
Epoch: 0007 nll_train: 11070.7473958333 kl_train: -0.0988797015 mse_train: 0.0582670931 acc_train: 0.5208333333 nll_val: 10347.2259114583 kl_val: -0.0787825646 mse_val: 0.0544590813 acc_val: 0.5000000000 time: 0.4994s
Best model so far, saving...
Epoch: 0008 nll_train: 11067.7145589193 kl_train: -0.0937253569 mse_train: 0.0582511293 acc_train: 0.4791666667 nll_val: 10318.8147786458 kl_val: -0.0814041595 mse_val: 0.0543095494 acc_val: 0.3333333333 time: 0.4995s
Best model so far, saving...
Epoch: 0009 nll_train: 11070.8957112630 kl_train: -0.0947430378 mse_train: 0.0582678712 acc_train: 0.4791666667 nll_val: 10269.5550130208 kl_val: -0.0763873557 mse_val: 0.0540502928 acc_val: 0.5000000000 time: 0.5404s
Best model so far, saving...
Epoch: 0010 nll_train: 11093.3343098958 kl_train: -0.0947410107 mse_train: 0.0583859715 acc_train: 0.4791666667 nll_val: 10361.5110677083 kl_val: -0.0736059422 mse_val: 0.0545342689 acc_val: 0.3333333333 time: 0.5349s
Epoch: 0011 nll_train: 11115.6769205729 kl_train: -0.0968678140 mse_train: 0.0585035626 acc_train: 0.5208333333 nll_val: 10543.0032552083 kl_val: -0.0798732129 mse_val: 0.0554894879 acc_val: 0.5000000000 time: 0.5313s
Epoch: 0012 nll_train: 11127.8783365885 kl_train: -0.1006927782 mse_train: 0.0585677818 acc_train: 0.5208333333 nll_val: 10651.7158203125 kl_val: -0.0806738834 mse_val: 0.0560616603 acc_val: 0.3333333333 time: 0.5395s
Epoch: 0013 nll_train: 11088.3935546875 kl_train: -0.0996947475 mse_train: 0.0583599651 acc_train: 0.4791666667 nll_val: 10467.8639322917 kl_val: -0.0893446480 mse_val: 0.0550940186 acc_val: 0.5000000000 time: 0.5379s
Epoch: 0014 nll_train: 11055.1423746745 kl_train: -0.0954373889 mse_train: 0.0581849612 acc_train: 0.4791666667 nll_val: 10442.4427083333 kl_val: -0.0862319519 mse_val: 0.0549602211 acc_val: 0.3333333333 time: 0.5323s
Epoch: 0015 nll_train: 11041.8802897135 kl_train: -0.0921742109 mse_train: 0.0581151595 acc_train: 0.5208333333 nll_val: 10444.7190755208 kl_val: -0.0769731477 mse_val: 0.0549722053 acc_val: 0.3333333333 time: 0.5371s
Epoch: 0016 nll_train: 11044.6376546224 kl_train: -0.0919927759 mse_train: 0.0581296718 acc_train: 0.4791666667 nll_val: 10468.2841796875 kl_val: -0.0861710583 mse_val: 0.0550962289 acc_val: 0.3333333333 time: 0.5431s
Epoch: 0017 nll_train: 11043.1388346354 kl_train: -0.0948059947 mse_train: 0.0581217844 acc_train: 0.4583333333 nll_val: 10431.9856770833 kl_val: -0.0828427188 mse_val: 0.0549051898 acc_val: 0.5000000000 time: 0.5195s
Epoch: 0018 nll_train: 11037.0730794271 kl_train: -0.0985910573 mse_train: 0.0580898582 acc_train: 0.5000000000 nll_val: 10435.8541666667 kl_val: -0.0911952232 mse_val: 0.0549255461 acc_val: 0.3333333333 time: 0.5285s
Epoch: 0019 nll_train: 11031.9191487630 kl_train: -0.1151345801 mse_train: 0.0580627325 acc_train: 0.5208333333 nll_val: 10447.0481770833 kl_val: -0.0920060600 mse_val: 0.0549844615 acc_val: 0.3333333333 time: 0.5400s
Epoch: 0020 nll_train: 11024.7502848307 kl_train: -0.1060591492 mse_train: 0.0580250017 acc_train: 0.4791666667 nll_val: 10398.9944661458 kl_val: -0.0931090725 mse_val: 0.0547315478 acc_val: 0.3333333333 time: 0.5299s
Epoch: 0021 nll_train: 11015.1844075521 kl_train: -0.1077183488 mse_train: 0.0579746553 acc_train: 0.5000000000 nll_val: 10446.2890625000 kl_val: -0.0959938429 mse_val: 0.0549804655 acc_val: 0.3333333333 time: 0.5357s
Epoch: 0022 nll_train: 11023.8658854167 kl_train: -0.1087224095 mse_train: 0.0580203474 acc_train: 0.5416666667 nll_val: 10444.3147786458 kl_val: -0.1066957066 mse_val: 0.0549700769 acc_val: 0.5000000000 time: 0.5276s
Epoch: 0023 nll_train: 11014.4902750651 kl_train: -0.1188229738 mse_train: 0.0579710002 acc_train: 0.4791666667 nll_val: 10419.5582682292 kl_val: -0.0884593613 mse_val: 0.0548397774 acc_val: 0.3333333333 time: 0.5351s
Epoch: 0024 nll_train: 11010.1553141276 kl_train: -0.1065800193 mse_train: 0.0579481873 acc_train: 0.5416666667 nll_val: 10426.7047526042 kl_val: -0.0888497954 mse_val: 0.0548773929 acc_val: 0.5000000000 time: 0.5278s
Epoch: 0025 nll_train: 11010.7365315755 kl_train: -0.1103173026 mse_train: 0.0579512453 acc_train: 0.5208333333 nll_val: 10428.9277343750 kl_val: -0.0958190312 mse_val: 0.0548890891 acc_val: 0.3333333333 time: 0.5352s
Epoch: 0026 nll_train: 11011.6649576823 kl_train: -0.1134601214 mse_train: 0.0579561307 acc_train: 0.4375000000 nll_val: 10397.3798828125 kl_val: -0.0977371509 mse_val: 0.0547230492 acc_val: 0.3333333333 time: 0.5293s
Epoch: 0027 nll_train: 11007.4889729818 kl_train: -0.1147513951 mse_train: 0.0579341529 acc_train: 0.4583333333 nll_val: 10411.1087239583 kl_val: -0.1043148190 mse_val: 0.0547953087 acc_val: 0.3333333333 time: 0.5301s
Epoch: 0028 nll_train: 11002.5802815755 kl_train: -0.1143259689 mse_train: 0.0579083185 acc_train: 0.5000000000 nll_val: 10384.2314453125 kl_val: -0.0967545124 mse_val: 0.0546538457 acc_val: 0.3333333333 time: 0.5321s
Epoch: 0029 nll_train: 10999.8262125651 kl_train: -0.1062974713 mse_train: 0.0578938217 acc_train: 0.4791666667 nll_val: 10413.7444661458 kl_val: -0.0867972250 mse_val: 0.0548091841 acc_val: 0.3333333333 time: 0.5328s
Epoch: 0030 nll_train: 11000.5609537760 kl_train: -0.1024142380 mse_train: 0.0578976904 acc_train: 0.5208333333 nll_val: 10399.0719401042 kl_val: -0.0933259142 mse_val: 0.0547319551 acc_val: 0.5000000000 time: 0.5400s
Epoch: 0031 nll_train: 11000.1384684245 kl_train: -0.1207644107 mse_train: 0.0578954662 acc_train: 0.5208333333 nll_val: 10439.4599609375 kl_val: -0.1114664351 mse_val: 0.0549445252 acc_val: 0.3333333333 time: 0.5329s
Epoch: 0032 nll_train: 10994.3512369792 kl_train: -0.1268004182 mse_train: 0.0578650078 acc_train: 0.5000000000 nll_val: 10393.5553385417 kl_val: -0.1183869814 mse_val: 0.0547029215 acc_val: 0.3333333333 time: 0.5317s
Epoch: 0033 nll_train: 10992.0901285807 kl_train: -0.1331059104 mse_train: 0.0578531080 acc_train: 0.4791666667 nll_val: 10423.0289713542 kl_val: -0.1180365632 mse_val: 0.0548580438 acc_val: 0.5000000000 time: 0.5302s
Epoch: 0034 nll_train: 10991.5193277995 kl_train: -0.1280140840 mse_train: 0.0578501015 acc_train: 0.4375000000 nll_val: 10417.1194661458 kl_val: -0.1118343621 mse_val: 0.0548269451 acc_val: 0.3333333333 time: 0.5311s
Epoch: 0035 nll_train: 10988.6567382812 kl_train: -0.1309186313 mse_train: 0.0578350364 acc_train: 0.5000000000 nll_val: 10385.3876953125 kl_val: -0.1064169084 mse_val: 0.0546599329 acc_val: 0.3333333333 time: 0.5299s
Epoch: 0036 nll_train: 10980.3424886068 kl_train: -0.1242819702 mse_train: 0.0577912747 acc_train: 0.5000000000 nll_val: 10376.1175130208 kl_val: -0.1061235281 mse_val: 0.0546111427 acc_val: 0.3333333333 time: 0.5368s
Epoch: 0037 nll_train: 10981.5497233073 kl_train: -0.1366721761 mse_train: 0.0577976303 acc_train: 0.4583333333 nll_val: 10394.0579427083 kl_val: -0.1140929299 mse_val: 0.0547055627 acc_val: 0.3333333333 time: 0.5285s
Epoch: 0038 nll_train: 10977.4961751302 kl_train: -0.1349484290 mse_train: 0.0577762964 acc_train: 0.4583333333 nll_val: 10409.2932942708 kl_val: -0.1312197919 mse_val: 0.0547857533 acc_val: 0.3333333333 time: 0.5363s
Epoch: 0039 nll_train: 10977.8377685547 kl_train: -0.1400443014 mse_train: 0.0577780937 acc_train: 0.4583333333 nll_val: 10398.3922526042 kl_val: -0.1220854968 mse_val: 0.0547283776 acc_val: 0.3333333333 time: 0.5296s
Epoch: 0040 nll_train: 10978.6361897786 kl_train: -0.1245355783 mse_train: 0.0577822969 acc_train: 0.4583333333 nll_val: 10372.1748046875 kl_val: -0.1120317479 mse_val: 0.0545903941 acc_val: 0.3333333333 time: 0.5344s
Epoch: 0041 nll_train: 10972.8166910807 kl_train: -0.1318329452 mse_train: 0.0577516685 acc_train: 0.4791666667 nll_val: 10366.0107421875 kl_val: -0.1090223938 mse_val: 0.0545579505 acc_val: 0.5000000000 time: 0.5418s
Epoch: 0042 nll_train: 10971.3348795573 kl_train: -0.1284097247 mse_train: 0.0577438682 acc_train: 0.4791666667 nll_val: 10422.2747395833 kl_val: -0.1127707387 mse_val: 0.0548540801 acc_val: 0.3333333333 time: 0.5260s
Epoch: 0043 nll_train: 10985.9867757161 kl_train: -0.1343590543 mse_train: 0.0578209843 acc_train: 0.4375000000 nll_val: 10412.0426432292 kl_val: -0.1008267552 mse_val: 0.0548002211 acc_val: 0.5000000000 time: 0.5331s
Epoch: 0044 nll_train: 10972.5679931641 kl_train: -0.1307356963 mse_train: 0.0577503581 acc_train: 0.4583333333 nll_val: 10409.4462890625 kl_val: -0.1093712362 mse_val: 0.0547865555 acc_val: 0.3333333333 time: 0.5308s
Epoch: 0045 nll_train: 10979.7667236328 kl_train: -0.1405037117 mse_train: 0.0577882466 acc_train: 0.5000000000 nll_val: 10390.0921223958 kl_val: -0.1177308982 mse_val: 0.0546846961 acc_val: 0.3333333333 time: 0.5332s
Epoch: 0046 nll_train: 10980.5506591797 kl_train: -0.1361658244 mse_train: 0.0577923721 acc_train: 0.4791666667 nll_val: 10460.4843750000 kl_val: -0.1238609999 mse_val: 0.0550551787 acc_val: 0.3333333333 time: 0.5279s
Epoch: 0047 nll_train: 10980.2036132812 kl_train: -0.1422800357 mse_train: 0.0577905454 acc_train: 0.4583333333 nll_val: 10419.1448567708 kl_val: -0.1176543025 mse_val: 0.0548376044 acc_val: 0.5000000000 time: 0.5337s
Epoch: 0048 nll_train: 10963.9446207682 kl_train: -0.1365630180 mse_train: 0.0577049729 acc_train: 0.4791666667 nll_val: 10396.5820312500 kl_val: -0.1348007719 mse_val: 0.0547188545 acc_val: 0.3333333333 time: 0.5251s
Epoch: 0049 nll_train: 10970.2625732422 kl_train: -0.1310100698 mse_train: 0.0577382258 acc_train: 0.4791666667 nll_val: 10370.3776041667 kl_val: -0.1192360123 mse_val: 0.0545809368 acc_val: 0.3333333333 time: 0.5330s
Epoch: 0050 nll_train: 10963.4167480469 kl_train: -0.1471176588 mse_train: 0.0577021940 acc_train: 0.4791666667 nll_val: 10390.0426432292 kl_val: -0.1327269028 mse_val: 0.0546844291 acc_val: 0.5000000000 time: 0.5290s
Epoch: 0051 nll_train: 10966.7051595052 kl_train: -0.1321143564 mse_train: 0.0577195010 acc_train: 0.5000000000 nll_val: 10388.0830078125 kl_val: -0.1083218207 mse_val: 0.0546741212 acc_val: 0.3333333333 time: 0.5323s
Epoch: 0052 nll_train: 10967.8473307292 kl_train: -0.1197287397 mse_train: 0.0577255137 acc_train: 0.5208333333 nll_val: 10425.1464843750 kl_val: -0.1168847320 mse_val: 0.0548691861 acc_val: 0.3333333333 time: 0.5323s
Epoch: 0053 nll_train: 10953.9798583984 kl_train: -0.1428279793 mse_train: 0.0576525268 acc_train: 0.5000000000 nll_val: 10412.9453125000 kl_val: -0.1318604747 mse_val: 0.0548049721 acc_val: 0.3333333333 time: 0.5318s
Epoch: 0054 nll_train: 10957.5040690104 kl_train: -0.1543633351 mse_train: 0.0576710737 acc_train: 0.4791666667 nll_val: 10452.9225260417 kl_val: -0.1311554064 mse_val: 0.0550153827 acc_val: 0.3333333333 time: 0.5376s
Epoch: 0055 nll_train: 10962.0192871094 kl_train: -0.1385224980 mse_train: 0.0576948395 acc_train: 0.4583333333 nll_val: 10462.4837239583 kl_val: -0.1339780589 mse_val: 0.0550657014 acc_val: 0.3333333333 time: 0.5316s
Epoch: 0056 nll_train: 10960.9493408203 kl_train: -0.1464612621 mse_train: 0.0576892081 acc_train: 0.4791666667 nll_val: 10438.3639322917 kl_val: -0.1269033551 mse_val: 0.0549387522 acc_val: 0.3333333333 time: 0.5324s
Epoch: 0057 nll_train: 10960.0369873047 kl_train: -0.1440030644 mse_train: 0.0576844051 acc_train: 0.4583333333 nll_val: 10471.1682942708 kl_val: -0.1290218830 mse_val: 0.0551114120 acc_val: 0.3333333333 time: 0.5304s
Epoch: 0058 nll_train: 10955.6422526042 kl_train: -0.1434476950 mse_train: 0.0576612763 acc_train: 0.5208333333 nll_val: 10418.8281250000 kl_val: -0.1433797429 mse_val: 0.0548359392 acc_val: 0.3333333333 time: 0.5359s
Epoch: 0059 nll_train: 10951.3250325521 kl_train: -0.1485459308 mse_train: 0.0576385542 acc_train: 0.4583333333 nll_val: 10448.9264322917 kl_val: -0.1276127944 mse_val: 0.0549943497 acc_val: 0.3333333333 time: 0.5286s
Epoch: 0060 nll_train: 10957.1908772786 kl_train: -0.1376156344 mse_train: 0.0576694251 acc_train: 0.4791666667 nll_val: 10432.2861328125 kl_val: -0.1141217798 mse_val: 0.0549067644 acc_val: 0.3333333333 time: 0.5350s
Epoch: 0061 nll_train: 10951.4387613932 kl_train: -0.1416658536 mse_train: 0.0576391513 acc_train: 0.4583333333 nll_val: 10408.2747395833 kl_val: -0.1308928728 mse_val: 0.0547803914 acc_val: 0.3333333333 time: 0.5275s
Epoch: 0062 nll_train: 10948.2524820964 kl_train: -0.1551626176 mse_train: 0.0576223832 acc_train: 0.5208333333 nll_val: 10424.8281250000 kl_val: -0.1480004266 mse_val: 0.0548675098 acc_val: 0.3333333333 time: 0.5326s
Epoch: 0063 nll_train: 10943.8400065104 kl_train: -0.1639051062 mse_train: 0.0575991580 acc_train: 0.4583333333 nll_val: 10449.2207031250 kl_val: -0.1438908180 mse_val: 0.0549958994 acc_val: 0.3333333333 time: 0.5324s
Epoch: 0064 nll_train: 10947.3282877604 kl_train: -0.1534755292 mse_train: 0.0576175164 acc_train: 0.4791666667 nll_val: 10422.6012369792 kl_val: -0.1423864687 mse_val: 0.0548557925 acc_val: 0.3333333333 time: 0.5319s
Epoch: 0065 nll_train: 10943.0498860677 kl_train: -0.1553216707 mse_train: 0.0575950008 acc_train: 0.4791666667 nll_val: 10456.6660156250 kl_val: -0.1553282440 mse_val: 0.0550350795 acc_val: 0.3333333333 time: 0.5288s
Epoch: 0066 nll_train: 10954.0092366536 kl_train: -0.1547448707 mse_train: 0.0576526810 acc_train: 0.5416666667 nll_val: 10506.2565104167 kl_val: -0.1348020335 mse_val: 0.0552960895 acc_val: 0.3333333333 time: 0.5200s
Epoch: 0067 nll_train: 10956.9325358073 kl_train: -0.1538866749 mse_train: 0.0576680677 acc_train: 0.4791666667 nll_val: 10401.0937500000 kl_val: -0.1452226192 mse_val: 0.0547425983 acc_val: 0.3333333333 time: 0.5368s
Epoch: 0068 nll_train: 10943.4836425781 kl_train: -0.1493021927 mse_train: 0.0575972823 acc_train: 0.5000000000 nll_val: 10366.3557942708 kl_val: -0.1291944943 mse_val: 0.0545597672 acc_val: 0.3333333333 time: 0.5287s
Epoch: 0069 nll_train: 10938.9023437500 kl_train: -0.1584489519 mse_train: 0.0575731701 acc_train: 0.5208333333 nll_val: 10428.9648437500 kl_val: -0.1661921044 mse_val: 0.0548892866 acc_val: 0.3333333333 time: 0.5355s
Epoch: 0070 nll_train: 10940.8044026693 kl_train: -0.1587069587 mse_train: 0.0575831826 acc_train: 0.5000000000 nll_val: 10450.4098307292 kl_val: -0.1548606207 mse_val: 0.0550021591 acc_val: 0.3333333333 time: 0.5291s
Epoch: 0071 nll_train: 10937.5903320312 kl_train: -0.1611139014 mse_train: 0.0575662645 acc_train: 0.5000000000 nll_val: 10445.3108723958 kl_val: -0.1617629329 mse_val: 0.0549753221 acc_val: 0.3333333333 time: 0.5317s
Epoch: 0072 nll_train: 10937.7950846354 kl_train: -0.1666463672 mse_train: 0.0575673416 acc_train: 0.4583333333 nll_val: 10430.3199869792 kl_val: -0.1584707151 mse_val: 0.0548964180 acc_val: 0.3333333333 time: 0.5316s
Epoch: 0073 nll_train: 10930.1866455078 kl_train: -0.1683022374 mse_train: 0.0575272995 acc_train: 0.5208333333 nll_val: 10440.2037760417 kl_val: -0.1536859398 mse_val: 0.0549484392 acc_val: 0.3333333333 time: 0.5342s
Epoch: 0074 nll_train: 10931.2172444661 kl_train: -0.1676185743 mse_train: 0.0575327223 acc_train: 0.4583333333 nll_val: 10456.7887369792 kl_val: -0.1596877476 mse_val: 0.0550357302 acc_val: 0.3333333333 time: 0.5345s
Epoch: 0075 nll_train: 10937.4346923828 kl_train: -0.1717616388 mse_train: 0.0575654457 acc_train: 0.5208333333 nll_val: 10432.4397786458 kl_val: -0.1434915612 mse_val: 0.0549075802 acc_val: 0.3333333333 time: 0.5291s
Epoch: 0076 nll_train: 10932.3171793620 kl_train: -0.1557747734 mse_train: 0.0575385120 acc_train: 0.4583333333 nll_val: 10459.5039062500 kl_val: -0.1367889370 mse_val: 0.0550500204 acc_val: 0.3333333333 time: 0.5326s
Epoch: 0077 nll_train: 10929.1861979167 kl_train: -0.1534327064 mse_train: 0.0575220327 acc_train: 0.5416666667 nll_val: 10459.0488281250 kl_val: -0.1576446742 mse_val: 0.0550476263 acc_val: 0.3333333333 time: 0.5277s
Epoch: 0078 nll_train: 10929.9726562500 kl_train: -0.1610269717 mse_train: 0.0575261712 acc_train: 0.5000000000 nll_val: 10438.0579427083 kl_val: -0.1515726720 mse_val: 0.0549371441 acc_val: 0.3333333333 time: 0.5495s
Epoch: 0079 nll_train: 10936.2528076172 kl_train: -0.1506513460 mse_train: 0.0575592259 acc_train: 0.4791666667 nll_val: 10472.2965494792 kl_val: -0.1260990153 mse_val: 0.0551173488 acc_val: 0.3333333333 time: 0.5298s
Epoch: 0080 nll_train: 10931.1318359375 kl_train: -0.1485530498 mse_train: 0.0575322720 acc_train: 0.5000000000 nll_val: 10460.2539062500 kl_val: -0.1265458986 mse_val: 0.0550539692 acc_val: 0.3333333333 time: 0.5387s
Epoch: 0081 nll_train: 10923.5887451172 kl_train: -0.1584920129 mse_train: 0.0574925733 acc_train: 0.4583333333 nll_val: 10512.7220052083 kl_val: -0.1740524496 mse_val: 0.0553301126 acc_val: 0.3333333333 time: 0.5261s
Epoch: 0082 nll_train: 10918.2013753255 kl_train: -0.1724930219 mse_train: 0.0574642173 acc_train: 0.5833333333 nll_val: 10459.3821614583 kl_val: -0.1666197951 mse_val: 0.0550493759 acc_val: 0.3333333333 time: 0.5371s
Epoch: 0083 nll_train: 10926.2031656901 kl_train: -0.1864543452 mse_train: 0.0575063333 acc_train: 0.5000000000 nll_val: 10476.7353515625 kl_val: -0.1671964675 mse_val: 0.0551407126 acc_val: 0.3333333333 time: 0.5285s
Epoch: 0084 nll_train: 10920.7666015625 kl_train: -0.1636045814 mse_train: 0.0574777195 acc_train: 0.5208333333 nll_val: 10465.9860026042 kl_val: -0.1402309661 mse_val: 0.0550841366 acc_val: 0.3333333333 time: 0.5315s
Epoch: 0085 nll_train: 10928.3258870443 kl_train: -0.1614949005 mse_train: 0.0575175046 acc_train: 0.5000000000 nll_val: 10483.2783203125 kl_val: -0.1648378273 mse_val: 0.0551751480 acc_val: 0.3333333333 time: 0.5322s
Epoch: 0086 nll_train: 10934.8801676432 kl_train: -0.1716510750 mse_train: 0.0575520000 acc_train: 0.5416666667 nll_val: 10447.7880859375 kl_val: -0.1422157983 mse_val: 0.0549883619 acc_val: 0.3333333333 time: 0.5330s
Epoch: 0087 nll_train: 10924.4783528646 kl_train: -0.1694531121 mse_train: 0.0574972547 acc_train: 0.5000000000 nll_val: 10468.4238281250 kl_val: -0.1667006686 mse_val: 0.0550969703 acc_val: 0.3333333333 time: 0.5328s
Epoch: 0088 nll_train: 10918.4090576172 kl_train: -0.1637472914 mse_train: 0.0574653115 acc_train: 0.4791666667 nll_val: 10477.2932942708 kl_val: -0.1590129932 mse_val: 0.0551436494 acc_val: 0.3333333333 time: 0.5328s
Epoch: 0089 nll_train: 10914.3861083984 kl_train: -0.1712128970 mse_train: 0.0574441371 acc_train: 0.4791666667 nll_val: 10434.0953776042 kl_val: -0.1653958981 mse_val: 0.0549162899 acc_val: 0.3333333333 time: 0.5272s
Epoch: 0090 nll_train: 10924.2266845703 kl_train: -0.1644632987 mse_train: 0.0574959309 acc_train: 0.5000000000 nll_val: 10430.1302083333 kl_val: -0.1319073886 mse_val: 0.0548954246 acc_val: 0.3333333333 time: 0.5264s
Epoch: 0091 nll_train: 10925.4895833333 kl_train: -0.1736234346 mse_train: 0.0575025780 acc_train: 0.5000000000 nll_val: 10423.7054036458 kl_val: -0.1554287771 mse_val: 0.0548616039 acc_val: 0.3333333333 time: 0.5419s
Epoch: 0092 nll_train: 10915.8733317057 kl_train: -0.1634679626 mse_train: 0.0574519654 acc_train: 0.5000000000 nll_val: 10464.8362630208 kl_val: -0.1512435277 mse_val: 0.0550780868 acc_val: 0.3333333333 time: 0.5294s
Epoch: 0093 nll_train: 10918.0448404948 kl_train: -0.1515134753 mse_train: 0.0574633953 acc_train: 0.5208333333 nll_val: 10464.4130859375 kl_val: -0.1354820927 mse_val: 0.0550758565 acc_val: 0.3333333333 time: 0.5332s
Epoch: 0094 nll_train: 10919.9103597005 kl_train: -0.1680781245 mse_train: 0.0574732126 acc_train: 0.5000000000 nll_val: 10513.3079427083 kl_val: -0.1625491033 mse_val: 0.0553331984 acc_val: 0.3333333333 time: 0.5288s
Epoch: 0095 nll_train: 10922.9894205729 kl_train: -0.1572079593 mse_train: 0.0574894183 acc_train: 0.4791666667 nll_val: 10454.0205078125 kl_val: -0.1509280230 mse_val: 0.0550211618 acc_val: 0.3333333333 time: 0.5291s
Epoch: 0096 nll_train: 10919.2101236979 kl_train: -0.1797000964 mse_train: 0.0574695262 acc_train: 0.5000000000 nll_val: 10458.4908854167 kl_val: -0.1426355789 mse_val: 0.0550446870 acc_val: 0.3333333333 time: 0.5374s
Epoch: 0097 nll_train: 10921.7536214193 kl_train: -0.1729659907 mse_train: 0.0574829141 acc_train: 0.5625000000 nll_val: 10477.3590494792 kl_val: -0.1724381447 mse_val: 0.0551439946 acc_val: 0.3333333333 time: 0.5290s
Epoch: 0098 nll_train: 10917.8891601562 kl_train: -0.1619821303 mse_train: 0.0574625749 acc_train: 0.5208333333 nll_val: 10421.0572916667 kl_val: -0.1377484749 mse_val: 0.0548476689 acc_val: 0.3333333333 time: 0.5366s
Epoch: 0099 nll_train: 10911.8544921875 kl_train: -0.1552598815 mse_train: 0.0574308143 acc_train: 0.5208333333 nll_val: 10416.9625651042 kl_val: -0.1448704377 mse_val: 0.0548261181 acc_val: 0.3333333333 time: 0.5288s
Epoch: 0100 nll_train: 10923.9154459635 kl_train: -0.1680687113 mse_train: 0.0574942920 acc_train: 0.5000000000 nll_val: 10427.5973307292 kl_val: -0.1428381354 mse_val: 0.0548820893 acc_val: 0.3333333333 time: 0.5382s
Epoch: 0101 nll_train: 10917.0435791016 kl_train: -0.1564717501 mse_train: 0.0574581258 acc_train: 0.5000000000 nll_val: 10415.3645833333 kl_val: -0.1328586588 mse_val: 0.0548177113 acc_val: 0.3333333333 time: 0.5355s
Epoch: 0102 nll_train: 10908.6163736979 kl_train: -0.1588708848 mse_train: 0.0574137707 acc_train: 0.5208333333 nll_val: 10411.9964192708 kl_val: -0.1634076908 mse_val: 0.0547999802 acc_val: 0.3333333333 time: 0.5347s
Epoch: 0103 nll_train: 10906.5352783203 kl_train: -0.1615886983 mse_train: 0.0574028172 acc_train: 0.4791666667 nll_val: 10473.2112630208 kl_val: -0.1748362432 mse_val: 0.0551221631 acc_val: 0.3333333333 time: 0.5286s
Epoch: 0104 nll_train: 10913.3203938802 kl_train: -0.1698533014 mse_train: 0.0574385299 acc_train: 0.4791666667 nll_val: 10429.7138671875 kl_val: -0.1337981448 mse_val: 0.0548932267 acc_val: 0.3333333333 time: 0.5357s
Epoch: 0105 nll_train: 10914.4746500651 kl_train: -0.1525524833 mse_train: 0.0574446033 acc_train: 0.4583333333 nll_val: 10439.9749348958 kl_val: -0.1726516932 mse_val: 0.0549472372 acc_val: 0.3333333333 time: 0.5312s
Epoch: 0106 nll_train: 10919.9325764974 kl_train: -0.1756714291 mse_train: 0.0574733295 acc_train: 0.4791666667 nll_val: 10440.3886718750 kl_val: -0.1674570218 mse_val: 0.0549494140 acc_val: 0.3333333333 time: 0.5342s
Epoch: 0107 nll_train: 10912.2924397786 kl_train: -0.1652759612 mse_train: 0.0574331187 acc_train: 0.5208333333 nll_val: 10440.5872395833 kl_val: -0.1714712034 mse_val: 0.0549504633 acc_val: 0.3333333333 time: 0.5332s
Epoch: 0108 nll_train: 10898.5423990885 kl_train: -0.1683873624 mse_train: 0.0573607500 acc_train: 0.5000000000 nll_val: 10395.3385416667 kl_val: -0.1604065175 mse_val: 0.0547123067 acc_val: 0.3333333333 time: 0.5278s
Epoch: 0109 nll_train: 10918.8343505859 kl_train: -0.1694932713 mse_train: 0.0574675490 acc_train: 0.4583333333 nll_val: 10495.8007812500 kl_val: -0.1842751801 mse_val: 0.0552410570 acc_val: 0.3333333333 time: 0.5378s
Epoch: 0110 nll_train: 10919.5428873698 kl_train: -0.1759451873 mse_train: 0.0574712791 acc_train: 0.4791666667 nll_val: 10432.9505208333 kl_val: -0.1520125717 mse_val: 0.0549102686 acc_val: 0.3333333333 time: 0.5284s
Epoch: 0111 nll_train: 10909.8674723307 kl_train: -0.1593227129 mse_train: 0.0574203554 acc_train: 0.4791666667 nll_val: 10375.6884765625 kl_val: -0.1548235069 mse_val: 0.0546088864 acc_val: 0.3333333333 time: 0.5366s
Epoch: 0112 nll_train: 10910.7476806641 kl_train: -0.1612692739 mse_train: 0.0574249888 acc_train: 0.4375000000 nll_val: 10406.3470052083 kl_val: -0.1533837567 mse_val: 0.0547702474 acc_val: 0.3333333333 time: 0.5301s
Epoch: 0113 nll_train: 10909.0156250000 kl_train: -0.1628731070 mse_train: 0.0574158723 acc_train: 0.5000000000 nll_val: 10450.7070312500 kl_val: -0.1639041776 mse_val: 0.0550037213 acc_val: 0.3333333333 time: 0.5344s
Epoch: 0114 nll_train: 10907.8581949870 kl_train: -0.1619856621 mse_train: 0.0574097807 acc_train: 0.4791666667 nll_val: 10372.8587239583 kl_val: -0.1462975567 mse_val: 0.0545939890 acc_val: 0.3333333333 time: 0.5307s
Epoch: 0115 nll_train: 10912.5444335938 kl_train: -0.1766067011 mse_train: 0.0574344449 acc_train: 0.4791666667 nll_val: 10416.2539062500 kl_val: -0.1887716775 mse_val: 0.0548223878 acc_val: 0.3333333333 time: 0.5341s
Epoch: 0116 nll_train: 10898.5245768229 kl_train: -0.1647225072 mse_train: 0.0573606564 acc_train: 0.4166666667 nll_val: 10377.6321614583 kl_val: -0.1640173619 mse_val: 0.0546191161 acc_val: 0.3333333333 time: 0.5566s
Epoch: 0117 nll_train: 10900.0926513672 kl_train: -0.1732752292 mse_train: 0.0573689075 acc_train: 0.4791666667 nll_val: 10433.0100911458 kl_val: -0.1798903421 mse_val: 0.0549105766 acc_val: 0.3333333333 time: 0.5307s
Epoch: 0118 nll_train: 10909.1373697917 kl_train: -0.1558834249 mse_train: 0.0574165120 acc_train: 0.4791666667 nll_val: 10384.7744140625 kl_val: -0.1555281232 mse_val: 0.0546567055 acc_val: 0.3333333333 time: 0.5486s
Epoch: 0119 nll_train: 10892.0410970052 kl_train: -0.1632210892 mse_train: 0.0573265327 acc_train: 0.4583333333 nll_val: 10359.5302734375 kl_val: -0.1733722091 mse_val: 0.0545238455 acc_val: 0.3333333333 time: 0.5290s
Epoch: 0120 nll_train: 10890.7343343099 kl_train: -0.1572287402 mse_train: 0.0573196560 acc_train: 0.4791666667 nll_val: 10402.3157552083 kl_val: -0.1631833464 mse_val: 0.0547490281 acc_val: 0.3333333333 time: 0.5355s
Epoch: 0121 nll_train: 10883.6389973958 kl_train: -0.1726031719 mse_train: 0.0572823103 acc_train: 0.4791666667 nll_val: 10429.4977213542 kl_val: -0.1648714393 mse_val: 0.0548920954 acc_val: 0.3333333333 time: 0.5284s
Epoch: 0122 nll_train: 10890.1791992188 kl_train: -0.1510957389 mse_train: 0.0573167335 acc_train: 0.5000000000 nll_val: 10397.6796875000 kl_val: -0.1641361887 mse_val: 0.0547246337 acc_val: 0.3333333333 time: 0.5312s
Epoch: 0123 nll_train: 10893.9180908203 kl_train: -0.1823647531 mse_train: 0.0573364102 acc_train: 0.4583333333 nll_val: 10439.7093098958 kl_val: -0.1764875079 mse_val: 0.0549458414 acc_val: 0.3333333333 time: 0.5318s
Epoch: 0124 nll_train: 10895.5775960286 kl_train: -0.1817109939 mse_train: 0.0573451455 acc_train: 0.4791666667 nll_val: 10401.3863932292 kl_val: -0.1822951039 mse_val: 0.0547441368 acc_val: 0.3333333333 time: 0.5294s
Epoch: 0125 nll_train: 10902.0564371745 kl_train: -0.1499340823 mse_train: 0.0573792442 acc_train: 0.4791666667 nll_val: 10424.5159505208 kl_val: -0.1597441336 mse_val: 0.0548658744 acc_val: 0.3333333333 time: 0.5382s
Epoch: 0126 nll_train: 10899.7357584635 kl_train: -0.1789006249 mse_train: 0.0573670312 acc_train: 0.4583333333 nll_val: 10448.6331380208 kl_val: -0.1921812594 mse_val: 0.0549928024 acc_val: 0.3333333333 time: 0.5510s
Epoch: 0127 nll_train: 10893.7982584635 kl_train: -0.1738041136 mse_train: 0.0573357805 acc_train: 0.4791666667 nll_val: 10415.6350911458 kl_val: -0.1770391141 mse_val: 0.0548191306 acc_val: 0.3333333333 time: 0.5334s
Epoch: 0128 nll_train: 10895.0783691406 kl_train: -0.1746527633 mse_train: 0.0573425181 acc_train: 0.4583333333 nll_val: 10353.1292317708 kl_val: -0.1508511702 mse_val: 0.0544901515 acc_val: 0.3333333333 time: 0.5304s
Epoch: 0129 nll_train: 10905.1789143880 kl_train: -0.1541378774 mse_train: 0.0573956788 acc_train: 0.5000000000 nll_val: 10408.8359375000 kl_val: -0.1425645923 mse_val: 0.0547833492 acc_val: 0.3333333333 time: 0.5356s
Epoch: 0130 nll_train: 10887.9451497396 kl_train: -0.1821349552 mse_train: 0.0573049754 acc_train: 0.4375000000 nll_val: 10358.3421223958 kl_val: -0.1813275789 mse_val: 0.0545175920 acc_val: 0.3333333333 time: 0.5270s
Epoch: 0131 nll_train: 10884.7196858724 kl_train: -0.1655313199 mse_train: 0.0572879988 acc_train: 0.4583333333 nll_val: 10405.8873697917 kl_val: -0.1825124820 mse_val: 0.0547678272 acc_val: 0.3333333333 time: 0.5377s
Epoch: 0132 nll_train: 10878.0184733073 kl_train: -0.1726874486 mse_train: 0.0572527293 acc_train: 0.5208333333 nll_val: 10441.1298828125 kl_val: -0.1560991804 mse_val: 0.0549533144 acc_val: 0.3333333333 time: 0.5297s
Epoch: 0133 nll_train: 10888.7959798177 kl_train: -0.1740580853 mse_train: 0.0573094528 acc_train: 0.5416666667 nll_val: 10447.0947265625 kl_val: -0.1801508963 mse_val: 0.0549847099 acc_val: 0.3333333333 time: 0.5338s
Epoch: 0134 nll_train: 10890.8589274089 kl_train: -0.1822352267 mse_train: 0.0573203123 acc_train: 0.4791666667 nll_val: 10401.9921875000 kl_val: -0.1752463058 mse_val: 0.0547473257 acc_val: 0.3333333333 time: 0.5317s
Epoch: 0135 nll_train: 10889.7964680990 kl_train: -0.1846256688 mse_train: 0.0573147183 acc_train: 0.5000000000 nll_val: 10399.6920572917 kl_val: -0.1647400061 mse_val: 0.0547352172 acc_val: 0.3333333333 time: 0.5327s
Epoch: 0136 nll_train: 10874.8611246745 kl_train: -0.1615787012 mse_train: 0.0572361127 acc_train: 0.5000000000 nll_val: 10399.5550130208 kl_val: -0.1657114228 mse_val: 0.0547345032 acc_val: 0.3333333333 time: 0.5406s
Epoch: 0137 nll_train: 10875.2915039062 kl_train: -0.1635867336 mse_train: 0.0572383758 acc_train: 0.4791666667 nll_val: 10391.4687500000 kl_val: -0.1657737394 mse_val: 0.0546919430 acc_val: 0.3333333333 time: 0.5424s
Epoch: 0138 nll_train: 10884.8444417318 kl_train: -0.1610482112 mse_train: 0.0572886565 acc_train: 0.4791666667 nll_val: 10347.3919270833 kl_val: -0.1551825975 mse_val: 0.0544599580 acc_val: 0.3333333333 time: 0.5111s
Epoch: 0139 nll_train: 10903.1523437500 kl_train: -0.1740040611 mse_train: 0.0573850127 acc_train: 0.5208333333 nll_val: 10458.7496744792 kl_val: -0.1679020425 mse_val: 0.0550460505 acc_val: 0.3333333333 time: 0.4940s
Epoch: 0140 nll_train: 10904.2496744792 kl_train: -0.1543560727 mse_train: 0.0573907875 acc_train: 0.4583333333 nll_val: 10442.4615885417 kl_val: -0.1581336583 mse_val: 0.0549603229 acc_val: 0.3333333333 time: 0.5086s
Epoch: 0141 nll_train: 10904.1683756510 kl_train: -0.1725055318 mse_train: 0.0573903603 acc_train: 0.4583333333 nll_val: 10410.7897135417 kl_val: -0.1710359007 mse_val: 0.0547936310 acc_val: 0.3333333333 time: 0.5007s
Epoch: 0142 nll_train: 10900.2872721354 kl_train: -0.1686613265 mse_train: 0.0573699325 acc_train: 0.5000000000 nll_val: 10406.8092447917 kl_val: -0.1496269181 mse_val: 0.0547726800 acc_val: 0.3333333333 time: 0.5131s
Epoch: 0143 nll_train: 10880.5160725911 kl_train: -0.1701968998 mse_train: 0.0572658748 acc_train: 0.4166666667 nll_val: 10435.8564453125 kl_val: -0.1808976059 mse_val: 0.0549255585 acc_val: 0.3333333333 time: 0.4940s
Epoch: 0144 nll_train: 10873.8266601562 kl_train: -0.1759980681 mse_train: 0.0572306664 acc_train: 0.4583333333 nll_val: 10422.7913411458 kl_val: -0.1647157942 mse_val: 0.0548567958 acc_val: 0.3333333333 time: 0.4946s
Epoch: 0145 nll_train: 10876.6876220703 kl_train: -0.1688476900 mse_train: 0.0572457242 acc_train: 0.4583333333 nll_val: 10433.6998697917 kl_val: -0.1689246198 mse_val: 0.0549142087 acc_val: 0.3333333333 time: 0.4935s
Epoch: 0146 nll_train: 10868.4663899740 kl_train: -0.1798473215 mse_train: 0.0572024567 acc_train: 0.4791666667 nll_val: 10457.2255859375 kl_val: -0.1818025510 mse_val: 0.0550380237 acc_val: 0.3333333333 time: 0.4964s
Epoch: 0147 nll_train: 10887.5426432292 kl_train: -0.1748036407 mse_train: 0.0573028567 acc_train: 0.5208333333 nll_val: 10372.9869791667 kl_val: -0.1453331932 mse_val: 0.0545946707 acc_val: 0.3333333333 time: 0.4933s
Epoch: 0148 nll_train: 10883.8406982422 kl_train: -0.1624094785 mse_train: 0.0572833722 acc_train: 0.3958333333 nll_val: 10492.7477213542 kl_val: -0.1876074125 mse_val: 0.0552249923 acc_val: 0.3333333333 time: 0.4992s
Epoch: 0149 nll_train: 10877.3287760417 kl_train: -0.1702576308 mse_train: 0.0572490990 acc_train: 0.4583333333 nll_val: 10406.9550781250 kl_val: -0.1627564008 mse_val: 0.0547734449 acc_val: 0.3333333333 time: 0.4970s
Epoch: 0150 nll_train: 10876.1070963542 kl_train: -0.1684478031 mse_train: 0.0572426698 acc_train: 0.3958333333 nll_val: 10446.2555338542 kl_val: -0.1760108123 mse_val: 0.0549802917 acc_val: 0.3333333333 time: 0.4954s
Epoch: 0151 nll_train: 10890.0468343099 kl_train: -0.1607213045 mse_train: 0.0573160377 acc_train: 0.4791666667 nll_val: 10398.3372395833 kl_val: -0.1570217609 mse_val: 0.0547280895 acc_val: 0.3333333333 time: 0.4953s
Epoch: 0152 nll_train: 10880.5063883464 kl_train: -0.1582022545 mse_train: 0.0572658225 acc_train: 0.3750000000 nll_val: 10409.4501953125 kl_val: -0.1693976372 mse_val: 0.0547865803 acc_val: 0.3333333333 time: 0.4975s
Epoch: 0153 nll_train: 10879.5166829427 kl_train: -0.1665659988 mse_train: 0.0572606139 acc_train: 0.5000000000 nll_val: 10390.0686848958 kl_val: -0.1730970542 mse_val: 0.0546845694 acc_val: 0.3333333333 time: 0.4948s
Epoch: 0154 nll_train: 10872.8899332682 kl_train: -0.1731521608 mse_train: 0.0572257365 acc_train: 0.4166666667 nll_val: 10487.5117187500 kl_val: -0.1811720182 mse_val: 0.0551974302 acc_val: 0.3333333333 time: 0.4971s
Epoch: 0155 nll_train: 10866.5177408854 kl_train: -0.1725510424 mse_train: 0.0571921999 acc_train: 0.4583333333 nll_val: 10439.5074869792 kl_val: -0.1823906203 mse_val: 0.0549447760 acc_val: 0.3333333333 time: 0.4928s
Epoch: 0156 nll_train: 10859.3605957031 kl_train: -0.1609076958 mse_train: 0.0571545286 acc_train: 0.5000000000 nll_val: 10399.3209635417 kl_val: -0.1756612112 mse_val: 0.0547332677 acc_val: 0.3333333333 time: 0.4975s
Epoch: 0157 nll_train: 10872.6859944661 kl_train: -0.1669807375 mse_train: 0.0572246630 acc_train: 0.5000000000 nll_val: 10362.0253906250 kl_val: -0.1277629087 mse_val: 0.0545369734 acc_val: 0.3333333333 time: 0.4944s
Epoch: 0158 nll_train: 10891.9037679036 kl_train: -0.1727295496 mse_train: 0.0573258088 acc_train: 0.4583333333 nll_val: 10430.6090494792 kl_val: -0.1861573011 mse_val: 0.0548979416 acc_val: 0.3333333333 time: 0.4957s
Epoch: 0159 nll_train: 10902.9823811849 kl_train: -0.1600759778 mse_train: 0.0573841163 acc_train: 0.4375000000 nll_val: 10428.7871093750 kl_val: -0.1745241284 mse_val: 0.0548883478 acc_val: 0.3333333333 time: 0.4953s
Epoch: 0160 nll_train: 10900.4826660156 kl_train: -0.1606148587 mse_train: 0.0573709612 acc_train: 0.5000000000 nll_val: 10402.5371093750 kl_val: -0.1860604137 mse_val: 0.0547501966 acc_val: 0.3333333333 time: 0.4963s
Epoch: 0161 nll_train: 10878.2451171875 kl_train: -0.1614517228 mse_train: 0.0572539217 acc_train: 0.5208333333 nll_val: 10376.9342447917 kl_val: -0.1597030163 mse_val: 0.0546154405 acc_val: 0.3333333333 time: 0.4952s
Epoch: 0162 nll_train: 10868.3115234375 kl_train: -0.1559788085 mse_train: 0.0572016405 acc_train: 0.4791666667 nll_val: 10394.2259114583 kl_val: -0.1606439501 mse_val: 0.0547064530 acc_val: 0.3333333333 time: 0.4977s
Epoch: 0163 nll_train: 10871.2342936198 kl_train: -0.1547942441 mse_train: 0.0572170233 acc_train: 0.4166666667 nll_val: 10367.8727213542 kl_val: -0.1572357366 mse_val: 0.0545677493 acc_val: 0.3333333333 time: 0.5062s
Epoch: 0164 nll_train: 10884.4663899740 kl_train: -0.1545700763 mse_train: 0.0572866644 acc_train: 0.4583333333 nll_val: 10368.9511718750 kl_val: -0.1637294739 mse_val: 0.0545734242 acc_val: 0.3333333333 time: 0.4966s
Epoch: 0165 nll_train: 10887.5071207682 kl_train: -0.1682095292 mse_train: 0.0573026701 acc_train: 0.4375000000 nll_val: 10378.3317057292 kl_val: -0.1644408430 mse_val: 0.0546227942 acc_val: 0.3333333333 time: 0.4943s
Epoch: 0166 nll_train: 10891.0033365885 kl_train: -0.1553729977 mse_train: 0.0573210702 acc_train: 0.5208333333 nll_val: 10390.6523437500 kl_val: -0.1726993521 mse_val: 0.0546876428 acc_val: 0.3333333333 time: 0.4964s
Epoch: 0167 nll_train: 10895.3222656250 kl_train: -0.1644775818 mse_train: 0.0573438015 acc_train: 0.4583333333 nll_val: 10382.5921223958 kl_val: -0.1676212301 mse_val: 0.0546452180 acc_val: 0.3333333333 time: 0.4945s
Epoch: 0168 nll_train: 10878.2546793620 kl_train: -0.1641079153 mse_train: 0.0572539723 acc_train: 0.4583333333 nll_val: 10365.1018880208 kl_val: -0.1658679793 mse_val: 0.0545531623 acc_val: 0.3333333333 time: 0.4955s
Epoch: 0169 nll_train: 10880.5113932292 kl_train: -0.1669078724 mse_train: 0.0572658502 acc_train: 0.4791666667 nll_val: 10467.8818359375 kl_val: -0.1740011151 mse_val: 0.0550941154 acc_val: 0.3333333333 time: 0.4955s
Epoch: 0170 nll_train: 10902.0800374349 kl_train: -0.1404064925 mse_train: 0.0573793699 acc_train: 0.4791666667 nll_val: 10413.6943359375 kl_val: -0.1268438647 mse_val: 0.0548089196 acc_val: 0.3333333333 time: 0.4982s
Epoch: 0171 nll_train: 10890.9352620443 kl_train: -0.1438785366 mse_train: 0.0573207118 acc_train: 0.5208333333 nll_val: 10430.8694661458 kl_val: -0.1671030199 mse_val: 0.0548993076 acc_val: 0.3333333333 time: 0.4952s
Epoch: 0172 nll_train: 10904.7359619141 kl_train: -0.1907047896 mse_train: 0.0573933477 acc_train: 0.4583333333 nll_val: 10408.2063802083 kl_val: -0.1709367236 mse_val: 0.0547800300 acc_val: 0.5000000000 time: 0.4984s
Epoch: 0173 nll_train: 10906.7656250000 kl_train: -0.1784980791 mse_train: 0.0574040300 acc_train: 0.4375000000 nll_val: 10456.0755208333 kl_val: -0.1872089356 mse_val: 0.0550319788 acc_val: 0.3333333333 time: 0.4966s
Epoch: 0174 nll_train: 10893.8258870443 kl_train: -0.1542125537 mse_train: 0.0573359259 acc_train: 0.5000000000 nll_val: 10399.8668619792 kl_val: -0.1671974162 mse_val: 0.0547361386 acc_val: 0.3333333333 time: 0.4959s
Epoch: 0175 nll_train: 10890.4212646484 kl_train: -0.1639983139 mse_train: 0.0573180066 acc_train: 0.5000000000 nll_val: 10371.6907552083 kl_val: -0.1559632793 mse_val: 0.0545878410 acc_val: 0.3333333333 time: 0.4935s
Epoch: 0176 nll_train: 10881.1950683594 kl_train: -0.1740494442 mse_train: 0.0572694478 acc_train: 0.4375000000 nll_val: 10441.7148437500 kl_val: -0.1873120939 mse_val: 0.0549563964 acc_val: 0.3333333333 time: 0.4961s
Epoch: 0177 nll_train: 10880.0399169922 kl_train: -0.1774276868 mse_train: 0.0572633678 acc_train: 0.4375000000 nll_val: 10370.5628255208 kl_val: -0.1745848656 mse_val: 0.0545819091 acc_val: 0.3333333333 time: 0.4957s
Epoch: 0178 nll_train: 10881.9431152344 kl_train: -0.1602134000 mse_train: 0.0572733845 acc_train: 0.4583333333 nll_val: 10425.2597656250 kl_val: -0.1739884516 mse_val: 0.0548697896 acc_val: 0.3333333333 time: 0.4984s
Epoch: 0179 nll_train: 10886.2749837240 kl_train: -0.1745379691 mse_train: 0.0572961847 acc_train: 0.4791666667 nll_val: 10317.4277343750 kl_val: -0.1803480486 mse_val: 0.0543022491 acc_val: 0.3333333333 time: 0.4963s
Epoch: 0180 nll_train: 10884.2180989583 kl_train: -0.1652072544 mse_train: 0.0572853579 acc_train: 0.4166666667 nll_val: 10473.6660156250 kl_val: -0.1958292723 mse_val: 0.0551245548 acc_val: 0.3333333333 time: 0.4943s
Epoch: 0181 nll_train: 10885.2986246745 kl_train: -0.1809497544 mse_train: 0.0572910461 acc_train: 0.4583333333 nll_val: 10432.3714192708 kl_val: -0.1908798739 mse_val: 0.0549072189 acc_val: 0.3333333333 time: 0.4952s
Epoch: 0182 nll_train: 10885.6498616536 kl_train: -0.1683217923 mse_train: 0.0572928946 acc_train: 0.4375000000 nll_val: 10492.1490885417 kl_val: -0.1736702671 mse_val: 0.0552218370 acc_val: 0.3333333333 time: 0.4948s
Epoch: 0183 nll_train: 10886.8665364583 kl_train: -0.1655619889 mse_train: 0.0572992976 acc_train: 0.4375000000 nll_val: 10348.6233723958 kl_val: -0.1723677615 mse_val: 0.0544664338 acc_val: 0.3333333333 time: 0.5033s
Epoch: 0184 nll_train: 10865.3863932292 kl_train: -0.1712466388 mse_train: 0.0571862427 acc_train: 0.4583333333 nll_val: 10400.2174479167 kl_val: -0.1803790629 mse_val: 0.0547379876 acc_val: 0.3333333333 time: 0.4961s
Epoch: 0185 nll_train: 10868.3114420573 kl_train: -0.1611379456 mse_train: 0.0572016394 acc_train: 0.5208333333 nll_val: 10398.9554036458 kl_val: -0.1454927350 mse_val: 0.0547313429 acc_val: 0.3333333333 time: 0.4969s
Epoch: 0186 nll_train: 10881.0006510417 kl_train: -0.1647838978 mse_train: 0.0572684252 acc_train: 0.3750000000 nll_val: 10404.4593098958 kl_val: -0.1609316096 mse_val: 0.0547603133 acc_val: 0.3333333333 time: 0.4934s
Epoch: 0187 nll_train: 10889.2333984375 kl_train: -0.1706351756 mse_train: 0.0573117562 acc_train: 0.4166666667 nll_val: 10425.3720703125 kl_val: -0.1929053143 mse_val: 0.0548703795 acc_val: 0.3333333333 time: 0.4970s
Epoch: 0188 nll_train: 10891.6273193359 kl_train: -0.1660205216 mse_train: 0.0573243536 acc_train: 0.5000000000 nll_val: 10428.3678385417 kl_val: -0.1792598739 mse_val: 0.0548861461 acc_val: 0.3333333333 time: 0.4953s
Epoch: 0189 nll_train: 10866.6834716797 kl_train: -0.1626202154 mse_train: 0.0571930727 acc_train: 0.4791666667 nll_val: 10452.9830729167 kl_val: -0.1768475100 mse_val: 0.0550157006 acc_val: 0.3333333333 time: 0.4957s
Epoch: 0190 nll_train: 10862.4937744141 kl_train: -0.1467466913 mse_train: 0.0571710199 acc_train: 0.5000000000 nll_val: 10530.1168619792 kl_val: -0.1479646439 mse_val: 0.0554216690 acc_val: 0.3333333333 time: 0.4968s
Epoch: 0191 nll_train: 10850.1002604167 kl_train: -0.1443665937 mse_train: 0.0571057913 acc_train: 0.4791666667 nll_val: 10402.2242838542 kl_val: -0.1510002986 mse_val: 0.0547485463 acc_val: 0.3333333333 time: 0.4960s
Epoch: 0192 nll_train: 10855.9281412760 kl_train: -0.1571705711 mse_train: 0.0571364636 acc_train: 0.4791666667 nll_val: 10421.7542317708 kl_val: -0.1530520742 mse_val: 0.0548513345 acc_val: 0.3333333333 time: 0.5371s
Epoch: 0193 nll_train: 10858.9317220052 kl_train: -0.1760945547 mse_train: 0.0571522736 acc_train: 0.4583333333 nll_val: 10483.9335937500 kl_val: -0.1769145479 mse_val: 0.0551785976 acc_val: 0.3333333333 time: 0.5184s
Epoch: 0194 nll_train: 10870.9267985026 kl_train: -0.1565557057 mse_train: 0.0572154058 acc_train: 0.4583333333 nll_val: 10383.9833984375 kl_val: -0.1587527444 mse_val: 0.0546525419 acc_val: 0.3333333333 time: 0.4938s
Epoch: 0195 nll_train: 10887.7208658854 kl_train: -0.1357815123 mse_train: 0.0573037942 acc_train: 0.5000000000 nll_val: 10489.0237630208 kl_val: -0.1403855383 mse_val: 0.0552053923 acc_val: 0.3333333333 time: 0.4952s
Epoch: 0196 nll_train: 10893.5738932292 kl_train: -0.1493565099 mse_train: 0.0573345994 acc_train: 0.5208333333 nll_val: 10556.6464843750 kl_val: -0.1738306557 mse_val: 0.0555612966 acc_val: 0.3333333333 time: 0.4976s
Epoch: 0197 nll_train: 10881.3273518880 kl_train: -0.1844233169 mse_train: 0.0572701449 acc_train: 0.4375000000 nll_val: 10350.0126953125 kl_val: -0.1867258102 mse_val: 0.0544737491 acc_val: 0.3333333333 time: 0.4946s
Epoch: 0198 nll_train: 10861.4444580078 kl_train: -0.1682314463 mse_train: 0.0571654961 acc_train: 0.5000000000 nll_val: 10413.3365885417 kl_val: -0.1596990526 mse_val: 0.0548070359 acc_val: 0.3333333333 time: 0.4955s
Epoch: 0199 nll_train: 10854.1387939453 kl_train: -0.1570971341 mse_train: 0.0571270458 acc_train: 0.4375000000 nll_val: 10471.0315755208 kl_val: -0.1631083488 mse_val: 0.0551106905 acc_val: 0.3333333333 time: 0.4957s
Optimization finished
Best epoch 9
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 10871.4029947917 kl_test: -0.0820858541 mse_test: 0.0572179121 acc_test: 0.5000000000
MSE: [ 0.056446690112 , 0.055221367627 , 0.056328535080 , 0.056785386056 , 0.056769374758 , 0.057300645858 , 0.056510318071 , 0.056259710342 , 0.056851159781 , 0.056501854211 , 0.055979911238 , 0.057325661182 , 0.056033182889 , 0.056384596974 , 0.056536495686 , 0.056404415518 , 0.054813742638 , 0.054232239723 , 0.054612785578 ]
Accuracy for experiment id 1 is 0.25
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 16178.4335530599 kl_train: -0.3418984842 mse_train: 0.0851496514 acc_train: 0.4791666667 nll_val: 18503.9804687500 kl_val: -0.4585913420 mse_val: 0.0973893628 acc_val: 0.5000000000 time: 0.5059s
Best model so far, saving...
Epoch: 0001 nll_train: 12806.2631022135 kl_train: -0.4323493429 mse_train: 0.0674013863 acc_train: 0.5000000000 nll_val: 14426.8746744792 kl_val: -0.4449154536 mse_val: 0.0759309158 acc_val: 0.5000000000 time: 0.5015s
Best model so far, saving...
Epoch: 0002 nll_train: 12284.9912109375 kl_train: -0.3906471444 mse_train: 0.0646578477 acc_train: 0.5208333333 nll_val: 13747.8330078125 kl_val: -0.3505473634 mse_val: 0.0723570113 acc_val: 0.3333333333 time: 0.5045s
Best model so far, saving...
Epoch: 0003 nll_train: 12062.5425618490 kl_train: -0.3298715800 mse_train: 0.0634870666 acc_train: 0.5416666667 nll_val: 13416.5120442708 kl_val: -0.3092627823 mse_val: 0.0706132203 acc_val: 0.3333333333 time: 0.5014s
Best model so far, saving...
Epoch: 0004 nll_train: 12035.2357584635 kl_train: -0.2754784053 mse_train: 0.0633433463 acc_train: 0.4583333333 nll_val: 13341.9573567708 kl_val: -0.2414685885 mse_val: 0.0702208281 acc_val: 0.5000000000 time: 0.5022s
Best model so far, saving...
Epoch: 0005 nll_train: 12021.3150227865 kl_train: -0.2268264567 mse_train: 0.0632700791 acc_train: 0.4791666667 nll_val: 13274.9674479167 kl_val: -0.2004796316 mse_val: 0.0698682492 acc_val: 0.5000000000 time: 0.5008s
Best model so far, saving...
Epoch: 0006 nll_train: 12048.2721761068 kl_train: -0.2032959846 mse_train: 0.0634119594 acc_train: 0.5000000000 nll_val: 13286.0449218750 kl_val: -0.1692972084 mse_val: 0.0699265500 acc_val: 0.5000000000 time: 0.5038s
Epoch: 0007 nll_train: 12055.9388834635 kl_train: -0.1765448209 mse_train: 0.0634523105 acc_train: 0.5833333333 nll_val: 13435.5709635417 kl_val: -0.1554679622 mse_val: 0.0707135325 acc_val: 0.5000000000 time: 0.4998s
Epoch: 0008 nll_train: 11978.5572509766 kl_train: -0.1671486062 mse_train: 0.0630450388 acc_train: 0.4583333333 nll_val: 13027.9270833333 kl_val: -0.1499474446 mse_val: 0.0685680384 acc_val: 0.6666666667 time: 0.5550s
Best model so far, saving...
Epoch: 0009 nll_train: 11901.9233398438 kl_train: -0.1475236568 mse_train: 0.0626417027 acc_train: 0.5208333333 nll_val: 13023.9342447917 kl_val: -0.1387941092 mse_val: 0.0685470228 acc_val: 0.6666666667 time: 0.5568s
Best model so far, saving...
Epoch: 0010 nll_train: 11904.2561035156 kl_train: -0.1372085791 mse_train: 0.0626539805 acc_train: 0.5000000000 nll_val: 12831.3889973958 kl_val: -0.1142041683 mse_val: 0.0675336247 acc_val: 0.6666666667 time: 0.5556s
Best model so far, saving...
Epoch: 0011 nll_train: 11892.6597493490 kl_train: -0.1261210069 mse_train: 0.0625929444 acc_train: 0.5208333333 nll_val: 12840.0074869792 kl_val: -0.1082778995 mse_val: 0.0675789913 acc_val: 0.5000000000 time: 0.5408s
Epoch: 0012 nll_train: 11890.6568603516 kl_train: -0.1164061089 mse_train: 0.0625824048 acc_train: 0.5000000000 nll_val: 12726.7552083333 kl_val: -0.1072156057 mse_val: 0.0669829200 acc_val: 0.6666666667 time: 0.4982s
Best model so far, saving...
Epoch: 0013 nll_train: 11877.4449055990 kl_train: -0.1187326517 mse_train: 0.0625128701 acc_train: 0.4791666667 nll_val: 12706.8190104167 kl_val: -0.1104547009 mse_val: 0.0668779959 acc_val: 0.5000000000 time: 0.5054s
Best model so far, saving...
Epoch: 0014 nll_train: 11882.9940185547 kl_train: -0.1192370684 mse_train: 0.0625420731 acc_train: 0.5000000000 nll_val: 12690.5364583333 kl_val: -0.1114865616 mse_val: 0.0667922969 acc_val: 0.6666666667 time: 0.5060s
Best model so far, saving...
Epoch: 0015 nll_train: 11880.2034912109 kl_train: -0.1143722022 mse_train: 0.0625273882 acc_train: 0.5000000000 nll_val: 12708.9404296875 kl_val: -0.0963382671 mse_val: 0.0668891594 acc_val: 0.5000000000 time: 0.4993s
Epoch: 0016 nll_train: 11881.1491292318 kl_train: -0.1120516794 mse_train: 0.0625323636 acc_train: 0.5208333333 nll_val: 12686.1852213542 kl_val: -0.0876428013 mse_val: 0.0667693913 acc_val: 0.5000000000 time: 0.4987s
Best model so far, saving...
Epoch: 0017 nll_train: 11869.9582926432 kl_train: -0.1042015239 mse_train: 0.0624734643 acc_train: 0.5416666667 nll_val: 12512.1949869792 kl_val: -0.0957165311 mse_val: 0.0658536541 acc_val: 0.6666666667 time: 0.5008s
Best model so far, saving...
Epoch: 0018 nll_train: 11859.6009521484 kl_train: -0.1071777555 mse_train: 0.0624189521 acc_train: 0.5625000000 nll_val: 12518.4931640625 kl_val: -0.0960648259 mse_val: 0.0658868055 acc_val: 0.5000000000 time: 0.5051s
Epoch: 0019 nll_train: 11859.9127197266 kl_train: -0.1134362739 mse_train: 0.0624205922 acc_train: 0.5000000000 nll_val: 12525.6757812500 kl_val: -0.1018242116 mse_val: 0.0659246085 acc_val: 0.5000000000 time: 0.5305s
Epoch: 0020 nll_train: 11855.4729817708 kl_train: -0.1124573778 mse_train: 0.0623972261 acc_train: 0.5000000000 nll_val: 12581.5957031250 kl_val: -0.0906706018 mse_val: 0.0662189250 acc_val: 0.5000000000 time: 0.5381s
Epoch: 0021 nll_train: 11849.6207682292 kl_train: -0.1094909996 mse_train: 0.0623664241 acc_train: 0.5833333333 nll_val: 12471.6087239583 kl_val: -0.0843212754 mse_val: 0.0656400460 acc_val: 0.6666666667 time: 0.5007s
Best model so far, saving...
Epoch: 0022 nll_train: 11851.0663248698 kl_train: -0.0994165552 mse_train: 0.0623740333 acc_train: 0.5833333333 nll_val: 12423.3525390625 kl_val: -0.0892704452 mse_val: 0.0653860644 acc_val: 0.6666666667 time: 0.5013s
Best model so far, saving...
Epoch: 0023 nll_train: 11846.7838948568 kl_train: -0.0937512765 mse_train: 0.0623514939 acc_train: 0.5833333333 nll_val: 12406.2496744792 kl_val: -0.0765282313 mse_val: 0.0652960502 acc_val: 0.5000000000 time: 0.4997s
Best model so far, saving...
Epoch: 0024 nll_train: 11843.6730550130 kl_train: -0.0932129932 mse_train: 0.0623351224 acc_train: 0.5625000000 nll_val: 12418.5693359375 kl_val: -0.0724496593 mse_val: 0.0653608913 acc_val: 0.6666666667 time: 0.5018s
Epoch: 0025 nll_train: 11840.5580647786 kl_train: -0.0972497302 mse_train: 0.0623187291 acc_train: 0.5833333333 nll_val: 12407.1637369792 kl_val: -0.0849849433 mse_val: 0.0653008583 acc_val: 0.5000000000 time: 0.5001s
Epoch: 0026 nll_train: 11836.2006022135 kl_train: -0.1000639809 mse_train: 0.0622957945 acc_train: 0.5625000000 nll_val: 12377.8157552083 kl_val: -0.0898301055 mse_val: 0.0651464015 acc_val: 0.5000000000 time: 0.4978s
Best model so far, saving...
Epoch: 0027 nll_train: 11836.3057454427 kl_train: -0.1006265779 mse_train: 0.0622963455 acc_train: 0.5416666667 nll_val: 12365.3509114583 kl_val: -0.0826254264 mse_val: 0.0650807942 acc_val: 0.6666666667 time: 0.4999s
Best model so far, saving...
Epoch: 0028 nll_train: 11835.5864257812 kl_train: -0.0990799960 mse_train: 0.0622925602 acc_train: 0.5208333333 nll_val: 12350.5758463542 kl_val: -0.0823162012 mse_val: 0.0650030275 acc_val: 0.5000000000 time: 0.5012s
Best model so far, saving...
Epoch: 0029 nll_train: 11831.5242513021 kl_train: -0.0973405611 mse_train: 0.0622711804 acc_train: 0.5208333333 nll_val: 12378.0677083333 kl_val: -0.0819084284 mse_val: 0.0651477252 acc_val: 0.6666666667 time: 0.5008s
Epoch: 0030 nll_train: 11839.0639648438 kl_train: -0.0986775439 mse_train: 0.0623108622 acc_train: 0.5416666667 nll_val: 12335.8723958333 kl_val: -0.0770686467 mse_val: 0.0649256396 acc_val: 0.5000000000 time: 0.5053s
Best model so far, saving...
Epoch: 0031 nll_train: 11829.0941162109 kl_train: -0.1062887187 mse_train: 0.0622583909 acc_train: 0.5208333333 nll_val: 12334.3942057292 kl_val: -0.0863927330 mse_val: 0.0649178661 acc_val: 0.6666666667 time: 0.5290s
Best model so far, saving...
Epoch: 0032 nll_train: 11818.3931884766 kl_train: -0.1070109140 mse_train: 0.0622020688 acc_train: 0.5000000000 nll_val: 12344.8701171875 kl_val: -0.0697840862 mse_val: 0.0649730017 acc_val: 0.6666666667 time: 0.5023s
Epoch: 0033 nll_train: 11821.0317382812 kl_train: -0.1012555656 mse_train: 0.0622159575 acc_train: 0.5833333333 nll_val: 12296.0270182292 kl_val: -0.0861361300 mse_val: 0.0647159306 acc_val: 0.6666666667 time: 0.4992s
Best model so far, saving...
Epoch: 0034 nll_train: 11822.1088867188 kl_train: -0.1024054612 mse_train: 0.0622216261 acc_train: 0.5625000000 nll_val: 12350.1744791667 kl_val: -0.0837842027 mse_val: 0.0650009165 acc_val: 0.6666666667 time: 0.4994s
Epoch: 0035 nll_train: 11820.6850992839 kl_train: -0.1036335266 mse_train: 0.0622141332 acc_train: 0.5208333333 nll_val: 12312.9765625000 kl_val: -0.0744925775 mse_val: 0.0648051389 acc_val: 0.6666666667 time: 0.5000s
Epoch: 0036 nll_train: 11815.2585449219 kl_train: -0.0994431629 mse_train: 0.0621855706 acc_train: 0.5625000000 nll_val: 12313.2014973958 kl_val: -0.0752705398 mse_val: 0.0648063247 acc_val: 0.6666666667 time: 0.4988s
Epoch: 0037 nll_train: 11812.0434570312 kl_train: -0.1028522213 mse_train: 0.0621686509 acc_train: 0.5416666667 nll_val: 12283.7434895833 kl_val: -0.0797475850 mse_val: 0.0646512782 acc_val: 0.6666666667 time: 0.4982s
Best model so far, saving...
Epoch: 0038 nll_train: 11817.1048583984 kl_train: -0.1131832957 mse_train: 0.0621952897 acc_train: 0.5000000000 nll_val: 12303.3789062500 kl_val: -0.0802572767 mse_val: 0.0647546214 acc_val: 0.6666666667 time: 0.5197s
Epoch: 0039 nll_train: 11817.3557128906 kl_train: -0.1161945981 mse_train: 0.0621966080 acc_train: 0.5208333333 nll_val: 12270.6389973958 kl_val: -0.0917369847 mse_val: 0.0645823106 acc_val: 0.6666666667 time: 0.4958s
Best model so far, saving...
Epoch: 0040 nll_train: 11812.5892333984 kl_train: -0.1243661859 mse_train: 0.0621715241 acc_train: 0.5625000000 nll_val: 12260.5771484375 kl_val: -0.0965830460 mse_val: 0.0645293519 acc_val: 0.6666666667 time: 0.5536s
Best model so far, saving...
Epoch: 0041 nll_train: 11811.0216471354 kl_train: -0.1213867469 mse_train: 0.0621632726 acc_train: 0.5000000000 nll_val: 12283.8430989583 kl_val: -0.0833860400 mse_val: 0.0646518022 acc_val: 0.6666666667 time: 0.5155s
Epoch: 0042 nll_train: 11807.2299804688 kl_train: -0.1176765999 mse_train: 0.0621433153 acc_train: 0.5416666667 nll_val: 12306.9290364583 kl_val: -0.0901173055 mse_val: 0.0647733112 acc_val: 0.5000000000 time: 0.4990s
Epoch: 0043 nll_train: 11808.2909342448 kl_train: -0.1183526472 mse_train: 0.0621488999 acc_train: 0.5625000000 nll_val: 12235.6617838542 kl_val: -0.0764263806 mse_val: 0.0643982142 acc_val: 0.6666666667 time: 0.4983s
Best model so far, saving...
Epoch: 0044 nll_train: 11808.4600830078 kl_train: -0.1080517353 mse_train: 0.0621497906 acc_train: 0.5416666667 nll_val: 12294.1181640625 kl_val: -0.0802604146 mse_val: 0.0647058797 acc_val: 0.6666666667 time: 0.5014s
Epoch: 0045 nll_train: 11804.6363525391 kl_train: -0.1192525315 mse_train: 0.0621296656 acc_train: 0.5208333333 nll_val: 12251.9645182292 kl_val: -0.0852520764 mse_val: 0.0644840213 acc_val: 0.6666666667 time: 0.5017s
Epoch: 0046 nll_train: 11805.2683512370 kl_train: -0.1150965060 mse_train: 0.0621329926 acc_train: 0.5208333333 nll_val: 12236.9713541667 kl_val: -0.0832644316 mse_val: 0.0644051097 acc_val: 0.6666666667 time: 0.4992s
Epoch: 0047 nll_train: 11801.6588541667 kl_train: -0.1207079748 mse_train: 0.0621139938 acc_train: 0.5000000000 nll_val: 12248.8069661458 kl_val: -0.0805635365 mse_val: 0.0644674040 acc_val: 0.6666666667 time: 0.5021s
Epoch: 0048 nll_train: 11796.4136555990 kl_train: -0.1209861816 mse_train: 0.0620863877 acc_train: 0.5625000000 nll_val: 12288.3222656250 kl_val: -0.0928828890 mse_val: 0.0646753820 acc_val: 0.6666666667 time: 0.4974s
Epoch: 0049 nll_train: 11799.2218017578 kl_train: -0.1320668161 mse_train: 0.0621011675 acc_train: 0.5416666667 nll_val: 12307.2955729167 kl_val: -0.0977196867 mse_val: 0.0647752397 acc_val: 0.6666666667 time: 0.4982s
Epoch: 0050 nll_train: 11796.6958821615 kl_train: -0.1265231917 mse_train: 0.0620878749 acc_train: 0.5625000000 nll_val: 12262.9511718750 kl_val: -0.0926362909 mse_val: 0.0645418502 acc_val: 0.6666666667 time: 0.4988s
Epoch: 0051 nll_train: 11792.1870930990 kl_train: -0.1252433105 mse_train: 0.0620641421 acc_train: 0.5625000000 nll_val: 12258.3453776042 kl_val: -0.0924438039 mse_val: 0.0645176036 acc_val: 0.6666666667 time: 0.4971s
Epoch: 0052 nll_train: 11794.1782633464 kl_train: -0.1229414791 mse_train: 0.0620746238 acc_train: 0.5416666667 nll_val: 12264.7382812500 kl_val: -0.0913770174 mse_val: 0.0645512504 acc_val: 0.6666666667 time: 0.4958s
Epoch: 0053 nll_train: 11796.4287516276 kl_train: -0.1144086296 mse_train: 0.0620864683 acc_train: 0.5208333333 nll_val: 12234.7164713542 kl_val: -0.0783892696 mse_val: 0.0643932422 acc_val: 0.6666666667 time: 0.5012s
Best model so far, saving...
Epoch: 0054 nll_train: 11793.4711507161 kl_train: -0.1172151314 mse_train: 0.0620708998 acc_train: 0.5416666667 nll_val: 12253.7968750000 kl_val: -0.0876835808 mse_val: 0.0644936611 acc_val: 0.6666666667 time: 0.5013s
Epoch: 0055 nll_train: 11789.4312337240 kl_train: -0.1170572580 mse_train: 0.0620496388 acc_train: 0.5208333333 nll_val: 12250.6093750000 kl_val: -0.0851537473 mse_val: 0.0644768936 acc_val: 0.6666666667 time: 0.4979s
Epoch: 0056 nll_train: 11790.5153808594 kl_train: -0.1183511596 mse_train: 0.0620553455 acc_train: 0.5000000000 nll_val: 12240.1666666667 kl_val: -0.0817381404 mse_val: 0.0644219294 acc_val: 0.6666666667 time: 0.4953s
Epoch: 0057 nll_train: 11786.6984049479 kl_train: -0.1232030572 mse_train: 0.0620352561 acc_train: 0.5208333333 nll_val: 12282.2337239583 kl_val: -0.1040796687 mse_val: 0.0646433334 acc_val: 0.6666666667 time: 0.4999s
Epoch: 0058 nll_train: 11788.7178548177 kl_train: -0.1331872561 mse_train: 0.0620458831 acc_train: 0.5208333333 nll_val: 12298.9817708333 kl_val: -0.0955660890 mse_val: 0.0647314799 acc_val: 0.6666666667 time: 0.4979s
Epoch: 0059 nll_train: 11793.3020426432 kl_train: -0.1250930298 mse_train: 0.0620700112 acc_train: 0.5000000000 nll_val: 12226.8375651042 kl_val: -0.0922548970 mse_val: 0.0643517710 acc_val: 0.6666666667 time: 0.5101s
Best model so far, saving...
Epoch: 0060 nll_train: 11783.4855143229 kl_train: -0.1259097963 mse_train: 0.0620183446 acc_train: 0.5208333333 nll_val: 12257.7978515625 kl_val: -0.1036545634 mse_val: 0.0645147214 acc_val: 0.6666666667 time: 0.5196s
Epoch: 0061 nll_train: 11789.9312337240 kl_train: -0.1340453923 mse_train: 0.0620522701 acc_train: 0.5833333333 nll_val: 12245.2337239583 kl_val: -0.0979470722 mse_val: 0.0644485963 acc_val: 0.6666666667 time: 0.4987s
Epoch: 0062 nll_train: 11788.4947509766 kl_train: -0.1331544491 mse_train: 0.0620447099 acc_train: 0.5208333333 nll_val: 12240.7356770833 kl_val: -0.1045008078 mse_val: 0.0644249233 acc_val: 0.6666666667 time: 0.4963s
Epoch: 0063 nll_train: 11783.5093587240 kl_train: -0.1348823948 mse_train: 0.0620184684 acc_train: 0.5208333333 nll_val: 12243.3964843750 kl_val: -0.1007598639 mse_val: 0.0644389254 acc_val: 0.6666666667 time: 0.4992s
Epoch: 0064 nll_train: 11783.7624918620 kl_train: -0.1417834060 mse_train: 0.0620198039 acc_train: 0.5416666667 nll_val: 12232.3994140625 kl_val: -0.0951400101 mse_val: 0.0643810518 acc_val: 0.6666666667 time: 0.4979s
Epoch: 0065 nll_train: 11780.1827799479 kl_train: -0.1270700274 mse_train: 0.0620009632 acc_train: 0.5625000000 nll_val: 12239.5989583333 kl_val: -0.0913316632 mse_val: 0.0644189393 acc_val: 0.6666666667 time: 0.5007s
Epoch: 0066 nll_train: 11777.0367431641 kl_train: -0.1297224325 mse_train: 0.0619844032 acc_train: 0.5625000000 nll_val: 12233.5081380208 kl_val: -0.0976518740 mse_val: 0.0643868844 acc_val: 0.6666666667 time: 0.4962s
Epoch: 0067 nll_train: 11776.1031087240 kl_train: -0.1406528077 mse_train: 0.0619794900 acc_train: 0.5416666667 nll_val: 12229.6988932292 kl_val: -0.0973633329 mse_val: 0.0643668398 acc_val: 0.6666666667 time: 0.4954s
Epoch: 0068 nll_train: 11777.8068033854 kl_train: -0.1342953558 mse_train: 0.0619884577 acc_train: 0.5416666667 nll_val: 12210.7454427083 kl_val: -0.0937588351 mse_val: 0.0642670753 acc_val: 0.6666666667 time: 0.4956s
Best model so far, saving...
Epoch: 0069 nll_train: 11777.3266194661 kl_train: -0.1386114107 mse_train: 0.0619859284 acc_train: 0.5416666667 nll_val: 12209.6930338542 kl_val: -0.1023941984 mse_val: 0.0642615358 acc_val: 0.6666666667 time: 0.5520s
Best model so far, saving...
Epoch: 0070 nll_train: 11778.2504475911 kl_train: -0.1510441024 mse_train: 0.0619907932 acc_train: 0.5625000000 nll_val: 12217.5003255208 kl_val: -0.1143510242 mse_val: 0.0643026295 acc_val: 0.6666666667 time: 0.5208s
Epoch: 0071 nll_train: 11775.7155761719 kl_train: -0.1474657481 mse_train: 0.0619774509 acc_train: 0.5416666667 nll_val: 12247.6464843750 kl_val: -0.1005120600 mse_val: 0.0644612970 acc_val: 0.6666666667 time: 0.4959s
Epoch: 0072 nll_train: 11775.2123209635 kl_train: -0.1388233947 mse_train: 0.0619748027 acc_train: 0.6041666667 nll_val: 12233.6861979167 kl_val: -0.1078035099 mse_val: 0.0643878269 acc_val: 0.6666666667 time: 0.4979s
Epoch: 0073 nll_train: 11771.3299153646 kl_train: -0.1434184372 mse_train: 0.0619543679 acc_train: 0.5416666667 nll_val: 12224.7363281250 kl_val: -0.0995508383 mse_val: 0.0643407206 acc_val: 0.6666666667 time: 0.4991s
Epoch: 0074 nll_train: 11776.8209228516 kl_train: -0.1544555770 mse_train: 0.0619832690 acc_train: 0.5625000000 nll_val: 12246.9944661458 kl_val: -0.1063015300 mse_val: 0.0644578636 acc_val: 0.6666666667 time: 0.4991s
Epoch: 0075 nll_train: 11776.8758951823 kl_train: -0.1462199039 mse_train: 0.0619835576 acc_train: 0.5208333333 nll_val: 12180.3382161458 kl_val: -0.0886533012 mse_val: 0.0641070406 acc_val: 0.6666666667 time: 0.4956s
Best model so far, saving...
Epoch: 0076 nll_train: 11771.1107991536 kl_train: -0.1324543562 mse_train: 0.0619532142 acc_train: 0.5625000000 nll_val: 12261.1533203125 kl_val: -0.1111617883 mse_val: 0.0645323843 acc_val: 0.6666666667 time: 0.5063s
Epoch: 0077 nll_train: 11777.0741373698 kl_train: -0.1651052171 mse_train: 0.0619846017 acc_train: 0.5416666667 nll_val: 12222.9863281250 kl_val: -0.1073053665 mse_val: 0.0643315067 acc_val: 0.6666666667 time: 0.4993s
Epoch: 0078 nll_train: 11773.1309814453 kl_train: -0.1504495507 mse_train: 0.0619638468 acc_train: 0.5000000000 nll_val: 12262.8365885417 kl_val: -0.1062375990 mse_val: 0.0645412443 acc_val: 0.6666666667 time: 0.4966s
Epoch: 0079 nll_train: 11770.6024576823 kl_train: -0.1639634551 mse_train: 0.0619505395 acc_train: 0.5208333333 nll_val: 12217.5947265625 kl_val: -0.1226407637 mse_val: 0.0643031299 acc_val: 0.6666666667 time: 0.4984s
Epoch: 0080 nll_train: 11770.8924560547 kl_train: -0.1667730892 mse_train: 0.0619520661 acc_train: 0.5416666667 nll_val: 12226.3818359375 kl_val: -0.1058528374 mse_val: 0.0643493719 acc_val: 0.6666666667 time: 0.4955s
Epoch: 0081 nll_train: 11762.5111490885 kl_train: -0.1653178136 mse_train: 0.0619079542 acc_train: 0.4791666667 nll_val: 12216.2867838542 kl_val: -0.1186421290 mse_val: 0.0642962394 acc_val: 0.6666666667 time: 0.4975s
Epoch: 0082 nll_train: 11757.2683512370 kl_train: -0.1603565213 mse_train: 0.0618803605 acc_train: 0.4791666667 nll_val: 12227.9378255208 kl_val: -0.1148073028 mse_val: 0.0643575688 acc_val: 0.6666666667 time: 0.5021s
Epoch: 0083 nll_train: 11757.4760335286 kl_train: -0.1669404671 mse_train: 0.0618814530 acc_train: 0.4375000000 nll_val: 12236.6985677083 kl_val: -0.1044552450 mse_val: 0.0644036755 acc_val: 0.6666666667 time: 0.4959s
Epoch: 0084 nll_train: 11756.8746337891 kl_train: -0.1677777711 mse_train: 0.0618782858 acc_train: 0.4791666667 nll_val: 12280.1116536458 kl_val: -0.1026403258 mse_val: 0.0646321674 acc_val: 0.5000000000 time: 0.4971s
Epoch: 0085 nll_train: 11760.7977701823 kl_train: -0.1688087030 mse_train: 0.0618989343 acc_train: 0.4791666667 nll_val: 12318.1653645833 kl_val: -0.1134110118 mse_val: 0.0648324514 acc_val: 0.6666666667 time: 0.4976s
Epoch: 0086 nll_train: 11766.0724283854 kl_train: -0.1690312605 mse_train: 0.0619266980 acc_train: 0.5208333333 nll_val: 12274.2190755208 kl_val: -0.1178758790 mse_val: 0.0646011544 acc_val: 0.6666666667 time: 0.5093s
Epoch: 0087 nll_train: 11760.7064208984 kl_train: -0.1554008421 mse_train: 0.0618984550 acc_train: 0.5416666667 nll_val: 12273.9759114583 kl_val: -0.1060675606 mse_val: 0.0645998716 acc_val: 0.6666666667 time: 0.5178s
Epoch: 0088 nll_train: 11760.3190511068 kl_train: -0.1626425302 mse_train: 0.0618964182 acc_train: 0.4791666667 nll_val: 12271.8942057292 kl_val: -0.1118238196 mse_val: 0.0645889143 acc_val: 0.6666666667 time: 0.5034s
Epoch: 0089 nll_train: 11756.4388834635 kl_train: -0.1624211380 mse_train: 0.0618759954 acc_train: 0.4791666667 nll_val: 12226.2975260417 kl_val: -0.1136299123 mse_val: 0.0643489336 acc_val: 0.6666666667 time: 0.5017s
Epoch: 0090 nll_train: 11752.0074462891 kl_train: -0.1621780073 mse_train: 0.0618526707 acc_train: 0.4583333333 nll_val: 12262.7796223958 kl_val: -0.1070239569 mse_val: 0.0645409400 acc_val: 0.5000000000 time: 0.5096s
Epoch: 0091 nll_train: 11756.4685058594 kl_train: -0.1675253976 mse_train: 0.0618761505 acc_train: 0.4375000000 nll_val: 12294.6868489583 kl_val: -0.1070897977 mse_val: 0.0647088798 acc_val: 0.5000000000 time: 0.5214s
Epoch: 0092 nll_train: 11757.8880208333 kl_train: -0.1786739429 mse_train: 0.0618836218 acc_train: 0.4791666667 nll_val: 12284.0677083333 kl_val: -0.1212419098 mse_val: 0.0646529868 acc_val: 0.6666666667 time: 0.4979s
Epoch: 0093 nll_train: 11762.9862060547 kl_train: -0.1657759175 mse_train: 0.0619104539 acc_train: 0.4375000000 nll_val: 12287.1949869792 kl_val: -0.1087816283 mse_val: 0.0646694476 acc_val: 0.6666666667 time: 0.4996s
Epoch: 0094 nll_train: 11753.4916992188 kl_train: -0.1766194658 mse_train: 0.0618604830 acc_train: 0.4375000000 nll_val: 12235.6621093750 kl_val: -0.1226627243 mse_val: 0.0643982217 acc_val: 0.6666666667 time: 0.4995s
Epoch: 0095 nll_train: 11746.1048583984 kl_train: -0.1720052535 mse_train: 0.0618216054 acc_train: 0.4583333333 nll_val: 12268.6669921875 kl_val: -0.1208143135 mse_val: 0.0645719307 acc_val: 0.5000000000 time: 0.4975s
Epoch: 0096 nll_train: 11770.9245605469 kl_train: -0.1792336920 mse_train: 0.0619522350 acc_train: 0.4375000000 nll_val: 12296.2089843750 kl_val: -0.1080803052 mse_val: 0.0647168842 acc_val: 0.5000000000 time: 0.5328s
Epoch: 0097 nll_train: 11760.5003255208 kl_train: -0.1698216839 mse_train: 0.0618973703 acc_train: 0.5208333333 nll_val: 12241.8697916667 kl_val: -0.0981371738 mse_val: 0.0644308900 acc_val: 0.5000000000 time: 0.4993s
Epoch: 0098 nll_train: 11755.4193115234 kl_train: -0.1850056772 mse_train: 0.0618706273 acc_train: 0.4375000000 nll_val: 12338.0325520833 kl_val: -0.1354996885 mse_val: 0.0649370116 acc_val: 0.5000000000 time: 0.4997s
Epoch: 0099 nll_train: 11756.1558837891 kl_train: -0.1602600204 mse_train: 0.0618745062 acc_train: 0.4791666667 nll_val: 12307.8395182292 kl_val: -0.0933649937 mse_val: 0.0647781007 acc_val: 0.5000000000 time: 0.5174s
Epoch: 0100 nll_train: 11767.8151041667 kl_train: -0.1676913649 mse_train: 0.0619358697 acc_train: 0.4583333333 nll_val: 12339.7926432292 kl_val: -0.1316262384 mse_val: 0.0649462727 acc_val: 0.5000000000 time: 0.4994s
Epoch: 0101 nll_train: 11758.5188395182 kl_train: -0.1859141644 mse_train: 0.0618869405 acc_train: 0.5000000000 nll_val: 12325.3183593750 kl_val: -0.1216879090 mse_val: 0.0648700967 acc_val: 0.6666666667 time: 0.4971s
Epoch: 0102 nll_train: 11747.7712402344 kl_train: -0.1621964288 mse_train: 0.0618303749 acc_train: 0.5000000000 nll_val: 12215.4436848958 kl_val: -0.1076948158 mse_val: 0.0642918025 acc_val: 0.5000000000 time: 0.4984s
Epoch: 0103 nll_train: 11763.5028889974 kl_train: -0.1746141532 mse_train: 0.0619131733 acc_train: 0.5208333333 nll_val: 12171.7031250000 kl_val: -0.1042183712 mse_val: 0.0640615945 acc_val: 0.6666666667 time: 0.4973s
Best model so far, saving...
Epoch: 0104 nll_train: 11755.7749837240 kl_train: -0.1625051610 mse_train: 0.0618724992 acc_train: 0.4375000000 nll_val: 12205.4322916667 kl_val: -0.1106496242 mse_val: 0.0642391170 acc_val: 0.5000000000 time: 0.5176s
Epoch: 0105 nll_train: 11743.6333414714 kl_train: -0.1739643856 mse_train: 0.0618085985 acc_train: 0.4791666667 nll_val: 12199.3356119792 kl_val: -0.1050325632 mse_val: 0.0642070298 acc_val: 0.5000000000 time: 0.4973s
Epoch: 0106 nll_train: 11755.8267415365 kl_train: -0.1810028435 mse_train: 0.0618727729 acc_train: 0.5208333333 nll_val: 12209.5078125000 kl_val: -0.1109852195 mse_val: 0.0642605647 acc_val: 0.5000000000 time: 0.5002s
Epoch: 0107 nll_train: 11746.6348876953 kl_train: -0.1659964814 mse_train: 0.0618243945 acc_train: 0.4583333333 nll_val: 12273.1699218750 kl_val: -0.1020633777 mse_val: 0.0645956323 acc_val: 0.5000000000 time: 0.4973s
Epoch: 0108 nll_train: 11760.2993977865 kl_train: -0.1721479421 mse_train: 0.0618963121 acc_train: 0.4791666667 nll_val: 12445.9140625000 kl_val: -0.1427260588 mse_val: 0.0655048067 acc_val: 0.5000000000 time: 0.4973s
Epoch: 0109 nll_train: 11775.2469075521 kl_train: -0.1840206999 mse_train: 0.0619749843 acc_train: 0.4791666667 nll_val: 12525.6682942708 kl_val: -0.1269191802 mse_val: 0.0659245687 acc_val: 0.5000000000 time: 0.5018s
Epoch: 0110 nll_train: 11762.5867513021 kl_train: -0.1703815870 mse_train: 0.0619083509 acc_train: 0.5000000000 nll_val: 12445.9062500000 kl_val: -0.1193450515 mse_val: 0.0655047707 acc_val: 0.5000000000 time: 0.4982s
Epoch: 0111 nll_train: 11758.5761311849 kl_train: -0.1818455247 mse_train: 0.0618872433 acc_train: 0.5000000000 nll_val: 12297.2841796875 kl_val: -0.1167524407 mse_val: 0.0647225467 acc_val: 0.5000000000 time: 0.4989s
Epoch: 0112 nll_train: 11764.5324300130 kl_train: -0.1707287959 mse_train: 0.0619185915 acc_train: 0.5208333333 nll_val: 12288.4651692708 kl_val: -0.1202075928 mse_val: 0.0646761321 acc_val: 0.5000000000 time: 0.5001s
Epoch: 0113 nll_train: 11764.9579671224 kl_train: -0.1801943363 mse_train: 0.0619208328 acc_train: 0.5000000000 nll_val: 12394.4938151042 kl_val: -0.1172869988 mse_val: 0.0652341756 acc_val: 0.6666666667 time: 0.5000s
Epoch: 0114 nll_train: 11750.6297607422 kl_train: -0.1713570841 mse_train: 0.0618454210 acc_train: 0.5000000000 nll_val: 12529.5403645833 kl_val: -0.1320966755 mse_val: 0.0659449473 acc_val: 0.5000000000 time: 0.4998s
Epoch: 0115 nll_train: 11741.0370279948 kl_train: -0.1748767253 mse_train: 0.0617949326 acc_train: 0.5208333333 nll_val: 12545.7910156250 kl_val: -0.1357063577 mse_val: 0.0660304787 acc_val: 0.5000000000 time: 0.4947s
Epoch: 0116 nll_train: 11740.2445475260 kl_train: -0.1492971138 mse_train: 0.0617907615 acc_train: 0.5000000000 nll_val: 12398.3535156250 kl_val: -0.0971841862 mse_val: 0.0652544933 acc_val: 0.5000000000 time: 0.5002s
Epoch: 0117 nll_train: 11737.9546305339 kl_train: -0.1737020503 mse_train: 0.0617787068 acc_train: 0.4583333333 nll_val: 12455.0530598958 kl_val: -0.1331869103 mse_val: 0.0655529102 acc_val: 0.5000000000 time: 0.5002s
Epoch: 0118 nll_train: 11737.1982828776 kl_train: -0.1690016525 mse_train: 0.0617747279 acc_train: 0.5000000000 nll_val: 12426.5305989583 kl_val: -0.1116512989 mse_val: 0.0654027934 acc_val: 0.6666666667 time: 0.5012s
Epoch: 0119 nll_train: 11737.5480143229 kl_train: -0.1655340524 mse_train: 0.0617765685 acc_train: 0.5000000000 nll_val: 12479.2216796875 kl_val: -0.1284629889 mse_val: 0.0656801127 acc_val: 0.5000000000 time: 0.4978s
Epoch: 0120 nll_train: 11750.5441080729 kl_train: -0.1725511067 mse_train: 0.0618449695 acc_train: 0.5000000000 nll_val: 12316.9036458333 kl_val: -0.1076513653 mse_val: 0.0648258055 acc_val: 0.5000000000 time: 0.4988s
Epoch: 0121 nll_train: 11725.1873779297 kl_train: -0.1815715066 mse_train: 0.0617115130 acc_train: 0.4791666667 nll_val: 12411.5367838542 kl_val: -0.1377338208 mse_val: 0.0653238756 acc_val: 0.5000000000 time: 0.4965s
Epoch: 0122 nll_train: 11729.9655354818 kl_train: -0.1897503907 mse_train: 0.0617366607 acc_train: 0.5000000000 nll_val: 12354.8886718750 kl_val: -0.1220939383 mse_val: 0.0650257282 acc_val: 0.5000000000 time: 0.4994s
Epoch: 0123 nll_train: 11738.5674641927 kl_train: -0.1621696545 mse_train: 0.0617819354 acc_train: 0.5000000000 nll_val: 12319.7438151042 kl_val: -0.1148283792 mse_val: 0.0648407551 acc_val: 0.5000000000 time: 0.4956s
Epoch: 0124 nll_train: 11748.2564697266 kl_train: -0.1626685696 mse_train: 0.0618329304 acc_train: 0.5000000000 nll_val: 12205.4586588542 kl_val: -0.1029008813 mse_val: 0.0642392524 acc_val: 0.5000000000 time: 0.4995s
Epoch: 0125 nll_train: 11740.0837809245 kl_train: -0.1552621284 mse_train: 0.0617899145 acc_train: 0.5000000000 nll_val: 12209.5117187500 kl_val: -0.0889371534 mse_val: 0.0642605896 acc_val: 0.5000000000 time: 0.4977s
Epoch: 0126 nll_train: 11731.6425374349 kl_train: -0.1813783726 mse_train: 0.0617454861 acc_train: 0.5000000000 nll_val: 12252.7887369792 kl_val: -0.1344150131 mse_val: 0.0644883600 acc_val: 0.5000000000 time: 0.4971s
Epoch: 0127 nll_train: 11742.6648763021 kl_train: -0.1821697683 mse_train: 0.0618035005 acc_train: 0.5000000000 nll_val: 12208.5494791667 kl_val: -0.1127230873 mse_val: 0.0642555257 acc_val: 0.5000000000 time: 0.4985s
Epoch: 0128 nll_train: 11743.0495605469 kl_train: -0.1617778664 mse_train: 0.0618055246 acc_train: 0.5000000000 nll_val: 12383.1409505208 kl_val: -0.1165380987 mse_val: 0.0651744219 acc_val: 0.5000000000 time: 0.4998s
Epoch: 0129 nll_train: 11734.8533935547 kl_train: -0.1729505726 mse_train: 0.0617623849 acc_train: 0.5000000000 nll_val: 12456.9111328125 kl_val: -0.1108320877 mse_val: 0.0655626891 acc_val: 0.5000000000 time: 0.4973s
Epoch: 0130 nll_train: 11737.1029052734 kl_train: -0.1690509425 mse_train: 0.0617742270 acc_train: 0.5000000000 nll_val: 12361.6494140625 kl_val: -0.1324769457 mse_val: 0.0650613122 acc_val: 0.5000000000 time: 0.4988s
Epoch: 0131 nll_train: 11715.0322265625 kl_train: -0.1729259230 mse_train: 0.0616580662 acc_train: 0.5000000000 nll_val: 12534.5100911458 kl_val: -0.1299930091 mse_val: 0.0659711050 acc_val: 0.5000000000 time: 0.4995s
Epoch: 0132 nll_train: 11725.0777180990 kl_train: -0.1656800204 mse_train: 0.0617109349 acc_train: 0.5000000000 nll_val: 12398.4423828125 kl_val: -0.1102064910 mse_val: 0.0652549565 acc_val: 0.5000000000 time: 0.5002s
Epoch: 0133 nll_train: 11705.5700276693 kl_train: -0.1760470942 mse_train: 0.0616082647 acc_train: 0.5000000000 nll_val: 12311.2288411458 kl_val: -0.1309465443 mse_val: 0.0647959448 acc_val: 0.5000000000 time: 0.4988s
Epoch: 0134 nll_train: 11739.0229492188 kl_train: -0.1739407548 mse_train: 0.0617843320 acc_train: 0.5000000000 nll_val: 12251.4563802083 kl_val: -0.0981968641 mse_val: 0.0644813466 acc_val: 0.5000000000 time: 0.4983s
Epoch: 0135 nll_train: 11723.7270914714 kl_train: -0.1511593293 mse_train: 0.0617038268 acc_train: 0.5000000000 nll_val: 12285.2034505208 kl_val: -0.1071523987 mse_val: 0.0646589597 acc_val: 0.5000000000 time: 0.4974s
Epoch: 0136 nll_train: 11721.6605224609 kl_train: -0.1682539185 mse_train: 0.0616929514 acc_train: 0.5000000000 nll_val: 12341.2330729167 kl_val: -0.1240416517 mse_val: 0.0649538574 acc_val: 0.5000000000 time: 0.5001s
Epoch: 0137 nll_train: 11715.7869466146 kl_train: -0.1706887875 mse_train: 0.0616620361 acc_train: 0.5000000000 nll_val: 12326.9547526042 kl_val: -0.1228089072 mse_val: 0.0648787084 acc_val: 0.5000000000 time: 0.5101s
Epoch: 0138 nll_train: 11710.4851074219 kl_train: -0.1746786690 mse_train: 0.0616341329 acc_train: 0.4791666667 nll_val: 12318.7529296875 kl_val: -0.1210831093 mse_val: 0.0648355385 acc_val: 0.5000000000 time: 0.5550s
Epoch: 0139 nll_train: 11727.9908854167 kl_train: -0.1879503249 mse_train: 0.0617262673 acc_train: 0.5000000000 nll_val: 12462.5019531250 kl_val: -0.1478583490 mse_val: 0.0655921139 acc_val: 0.5000000000 time: 0.5292s
Epoch: 0140 nll_train: 11724.4811604818 kl_train: -0.1764313964 mse_train: 0.0617077961 acc_train: 0.5000000000 nll_val: 12514.1477864583 kl_val: -0.1237576207 mse_val: 0.0658639322 acc_val: 0.5000000000 time: 0.4986s
Epoch: 0141 nll_train: 11706.5354003906 kl_train: -0.1633677594 mse_train: 0.0616133447 acc_train: 0.5000000000 nll_val: 12475.1829427083 kl_val: -0.1286387990 mse_val: 0.0656588562 acc_val: 0.5000000000 time: 0.4971s
Epoch: 0142 nll_train: 11713.5489501953 kl_train: -0.1736322846 mse_train: 0.0616502565 acc_train: 0.4791666667 nll_val: 12428.1975911458 kl_val: -0.1295190255 mse_val: 0.0654115677 acc_val: 0.5000000000 time: 0.4998s
Epoch: 0143 nll_train: 11705.2989095052 kl_train: -0.1756184331 mse_train: 0.0616068367 acc_train: 0.4791666667 nll_val: 12451.3167317708 kl_val: -0.1330679245 mse_val: 0.0655332456 acc_val: 0.5000000000 time: 0.4995s
Epoch: 0144 nll_train: 11703.9150390625 kl_train: -0.1765994808 mse_train: 0.0615995529 acc_train: 0.5000000000 nll_val: 12525.1699218750 kl_val: -0.1146603090 mse_val: 0.0659219461 acc_val: 0.5000000000 time: 0.4985s
Epoch: 0145 nll_train: 11707.3758951823 kl_train: -0.1741234052 mse_train: 0.0616177687 acc_train: 0.4791666667 nll_val: 12431.9638671875 kl_val: -0.1177861492 mse_val: 0.0654313850 acc_val: 0.5000000000 time: 0.5023s
Epoch: 0146 nll_train: 11705.0533854167 kl_train: -0.1563481806 mse_train: 0.0616055421 acc_train: 0.5000000000 nll_val: 12388.4059244792 kl_val: -0.1035261725 mse_val: 0.0652021368 acc_val: 0.5000000000 time: 0.4986s
Epoch: 0147 nll_train: 11729.5676269531 kl_train: -0.1534141532 mse_train: 0.0617345668 acc_train: 0.5000000000 nll_val: 12269.6041666667 kl_val: -0.0970865078 mse_val: 0.0645768667 acc_val: 0.5000000000 time: 0.4955s
Epoch: 0148 nll_train: 11710.5713704427 kl_train: -0.1561949638 mse_train: 0.0616345860 acc_train: 0.5000000000 nll_val: 12349.5426432292 kl_val: -0.1179847804 mse_val: 0.0649975948 acc_val: 0.5000000000 time: 0.5025s
Epoch: 0149 nll_train: 11699.4189046224 kl_train: -0.1653382418 mse_train: 0.0615758885 acc_train: 0.5000000000 nll_val: 12340.8219401042 kl_val: -0.1126662443 mse_val: 0.0649516967 acc_val: 0.5000000000 time: 0.4965s
Epoch: 0150 nll_train: 11710.2403971354 kl_train: -0.1493976042 mse_train: 0.0616328443 acc_train: 0.5000000000 nll_val: 12344.8212890625 kl_val: -0.0996498093 mse_val: 0.0649727422 acc_val: 0.5000000000 time: 0.4990s
Epoch: 0151 nll_train: 11697.5959879557 kl_train: -0.1519050567 mse_train: 0.0615662962 acc_train: 0.5000000000 nll_val: 12308.7789713542 kl_val: -0.0982150249 mse_val: 0.0647830491 acc_val: 0.5000000000 time: 0.5235s
Epoch: 0152 nll_train: 11686.6733805339 kl_train: -0.1668069170 mse_train: 0.0615088084 acc_train: 0.4583333333 nll_val: 12454.6468098958 kl_val: -0.1038618187 mse_val: 0.0655507731 acc_val: 0.5000000000 time: 0.4980s
Epoch: 0153 nll_train: 11682.5296630859 kl_train: -0.1698926880 mse_train: 0.0614869986 acc_train: 0.4791666667 nll_val: 12332.8424479167 kl_val: -0.1453608672 mse_val: 0.0649096953 acc_val: 0.5000000000 time: 0.4989s
Epoch: 0154 nll_train: 11696.3271891276 kl_train: -0.1748663690 mse_train: 0.0615596174 acc_train: 0.5000000000 nll_val: 12350.5198567708 kl_val: -0.1092654678 mse_val: 0.0650027382 acc_val: 0.5000000000 time: 0.4985s
Epoch: 0155 nll_train: 11680.6397705078 kl_train: -0.1680512733 mse_train: 0.0614770505 acc_train: 0.5000000000 nll_val: 12373.9466145833 kl_val: -0.1218849011 mse_val: 0.0651260354 acc_val: 0.5000000000 time: 0.4996s
Epoch: 0156 nll_train: 11670.6318766276 kl_train: -0.1695002193 mse_train: 0.0614243786 acc_train: 0.5000000000 nll_val: 12417.8401692708 kl_val: -0.1232461383 mse_val: 0.0653570555 acc_val: 0.5000000000 time: 0.5031s
Epoch: 0157 nll_train: 11697.1808268229 kl_train: -0.1827123308 mse_train: 0.0615641096 acc_train: 0.4791666667 nll_val: 12662.8138020833 kl_val: -0.1425517077 mse_val: 0.0666463884 acc_val: 0.5000000000 time: 0.4971s
Epoch: 0158 nll_train: 11702.2951660156 kl_train: -0.1914786939 mse_train: 0.0615910276 acc_train: 0.4791666667 nll_val: 12609.9947916667 kl_val: -0.1210729082 mse_val: 0.0663683948 acc_val: 0.5000000000 time: 0.4988s
Epoch: 0159 nll_train: 11687.2095540365 kl_train: -0.1632126396 mse_train: 0.0615116302 acc_train: 0.5000000000 nll_val: 12752.8450520833 kl_val: -0.1326367048 mse_val: 0.0671202342 acc_val: 0.5000000000 time: 0.5302s
Epoch: 0160 nll_train: 11709.2853190104 kl_train: -0.1578837497 mse_train: 0.0616278175 acc_train: 0.5000000000 nll_val: 12665.1282552083 kl_val: -0.1046983662 mse_val: 0.0666585714 acc_val: 0.5000000000 time: 0.5062s
Epoch: 0161 nll_train: 11707.4764404297 kl_train: -0.1499223777 mse_train: 0.0616182971 acc_train: 0.5000000000 nll_val: 12341.3570963542 kl_val: -0.0977363947 mse_val: 0.0649545118 acc_val: 0.5000000000 time: 0.4998s
Epoch: 0162 nll_train: 11718.2756347656 kl_train: -0.1267861860 mse_train: 0.0616751357 acc_train: 0.5000000000 nll_val: 12271.9899088542 kl_val: -0.0826718944 mse_val: 0.0645894185 acc_val: 0.5000000000 time: 0.4980s
Epoch: 0163 nll_train: 11730.2608235677 kl_train: -0.1383274412 mse_train: 0.0617382151 acc_train: 0.5000000000 nll_val: 12304.1604817708 kl_val: -0.0882542258 mse_val: 0.0647587379 acc_val: 0.5000000000 time: 0.4956s
Epoch: 0164 nll_train: 11722.3237711589 kl_train: -0.1476835106 mse_train: 0.0616964425 acc_train: 0.5000000000 nll_val: 12349.0273437500 kl_val: -0.1021863458 mse_val: 0.0649948766 acc_val: 0.5000000000 time: 0.4966s
Epoch: 0165 nll_train: 11690.7222900391 kl_train: -0.1592515381 mse_train: 0.0615301182 acc_train: 0.5000000000 nll_val: 12365.7913411458 kl_val: -0.1047356091 mse_val: 0.0650831113 acc_val: 0.5000000000 time: 0.5340s
Epoch: 0166 nll_train: 11688.3132731120 kl_train: -0.1821766260 mse_train: 0.0615174390 acc_train: 0.5000000000 nll_val: 12360.1748046875 kl_val: -0.1303013017 mse_val: 0.0650535536 acc_val: 0.5000000000 time: 0.5011s
Epoch: 0167 nll_train: 11695.8245442708 kl_train: -0.1770277855 mse_train: 0.0615569723 acc_train: 0.5000000000 nll_val: 12279.3170572917 kl_val: -0.1249949361 mse_val: 0.0646279852 acc_val: 0.5000000000 time: 0.4989s
Epoch: 0168 nll_train: 11693.9409179688 kl_train: -0.1697373788 mse_train: 0.0615470569 acc_train: 0.5000000000 nll_val: 12242.6917317708 kl_val: -0.0992555842 mse_val: 0.0644352188 acc_val: 0.5000000000 time: 0.4985s
Epoch: 0169 nll_train: 11703.5109456380 kl_train: -0.1659766727 mse_train: 0.0615974260 acc_train: 0.5000000000 nll_val: 12309.7695312500 kl_val: -0.0998415488 mse_val: 0.0647882620 acc_val: 0.5000000000 time: 0.4997s
Epoch: 0170 nll_train: 11703.0526529948 kl_train: -0.1659180087 mse_train: 0.0615950148 acc_train: 0.5000000000 nll_val: 12400.6910807292 kl_val: -0.1477609314 mse_val: 0.0652667930 acc_val: 0.5000000000 time: 0.4973s
Epoch: 0171 nll_train: 11697.1247965495 kl_train: -0.1671750533 mse_train: 0.0615638148 acc_train: 0.5000000000 nll_val: 12293.6022135417 kl_val: -0.1059907774 mse_val: 0.0647031652 acc_val: 0.5000000000 time: 0.4966s
Epoch: 0172 nll_train: 11686.4119466146 kl_train: -0.1667222778 mse_train: 0.0615074319 acc_train: 0.5000000000 nll_val: 12365.3369140625 kl_val: -0.1094115923 mse_val: 0.0650807209 acc_val: 0.5000000000 time: 0.4990s
Epoch: 0173 nll_train: 11678.3646647135 kl_train: -0.1593278224 mse_train: 0.0614650780 acc_train: 0.5000000000 nll_val: 12386.0136718750 kl_val: -0.1188958089 mse_val: 0.0651895429 acc_val: 0.5000000000 time: 0.4994s
Epoch: 0174 nll_train: 11681.2779541016 kl_train: -0.1568187680 mse_train: 0.0614804109 acc_train: 0.5000000000 nll_val: 12305.5052083333 kl_val: -0.0862252774 mse_val: 0.0647658147 acc_val: 0.5000000000 time: 0.5002s
Epoch: 0175 nll_train: 11699.9112955729 kl_train: -0.1433739560 mse_train: 0.0615784805 acc_train: 0.5000000000 nll_val: 12231.9456380208 kl_val: -0.0933320721 mse_val: 0.0643786627 acc_val: 0.5000000000 time: 0.4961s
Epoch: 0176 nll_train: 11690.5517578125 kl_train: -0.1616609832 mse_train: 0.0615292201 acc_train: 0.5000000000 nll_val: 12303.3710937500 kl_val: -0.1032133537 mse_val: 0.0647545842 acc_val: 0.5000000000 time: 0.4983s
Epoch: 0177 nll_train: 11688.0222981771 kl_train: -0.1491154066 mse_train: 0.0615159068 acc_train: 0.5000000000 nll_val: 12286.2932942708 kl_val: -0.0867478463 mse_val: 0.0646647016 acc_val: 0.5000000000 time: 0.5133s
Epoch: 0178 nll_train: 11692.1762695312 kl_train: -0.1482557850 mse_train: 0.0615377709 acc_train: 0.5000000000 nll_val: 12334.4124348958 kl_val: -0.0942765151 mse_val: 0.0649179543 acc_val: 0.5000000000 time: 0.4974s
Epoch: 0179 nll_train: 11675.3330485026 kl_train: -0.1627911916 mse_train: 0.0614491212 acc_train: 0.5000000000 nll_val: 12296.8977864583 kl_val: -0.1011905931 mse_val: 0.0647205176 acc_val: 0.5000000000 time: 0.5016s
Epoch: 0180 nll_train: 11690.9794514974 kl_train: -0.1558532317 mse_train: 0.0615314726 acc_train: 0.5000000000 nll_val: 12298.8629557292 kl_val: -0.0975761935 mse_val: 0.0647308591 acc_val: 0.5000000000 time: 0.4979s
Epoch: 0181 nll_train: 11688.9378662109 kl_train: -0.1378456041 mse_train: 0.0615207263 acc_train: 0.5000000000 nll_val: 12290.1569010417 kl_val: -0.1059457337 mse_val: 0.0646850343 acc_val: 0.5000000000 time: 0.4965s
Epoch: 0182 nll_train: 11686.5351562500 kl_train: -0.1648756713 mse_train: 0.0615080798 acc_train: 0.5000000000 nll_val: 12327.3945312500 kl_val: -0.1083156268 mse_val: 0.0648810218 acc_val: 0.5000000000 time: 0.4959s
Epoch: 0183 nll_train: 11675.0357666016 kl_train: -0.1785756834 mse_train: 0.0614475583 acc_train: 0.5000000000 nll_val: 12350.1546223958 kl_val: -0.1310690567 mse_val: 0.0650008110 acc_val: 0.5000000000 time: 0.5003s
Epoch: 0184 nll_train: 11698.0136311849 kl_train: -0.1560012608 mse_train: 0.0615684946 acc_train: 0.5000000000 nll_val: 12295.0455729167 kl_val: -0.1111120284 mse_val: 0.0647107636 acc_val: 0.5000000000 time: 0.5019s
Epoch: 0185 nll_train: 11691.9737141927 kl_train: -0.1692898075 mse_train: 0.0615367036 acc_train: 0.5000000000 nll_val: 12360.0680338542 kl_val: -0.1179507673 mse_val: 0.0650529849 acc_val: 0.5000000000 time: 0.5154s
Epoch: 0186 nll_train: 11693.2333577474 kl_train: -0.1611470773 mse_train: 0.0615433340 acc_train: 0.5000000000 nll_val: 12295.2666015625 kl_val: -0.1049066956 mse_val: 0.0647119296 acc_val: 0.5000000000 time: 0.5101s
Epoch: 0187 nll_train: 11719.9310302734 kl_train: -0.1583261490 mse_train: 0.0616838480 acc_train: 0.5000000000 nll_val: 12289.1438802083 kl_val: -0.0915388726 mse_val: 0.0646797009 acc_val: 0.5000000000 time: 0.4987s
Epoch: 0188 nll_train: 11690.2842203776 kl_train: -0.1672896997 mse_train: 0.0615278110 acc_train: 0.5000000000 nll_val: 12295.6194661458 kl_val: -0.1281193842 mse_val: 0.0647137848 acc_val: 0.5000000000 time: 0.5005s
Epoch: 0189 nll_train: 11692.9379475911 kl_train: -0.1897414140 mse_train: 0.0615417797 acc_train: 0.4791666667 nll_val: 12339.2555338542 kl_val: -0.1219802424 mse_val: 0.0649434490 acc_val: 0.5000000000 time: 0.5028s
Epoch: 0190 nll_train: 11686.3244222005 kl_train: -0.1597035111 mse_train: 0.0615069716 acc_train: 0.5000000000 nll_val: 12388.3792317708 kl_val: -0.1202075544 mse_val: 0.0652019915 acc_val: 0.5000000000 time: 0.4992s
Epoch: 0191 nll_train: 11659.5864257812 kl_train: -0.1529768171 mse_train: 0.0613662465 acc_train: 0.5000000000 nll_val: 12493.1953125000 kl_val: -0.1286076196 mse_val: 0.0657536623 acc_val: 0.5000000000 time: 0.4997s
Epoch: 0192 nll_train: 11668.0083007812 kl_train: -0.1800610879 mse_train: 0.0614105687 acc_train: 0.5000000000 nll_val: 12548.8098958333 kl_val: -0.1221754526 mse_val: 0.0660463658 acc_val: 0.5000000000 time: 0.4966s
Epoch: 0193 nll_train: 11695.5137939453 kl_train: -0.1800821802 mse_train: 0.0615553364 acc_train: 0.5000000000 nll_val: 12784.5670572917 kl_val: -0.1423392519 mse_val: 0.0672871917 acc_val: 0.5000000000 time: 0.4979s
Epoch: 0194 nll_train: 11702.3227539062 kl_train: -0.1754576294 mse_train: 0.0615911727 acc_train: 0.5000000000 nll_val: 12632.9856770833 kl_val: -0.1019821130 mse_val: 0.0664893960 acc_val: 0.5000000000 time: 0.4961s
Epoch: 0195 nll_train: 11657.5574137370 kl_train: -0.1350631369 mse_train: 0.0613555663 acc_train: 0.5000000000 nll_val: 12573.1819661458 kl_val: -0.1018982207 mse_val: 0.0661746413 acc_val: 0.5000000000 time: 0.5023s
Epoch: 0196 nll_train: 11662.7009277344 kl_train: -0.1475145938 mse_train: 0.0613826374 acc_train: 0.5000000000 nll_val: 12465.2854817708 kl_val: -0.1009864099 mse_val: 0.0656067654 acc_val: 0.5000000000 time: 0.4973s
Epoch: 0197 nll_train: 11655.8848470052 kl_train: -0.1753627285 mse_train: 0.0613467617 acc_train: 0.5000000000 nll_val: 12607.0891927083 kl_val: -0.1348462999 mse_val: 0.0663531013 acc_val: 0.5000000000 time: 0.5033s
Epoch: 0198 nll_train: 11678.3800455729 kl_train: -0.2042808576 mse_train: 0.0614651589 acc_train: 0.5000000000 nll_val: 12582.5618489583 kl_val: -0.1381610719 mse_val: 0.0662240113 acc_val: 0.5000000000 time: 0.4988s
Epoch: 0199 nll_train: 11666.6680908203 kl_train: -0.1791448519 mse_train: 0.0614035168 acc_train: 0.5000000000 nll_val: 12609.5537109375 kl_val: -0.1292790373 mse_val: 0.0663660727 acc_val: 0.5000000000 time: 0.4960s
Optimization finished
Best epoch 103
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 13979.8684895833 kl_test: -0.1502504672 mse_test: 0.0735782559 acc_test: 0.6666666667
MSE: [ 0.073024921119 , 0.071150064468 , 0.072034843266 , 0.071941964328 , 0.072493225336 , 0.072079442441 , 0.073733739555 , 0.073647543788 , 0.072057567537 , 0.072020336986 , 0.071954131126 , 0.071533046663 , 0.072552807629 , 0.073243804276 , 0.073542177677 , 0.073244877160 , 0.072028852999 , 0.069742359221 , 0.069344788790 ]
Accuracy for experiment id 2 is 0.25
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 19870.1101074219 kl_train: -0.2144407729 mse_train: 0.1045795272 acc_train: 0.5833333333 nll_val: 16549.8544921875 kl_val: -0.3606339594 mse_val: 0.0871044969 acc_val: 0.5000000000 time: 0.5016s
Best model so far, saving...
Epoch: 0001 nll_train: 15449.3875732422 kl_train: -0.2322527599 mse_train: 0.0813125645 acc_train: 0.5000000000 nll_val: 14910.7656250000 kl_val: -0.3393800457 mse_val: 0.0784777105 acc_val: 0.5000000000 time: 0.5025s
Best model so far, saving...
Epoch: 0002 nll_train: 14797.7140706380 kl_train: -0.3229456830 mse_train: 0.0778827065 acc_train: 0.5000000000 nll_val: 14391.1578776042 kl_val: -0.3463203510 mse_val: 0.0757429351 acc_val: 0.5000000000 time: 0.5150s
Best model so far, saving...
Epoch: 0003 nll_train: 14623.7400309245 kl_train: -0.3035780576 mse_train: 0.0769670516 acc_train: 0.5000000000 nll_val: 13866.0429687500 kl_val: -0.3193325400 mse_val: 0.0729791721 acc_val: 0.3333333333 time: 0.5019s
Best model so far, saving...
Epoch: 0004 nll_train: 14541.7726236979 kl_train: -0.2638984198 mse_train: 0.0765356459 acc_train: 0.5000000000 nll_val: 14093.5777994792 kl_val: -0.2372289151 mse_val: 0.0741767262 acc_val: 0.6666666667 time: 0.4991s
Epoch: 0005 nll_train: 14537.9833984375 kl_train: -0.2010302202 mse_train: 0.0765157022 acc_train: 0.5208333333 nll_val: 14636.3072916667 kl_val: -0.1685861150 mse_val: 0.0770331969 acc_val: 0.5000000000 time: 0.4964s
Epoch: 0006 nll_train: 14637.0535074870 kl_train: -0.1538008383 mse_train: 0.0770371243 acc_train: 0.3750000000 nll_val: 14795.9235026042 kl_val: -0.1136379639 mse_val: 0.0778732797 acc_val: 0.6666666667 time: 0.4957s
Epoch: 0007 nll_train: 14613.3506266276 kl_train: -0.1292183856 mse_train: 0.0769123721 acc_train: 0.3750000000 nll_val: 13639.4775390625 kl_val: -0.0924562886 mse_val: 0.0717867240 acc_val: 0.5000000000 time: 0.5037s
Best model so far, saving...
Epoch: 0008 nll_train: 14358.2473144531 kl_train: -0.1021500453 mse_train: 0.0755697225 acc_train: 0.4375000000 nll_val: 13404.8844401042 kl_val: -0.0748233174 mse_val: 0.0705520188 acc_val: 0.5000000000 time: 0.4996s
Best model so far, saving...
Epoch: 0009 nll_train: 14362.2696533203 kl_train: -0.0938279815 mse_train: 0.0755908927 acc_train: 0.4375000000 nll_val: 13009.3811848958 kl_val: -0.0609441002 mse_val: 0.0684704259 acc_val: 0.5000000000 time: 0.5019s
Best model so far, saving...
Epoch: 0010 nll_train: 14331.1806640625 kl_train: -0.0822454685 mse_train: 0.0754272674 acc_train: 0.4375000000 nll_val: 12837.6119791667 kl_val: -0.0711517309 mse_val: 0.0675663749 acc_val: 0.5000000000 time: 0.4993s
Best model so far, saving...
Epoch: 0011 nll_train: 14321.2865804036 kl_train: -0.0804717876 mse_train: 0.0753751937 acc_train: 0.3541666667 nll_val: 12763.7314453125 kl_val: -0.0732562592 mse_val: 0.0671775391 acc_val: 0.5000000000 time: 0.5006s
Best model so far, saving...
Epoch: 0012 nll_train: 14315.0462239583 kl_train: -0.0759336492 mse_train: 0.0753423500 acc_train: 0.3958333333 nll_val: 12712.1871744792 kl_val: -0.0702528358 mse_val: 0.0669062510 acc_val: 0.5000000000 time: 0.5037s
Best model so far, saving...
Epoch: 0013 nll_train: 14294.6990966797 kl_train: -0.0757663116 mse_train: 0.0752352597 acc_train: 0.4166666667 nll_val: 12766.5979817708 kl_val: -0.0622795373 mse_val: 0.0671926166 acc_val: 0.5000000000 time: 0.4996s
Epoch: 0014 nll_train: 14284.8892415365 kl_train: -0.0758456280 mse_train: 0.0751836269 acc_train: 0.4166666667 nll_val: 12734.2093098958 kl_val: -0.0791019648 mse_val: 0.0670221522 acc_val: 0.5000000000 time: 0.4978s
Epoch: 0015 nll_train: 14283.8098958333 kl_train: -0.0748104149 mse_train: 0.0751779474 acc_train: 0.3541666667 nll_val: 12681.1292317708 kl_val: -0.0656888721 mse_val: 0.0667427853 acc_val: 0.5000000000 time: 0.5019s
Best model so far, saving...
Epoch: 0016 nll_train: 14281.5562744141 kl_train: -0.0783850585 mse_train: 0.0751660851 acc_train: 0.4166666667 nll_val: 12706.8499348958 kl_val: -0.0736392612 mse_val: 0.0668781549 acc_val: 0.5000000000 time: 0.5007s
Epoch: 0017 nll_train: 14272.2240804036 kl_train: -0.0732949319 mse_train: 0.0751169681 acc_train: 0.3958333333 nll_val: 12717.7311197917 kl_val: -0.0628715381 mse_val: 0.0669354275 acc_val: 0.5000000000 time: 0.4980s
Epoch: 0018 nll_train: 14263.8072102865 kl_train: -0.0757694164 mse_train: 0.0750726707 acc_train: 0.3750000000 nll_val: 12664.0908203125 kl_val: -0.0677593152 mse_val: 0.0666531101 acc_val: 0.5000000000 time: 0.5014s
Best model so far, saving...
Epoch: 0019 nll_train: 14256.2789306641 kl_train: -0.0766185221 mse_train: 0.0750330482 acc_train: 0.3958333333 nll_val: 12719.9466145833 kl_val: -0.0596504249 mse_val: 0.0669470876 acc_val: 0.5000000000 time: 0.5038s
Epoch: 0020 nll_train: 14266.2715250651 kl_train: -0.0780493706 mse_train: 0.0750856406 acc_train: 0.4166666667 nll_val: 12727.7138671875 kl_val: -0.0721833482 mse_val: 0.0669879615 acc_val: 0.5000000000 time: 0.5000s
Epoch: 0021 nll_train: 14255.5685221354 kl_train: -0.0764090360 mse_train: 0.0750293089 acc_train: 0.4166666667 nll_val: 12702.3994140625 kl_val: -0.0575250909 mse_val: 0.0668547327 acc_val: 0.5000000000 time: 0.4979s
Epoch: 0022 nll_train: 14260.9767659505 kl_train: -0.0740244058 mse_train: 0.0750577742 acc_train: 0.3958333333 nll_val: 12685.2027994792 kl_val: -0.0595504704 mse_val: 0.0667642231 acc_val: 0.5000000000 time: 0.5141s
Epoch: 0023 nll_train: 14247.6003011068 kl_train: -0.0750581721 mse_train: 0.0749873699 acc_train: 0.3541666667 nll_val: 12720.2568359375 kl_val: -0.0582743796 mse_val: 0.0669487193 acc_val: 0.5000000000 time: 0.5133s
Epoch: 0024 nll_train: 14242.4239908854 kl_train: -0.0775818769 mse_train: 0.0749601284 acc_train: 0.3541666667 nll_val: 12736.5556640625 kl_val: -0.0631366285 mse_val: 0.0670345028 acc_val: 0.5000000000 time: 0.5029s
Epoch: 0025 nll_train: 14252.2411295573 kl_train: -0.0757543119 mse_train: 0.0750117951 acc_train: 0.3750000000 nll_val: 12721.5465494792 kl_val: -0.0705637510 mse_val: 0.0669555068 acc_val: 0.5000000000 time: 0.4989s
Epoch: 0026 nll_train: 14234.6612141927 kl_train: -0.0764166595 mse_train: 0.0749192722 acc_train: 0.3750000000 nll_val: 12672.6243489583 kl_val: -0.0624121614 mse_val: 0.0666980197 acc_val: 0.5000000000 time: 0.4989s
Epoch: 0027 nll_train: 14252.5282389323 kl_train: -0.0755183349 mse_train: 0.0750133082 acc_train: 0.4166666667 nll_val: 12718.6930338542 kl_val: -0.0636448314 mse_val: 0.0669404839 acc_val: 0.5000000000 time: 0.4967s
Epoch: 0028 nll_train: 14227.9678955078 kl_train: -0.0796647873 mse_train: 0.0748840437 acc_train: 0.3750000000 nll_val: 12763.6682942708 kl_val: -0.0700883915 mse_val: 0.0671772063 acc_val: 0.5000000000 time: 0.5021s
Epoch: 0029 nll_train: 14237.9104003906 kl_train: -0.0760209418 mse_train: 0.0749363707 acc_train: 0.4166666667 nll_val: 12767.7438151042 kl_val: -0.0748707478 mse_val: 0.0671986466 acc_val: 0.5000000000 time: 0.4971s
Epoch: 0030 nll_train: 14229.4001871745 kl_train: -0.0854025646 mse_train: 0.0748915800 acc_train: 0.3750000000 nll_val: 12744.3929036458 kl_val: -0.0622753091 mse_val: 0.0670757492 acc_val: 0.5000000000 time: 0.5005s
Epoch: 0031 nll_train: 14239.5805664062 kl_train: -0.0858014608 mse_train: 0.0749451608 acc_train: 0.3958333333 nll_val: 12788.7125651042 kl_val: -0.0729784382 mse_val: 0.0673090170 acc_val: 0.5000000000 time: 0.5039s
Epoch: 0032 nll_train: 14217.7931315104 kl_train: -0.0909631740 mse_train: 0.0748304908 acc_train: 0.4166666667 nll_val: 12754.8951822917 kl_val: -0.0794460600 mse_val: 0.0671310251 acc_val: 0.5000000000 time: 0.4972s
Epoch: 0033 nll_train: 14212.7271321615 kl_train: -0.0934679500 mse_train: 0.0748038283 acc_train: 0.3750000000 nll_val: 12763.4651692708 kl_val: -0.0811928585 mse_val: 0.0671761309 acc_val: 0.5000000000 time: 0.4996s
Epoch: 0034 nll_train: 14222.3494873047 kl_train: -0.0943548460 mse_train: 0.0748544702 acc_train: 0.3958333333 nll_val: 12778.3792317708 kl_val: -0.0768265575 mse_val: 0.0672546253 acc_val: 0.5000000000 time: 0.4996s
Epoch: 0035 nll_train: 14214.6451416016 kl_train: -0.0920063124 mse_train: 0.0748139210 acc_train: 0.4166666667 nll_val: 12815.0276692708 kl_val: -0.0807412490 mse_val: 0.0674475084 acc_val: 0.5000000000 time: 0.4995s
Epoch: 0036 nll_train: 14210.8696695964 kl_train: -0.0998134104 mse_train: 0.0747940528 acc_train: 0.4166666667 nll_val: 12804.2278645833 kl_val: -0.0896787792 mse_val: 0.0673906704 acc_val: 0.5000000000 time: 0.5015s
Epoch: 0037 nll_train: 14200.1592610677 kl_train: -0.1029676430 mse_train: 0.0747376814 acc_train: 0.3958333333 nll_val: 12786.1302083333 kl_val: -0.0874853631 mse_val: 0.0672954222 acc_val: 0.5000000000 time: 0.5075s
Epoch: 0038 nll_train: 14204.8955078125 kl_train: -0.0992537720 mse_train: 0.0747626079 acc_train: 0.3958333333 nll_val: 12796.4726562500 kl_val: -0.0837923338 mse_val: 0.0673498536 acc_val: 0.5000000000 time: 0.4998s
Epoch: 0039 nll_train: 14213.7858072917 kl_train: -0.0943656576 mse_train: 0.0748094007 acc_train: 0.3958333333 nll_val: 12800.9918619792 kl_val: -0.0783440024 mse_val: 0.0673736408 acc_val: 0.5000000000 time: 0.5003s
Epoch: 0040 nll_train: 14209.2613932292 kl_train: -0.0990119707 mse_train: 0.0747855874 acc_train: 0.3750000000 nll_val: 12760.3489583333 kl_val: -0.0903891077 mse_val: 0.0671597272 acc_val: 0.5000000000 time: 0.5119s
Epoch: 0041 nll_train: 14199.2095133464 kl_train: -0.0971044423 mse_train: 0.0747326827 acc_train: 0.3750000000 nll_val: 12769.3473307292 kl_val: -0.0911398207 mse_val: 0.0672070906 acc_val: 0.5000000000 time: 0.4985s
Epoch: 0042 nll_train: 14203.4495442708 kl_train: -0.0999805157 mse_train: 0.0747549984 acc_train: 0.3541666667 nll_val: 12766.6585286458 kl_val: -0.0773312772 mse_val: 0.0671929394 acc_val: 0.5000000000 time: 0.5002s
Epoch: 0043 nll_train: 14202.6965332031 kl_train: -0.0998879367 mse_train: 0.0747510353 acc_train: 0.4166666667 nll_val: 12807.1529947917 kl_val: -0.0995447785 mse_val: 0.0674060633 acc_val: 0.5000000000 time: 0.4997s
Epoch: 0044 nll_train: 14184.6156819661 kl_train: -0.1156351632 mse_train: 0.0746558718 acc_train: 0.4375000000 nll_val: 12857.6624348958 kl_val: -0.0982518742 mse_val: 0.0676719025 acc_val: 0.5000000000 time: 0.5009s
Epoch: 0045 nll_train: 14190.3277180990 kl_train: -0.1089044875 mse_train: 0.0746859349 acc_train: 0.4166666667 nll_val: 12844.4235026042 kl_val: -0.0925837706 mse_val: 0.0676022296 acc_val: 0.5000000000 time: 0.4987s
Epoch: 0046 nll_train: 14191.2392985026 kl_train: -0.1121341415 mse_train: 0.0746907340 acc_train: 0.3750000000 nll_val: 12858.1194661458 kl_val: -0.1060311124 mse_val: 0.0676743140 acc_val: 0.5000000000 time: 0.4992s
Epoch: 0047 nll_train: 14179.6302897135 kl_train: -0.1212856198 mse_train: 0.0746296324 acc_train: 0.3333333333 nll_val: 12826.1041666667 kl_val: -0.1044998194 mse_val: 0.0675058117 acc_val: 0.5000000000 time: 0.5006s
Epoch: 0048 nll_train: 14186.9189453125 kl_train: -0.1193197190 mse_train: 0.0746679964 acc_train: 0.3958333333 nll_val: 12842.7962239583 kl_val: -0.1060836588 mse_val: 0.0675936664 acc_val: 0.5000000000 time: 0.5015s
Epoch: 0049 nll_train: 14183.5076090495 kl_train: -0.1239889363 mse_train: 0.0746500408 acc_train: 0.3958333333 nll_val: 12864.8489583333 kl_val: -0.1044387420 mse_val: 0.0677097291 acc_val: 0.5000000000 time: 0.5005s
Epoch: 0050 nll_train: 14191.2924397786 kl_train: -0.1216992444 mse_train: 0.0746910134 acc_train: 0.3333333333 nll_val: 12869.0582682292 kl_val: -0.0965335692 mse_val: 0.0677318871 acc_val: 0.5000000000 time: 0.4997s
Epoch: 0051 nll_train: 14196.4916178385 kl_train: -0.1073655014 mse_train: 0.0747183776 acc_train: 0.3750000000 nll_val: 12824.5908203125 kl_val: -0.0882454986 mse_val: 0.0674978470 acc_val: 0.5000000000 time: 0.4960s
Epoch: 0052 nll_train: 14177.2827962240 kl_train: -0.1106276816 mse_train: 0.0746172772 acc_train: 0.4375000000 nll_val: 12897.0576171875 kl_val: -0.0945138633 mse_val: 0.0678792497 acc_val: 0.5000000000 time: 0.4993s
Epoch: 0053 nll_train: 14185.8677978516 kl_train: -0.1112939759 mse_train: 0.0746624634 acc_train: 0.3750000000 nll_val: 12867.3118489583 kl_val: -0.0900643145 mse_val: 0.0677226956 acc_val: 0.5000000000 time: 0.5167s
Epoch: 0054 nll_train: 14168.7128092448 kl_train: -0.1153742590 mse_train: 0.0745721736 acc_train: 0.4375000000 nll_val: 12833.3157552083 kl_val: -0.0957644408 mse_val: 0.0675437624 acc_val: 0.5000000000 time: 0.4998s
Epoch: 0055 nll_train: 14176.4501546224 kl_train: -0.1169741116 mse_train: 0.0746128984 acc_train: 0.4375000000 nll_val: 12924.5706380208 kl_val: -0.1004441902 mse_val: 0.0680240567 acc_val: 0.5000000000 time: 0.4988s
Epoch: 0056 nll_train: 14180.1979980469 kl_train: -0.1253774706 mse_train: 0.0746326211 acc_train: 0.3750000000 nll_val: 12884.1793619792 kl_val: -0.0989210407 mse_val: 0.0678114643 acc_val: 0.5000000000 time: 0.5021s
Epoch: 0057 nll_train: 14170.3480631510 kl_train: -0.1278769265 mse_train: 0.0745807781 acc_train: 0.3750000000 nll_val: 12846.2802734375 kl_val: -0.1030729190 mse_val: 0.0676119998 acc_val: 0.5000000000 time: 0.5010s
Epoch: 0058 nll_train: 14168.7080078125 kl_train: -0.1289932827 mse_train: 0.0745721478 acc_train: 0.3541666667 nll_val: 12802.0162760417 kl_val: -0.1044376343 mse_val: 0.0673790326 acc_val: 0.5000000000 time: 0.4999s
Epoch: 0059 nll_train: 14156.5528971354 kl_train: -0.1268599487 mse_train: 0.0745081740 acc_train: 0.3958333333 nll_val: 12873.8681640625 kl_val: -0.1125223612 mse_val: 0.0677571967 acc_val: 0.5000000000 time: 0.4944s
Epoch: 0060 nll_train: 14193.6175537109 kl_train: -0.1265573191 mse_train: 0.0747032498 acc_train: 0.3958333333 nll_val: 12872.8437500000 kl_val: -0.0964329690 mse_val: 0.0677518050 acc_val: 0.5000000000 time: 0.5000s
Epoch: 0061 nll_train: 14153.8769531250 kl_train: -0.1120346443 mse_train: 0.0744940909 acc_train: 0.3750000000 nll_val: 12851.5488281250 kl_val: -0.1027307461 mse_val: 0.0676397309 acc_val: 0.5000000000 time: 0.4970s
Epoch: 0062 nll_train: 14176.4407552083 kl_train: -0.1273615606 mse_train: 0.0746128469 acc_train: 0.3958333333 nll_val: 12828.5289713542 kl_val: -0.1160667563 mse_val: 0.0675185695 acc_val: 0.5000000000 time: 0.5094s
Epoch: 0063 nll_train: 14155.5380452474 kl_train: -0.1330829135 mse_train: 0.0745028326 acc_train: 0.4166666667 nll_val: 12862.9202473958 kl_val: -0.1098805641 mse_val: 0.0676995789 acc_val: 0.5000000000 time: 0.4962s
Epoch: 0064 nll_train: 14160.7111002604 kl_train: -0.1453396278 mse_train: 0.0745300585 acc_train: 0.2916666667 nll_val: 12880.0221354167 kl_val: -0.1193668395 mse_val: 0.0677895844 acc_val: 0.5000000000 time: 0.4953s
Epoch: 0065 nll_train: 14145.9641113281 kl_train: -0.1415340286 mse_train: 0.0744524437 acc_train: 0.4166666667 nll_val: 12886.5026041667 kl_val: -0.1258590867 mse_val: 0.0678236956 acc_val: 0.5000000000 time: 0.4963s
Epoch: 0066 nll_train: 14159.3399251302 kl_train: -0.1417025129 mse_train: 0.0745228417 acc_train: 0.3958333333 nll_val: 12889.8603515625 kl_val: -0.1099177897 mse_val: 0.0678413659 acc_val: 0.5000000000 time: 0.5007s
Epoch: 0067 nll_train: 14147.6823730469 kl_train: -0.1437460010 mse_train: 0.0744614862 acc_train: 0.3750000000 nll_val: 12854.3144531250 kl_val: -0.1277926837 mse_val: 0.0676542843 acc_val: 0.5000000000 time: 0.4983s
Epoch: 0068 nll_train: 14142.6890055339 kl_train: -0.1511827658 mse_train: 0.0744352058 acc_train: 0.3958333333 nll_val: 12952.5849609375 kl_val: -0.1159398307 mse_val: 0.0681715012 acc_val: 0.5000000000 time: 0.4987s
Epoch: 0069 nll_train: 14145.9239501953 kl_train: -0.1540951704 mse_train: 0.0744522326 acc_train: 0.3541666667 nll_val: 12888.5976562500 kl_val: -0.1097383574 mse_val: 0.0678347250 acc_val: 0.5000000000 time: 0.4997s
Epoch: 0070 nll_train: 14141.8430175781 kl_train: -0.1416796735 mse_train: 0.0744307513 acc_train: 0.4166666667 nll_val: 12998.5276692708 kl_val: -0.1250262683 mse_val: 0.0684133023 acc_val: 0.5000000000 time: 0.4995s
Epoch: 0071 nll_train: 14144.6447753906 kl_train: -0.1506523102 mse_train: 0.0744454963 acc_train: 0.4166666667 nll_val: 12926.3274739583 kl_val: -0.1232617895 mse_val: 0.0680333003 acc_val: 0.5000000000 time: 0.4973s
Epoch: 0072 nll_train: 14145.6625569661 kl_train: -0.1371115847 mse_train: 0.0744508555 acc_train: 0.3958333333 nll_val: 12903.6022135417 kl_val: -0.1101952965 mse_val: 0.0679136937 acc_val: 0.5000000000 time: 0.5079s
Epoch: 0073 nll_train: 14143.9086914062 kl_train: -0.1381322493 mse_train: 0.0744416236 acc_train: 0.3750000000 nll_val: 12934.6165364583 kl_val: -0.1316060921 mse_val: 0.0680769260 acc_val: 0.5000000000 time: 0.4999s
Epoch: 0074 nll_train: 14118.2435302734 kl_train: -0.1589452829 mse_train: 0.0743065470 acc_train: 0.3958333333 nll_val: 12931.2978515625 kl_val: -0.1461406151 mse_val: 0.0680594593 acc_val: 0.5000000000 time: 0.5000s
Epoch: 0075 nll_train: 14120.9310709635 kl_train: -0.1663499875 mse_train: 0.0743206898 acc_train: 0.4583333333 nll_val: 12943.7805989583 kl_val: -0.1152702322 mse_val: 0.0681251610 acc_val: 0.5000000000 time: 0.4948s
Epoch: 0076 nll_train: 14138.6135253906 kl_train: -0.1435266171 mse_train: 0.0744137547 acc_train: 0.3958333333 nll_val: 12908.8557942708 kl_val: -0.1239780337 mse_val: 0.0679413428 acc_val: 0.5000000000 time: 0.4987s
Epoch: 0077 nll_train: 14135.3344319661 kl_train: -0.1529538995 mse_train: 0.0743964982 acc_train: 0.3333333333 nll_val: 12957.6194661458 kl_val: -0.1267564148 mse_val: 0.0681979979 acc_val: 0.5000000000 time: 0.4967s
Epoch: 0078 nll_train: 14119.7049560547 kl_train: -0.1477154816 mse_train: 0.0743142376 acc_train: 0.3541666667 nll_val: 12992.0791015625 kl_val: -0.1226211637 mse_val: 0.0683793599 acc_val: 0.5000000000 time: 0.5075s
Epoch: 0079 nll_train: 14123.2698160807 kl_train: -0.1445591999 mse_train: 0.0743329978 acc_train: 0.4375000000 nll_val: 12973.8990885417 kl_val: -0.1279970581 mse_val: 0.0682836746 acc_val: 0.5000000000 time: 0.5554s
Epoch: 0080 nll_train: 14122.6138102214 kl_train: -0.1679193741 mse_train: 0.0743295473 acc_train: 0.4166666667 nll_val: 12983.5914713542 kl_val: -0.1287773450 mse_val: 0.0683346912 acc_val: 0.5000000000 time: 0.5504s
Epoch: 0081 nll_train: 14132.1963704427 kl_train: -0.1537571053 mse_train: 0.0743799821 acc_train: 0.4583333333 nll_val: 12973.9003906250 kl_val: -0.1156093900 mse_val: 0.0682836846 acc_val: 0.5000000000 time: 0.4972s
Epoch: 0082 nll_train: 14125.6730550130 kl_train: -0.1529126804 mse_train: 0.0743456489 acc_train: 0.4166666667 nll_val: 13010.8857421875 kl_val: -0.1349606862 mse_val: 0.0684783459 acc_val: 0.5000000000 time: 0.5017s
Epoch: 0083 nll_train: 14126.0026041667 kl_train: -0.1479340531 mse_train: 0.0743473837 acc_train: 0.3958333333 nll_val: 12972.2480468750 kl_val: -0.1130288939 mse_val: 0.0682749848 acc_val: 0.5000000000 time: 0.4983s
Epoch: 0084 nll_train: 14133.8525797526 kl_train: -0.1601981285 mse_train: 0.0743886977 acc_train: 0.3958333333 nll_val: 12969.6113281250 kl_val: -0.1233043373 mse_val: 0.0682611118 acc_val: 0.5000000000 time: 0.5186s
Epoch: 0085 nll_train: 14121.9985758464 kl_train: -0.1469955351 mse_train: 0.0743263097 acc_train: 0.3958333333 nll_val: 12985.6917317708 kl_val: -0.1169097275 mse_val: 0.0683457454 acc_val: 0.5000000000 time: 0.5060s
Epoch: 0086 nll_train: 14106.1302083333 kl_train: -0.1560791650 mse_train: 0.0742427905 acc_train: 0.3750000000 nll_val: 13060.7851562500 kl_val: -0.1371119320 mse_val: 0.0687409739 acc_val: 0.5000000000 time: 0.5006s
Epoch: 0087 nll_train: 14109.9016927083 kl_train: -0.1466169789 mse_train: 0.0742626414 acc_train: 0.3958333333 nll_val: 13006.6458333333 kl_val: -0.1311484526 mse_val: 0.0684560314 acc_val: 0.5000000000 time: 0.4986s
Epoch: 0088 nll_train: 14118.2239583333 kl_train: -0.1492709896 mse_train: 0.0743064427 acc_train: 0.4583333333 nll_val: 13015.3779296875 kl_val: -0.1116592288 mse_val: 0.0685019915 acc_val: 0.5000000000 time: 0.5188s
Epoch: 0089 nll_train: 14103.6588541667 kl_train: -0.1520810959 mse_train: 0.0742297834 acc_train: 0.3750000000 nll_val: 13065.4550781250 kl_val: -0.1408800135 mse_val: 0.0687655533 acc_val: 0.5000000000 time: 0.5074s
Epoch: 0090 nll_train: 14100.8693033854 kl_train: -0.1738891328 mse_train: 0.0742151023 acc_train: 0.3958333333 nll_val: 13125.5227864583 kl_val: -0.1536548883 mse_val: 0.0690817013 acc_val: 0.5000000000 time: 0.4979s
Epoch: 0091 nll_train: 14108.1549886068 kl_train: -0.1604706446 mse_train: 0.0742534477 acc_train: 0.3958333333 nll_val: 12936.0201822917 kl_val: -0.1320925703 mse_val: 0.0680843145 acc_val: 0.5000000000 time: 0.5030s
Epoch: 0092 nll_train: 14105.7782389323 kl_train: -0.1716093129 mse_train: 0.0742409384 acc_train: 0.3958333333 nll_val: 13094.4570312500 kl_val: -0.1481550485 mse_val: 0.0689181909 acc_val: 0.5000000000 time: 0.4974s
Epoch: 0093 nll_train: 14110.5948486328 kl_train: -0.1625786976 mse_train: 0.0742662900 acc_train: 0.3958333333 nll_val: 13098.5813802083 kl_val: -0.1530946543 mse_val: 0.0689399044 acc_val: 0.5000000000 time: 0.5025s
Epoch: 0094 nll_train: 14103.7551269531 kl_train: -0.1643619418 mse_train: 0.0742302919 acc_train: 0.4583333333 nll_val: 13102.7291666667 kl_val: -0.1365529895 mse_val: 0.0689617346 acc_val: 0.5000000000 time: 0.5018s
Epoch: 0095 nll_train: 14101.9922281901 kl_train: -0.1506633870 mse_train: 0.0742210125 acc_train: 0.4166666667 nll_val: 13017.7054036458 kl_val: -0.1536740412 mse_val: 0.0685142353 acc_val: 0.5000000000 time: 0.4998s
Epoch: 0096 nll_train: 14094.9055175781 kl_train: -0.1675770537 mse_train: 0.0741837136 acc_train: 0.4166666667 nll_val: 13053.6861979167 kl_val: -0.1557628761 mse_val: 0.0687036067 acc_val: 0.5000000000 time: 0.4985s
Epoch: 0097 nll_train: 14112.0244954427 kl_train: -0.1615204588 mse_train: 0.0742738151 acc_train: 0.4166666667 nll_val: 13054.5608723958 kl_val: -0.1504553606 mse_val: 0.0687082162 acc_val: 0.5000000000 time: 0.4996s
Epoch: 0098 nll_train: 14096.0482991536 kl_train: -0.1464846429 mse_train: 0.0741897291 acc_train: 0.3750000000 nll_val: 13058.2542317708 kl_val: -0.1333843519 mse_val: 0.0687276497 acc_val: 0.5000000000 time: 0.5002s
Epoch: 0099 nll_train: 14092.0896402995 kl_train: -0.1557092757 mse_train: 0.0741688941 acc_train: 0.3541666667 nll_val: 13065.8245442708 kl_val: -0.1630408168 mse_val: 0.0687675004 acc_val: 0.5000000000 time: 0.4985s
Epoch: 0100 nll_train: 14093.8757731120 kl_train: -0.1690048700 mse_train: 0.0741782927 acc_train: 0.4375000000 nll_val: 13114.2783203125 kl_val: -0.1329502116 mse_val: 0.0690225189 acc_val: 0.5000000000 time: 0.4985s
Epoch: 0101 nll_train: 14095.8015950521 kl_train: -0.1555583247 mse_train: 0.0741884311 acc_train: 0.4166666667 nll_val: 12989.7952473958 kl_val: -0.1255982692 mse_val: 0.0683673397 acc_val: 0.5000000000 time: 0.5081s
Epoch: 0102 nll_train: 14110.6501057943 kl_train: -0.1610557539 mse_train: 0.0742665802 acc_train: 0.3750000000 nll_val: 13133.1891276042 kl_val: -0.1594625264 mse_val: 0.0691220487 acc_val: 0.5000000000 time: 0.5116s
Epoch: 0103 nll_train: 14099.9230550130 kl_train: -0.1566504048 mse_train: 0.0742101222 acc_train: 0.3958333333 nll_val: 13049.1432291667 kl_val: -0.1368261923 mse_val: 0.0686797003 acc_val: 0.5000000000 time: 0.5001s
Epoch: 0104 nll_train: 14096.8750406901 kl_train: -0.1584940891 mse_train: 0.0741940793 acc_train: 0.4166666667 nll_val: 13056.9850260417 kl_val: -0.1705816835 mse_val: 0.0687209815 acc_val: 0.5000000000 time: 0.4986s
Epoch: 0105 nll_train: 14089.4083251953 kl_train: -0.1728945983 mse_train: 0.0741547812 acc_train: 0.3958333333 nll_val: 13047.1471354167 kl_val: -0.1546860288 mse_val: 0.0686691975 acc_val: 0.5000000000 time: 0.5004s
Epoch: 0106 nll_train: 14084.6802164714 kl_train: -0.1571705459 mse_train: 0.0741298959 acc_train: 0.3958333333 nll_val: 13071.9163411458 kl_val: -0.1526262114 mse_val: 0.0687995578 acc_val: 0.5000000000 time: 0.5085s
Epoch: 0107 nll_train: 14081.0281982422 kl_train: -0.1660715869 mse_train: 0.0741106756 acc_train: 0.4375000000 nll_val: 13093.6735026042 kl_val: -0.1367718329 mse_val: 0.0689140732 acc_val: 0.5000000000 time: 0.5091s
Epoch: 0108 nll_train: 14086.4685465495 kl_train: -0.1666061319 mse_train: 0.0741393085 acc_train: 0.3541666667 nll_val: 13054.4537760417 kl_val: -0.1515222639 mse_val: 0.0687076524 acc_val: 0.5000000000 time: 0.5002s
Epoch: 0109 nll_train: 14089.3862304688 kl_train: -0.1537556484 mse_train: 0.0741546654 acc_train: 0.3958333333 nll_val: 13003.9778645833 kl_val: -0.1406319390 mse_val: 0.0684419870 acc_val: 0.5000000000 time: 0.5003s
Epoch: 0110 nll_train: 14084.9989013672 kl_train: -0.1600685505 mse_train: 0.0741315732 acc_train: 0.4375000000 nll_val: 13146.8486328125 kl_val: -0.1799781422 mse_val: 0.0691939394 acc_val: 0.5000000000 time: 0.5001s
Epoch: 0111 nll_train: 14071.8271891276 kl_train: -0.1604343684 mse_train: 0.0740622478 acc_train: 0.4166666667 nll_val: 13048.5260416667 kl_val: -0.1453335087 mse_val: 0.0686764543 acc_val: 0.5000000000 time: 0.4965s
Epoch: 0112 nll_train: 14070.7936604818 kl_train: -0.1698026884 mse_train: 0.0740568088 acc_train: 0.4166666667 nll_val: 13146.7682291667 kl_val: -0.1622706205 mse_val: 0.0691935172 acc_val: 0.5000000000 time: 0.5031s
Epoch: 0113 nll_train: 14079.6807861328 kl_train: -0.1584220345 mse_train: 0.0741035842 acc_train: 0.4166666667 nll_val: 13098.0087890625 kl_val: -0.1613284896 mse_val: 0.0689368819 acc_val: 0.5000000000 time: 0.5125s
Epoch: 0114 nll_train: 14071.4758300781 kl_train: -0.1689748606 mse_train: 0.0740604000 acc_train: 0.3958333333 nll_val: 13117.0309244792 kl_val: -0.1595917294 mse_val: 0.0690370053 acc_val: 0.5000000000 time: 0.4986s
Epoch: 0115 nll_train: 14072.0973307292 kl_train: -0.1626326381 mse_train: 0.0740636714 acc_train: 0.3958333333 nll_val: 13134.5774739583 kl_val: -0.1579258045 mse_val: 0.0691293553 acc_val: 0.5000000000 time: 0.4994s
Epoch: 0116 nll_train: 14081.6654052734 kl_train: -0.1621943967 mse_train: 0.0741140296 acc_train: 0.4166666667 nll_val: 13122.0973307292 kl_val: -0.1540000290 mse_val: 0.0690636734 acc_val: 0.5000000000 time: 0.4972s
Epoch: 0117 nll_train: 14085.9028320312 kl_train: -0.1750236649 mse_train: 0.0741363307 acc_train: 0.4166666667 nll_val: 13066.4964192708 kl_val: -0.1450808793 mse_val: 0.0687710345 acc_val: 0.5000000000 time: 0.4995s
Epoch: 0118 nll_train: 14079.0629882812 kl_train: -0.1692585001 mse_train: 0.0741003317 acc_train: 0.3541666667 nll_val: 13235.8395182292 kl_val: -0.1775013109 mse_val: 0.0696623127 acc_val: 0.5000000000 time: 0.4981s
Epoch: 0119 nll_train: 14070.4232177734 kl_train: -0.1719015340 mse_train: 0.0740548602 acc_train: 0.3958333333 nll_val: 13193.5081380208 kl_val: -0.1664391011 mse_val: 0.0694395130 acc_val: 0.5000000000 time: 0.5156s
Epoch: 0120 nll_train: 14092.1849365234 kl_train: -0.1548940471 mse_train: 0.0741693955 acc_train: 0.5000000000 nll_val: 13031.2239583333 kl_val: -0.1596202006 mse_val: 0.0685853908 acc_val: 0.5000000000 time: 0.5124s
Epoch: 0121 nll_train: 14105.2827555339 kl_train: -0.1610762328 mse_train: 0.0742383317 acc_train: 0.4583333333 nll_val: 12970.0341796875 kl_val: -0.1567408691 mse_val: 0.0682633395 acc_val: 0.5000000000 time: 0.5006s
Epoch: 0122 nll_train: 14106.6383870443 kl_train: -0.1709613055 mse_train: 0.0742454653 acc_train: 0.3750000000 nll_val: 13107.4059244792 kl_val: -0.1613209993 mse_val: 0.0689863463 acc_val: 0.5000000000 time: 0.4983s
Epoch: 0123 nll_train: 14082.0159098307 kl_train: -0.1615242919 mse_train: 0.0741158746 acc_train: 0.3958333333 nll_val: 13080.2529296875 kl_val: -0.1692870359 mse_val: 0.0688434318 acc_val: 0.5000000000 time: 0.4997s
Epoch: 0124 nll_train: 14065.6947835286 kl_train: -0.1618913173 mse_train: 0.0740299740 acc_train: 0.3958333333 nll_val: 13144.2695312500 kl_val: -0.1795147508 mse_val: 0.0691803619 acc_val: 0.5000000000 time: 0.4984s
Epoch: 0125 nll_train: 14064.4710286458 kl_train: -0.1659143964 mse_train: 0.0740235317 acc_train: 0.4583333333 nll_val: 13153.8958333333 kl_val: -0.1868230253 mse_val: 0.0692310308 acc_val: 0.5000000000 time: 0.5020s
Epoch: 0126 nll_train: 14061.7515055339 kl_train: -0.1596096509 mse_train: 0.0740092179 acc_train: 0.4166666667 nll_val: 13093.3697916667 kl_val: -0.1372894943 mse_val: 0.0689124689 acc_val: 0.5000000000 time: 0.5043s
Epoch: 0127 nll_train: 14065.7231852214 kl_train: -0.1603507769 mse_train: 0.0740301233 acc_train: 0.3958333333 nll_val: 13194.1643880208 kl_val: -0.1616113186 mse_val: 0.0694429701 acc_val: 0.5000000000 time: 0.4997s
Epoch: 0128 nll_train: 14086.4936116536 kl_train: -0.1562006807 mse_train: 0.0741394401 acc_train: 0.3750000000 nll_val: 13140.2636718750 kl_val: -0.1808438102 mse_val: 0.0691592793 acc_val: 0.5000000000 time: 0.4978s
Epoch: 0129 nll_train: 14073.9899495443 kl_train: -0.1762351670 mse_train: 0.0740736310 acc_train: 0.4375000000 nll_val: 13166.2011718750 kl_val: -0.1494032443 mse_val: 0.0692957913 acc_val: 0.5000000000 time: 0.4971s
Epoch: 0130 nll_train: 14066.7551269531 kl_train: -0.1695537426 mse_train: 0.0740355545 acc_train: 0.4583333333 nll_val: 13232.7871093750 kl_val: -0.1525709083 mse_val: 0.0696462442 acc_val: 0.5000000000 time: 0.4997s
Epoch: 0131 nll_train: 14067.5214436849 kl_train: -0.1533294478 mse_train: 0.0740395853 acc_train: 0.4791666667 nll_val: 13110.3512369792 kl_val: -0.1631560624 mse_val: 0.0690018410 acc_val: 0.5000000000 time: 0.5010s
Epoch: 0132 nll_train: 14064.6392822266 kl_train: -0.1649337470 mse_train: 0.0740244184 acc_train: 0.4791666667 nll_val: 13130.4680989583 kl_val: -0.2054764281 mse_val: 0.0691077237 acc_val: 0.5000000000 time: 0.5008s
Epoch: 0133 nll_train: 14053.3953043620 kl_train: -0.1804313296 mse_train: 0.0739652369 acc_train: 0.4375000000 nll_val: 13127.6035156250 kl_val: -0.1684615612 mse_val: 0.0690926512 acc_val: 0.5000000000 time: 0.5009s
Epoch: 0134 nll_train: 14064.3849690755 kl_train: -0.1647129341 mse_train: 0.0740230779 acc_train: 0.3750000000 nll_val: 13155.0875651042 kl_val: -0.1577624232 mse_val: 0.0692373042 acc_val: 0.5000000000 time: 0.5041s
Epoch: 0135 nll_train: 14073.5529785156 kl_train: -0.1620684794 mse_train: 0.0740713309 acc_train: 0.3958333333 nll_val: 13062.0097656250 kl_val: -0.1661247512 mse_val: 0.0687474161 acc_val: 0.5000000000 time: 0.5177s
Epoch: 0136 nll_train: 14060.3036295573 kl_train: -0.1664308229 mse_train: 0.0740015979 acc_train: 0.3541666667 nll_val: 13138.0364583333 kl_val: -0.1750446906 mse_val: 0.0691475595 acc_val: 0.5000000000 time: 0.4996s
Epoch: 0137 nll_train: 14071.2107747396 kl_train: -0.1487241027 mse_train: 0.0740590049 acc_train: 0.4375000000 nll_val: 13173.4573567708 kl_val: -0.1663619429 mse_val: 0.0693339879 acc_val: 0.5000000000 time: 0.5121s
Epoch: 0138 nll_train: 14071.2255452474 kl_train: -0.1591240525 mse_train: 0.0740590825 acc_train: 0.4791666667 nll_val: 13071.3984375000 kl_val: -0.1775222570 mse_val: 0.0687968334 acc_val: 0.5000000000 time: 0.4993s
Epoch: 0139 nll_train: 14075.4385172526 kl_train: -0.1761059544 mse_train: 0.0740812548 acc_train: 0.3958333333 nll_val: 13098.4098307292 kl_val: -0.1723973751 mse_val: 0.0689389979 acc_val: 0.5000000000 time: 0.4991s
Epoch: 0140 nll_train: 14056.4357910156 kl_train: -0.1667559201 mse_train: 0.0739812429 acc_train: 0.3958333333 nll_val: 13169.2379557292 kl_val: -0.1646410724 mse_val: 0.0693117778 acc_val: 0.5000000000 time: 0.4991s
Epoch: 0141 nll_train: 14056.9534505208 kl_train: -0.1766540607 mse_train: 0.0739839673 acc_train: 0.4166666667 nll_val: 13158.8997395833 kl_val: -0.1870779296 mse_val: 0.0692573637 acc_val: 0.5000000000 time: 0.5020s
Epoch: 0142 nll_train: 14066.2081298828 kl_train: -0.1600575453 mse_train: 0.0740326752 acc_train: 0.4791666667 nll_val: 13148.4534505208 kl_val: -0.1544295649 mse_val: 0.0692023858 acc_val: 0.5000000000 time: 0.4998s
Epoch: 0143 nll_train: 14085.8485514323 kl_train: -0.1619329316 mse_train: 0.0741360451 acc_train: 0.3958333333 nll_val: 13124.8570963542 kl_val: -0.1760724485 mse_val: 0.0690781946 acc_val: 0.5000000000 time: 0.4994s
Epoch: 0144 nll_train: 14076.9861246745 kl_train: -0.1872901513 mse_train: 0.0740894008 acc_train: 0.3958333333 nll_val: 13031.9296875000 kl_val: -0.1750964473 mse_val: 0.0685890988 acc_val: 0.5000000000 time: 0.5001s
Epoch: 0145 nll_train: 14069.9236653646 kl_train: -0.1742167692 mse_train: 0.0740522311 acc_train: 0.3958333333 nll_val: 13211.0937500000 kl_val: -0.1800950666 mse_val: 0.0695320666 acc_val: 0.5000000000 time: 0.5014s
Epoch: 0146 nll_train: 14065.4544677734 kl_train: -0.1803111583 mse_train: 0.0740287108 acc_train: 0.4375000000 nll_val: 13066.3567708333 kl_val: -0.1592153907 mse_val: 0.0687702969 acc_val: 0.5000000000 time: 0.5179s
Epoch: 0147 nll_train: 14053.9633382161 kl_train: -0.1657459869 mse_train: 0.0739682289 acc_train: 0.4166666667 nll_val: 13219.8245442708 kl_val: -0.1794224530 mse_val: 0.0695780242 acc_val: 0.5000000000 time: 0.5089s
Epoch: 0148 nll_train: 14055.2609456380 kl_train: -0.1781010086 mse_train: 0.0739750589 acc_train: 0.3958333333 nll_val: 13242.9632161458 kl_val: -0.1774745286 mse_val: 0.0696998040 acc_val: 0.5000000000 time: 0.4992s
Epoch: 0149 nll_train: 14050.4995930990 kl_train: -0.1599105746 mse_train: 0.0739499979 acc_train: 0.4166666667 nll_val: 13207.9807942708 kl_val: -0.1663242032 mse_val: 0.0695156877 acc_val: 0.5000000000 time: 0.5013s
Epoch: 0150 nll_train: 14043.2690429688 kl_train: -0.1732071669 mse_train: 0.0739119422 acc_train: 0.3958333333 nll_val: 13182.1090494792 kl_val: -0.1727256229 mse_val: 0.0693795159 acc_val: 0.5000000000 time: 0.5009s
Epoch: 0151 nll_train: 14048.6617838542 kl_train: -0.1643001605 mse_train: 0.0739403258 acc_train: 0.4375000000 nll_val: 13237.4095052083 kl_val: -0.1685461054 mse_val: 0.0696705754 acc_val: 0.5000000000 time: 0.5002s
Epoch: 0152 nll_train: 14044.5290527344 kl_train: -0.1684114675 mse_train: 0.0739185732 acc_train: 0.4583333333 nll_val: 13222.8421223958 kl_val: -0.1838541130 mse_val: 0.0695939064 acc_val: 0.5000000000 time: 0.4985s
Epoch: 0153 nll_train: 14056.2530517578 kl_train: -0.1667277801 mse_train: 0.0739802790 acc_train: 0.4166666667 nll_val: 13313.2115885417 kl_val: -0.1646901319 mse_val: 0.0700695316 acc_val: 0.5000000000 time: 0.5005s
Epoch: 0154 nll_train: 14059.3424072266 kl_train: -0.1855262105 mse_train: 0.0739965402 acc_train: 0.3958333333 nll_val: 13288.6429036458 kl_val: -0.1931007604 mse_val: 0.0699402268 acc_val: 0.5000000000 time: 0.5257s
Epoch: 0155 nll_train: 14088.4653727214 kl_train: -0.1779939850 mse_train: 0.0741498185 acc_train: 0.4583333333 nll_val: 13212.2096354167 kl_val: -0.1623672694 mse_val: 0.0695379401 acc_val: 0.5000000000 time: 0.5555s
Epoch: 0156 nll_train: 14062.1196289062 kl_train: -0.1723399353 mse_train: 0.0740111566 acc_train: 0.3750000000 nll_val: 13236.0638020833 kl_val: -0.1933420201 mse_val: 0.0696634923 acc_val: 0.5000000000 time: 0.5044s
Epoch: 0157 nll_train: 14061.3695475260 kl_train: -0.1591013977 mse_train: 0.0740072063 acc_train: 0.4583333333 nll_val: 13222.4192708333 kl_val: -0.1487453779 mse_val: 0.0695916787 acc_val: 0.5000000000 time: 0.5000s
Epoch: 0158 nll_train: 14057.8666992188 kl_train: -0.1549209991 mse_train: 0.0739887723 acc_train: 0.4791666667 nll_val: 13261.8720703125 kl_val: -0.1597959449 mse_val: 0.0697993264 acc_val: 0.5000000000 time: 0.4997s
Epoch: 0159 nll_train: 14070.9636230469 kl_train: -0.1498131018 mse_train: 0.0740577023 acc_train: 0.5208333333 nll_val: 13112.9886067708 kl_val: -0.1434194396 mse_val: 0.0690157289 acc_val: 0.5000000000 time: 0.4998s
Epoch: 0160 nll_train: 14071.2243652344 kl_train: -0.1546891959 mse_train: 0.0740590747 acc_train: 0.4375000000 nll_val: 13251.2854817708 kl_val: -0.1613798141 mse_val: 0.0697436109 acc_val: 0.5000000000 time: 0.4968s
Epoch: 0161 nll_train: 14056.3735758464 kl_train: -0.1650231065 mse_train: 0.0739809141 acc_train: 0.3958333333 nll_val: 13450.2486979167 kl_val: -0.1975029856 mse_val: 0.0707907826 acc_val: 0.5000000000 time: 0.5004s
Epoch: 0162 nll_train: 14047.3924967448 kl_train: -0.1717560942 mse_train: 0.0739336455 acc_train: 0.4583333333 nll_val: 13348.2259114583 kl_val: -0.1771770914 mse_val: 0.0702538192 acc_val: 0.5000000000 time: 0.4981s
Epoch: 0163 nll_train: 14057.2980957031 kl_train: -0.1645602413 mse_train: 0.0739857790 acc_train: 0.3750000000 nll_val: 13320.2620442708 kl_val: -0.1525693287 mse_val: 0.0701066405 acc_val: 0.5000000000 time: 0.4991s
Epoch: 0164 nll_train: 14069.9290364583 kl_train: -0.1550780147 mse_train: 0.0740522590 acc_train: 0.4166666667 nll_val: 13274.1246744792 kl_val: -0.1605153332 mse_val: 0.0698638136 acc_val: 0.5000000000 time: 0.5010s
Epoch: 0165 nll_train: 14063.6514892578 kl_train: -0.1775119076 mse_train: 0.0740192207 acc_train: 0.4166666667 nll_val: 13276.3544921875 kl_val: -0.1781897247 mse_val: 0.0698755483 acc_val: 0.5000000000 time: 0.4978s
Epoch: 0166 nll_train: 14067.0011800130 kl_train: -0.1690760058 mse_train: 0.0740368481 acc_train: 0.3750000000 nll_val: 13273.9446614583 kl_val: -0.2113391062 mse_val: 0.0698628624 acc_val: 0.5000000000 time: 0.4993s
Epoch: 0167 nll_train: 14060.1002197266 kl_train: -0.1575934111 mse_train: 0.0740005278 acc_train: 0.3958333333 nll_val: 13490.1985677083 kl_val: -0.1668562889 mse_val: 0.0710010429 acc_val: 0.5000000000 time: 0.5019s
Epoch: 0168 nll_train: 14051.8601074219 kl_train: -0.1479688160 mse_train: 0.0739571583 acc_train: 0.3958333333 nll_val: 13524.0410156250 kl_val: -0.1490025868 mse_val: 0.0711791615 acc_val: 0.5000000000 time: 0.4997s
Epoch: 0169 nll_train: 14059.9882812500 kl_train: -0.1543971918 mse_train: 0.0739999386 acc_train: 0.4375000000 nll_val: 13086.6536458333 kl_val: -0.1563165238 mse_val: 0.0688771208 acc_val: 0.5000000000 time: 0.5061s
Epoch: 0170 nll_train: 14059.1969401042 kl_train: -0.1577804846 mse_train: 0.0739957746 acc_train: 0.4166666667 nll_val: 13278.2275390625 kl_val: -0.1825539370 mse_val: 0.0698854079 acc_val: 0.5000000000 time: 0.4994s
Epoch: 0171 nll_train: 14054.6413574219 kl_train: -0.1681292018 mse_train: 0.0739717980 acc_train: 0.4791666667 nll_val: 13228.0709635417 kl_val: -0.1575231800 mse_val: 0.0696214214 acc_val: 0.5000000000 time: 0.4985s
Epoch: 0172 nll_train: 14049.6206054688 kl_train: -0.1547706223 mse_train: 0.0739453717 acc_train: 0.4166666667 nll_val: 13347.7255859375 kl_val: -0.1900698443 mse_val: 0.0702511867 acc_val: 0.5000000000 time: 0.4964s
Epoch: 0173 nll_train: 14058.0634358724 kl_train: -0.1566138904 mse_train: 0.0739898073 acc_train: 0.4375000000 nll_val: 13286.2666015625 kl_val: -0.1634191001 mse_val: 0.0699277222 acc_val: 0.5000000000 time: 0.5023s
Epoch: 0174 nll_train: 14058.5032958984 kl_train: -0.1562359628 mse_train: 0.0739921232 acc_train: 0.4375000000 nll_val: 13232.1070963542 kl_val: -0.1924677094 mse_val: 0.0696426680 acc_val: 0.5000000000 time: 0.5008s
Epoch: 0175 nll_train: 14048.3681640625 kl_train: -0.1763649189 mse_train: 0.0739387805 acc_train: 0.3958333333 nll_val: 13300.4850260417 kl_val: -0.1928803921 mse_val: 0.0700025509 acc_val: 0.5000000000 time: 0.4965s
Epoch: 0176 nll_train: 14047.9287516276 kl_train: -0.1802459992 mse_train: 0.0739364674 acc_train: 0.4166666667 nll_val: 13406.8242187500 kl_val: -0.1686665763 mse_val: 0.0705622310 acc_val: 0.5000000000 time: 0.4993s
Epoch: 0177 nll_train: 14049.8920084635 kl_train: -0.1560885900 mse_train: 0.0739467998 acc_train: 0.4375000000 nll_val: 13337.6393229167 kl_val: -0.1770081023 mse_val: 0.0701981038 acc_val: 0.5000000000 time: 0.5004s
Epoch: 0178 nll_train: 14075.1024576823 kl_train: -0.1809504226 mse_train: 0.0740794884 acc_train: 0.3541666667 nll_val: 13280.3958333333 kl_val: -0.1729585876 mse_val: 0.0698968172 acc_val: 0.5000000000 time: 0.4983s
Epoch: 0179 nll_train: 14087.1866861979 kl_train: -0.1674952282 mse_train: 0.0741430884 acc_train: 0.4375000000 nll_val: 13372.2610677083 kl_val: -0.2005760968 mse_val: 0.0703803177 acc_val: 0.5000000000 time: 0.5740s
Epoch: 0180 nll_train: 14074.5897623698 kl_train: -0.1548506521 mse_train: 0.0740767891 acc_train: 0.4583333333 nll_val: 13403.2926432292 kl_val: -0.1382200172 mse_val: 0.0705436443 acc_val: 0.5000000000 time: 0.5034s
Epoch: 0181 nll_train: 14033.8105061849 kl_train: -0.1510028765 mse_train: 0.0738621621 acc_train: 0.4375000000 nll_val: 13299.1604817708 kl_val: -0.1745125850 mse_val: 0.0699955821 acc_val: 0.5000000000 time: 0.5015s
Epoch: 0182 nll_train: 14041.0449625651 kl_train: -0.1710028107 mse_train: 0.0739002367 acc_train: 0.4375000000 nll_val: 13254.0152994792 kl_val: -0.1829734544 mse_val: 0.0697579756 acc_val: 0.5000000000 time: 0.4993s
Epoch: 0183 nll_train: 14070.1727701823 kl_train: -0.1724463652 mse_train: 0.0740535402 acc_train: 0.3958333333 nll_val: 13283.1419270833 kl_val: -0.1566343606 mse_val: 0.0699112689 acc_val: 0.5000000000 time: 0.4984s
Epoch: 0184 nll_train: 14058.6113281250 kl_train: -0.1470268049 mse_train: 0.0739926922 acc_train: 0.3541666667 nll_val: 13348.1106770833 kl_val: -0.1476788471 mse_val: 0.0702532157 acc_val: 0.5000000000 time: 0.4974s
Epoch: 0185 nll_train: 14057.4226074219 kl_train: -0.1617769971 mse_train: 0.0739864344 acc_train: 0.3750000000 nll_val: 13514.5182291667 kl_val: -0.1857735962 mse_val: 0.0711290414 acc_val: 0.5000000000 time: 0.4995s
Epoch: 0186 nll_train: 14054.7417399089 kl_train: -0.1708876723 mse_train: 0.0739723242 acc_train: 0.4166666667 nll_val: 13445.8867187500 kl_val: -0.1779861053 mse_val: 0.0707678199 acc_val: 0.5000000000 time: 0.5035s
Epoch: 0187 nll_train: 14053.4718017578 kl_train: -0.1555116127 mse_train: 0.0739656414 acc_train: 0.4166666667 nll_val: 13377.5852864583 kl_val: -0.1710027506 mse_val: 0.0704083418 acc_val: 0.5000000000 time: 0.4957s
Epoch: 0188 nll_train: 14067.3283284505 kl_train: -0.1724265721 mse_train: 0.0740385711 acc_train: 0.4583333333 nll_val: 13412.7867838542 kl_val: -0.1480479886 mse_val: 0.0705936179 acc_val: 0.5000000000 time: 0.4963s
Epoch: 0189 nll_train: 14065.4007975260 kl_train: -0.1602060827 mse_train: 0.0740284249 acc_train: 0.4375000000 nll_val: 13241.0403645833 kl_val: -0.1850848744 mse_val: 0.0696896836 acc_val: 0.5000000000 time: 0.4960s
Epoch: 0190 nll_train: 14037.8999023438 kl_train: -0.1699717892 mse_train: 0.0738836844 acc_train: 0.4166666667 nll_val: 13304.5986328125 kl_val: -0.2001394928 mse_val: 0.0700242023 acc_val: 0.5000000000 time: 0.4987s
Epoch: 0191 nll_train: 14029.7601318359 kl_train: -0.1622092879 mse_train: 0.0738408441 acc_train: 0.3750000000 nll_val: 13438.3603515625 kl_val: -0.1600999335 mse_val: 0.0707282101 acc_val: 0.5000000000 time: 0.5240s
Epoch: 0192 nll_train: 14076.8955891927 kl_train: -0.1772387892 mse_train: 0.0740889246 acc_train: 0.4375000000 nll_val: 13318.5625000000 kl_val: -0.1551996320 mse_val: 0.0700976948 acc_val: 0.5000000000 time: 0.5095s
Epoch: 0193 nll_train: 14057.8794352214 kl_train: -0.1534154143 mse_train: 0.0739888403 acc_train: 0.4166666667 nll_val: 13114.9612630208 kl_val: -0.1707440615 mse_val: 0.0690261126 acc_val: 0.5000000000 time: 0.5052s
Epoch: 0194 nll_train: 14054.9596354167 kl_train: -0.1528203984 mse_train: 0.0739734713 acc_train: 0.4375000000 nll_val: 13311.6569010417 kl_val: -0.1808933367 mse_val: 0.0700613509 acc_val: 0.5000000000 time: 0.5198s
Epoch: 0195 nll_train: 14059.5627441406 kl_train: -0.1702693890 mse_train: 0.0739976993 acc_train: 0.3958333333 nll_val: 13192.4837239583 kl_val: -0.1702135950 mse_val: 0.0694341213 acc_val: 0.5000000000 time: 0.4994s
Epoch: 0196 nll_train: 14066.7132975260 kl_train: -0.1663830789 mse_train: 0.0740353325 acc_train: 0.4583333333 nll_val: 13136.7399088542 kl_val: -0.1894690494 mse_val: 0.0691407348 acc_val: 0.5000000000 time: 0.4959s
Epoch: 0197 nll_train: 14041.5084635417 kl_train: -0.1709334319 mse_train: 0.0739026768 acc_train: 0.4583333333 nll_val: 13269.4261067708 kl_val: -0.2076174070 mse_val: 0.0698390827 acc_val: 0.5000000000 time: 0.5022s
Epoch: 0198 nll_train: 14046.5196940104 kl_train: -0.1691950904 mse_train: 0.0739290522 acc_train: 0.4166666667 nll_val: 13259.2379557292 kl_val: -0.1381114870 mse_val: 0.0697854583 acc_val: 0.5000000000 time: 0.5010s
Epoch: 0199 nll_train: 14080.6626383464 kl_train: -0.1455692754 mse_train: 0.0741087499 acc_train: 0.4583333333 nll_val: 12770.1315104167 kl_val: -0.1546142896 mse_val: 0.0672112182 acc_val: 0.5000000000 time: 0.4996s
Optimization finished
Best epoch 18
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 14144.9231770833 kl_test: -0.0883900697 mse_test: 0.0744469588 acc_test: 0.6666666667
MSE: [ 0.075466811657 , 0.074763797224 , 0.075022414327 , 0.073273487389 , 0.073345303535 , 0.074196971953 , 0.073912099004 , 0.073868311942 , 0.073104232550 , 0.072976388037 , 0.073045559227 , 0.074183657765 , 0.073904797435 , 0.074660196900 , 0.073639996350 , 0.074362002313 , 0.073365993798 , 0.073061659932 , 0.073496423662 ]
Accuracy for experiment id 3 is 0.5
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 19921.3214111328 kl_train: -0.1725263537 mse_train: 0.1048490629 acc_train: 0.5833333333 nll_val: 17819.6940104167 kl_val: -0.2849475344 mse_val: 0.0937878614 acc_val: 0.5000000000 time: 0.5012s
Best model so far, saving...
Epoch: 0001 nll_train: 15879.5745442708 kl_train: -0.1595781480 mse_train: 0.0835767075 acc_train: 0.5000000000 nll_val: 15681.8821614583 kl_val: -0.1991922855 mse_val: 0.0825362230 acc_val: 0.5000000000 time: 0.5010s
Best model so far, saving...
Epoch: 0002 nll_train: 15199.8074544271 kl_train: -0.2630011855 mse_train: 0.0799989877 acc_train: 0.5000000000 nll_val: 14923.2561848958 kl_val: -0.2094531804 mse_val: 0.0785434569 acc_val: 0.5000000000 time: 0.5058s
Best model so far, saving...
Epoch: 0003 nll_train: 14975.3651936849 kl_train: -0.2213918443 mse_train: 0.0788177112 acc_train: 0.5625000000 nll_val: 14797.3209635417 kl_val: -0.1228000000 mse_val: 0.0778806359 acc_val: 0.5000000000 time: 0.5012s
Best model so far, saving...
Epoch: 0004 nll_train: 14820.5519205729 kl_train: -0.1347503007 mse_train: 0.0780029055 acc_train: 0.6250000000 nll_val: 14559.1507161458 kl_val: -0.0940784377 mse_val: 0.0766271080 acc_val: 0.5000000000 time: 0.5007s
Best model so far, saving...
Epoch: 0005 nll_train: 14633.0220947266 kl_train: -0.1152015515 mse_train: 0.0770159055 acc_train: 0.6041666667 nll_val: 14336.6259765625 kl_val: -0.0940546580 mse_val: 0.0754559226 acc_val: 0.5000000000 time: 0.5261s
Best model so far, saving...
Epoch: 0006 nll_train: 14601.6888834635 kl_train: -0.0973088583 mse_train: 0.0768509948 acc_train: 0.6041666667 nll_val: 14342.5911458333 kl_val: -0.0726752256 mse_val: 0.0754873181 acc_val: 0.5000000000 time: 0.5006s
Epoch: 0007 nll_train: 14586.3373616536 kl_train: -0.0867662840 mse_train: 0.0767701970 acc_train: 0.5833333333 nll_val: 14389.2737630208 kl_val: -0.0826269065 mse_val: 0.0757330234 acc_val: 0.5000000000 time: 0.4989s
Epoch: 0008 nll_train: 14551.6468098958 kl_train: -0.0874066359 mse_train: 0.0765876169 acc_train: 0.6875000000 nll_val: 14379.3623046875 kl_val: -0.0733816077 mse_val: 0.0756808569 acc_val: 0.5000000000 time: 0.5151s
Epoch: 0009 nll_train: 14550.2183024089 kl_train: -0.0781888569 mse_train: 0.0765800951 acc_train: 0.6666666667 nll_val: 14310.1136067708 kl_val: -0.0805971051 mse_val: 0.0753163820 acc_val: 0.5000000000 time: 0.5101s
Best model so far, saving...
Epoch: 0010 nll_train: 14561.9881591797 kl_train: -0.0791934407 mse_train: 0.0766420431 acc_train: 0.6666666667 nll_val: 14341.1136067708 kl_val: -0.0644992813 mse_val: 0.0754795472 acc_val: 0.5000000000 time: 0.5044s
Epoch: 0011 nll_train: 14534.7588704427 kl_train: -0.0767350195 mse_train: 0.0764987315 acc_train: 0.5208333333 nll_val: 14227.2656250000 kl_val: -0.0698686230 mse_val: 0.0748803429 acc_val: 0.5000000000 time: 0.5008s
Best model so far, saving...
Epoch: 0012 nll_train: 14530.3354085286 kl_train: -0.0724202349 mse_train: 0.0764754503 acc_train: 0.6458333333 nll_val: 14289.9365234375 kl_val: -0.0700853802 mse_val: 0.0752101913 acc_val: 0.5000000000 time: 0.5003s
Epoch: 0013 nll_train: 14523.0758870443 kl_train: -0.0689457334 mse_train: 0.0764372425 acc_train: 0.5833333333 nll_val: 14220.1988932292 kl_val: -0.0803784169 mse_val: 0.0748431546 acc_val: 0.5000000000 time: 0.4968s
Best model so far, saving...
Epoch: 0014 nll_train: 14524.4576009115 kl_train: -0.0758148002 mse_train: 0.0764445163 acc_train: 0.6041666667 nll_val: 14341.7151692708 kl_val: -0.0850308314 mse_val: 0.0754827112 acc_val: 0.5000000000 time: 0.5009s
Epoch: 0015 nll_train: 14520.2605387370 kl_train: -0.0831501003 mse_train: 0.0764224230 acc_train: 0.5416666667 nll_val: 14250.3522135417 kl_val: -0.0850712657 mse_val: 0.0750018532 acc_val: 0.5000000000 time: 0.5006s
Epoch: 0016 nll_train: 14513.7188720703 kl_train: -0.0858636390 mse_train: 0.0763879934 acc_train: 0.6250000000 nll_val: 14337.2451171875 kl_val: -0.0890369117 mse_val: 0.0754591847 acc_val: 0.5000000000 time: 0.4977s
Epoch: 0017 nll_train: 14502.8931070964 kl_train: -0.0843667332 mse_train: 0.0763310166 acc_train: 0.5625000000 nll_val: 14266.6113281250 kl_val: -0.0798801072 mse_val: 0.0750874231 acc_val: 0.5000000000 time: 0.5015s
Epoch: 0018 nll_train: 14501.8425699870 kl_train: -0.0893496014 mse_train: 0.0763254880 acc_train: 0.6041666667 nll_val: 14217.9147135417 kl_val: -0.0909538890 mse_val: 0.0748311318 acc_val: 0.5000000000 time: 0.4993s
Best model so far, saving...
Epoch: 0019 nll_train: 14491.8159586589 kl_train: -0.0940324090 mse_train: 0.0762727164 acc_train: 0.5625000000 nll_val: 14292.9401041667 kl_val: -0.0872234677 mse_val: 0.0752259990 acc_val: 0.5000000000 time: 0.5049s
Epoch: 0020 nll_train: 14485.7397867839 kl_train: -0.0902216667 mse_train: 0.0762407378 acc_train: 0.5625000000 nll_val: 14166.8570963542 kl_val: -0.1031546270 mse_val: 0.0745624080 acc_val: 0.5000000000 time: 0.4997s
Best model so far, saving...
Epoch: 0021 nll_train: 14491.6945393880 kl_train: -0.0933855617 mse_train: 0.0762720758 acc_train: 0.5416666667 nll_val: 14288.4801432292 kl_val: -0.0945775906 mse_val: 0.0752025296 acc_val: 0.5000000000 time: 0.5026s
Epoch: 0022 nll_train: 14483.6033121745 kl_train: -0.0971930965 mse_train: 0.0762294928 acc_train: 0.5625000000 nll_val: 14208.7451171875 kl_val: -0.0898433104 mse_val: 0.0747828645 acc_val: 0.5000000000 time: 0.4957s
Epoch: 0023 nll_train: 14482.7405192057 kl_train: -0.0929341655 mse_train: 0.0762249509 acc_train: 0.6041666667 nll_val: 14348.7646484375 kl_val: -0.0972919613 mse_val: 0.0755198151 acc_val: 0.5000000000 time: 0.4997s
Epoch: 0024 nll_train: 14476.8393147786 kl_train: -0.0868158115 mse_train: 0.0761938929 acc_train: 0.6458333333 nll_val: 14277.0875651042 kl_val: -0.0872926290 mse_val: 0.0751425649 acc_val: 0.5000000000 time: 0.4999s
Epoch: 0025 nll_train: 14467.4503173828 kl_train: -0.0887493192 mse_train: 0.0761444763 acc_train: 0.5833333333 nll_val: 14198.7451171875 kl_val: -0.0867100855 mse_val: 0.0747302348 acc_val: 0.5000000000 time: 0.4992s
Epoch: 0026 nll_train: 14474.4391682943 kl_train: -0.0986934618 mse_train: 0.0761812584 acc_train: 0.5416666667 nll_val: 14308.7815755208 kl_val: -0.0963416149 mse_val: 0.0753093784 acc_val: 0.5000000000 time: 0.4986s
Epoch: 0027 nll_train: 14471.7524007161 kl_train: -0.0977126140 mse_train: 0.0761671181 acc_train: 0.6041666667 nll_val: 14271.1233723958 kl_val: -0.1008479297 mse_val: 0.0751111731 acc_val: 0.5000000000 time: 0.5001s
Epoch: 0028 nll_train: 14471.6553548177 kl_train: -0.0994860325 mse_train: 0.0761666090 acc_train: 0.5625000000 nll_val: 14303.9472656250 kl_val: -0.1033415347 mse_val: 0.0752839347 acc_val: 0.5000000000 time: 0.5164s
Epoch: 0029 nll_train: 14461.6610921224 kl_train: -0.1036833981 mse_train: 0.0761140042 acc_train: 0.4791666667 nll_val: 14316.7200520833 kl_val: -0.1016629413 mse_val: 0.0753511538 acc_val: 0.5000000000 time: 0.5548s
Epoch: 0030 nll_train: 14453.9136149089 kl_train: -0.0981423731 mse_train: 0.0760732296 acc_train: 0.5416666667 nll_val: 14303.1119791667 kl_val: -0.1036545783 mse_val: 0.0752795363 acc_val: 0.5000000000 time: 0.5384s
Epoch: 0031 nll_train: 14466.4300537109 kl_train: -0.0939926541 mse_train: 0.0761391059 acc_train: 0.4791666667 nll_val: 14289.5458984375 kl_val: -0.0986642291 mse_val: 0.0752081325 acc_val: 0.5000000000 time: 0.5012s
Epoch: 0032 nll_train: 14458.5685221354 kl_train: -0.0819175836 mse_train: 0.0760977298 acc_train: 0.5416666667 nll_val: 14307.5751953125 kl_val: -0.0822855557 mse_val: 0.0753030231 acc_val: 0.5000000000 time: 0.5010s
Epoch: 0033 nll_train: 14446.4251708984 kl_train: -0.0907970965 mse_train: 0.0760338168 acc_train: 0.5208333333 nll_val: 14287.4970703125 kl_val: -0.0914908821 mse_val: 0.0751973515 acc_val: 0.5000000000 time: 0.4986s
Epoch: 0034 nll_train: 14459.5233561198 kl_train: -0.1004064955 mse_train: 0.0761027536 acc_train: 0.5208333333 nll_val: 14353.8401692708 kl_val: -0.1089925145 mse_val: 0.0755465254 acc_val: 0.5000000000 time: 0.5195s
Epoch: 0035 nll_train: 14441.1484781901 kl_train: -0.1008901230 mse_train: 0.0760060449 acc_train: 0.5000000000 nll_val: 14325.7451171875 kl_val: -0.0934205949 mse_val: 0.0753986562 acc_val: 0.5000000000 time: 0.5559s
Epoch: 0036 nll_train: 14442.0934651693 kl_train: -0.0970419155 mse_train: 0.0760110174 acc_train: 0.5208333333 nll_val: 14347.6979166667 kl_val: -0.1096260374 mse_val: 0.0755141998 acc_val: 0.5000000000 time: 0.5231s
Epoch: 0037 nll_train: 14443.0826009115 kl_train: -0.1087096675 mse_train: 0.0760162248 acc_train: 0.5208333333 nll_val: 14410.7884114583 kl_val: -0.1028862794 mse_val: 0.0758462573 acc_val: 0.5000000000 time: 0.5018s
Epoch: 0038 nll_train: 14439.9327392578 kl_train: -0.1037943646 mse_train: 0.0759996466 acc_train: 0.5625000000 nll_val: 14345.4082031250 kl_val: -0.1074322537 mse_val: 0.0755021522 acc_val: 0.5000000000 time: 0.5016s
Epoch: 0039 nll_train: 14444.9802246094 kl_train: -0.1036411372 mse_train: 0.0760262116 acc_train: 0.5625000000 nll_val: 14354.2955729167 kl_val: -0.0865733003 mse_val: 0.0755489195 acc_val: 0.5000000000 time: 0.4977s
Epoch: 0040 nll_train: 14442.3513997396 kl_train: -0.0935087677 mse_train: 0.0760123741 acc_train: 0.5833333333 nll_val: 14365.5761718750 kl_val: -0.0944442799 mse_val: 0.0756082957 acc_val: 0.5000000000 time: 0.5002s
Epoch: 0041 nll_train: 14437.5478922526 kl_train: -0.0905449083 mse_train: 0.0759870939 acc_train: 0.5416666667 nll_val: 14368.4593098958 kl_val: -0.1013035576 mse_val: 0.0756234651 acc_val: 0.5000000000 time: 0.5001s
Epoch: 0042 nll_train: 14431.5137532552 kl_train: -0.0959556183 mse_train: 0.0759553354 acc_train: 0.5416666667 nll_val: 14388.3649088542 kl_val: -0.0935940221 mse_val: 0.0757282327 acc_val: 0.5000000000 time: 0.4992s
Epoch: 0043 nll_train: 14438.4963378906 kl_train: -0.0939323266 mse_train: 0.0759920840 acc_train: 0.5416666667 nll_val: 14429.4801432292 kl_val: -0.1051188856 mse_val: 0.0759446298 acc_val: 0.5000000000 time: 0.5014s
Epoch: 0044 nll_train: 14421.9138183594 kl_train: -0.0978362138 mse_train: 0.0759048103 acc_train: 0.5416666667 nll_val: 14363.2607421875 kl_val: -0.0863395979 mse_val: 0.0755961090 acc_val: 0.5000000000 time: 0.5019s
Epoch: 0045 nll_train: 14434.4954020182 kl_train: -0.0991481406 mse_train: 0.0759710285 acc_train: 0.5416666667 nll_val: 14372.1207682292 kl_val: -0.1193950499 mse_val: 0.0756427422 acc_val: 0.5000000000 time: 0.4981s
Epoch: 0046 nll_train: 14446.4562581380 kl_train: -0.0983664066 mse_train: 0.0760339811 acc_train: 0.5833333333 nll_val: 14401.1067708333 kl_val: -0.0997668592 mse_val: 0.0757952978 acc_val: 0.5000000000 time: 0.5016s
Epoch: 0047 nll_train: 14429.5674235026 kl_train: -0.0883083556 mse_train: 0.0759450913 acc_train: 0.5000000000 nll_val: 14424.0003255208 kl_val: -0.0882953828 mse_val: 0.0759157886 acc_val: 0.5000000000 time: 0.4991s
Epoch: 0048 nll_train: 14428.5111897786 kl_train: -0.0897197595 mse_train: 0.0759395342 acc_train: 0.5416666667 nll_val: 14383.6064453125 kl_val: -0.1016064237 mse_val: 0.0757031913 acc_val: 0.5000000000 time: 0.4990s
Epoch: 0049 nll_train: 14430.8570556641 kl_train: -0.0976565800 mse_train: 0.0759518802 acc_train: 0.5416666667 nll_val: 14360.2574869792 kl_val: -0.1015400440 mse_val: 0.0755803039 acc_val: 0.5000000000 time: 0.5014s
Epoch: 0050 nll_train: 14420.6532389323 kl_train: -0.0974255358 mse_train: 0.0758981751 acc_train: 0.5416666667 nll_val: 14401.4033203125 kl_val: -0.1058933834 mse_val: 0.0757968600 acc_val: 0.5000000000 time: 0.4985s
Epoch: 0051 nll_train: 14411.3243815104 kl_train: -0.1043434578 mse_train: 0.0758490756 acc_train: 0.5625000000 nll_val: 14371.6569010417 kl_val: -0.1208803852 mse_val: 0.0756402984 acc_val: 0.5000000000 time: 0.5001s
Epoch: 0052 nll_train: 14420.1151936849 kl_train: -0.1133924232 mse_train: 0.0758953445 acc_train: 0.5208333333 nll_val: 14438.1106770833 kl_val: -0.1107120588 mse_val: 0.0759900535 acc_val: 0.5000000000 time: 0.4994s
Epoch: 0053 nll_train: 14411.3861897786 kl_train: -0.1095539698 mse_train: 0.0758494010 acc_train: 0.5833333333 nll_val: 14414.7724609375 kl_val: -0.1046058709 mse_val: 0.0758672282 acc_val: 0.5000000000 time: 0.4998s
Epoch: 0054 nll_train: 14411.2353922526 kl_train: -0.1047134104 mse_train: 0.0758486091 acc_train: 0.5208333333 nll_val: 14413.2737630208 kl_val: -0.1135890757 mse_val: 0.0758593356 acc_val: 0.5000000000 time: 0.4985s
Epoch: 0055 nll_train: 14408.3946533203 kl_train: -0.1024016029 mse_train: 0.0758336582 acc_train: 0.5416666667 nll_val: 14435.2337239583 kl_val: -0.1050961365 mse_val: 0.0759749139 acc_val: 0.5000000000 time: 0.5021s
Epoch: 0056 nll_train: 14413.0421549479 kl_train: -0.1020784561 mse_train: 0.0758581161 acc_train: 0.5208333333 nll_val: 14384.8483072917 kl_val: -0.1061597466 mse_val: 0.0757097304 acc_val: 0.5000000000 time: 0.4982s
Epoch: 0057 nll_train: 14404.8123372396 kl_train: -0.1053262241 mse_train: 0.0758148027 acc_train: 0.5416666667 nll_val: 14410.9173177083 kl_val: -0.1045258244 mse_val: 0.0758469303 acc_val: 0.5000000000 time: 0.4961s
Epoch: 0058 nll_train: 14410.0767008464 kl_train: -0.1102919560 mse_train: 0.0758425106 acc_train: 0.5625000000 nll_val: 14408.5869140625 kl_val: -0.0990416706 mse_val: 0.0758346667 acc_val: 0.5000000000 time: 0.5225s
Epoch: 0059 nll_train: 14409.8204752604 kl_train: -0.1089949282 mse_train: 0.0758411591 acc_train: 0.5416666667 nll_val: 14380.7070312500 kl_val: -0.0941150809 mse_val: 0.0756879275 acc_val: 0.5000000000 time: 0.4982s
Epoch: 0060 nll_train: 14395.0265706380 kl_train: -0.1067925983 mse_train: 0.0757632984 acc_train: 0.5416666667 nll_val: 14486.6637369792 kl_val: -0.1141492824 mse_val: 0.0762455985 acc_val: 0.5000000000 time: 0.4999s
Epoch: 0061 nll_train: 14392.5634358724 kl_train: -0.1087890435 mse_train: 0.0757503340 acc_train: 0.5625000000 nll_val: 14386.5501302083 kl_val: -0.1149649322 mse_val: 0.0757186860 acc_val: 0.5000000000 time: 0.5030s
Epoch: 0062 nll_train: 14383.1903076172 kl_train: -0.1077951618 mse_train: 0.0757010013 acc_train: 0.5833333333 nll_val: 14397.5872395833 kl_val: -0.1197243830 mse_val: 0.0757767732 acc_val: 0.5000000000 time: 0.4992s
Epoch: 0063 nll_train: 14387.0021158854 kl_train: -0.1117748314 mse_train: 0.0757210640 acc_train: 0.5416666667 nll_val: 14461.6383463542 kl_val: -0.0876076122 mse_val: 0.0761138871 acc_val: 0.5000000000 time: 0.5000s
Epoch: 0064 nll_train: 14382.2388916016 kl_train: -0.1135782606 mse_train: 0.0756959951 acc_train: 0.5416666667 nll_val: 14393.7926432292 kl_val: -0.1276613797 mse_val: 0.0757568032 acc_val: 0.5000000000 time: 0.5047s
Epoch: 0065 nll_train: 14397.3472086589 kl_train: -0.1117776964 mse_train: 0.0757755116 acc_train: 0.5208333333 nll_val: 14526.5891927083 kl_val: -0.1125639131 mse_val: 0.0764557347 acc_val: 0.5000000000 time: 0.5008s
Epoch: 0066 nll_train: 14392.8035074870 kl_train: -0.1049354492 mse_train: 0.0757515986 acc_train: 0.5416666667 nll_val: 14417.8063151042 kl_val: -0.1233568440 mse_val: 0.0758831923 acc_val: 0.5000000000 time: 0.4977s
Epoch: 0067 nll_train: 14388.6160888672 kl_train: -0.1115126422 mse_train: 0.0757295590 acc_train: 0.5625000000 nll_val: 14543.2047526042 kl_val: -0.1162961324 mse_val: 0.0765431821 acc_val: 0.5000000000 time: 0.5018s
Epoch: 0068 nll_train: 14386.4422200521 kl_train: -0.1084424946 mse_train: 0.0757181171 acc_train: 0.5833333333 nll_val: 14486.9534505208 kl_val: -0.1057150563 mse_val: 0.0762471234 acc_val: 0.5000000000 time: 0.5255s
Epoch: 0069 nll_train: 14382.2626546224 kl_train: -0.1033279955 mse_train: 0.0756961199 acc_train: 0.5208333333 nll_val: 14460.3219401042 kl_val: -0.1039808691 mse_val: 0.0761069556 acc_val: 0.5000000000 time: 0.4988s
Epoch: 0070 nll_train: 14386.6198730469 kl_train: -0.1020481425 mse_train: 0.0757190521 acc_train: 0.5625000000 nll_val: 14482.8177083333 kl_val: -0.1166553920 mse_val: 0.0762253553 acc_val: 0.5000000000 time: 0.4965s
Epoch: 0071 nll_train: 14381.6392415365 kl_train: -0.1077914505 mse_train: 0.0756928388 acc_train: 0.5416666667 nll_val: 14534.7760416667 kl_val: -0.1164808472 mse_val: 0.0764988189 acc_val: 0.5000000000 time: 0.4991s
Epoch: 0072 nll_train: 14384.2824707031 kl_train: -0.1053053498 mse_train: 0.0757067503 acc_train: 0.5416666667 nll_val: 14404.5351562500 kl_val: -0.1073492989 mse_val: 0.0758133456 acc_val: 0.5000000000 time: 0.4999s
Epoch: 0073 nll_train: 14380.4771321615 kl_train: -0.1053529894 mse_train: 0.0756867224 acc_train: 0.4791666667 nll_val: 14537.1041666667 kl_val: -0.1200632925 mse_val: 0.0765110751 acc_val: 0.5000000000 time: 0.5072s
Epoch: 0074 nll_train: 14373.7427978516 kl_train: -0.1084788901 mse_train: 0.0756512784 acc_train: 0.5208333333 nll_val: 14464.6429036458 kl_val: -0.1169729580 mse_val: 0.0761297022 acc_val: 0.5000000000 time: 0.5184s
Epoch: 0075 nll_train: 14371.4683024089 kl_train: -0.1106148030 mse_train: 0.0756393081 acc_train: 0.5416666667 nll_val: 14439.1490885417 kl_val: -0.1164152126 mse_val: 0.0759955247 acc_val: 0.5000000000 time: 0.4985s
Epoch: 0076 nll_train: 14369.5332845052 kl_train: -0.1083489371 mse_train: 0.0756291222 acc_train: 0.5208333333 nll_val: 14560.1901041667 kl_val: -0.1202335606 mse_val: 0.0766325742 acc_val: 0.5000000000 time: 0.4990s
Epoch: 0077 nll_train: 14375.9320882161 kl_train: -0.1032585148 mse_train: 0.0756627993 acc_train: 0.5416666667 nll_val: 14442.6904296875 kl_val: -0.1174940442 mse_val: 0.0760141561 acc_val: 0.5000000000 time: 0.4993s
Epoch: 0078 nll_train: 14374.7348632812 kl_train: -0.0973796342 mse_train: 0.0756565002 acc_train: 0.5208333333 nll_val: 14506.4449869792 kl_val: -0.1158008774 mse_val: 0.0763497079 acc_val: 0.5000000000 time: 0.5093s
Epoch: 0079 nll_train: 14384.9622802734 kl_train: -0.1055650885 mse_train: 0.0757103267 acc_train: 0.5833333333 nll_val: 14491.8085937500 kl_val: -0.1195841109 mse_val: 0.0762726714 acc_val: 0.5000000000 time: 0.5036s
Epoch: 0080 nll_train: 14366.9606119792 kl_train: -0.1134460292 mse_train: 0.0756155819 acc_train: 0.5625000000 nll_val: 14520.2402343750 kl_val: -0.1443573733 mse_val: 0.0764223188 acc_val: 0.5000000000 time: 0.5007s
Epoch: 0081 nll_train: 14379.3815917969 kl_train: -0.1155515236 mse_train: 0.0756809575 acc_train: 0.5208333333 nll_val: 14452.1634114583 kl_val: -0.1111091400 mse_val: 0.0760640154 acc_val: 0.5000000000 time: 0.4970s
Epoch: 0082 nll_train: 14378.2931722005 kl_train: -0.1064724478 mse_train: 0.0756752271 acc_train: 0.5416666667 nll_val: 14482.8811848958 kl_val: -0.1166548828 mse_val: 0.0762256881 acc_val: 0.5000000000 time: 0.4994s
Epoch: 0083 nll_train: 14372.4830322266 kl_train: -0.1050379691 mse_train: 0.0756446463 acc_train: 0.5208333333 nll_val: 14425.9013671875 kl_val: -0.1302769979 mse_val: 0.0759257997 acc_val: 0.5000000000 time: 0.4993s
Epoch: 0084 nll_train: 14362.1336669922 kl_train: -0.1134619415 mse_train: 0.0755901764 acc_train: 0.5208333333 nll_val: 14402.2578125000 kl_val: -0.1307246586 mse_val: 0.0758013551 acc_val: 0.5000000000 time: 0.4957s
Epoch: 0085 nll_train: 14366.7278645833 kl_train: -0.1129285735 mse_train: 0.0756143581 acc_train: 0.5416666667 nll_val: 14522.3180338542 kl_val: -0.1274618879 mse_val: 0.0764332513 acc_val: 0.5000000000 time: 0.5038s
Epoch: 0086 nll_train: 14364.0118408203 kl_train: -0.1104997092 mse_train: 0.0756000625 acc_train: 0.5833333333 nll_val: 14457.7252604167 kl_val: -0.1338099589 mse_val: 0.0760932838 acc_val: 0.5000000000 time: 0.4981s
Epoch: 0087 nll_train: 14361.5869140625 kl_train: -0.1196261588 mse_train: 0.0755872993 acc_train: 0.5625000000 nll_val: 14481.1643880208 kl_val: -0.1097086097 mse_val: 0.0762166530 acc_val: 0.5000000000 time: 0.4966s
Epoch: 0088 nll_train: 14368.7820638021 kl_train: -0.1061818004 mse_train: 0.0756251702 acc_train: 0.5416666667 nll_val: 14472.4384765625 kl_val: -0.1140742501 mse_val: 0.0761707301 acc_val: 0.5000000000 time: 0.4975s
Epoch: 0089 nll_train: 14365.9949951172 kl_train: -0.1045590127 mse_train: 0.0756105015 acc_train: 0.5833333333 nll_val: 14528.6705729167 kl_val: -0.1054365635 mse_val: 0.0764666895 acc_val: 0.5000000000 time: 0.4999s
Epoch: 0090 nll_train: 14355.8944091797 kl_train: -0.1086204307 mse_train: 0.0755573409 acc_train: 0.5625000000 nll_val: 14552.2337239583 kl_val: -0.1317185760 mse_val: 0.0765907044 acc_val: 0.5000000000 time: 0.4981s
Epoch: 0091 nll_train: 14349.3625488281 kl_train: -0.1205297858 mse_train: 0.0755229592 acc_train: 0.5208333333 nll_val: 14408.1129557292 kl_val: -0.1221542656 mse_val: 0.0758321707 acc_val: 0.5000000000 time: 0.5029s
Epoch: 0092 nll_train: 14361.9956868490 kl_train: -0.1130332338 mse_train: 0.0755894532 acc_train: 0.5625000000 nll_val: 14394.6647135417 kl_val: -0.1143829202 mse_val: 0.0757613952 acc_val: 0.3333333333 time: 0.4998s
Epoch: 0093 nll_train: 14356.3308105469 kl_train: -0.1129558588 mse_train: 0.0755596355 acc_train: 0.5833333333 nll_val: 14421.6718750000 kl_val: -0.1350027223 mse_val: 0.0759035349 acc_val: 0.5000000000 time: 0.5005s
Epoch: 0094 nll_train: 14334.3509114583 kl_train: -0.1230605209 mse_train: 0.0754439523 acc_train: 0.5625000000 nll_val: 14488.0677083333 kl_val: -0.1357487937 mse_val: 0.0762529895 acc_val: 0.5000000000 time: 0.4990s
Epoch: 0095 nll_train: 14334.2302652995 kl_train: -0.1212741015 mse_train: 0.0754433192 acc_train: 0.5416666667 nll_val: 14449.9570312500 kl_val: -0.1525135214 mse_val: 0.0760524025 acc_val: 0.5000000000 time: 0.4980s
Epoch: 0096 nll_train: 14356.0509033203 kl_train: -0.1204339874 mse_train: 0.0755581637 acc_train: 0.5625000000 nll_val: 14510.7412109375 kl_val: -0.1525936772 mse_val: 0.0763723205 acc_val: 0.5000000000 time: 0.5120s
Epoch: 0097 nll_train: 14340.2818196615 kl_train: -0.1310284731 mse_train: 0.0754751693 acc_train: 0.5833333333 nll_val: 14422.8642578125 kl_val: -0.1365313654 mse_val: 0.0759098108 acc_val: 0.5000000000 time: 0.5173s
Epoch: 0098 nll_train: 14348.7095133464 kl_train: -0.1180496910 mse_train: 0.0755195251 acc_train: 0.6041666667 nll_val: 14450.1223958333 kl_val: -0.1357165252 mse_val: 0.0760532742 acc_val: 0.5000000000 time: 0.5111s
Epoch: 0099 nll_train: 14349.6971028646 kl_train: -0.1219412889 mse_train: 0.0755247211 acc_train: 0.5625000000 nll_val: 14482.4069010417 kl_val: -0.1481196284 mse_val: 0.0762231896 acc_val: 0.5000000000 time: 0.4996s
Epoch: 0100 nll_train: 14352.2493082682 kl_train: -0.1234658379 mse_train: 0.0755381559 acc_train: 0.5625000000 nll_val: 14371.5068359375 kl_val: -0.1585803330 mse_val: 0.0756395111 acc_val: 0.5000000000 time: 0.4973s
Epoch: 0101 nll_train: 14348.4071044922 kl_train: -0.1309583324 mse_train: 0.0755179341 acc_train: 0.5625000000 nll_val: 14422.6578776042 kl_val: -0.1397339255 mse_val: 0.0759087205 acc_val: 0.5000000000 time: 0.4982s
Epoch: 0102 nll_train: 14345.0410970052 kl_train: -0.1153960560 mse_train: 0.0755002163 acc_train: 0.5208333333 nll_val: 14498.2985026042 kl_val: -0.1272223617 mse_val: 0.0763068323 acc_val: 0.5000000000 time: 0.5109s
Epoch: 0103 nll_train: 14338.2419840495 kl_train: -0.1168270288 mse_train: 0.0754644312 acc_train: 0.5625000000 nll_val: 14511.4427083333 kl_val: -0.1538884118 mse_val: 0.0763760110 acc_val: 0.5000000000 time: 0.5028s
Epoch: 0104 nll_train: 14357.3933512370 kl_train: -0.1273829776 mse_train: 0.0755652287 acc_train: 0.5833333333 nll_val: 14511.9046223958 kl_val: -0.1272016987 mse_val: 0.0763784399 acc_val: 0.5000000000 time: 0.5020s
Epoch: 0105 nll_train: 14359.2952067057 kl_train: -0.1145526554 mse_train: 0.0755752375 acc_train: 0.5625000000 nll_val: 14570.7929687500 kl_val: -0.1272044604 mse_val: 0.0766883766 acc_val: 0.5000000000 time: 0.5590s
Epoch: 0106 nll_train: 14342.2919108073 kl_train: -0.1143522781 mse_train: 0.0754857472 acc_train: 0.5208333333 nll_val: 14734.6604817708 kl_val: -0.1399697214 mse_val: 0.0775508409 acc_val: 0.5000000000 time: 0.5147s
Epoch: 0107 nll_train: 14336.3370768229 kl_train: -0.1186366952 mse_train: 0.0754544064 acc_train: 0.5625000000 nll_val: 14676.1780598958 kl_val: -0.1303358972 mse_val: 0.0772430375 acc_val: 0.5000000000 time: 0.4975s
Epoch: 0108 nll_train: 14323.1191406250 kl_train: -0.1146235401 mse_train: 0.0753848385 acc_train: 0.5416666667 nll_val: 14561.4241536458 kl_val: -0.1435306420 mse_val: 0.0766390761 acc_val: 0.5000000000 time: 0.5007s
Epoch: 0109 nll_train: 14319.7725016276 kl_train: -0.1255452627 mse_train: 0.0753672247 acc_train: 0.5833333333 nll_val: 14729.5130208333 kl_val: -0.1506294534 mse_val: 0.0775237506 acc_val: 0.5000000000 time: 0.4989s
Epoch: 0110 nll_train: 14314.7630615234 kl_train: -0.1208511883 mse_train: 0.0753408591 acc_train: 0.5208333333 nll_val: 14566.1988932292 kl_val: -0.1380337278 mse_val: 0.0766641994 acc_val: 0.5000000000 time: 0.4981s
Epoch: 0111 nll_train: 14317.6896565755 kl_train: -0.1236975441 mse_train: 0.0753562627 acc_train: 0.5416666667 nll_val: 14597.2122395833 kl_val: -0.1536079397 mse_val: 0.0768274317 acc_val: 0.5000000000 time: 0.4990s
Epoch: 0112 nll_train: 14325.8390706380 kl_train: -0.1369007770 mse_train: 0.0753991539 acc_train: 0.5833333333 nll_val: 14579.1637369792 kl_val: -0.1694783196 mse_val: 0.0767324393 acc_val: 0.5000000000 time: 0.4968s
Epoch: 0113 nll_train: 14318.6037597656 kl_train: -0.1217689278 mse_train: 0.0753610726 acc_train: 0.5416666667 nll_val: 14606.4433593750 kl_val: -0.1617878725 mse_val: 0.0768760194 acc_val: 0.5000000000 time: 0.4989s
Epoch: 0114 nll_train: 14308.4347737630 kl_train: -0.1263548280 mse_train: 0.0753075507 acc_train: 0.5416666667 nll_val: 14602.8870442708 kl_val: -0.1551240856 mse_val: 0.0768572961 acc_val: 0.5000000000 time: 0.4972s
Epoch: 0115 nll_train: 14321.8480224609 kl_train: -0.1311979253 mse_train: 0.0753781471 acc_train: 0.5625000000 nll_val: 14633.6406250000 kl_val: -0.1434136728 mse_val: 0.0770191600 acc_val: 0.5000000000 time: 0.4995s
Epoch: 0116 nll_train: 14328.1032307943 kl_train: -0.1228411480 mse_train: 0.0754110701 acc_train: 0.5625000000 nll_val: 14679.7249348958 kl_val: -0.1509465699 mse_val: 0.0772617037 acc_val: 0.5000000000 time: 0.5006s
Epoch: 0117 nll_train: 14305.0494384766 kl_train: -0.1215783282 mse_train: 0.0752897346 acc_train: 0.5000000000 nll_val: 14680.2454427083 kl_val: -0.1464498093 mse_val: 0.0772644530 acc_val: 0.5000000000 time: 0.4964s
Epoch: 0118 nll_train: 14321.3112386068 kl_train: -0.1196525786 mse_train: 0.0753753233 acc_train: 0.5625000000 nll_val: 14745.4794921875 kl_val: -0.1457061370 mse_val: 0.0776077881 acc_val: 0.5000000000 time: 0.4995s
Epoch: 0119 nll_train: 14304.6649576823 kl_train: -0.1269473322 mse_train: 0.0752877103 acc_train: 0.5416666667 nll_val: 14793.6751302083 kl_val: -0.1639600197 mse_val: 0.0778614481 acc_val: 0.5000000000 time: 0.5004s
Epoch: 0120 nll_train: 14328.4528808594 kl_train: -0.1317679249 mse_train: 0.0754129124 acc_train: 0.5416666667 nll_val: 14730.0572916667 kl_val: -0.1559094091 mse_val: 0.0775266190 acc_val: 0.5000000000 time: 0.5251s
Epoch: 0121 nll_train: 14311.6865641276 kl_train: -0.1197249116 mse_train: 0.0753246675 acc_train: 0.5416666667 nll_val: 14794.3688151042 kl_val: -0.1515118927 mse_val: 0.0778651014 acc_val: 0.5000000000 time: 0.5012s
Epoch: 0122 nll_train: 14317.2445068359 kl_train: -0.1259140121 mse_train: 0.0753539175 acc_train: 0.5416666667 nll_val: 14707.1276041667 kl_val: -0.1721204321 mse_val: 0.0774059320 acc_val: 0.5000000000 time: 0.5008s
Epoch: 0123 nll_train: 14322.8499348958 kl_train: -0.1354471891 mse_train: 0.0753834215 acc_train: 0.5833333333 nll_val: 14688.4013671875 kl_val: -0.1346149668 mse_val: 0.0773073733 acc_val: 0.5000000000 time: 0.4990s
Epoch: 0124 nll_train: 14318.7594807943 kl_train: -0.1246355224 mse_train: 0.0753618918 acc_train: 0.5208333333 nll_val: 14672.2418619792 kl_val: -0.1678073953 mse_val: 0.0772223224 acc_val: 0.5000000000 time: 0.5000s
Epoch: 0125 nll_train: 14315.5598144531 kl_train: -0.1322622910 mse_train: 0.0753450526 acc_train: 0.5625000000 nll_val: 14724.7503255208 kl_val: -0.1618626018 mse_val: 0.0774986868 acc_val: 0.5000000000 time: 0.4972s
Epoch: 0126 nll_train: 14303.4813639323 kl_train: -0.1332939484 mse_train: 0.0752814822 acc_train: 0.5625000000 nll_val: 14658.8730468750 kl_val: -0.1634646455 mse_val: 0.0771519641 acc_val: 0.5000000000 time: 0.5012s
Epoch: 0127 nll_train: 14326.5308837891 kl_train: -0.1305394488 mse_train: 0.0754027933 acc_train: 0.5833333333 nll_val: 14671.8642578125 kl_val: -0.1750086496 mse_val: 0.0772203381 acc_val: 0.5000000000 time: 0.4958s
Epoch: 0128 nll_train: 14313.0343017578 kl_train: -0.1317259039 mse_train: 0.0753317598 acc_train: 0.5833333333 nll_val: 14539.6311848958 kl_val: -0.1573924348 mse_val: 0.0765243744 acc_val: 0.5000000000 time: 0.5000s
Epoch: 0129 nll_train: 14313.3514811198 kl_train: -0.1269029511 mse_train: 0.0753334292 acc_train: 0.5625000000 nll_val: 14562.0380859375 kl_val: -0.1653631727 mse_val: 0.0766423071 acc_val: 0.5000000000 time: 0.4964s
Epoch: 0130 nll_train: 14296.1051432292 kl_train: -0.1288999878 mse_train: 0.0752426595 acc_train: 0.5833333333 nll_val: 14583.3932291667 kl_val: -0.1606642157 mse_val: 0.0767546992 acc_val: 0.5000000000 time: 0.5003s
Epoch: 0131 nll_train: 14287.2811279297 kl_train: -0.1415212414 mse_train: 0.0751962168 acc_train: 0.5625000000 nll_val: 14594.8186848958 kl_val: -0.1545618599 mse_val: 0.0768148328 acc_val: 0.5000000000 time: 0.4989s
Epoch: 0132 nll_train: 14289.5007731120 kl_train: -0.1340775971 mse_train: 0.0752078990 acc_train: 0.5416666667 nll_val: 14619.6197916667 kl_val: -0.1932041446 mse_val: 0.0769453670 acc_val: 0.5000000000 time: 0.5000s
Epoch: 0133 nll_train: 14296.9313964844 kl_train: -0.1476383841 mse_train: 0.0752470085 acc_train: 0.5625000000 nll_val: 14655.1640625000 kl_val: -0.1509071067 mse_val: 0.0771324436 acc_val: 0.5000000000 time: 0.4988s
Epoch: 0134 nll_train: 14286.4579264323 kl_train: -0.1359567995 mse_train: 0.0751918857 acc_train: 0.5833333333 nll_val: 14652.8841145833 kl_val: -0.1744600733 mse_val: 0.0771204407 acc_val: 0.5000000000 time: 0.5006s
Epoch: 0135 nll_train: 14312.3889567057 kl_train: -0.1407087334 mse_train: 0.0753283631 acc_train: 0.5208333333 nll_val: 14727.4202473958 kl_val: -0.1779632568 mse_val: 0.0775127361 acc_val: 0.5000000000 time: 0.5004s
Epoch: 0136 nll_train: 14304.2943522135 kl_train: -0.1427349391 mse_train: 0.0752857612 acc_train: 0.5416666667 nll_val: 14624.6842447917 kl_val: -0.1848798444 mse_val: 0.0769720227 acc_val: 0.5000000000 time: 0.5115s
Epoch: 0137 nll_train: 14280.4379475911 kl_train: -0.1331455045 mse_train: 0.0751601990 acc_train: 0.5625000000 nll_val: 14663.8909505208 kl_val: -0.1581134995 mse_val: 0.0771783739 acc_val: 0.3333333333 time: 0.4983s
Epoch: 0138 nll_train: 14301.2050781250 kl_train: -0.1301886132 mse_train: 0.0752695001 acc_train: 0.5416666667 nll_val: 14595.6142578125 kl_val: -0.1580152263 mse_val: 0.0768190200 acc_val: 0.5000000000 time: 0.5279s
Epoch: 0139 nll_train: 14305.8430175781 kl_train: -0.1314890403 mse_train: 0.0752939108 acc_train: 0.5416666667 nll_val: 14716.9147135417 kl_val: -0.1714427893 mse_val: 0.0774574528 acc_val: 0.5000000000 time: 0.5038s
Epoch: 0140 nll_train: 14305.5610758464 kl_train: -0.1421165167 mse_train: 0.0752924265 acc_train: 0.5416666667 nll_val: 14819.4329427083 kl_val: -0.1819791993 mse_val: 0.0779970114 acc_val: 0.3333333333 time: 0.4990s
Epoch: 0141 nll_train: 14297.2486572266 kl_train: -0.1295109997 mse_train: 0.0752486771 acc_train: 0.5416666667 nll_val: 14595.6168619792 kl_val: -0.1536273112 mse_val: 0.0768190374 acc_val: 0.5000000000 time: 0.4942s
Epoch: 0142 nll_train: 14313.0127360026 kl_train: -0.1340278791 mse_train: 0.0753316469 acc_train: 0.5208333333 nll_val: 14644.7086588542 kl_val: -0.1658408195 mse_val: 0.0770774086 acc_val: 0.5000000000 time: 0.4964s
Epoch: 0143 nll_train: 14300.6413981120 kl_train: -0.1400239651 mse_train: 0.0752665345 acc_train: 0.5833333333 nll_val: 14581.2376302083 kl_val: -0.1724691242 mse_val: 0.0767433544 acc_val: 0.5000000000 time: 0.4937s
Epoch: 0144 nll_train: 14290.0581461589 kl_train: -0.1396585667 mse_train: 0.0752108311 acc_train: 0.5625000000 nll_val: 14592.3636067708 kl_val: -0.1749491269 mse_val: 0.0768019135 acc_val: 0.3333333333 time: 0.4990s
Epoch: 0145 nll_train: 14295.3900146484 kl_train: -0.1336277033 mse_train: 0.0752388954 acc_train: 0.5416666667 nll_val: 14613.8736979167 kl_val: -0.1766004165 mse_val: 0.0769151251 acc_val: 0.3333333333 time: 0.4985s
Epoch: 0146 nll_train: 14292.7379964193 kl_train: -0.1406681486 mse_train: 0.0752249374 acc_train: 0.5000000000 nll_val: 14653.7278645833 kl_val: -0.1602525190 mse_val: 0.0771248788 acc_val: 0.5000000000 time: 0.4956s
Epoch: 0147 nll_train: 14284.3494059245 kl_train: -0.1350597588 mse_train: 0.0751807876 acc_train: 0.5416666667 nll_val: 14567.0520833333 kl_val: -0.1911941866 mse_val: 0.0766686921 acc_val: 0.5000000000 time: 0.5008s
Epoch: 0148 nll_train: 14315.9262288411 kl_train: -0.1475235958 mse_train: 0.0753469812 acc_train: 0.5208333333 nll_val: 14656.4430338542 kl_val: -0.1750329336 mse_val: 0.0771391715 acc_val: 0.5000000000 time: 0.4982s
Epoch: 0149 nll_train: 14289.6176350911 kl_train: -0.1388970315 mse_train: 0.0752085148 acc_train: 0.5416666667 nll_val: 14612.0019531250 kl_val: -0.1855007311 mse_val: 0.0769052704 acc_val: 0.5000000000 time: 0.4979s
Epoch: 0150 nll_train: 14310.7052001953 kl_train: -0.1461447061 mse_train: 0.0753195017 acc_train: 0.6041666667 nll_val: 14626.5322265625 kl_val: -0.1840880315 mse_val: 0.0769817457 acc_val: 0.5000000000 time: 0.5035s
Epoch: 0151 nll_train: 14293.1847330729 kl_train: -0.1400055513 mse_train: 0.0752272899 acc_train: 0.5625000000 nll_val: 14632.6217447917 kl_val: -0.1586186041 mse_val: 0.0770137931 acc_val: 0.5000000000 time: 0.4986s
Epoch: 0152 nll_train: 14323.4069010417 kl_train: -0.1414224068 mse_train: 0.0753863530 acc_train: 0.5625000000 nll_val: 14758.7063802083 kl_val: -0.1768682202 mse_val: 0.0776774039 acc_val: 0.3333333333 time: 0.4997s
Epoch: 0153 nll_train: 14302.6411946615 kl_train: -0.1440511714 mse_train: 0.0752770596 acc_train: 0.5208333333 nll_val: 14722.6725260417 kl_val: -0.1749289234 mse_val: 0.0774877469 acc_val: 0.5000000000 time: 0.4967s
Epoch: 0154 nll_train: 14308.1909179688 kl_train: -0.1234571588 mse_train: 0.0753062673 acc_train: 0.5000000000 nll_val: 14698.7848307292 kl_val: -0.1745529522 mse_val: 0.0773620208 acc_val: 0.3333333333 time: 0.5100s
Epoch: 0155 nll_train: 14332.2272135417 kl_train: -0.1377865939 mse_train: 0.0754327765 acc_train: 0.5625000000 nll_val: 14932.3955078125 kl_val: -0.1857625743 mse_val: 0.0785915504 acc_val: 0.3333333333 time: 0.5133s
Epoch: 0156 nll_train: 14299.0205485026 kl_train: -0.1441039598 mse_train: 0.0752580026 acc_train: 0.5208333333 nll_val: 14717.3307291667 kl_val: -0.1760256290 mse_val: 0.0774596284 acc_val: 0.3333333333 time: 0.5011s
Epoch: 0157 nll_train: 14323.5978190104 kl_train: -0.1534507233 mse_train: 0.0753873557 acc_train: 0.5000000000 nll_val: 14600.8125000000 kl_val: -0.1760471712 mse_val: 0.0768463810 acc_val: 0.1666666667 time: 0.5132s
Epoch: 0158 nll_train: 14296.6429036458 kl_train: -0.1419459421 mse_train: 0.0752454906 acc_train: 0.5208333333 nll_val: 14598.2177734375 kl_val: -0.1735166907 mse_val: 0.0768327241 acc_val: 0.3333333333 time: 0.4995s
Epoch: 0159 nll_train: 14291.6693115234 kl_train: -0.1304410207 mse_train: 0.0752193111 acc_train: 0.5416666667 nll_val: 14501.3704427083 kl_val: -0.1720149716 mse_val: 0.0763230001 acc_val: 0.5000000000 time: 0.5004s
Epoch: 0160 nll_train: 14316.7727050781 kl_train: -0.1335567579 mse_train: 0.0753514351 acc_train: 0.5625000000 nll_val: 14717.3245442708 kl_val: -0.2045043161 mse_val: 0.0774596060 acc_val: 0.3333333333 time: 0.4988s
Epoch: 0161 nll_train: 14326.2028808594 kl_train: -0.1380278110 mse_train: 0.0754010687 acc_train: 0.5000000000 nll_val: 14650.3958333333 kl_val: -0.1552676111 mse_val: 0.0771073451 acc_val: 0.3333333333 time: 0.4992s
Epoch: 0162 nll_train: 14304.5994466146 kl_train: -0.1394478430 mse_train: 0.0752873666 acc_train: 0.4791666667 nll_val: 14602.6412760417 kl_val: -0.1876619558 mse_val: 0.0768560072 acc_val: 0.5000000000 time: 0.4979s
Epoch: 0163 nll_train: 14301.6071370443 kl_train: -0.1401598160 mse_train: 0.0752716164 acc_train: 0.5625000000 nll_val: 14597.6582031250 kl_val: -0.1820905755 mse_val: 0.0768297787 acc_val: 0.5000000000 time: 0.5002s
Epoch: 0164 nll_train: 14305.7779541016 kl_train: -0.1446922589 mse_train: 0.0752935678 acc_train: 0.5000000000 nll_val: 14572.6028645833 kl_val: -0.2143916041 mse_val: 0.0766979083 acc_val: 0.1666666667 time: 0.4989s
Epoch: 0165 nll_train: 14315.3583170573 kl_train: -0.1354897609 mse_train: 0.0753439915 acc_train: 0.5208333333 nll_val: 14535.3652343750 kl_val: -0.1876903425 mse_val: 0.0765019208 acc_val: 0.1666666667 time: 0.4980s
Epoch: 0166 nll_train: 14342.3959960938 kl_train: -0.1422433571 mse_train: 0.0754862952 acc_train: 0.5416666667 nll_val: 14539.4104817708 kl_val: -0.1958040992 mse_val: 0.0765232146 acc_val: 0.3333333333 time: 0.5093s
Epoch: 0167 nll_train: 14321.2733561198 kl_train: -0.1392290870 mse_train: 0.0753751225 acc_train: 0.5208333333 nll_val: 14534.1240234375 kl_val: -0.2016976029 mse_val: 0.0764953891 acc_val: 0.1666666667 time: 0.5551s
Epoch: 0168 nll_train: 14331.5799967448 kl_train: -0.1456166223 mse_train: 0.0754293692 acc_train: 0.5208333333 nll_val: 14388.9830729167 kl_val: -0.1880886058 mse_val: 0.0757314861 acc_val: 0.5000000000 time: 0.5089s
Epoch: 0169 nll_train: 14316.5796712240 kl_train: -0.1403290220 mse_train: 0.0753504196 acc_train: 0.5208333333 nll_val: 14561.9394531250 kl_val: -0.2279581577 mse_val: 0.0766417881 acc_val: 0.1666666667 time: 0.4970s
Epoch: 0170 nll_train: 14338.7308349609 kl_train: -0.1382452996 mse_train: 0.0754670043 acc_train: 0.5416666667 nll_val: 14452.3027343750 kl_val: -0.2064251602 mse_val: 0.0760647456 acc_val: 0.5000000000 time: 0.5040s
Epoch: 0171 nll_train: 14362.7353922526 kl_train: -0.1512348548 mse_train: 0.0755933458 acc_train: 0.4791666667 nll_val: 14423.7187500000 kl_val: -0.1821546505 mse_val: 0.0759143109 acc_val: 0.5000000000 time: 0.4999s
Epoch: 0172 nll_train: 14327.9234619141 kl_train: -0.1346775747 mse_train: 0.0754101230 acc_train: 0.5625000000 nll_val: 14428.5354817708 kl_val: -0.2081420273 mse_val: 0.0759396553 acc_val: 0.5000000000 time: 0.4987s
Epoch: 0173 nll_train: 14375.4893391927 kl_train: -0.1455263771 mse_train: 0.0756604704 acc_train: 0.4791666667 nll_val: 14419.2903645833 kl_val: -0.2281450778 mse_val: 0.0758910005 acc_val: 0.3333333333 time: 0.5003s
Epoch: 0174 nll_train: 14348.3593750000 kl_train: -0.1406958456 mse_train: 0.0755176827 acc_train: 0.5000000000 nll_val: 14453.1731770833 kl_val: -0.2273107121 mse_val: 0.0760693302 acc_val: 0.1666666667 time: 0.5021s
Epoch: 0175 nll_train: 14378.7228190104 kl_train: -0.1457635461 mse_train: 0.0756774900 acc_train: 0.5000000000 nll_val: 14523.9101562500 kl_val: -0.2180830091 mse_val: 0.0764416332 acc_val: 0.3333333333 time: 0.5003s
Epoch: 0176 nll_train: 14364.1734212240 kl_train: -0.1461601621 mse_train: 0.0756009131 acc_train: 0.5208333333 nll_val: 14610.0419921875 kl_val: -0.2474789868 mse_val: 0.0768949563 acc_val: 0.3333333333 time: 0.4994s
Epoch: 0177 nll_train: 14372.7861328125 kl_train: -0.1460600239 mse_train: 0.0756462427 acc_train: 0.5416666667 nll_val: 14413.0374348958 kl_val: -0.1912983010 mse_val: 0.0758580888 acc_val: 0.5000000000 time: 0.4970s
Epoch: 0178 nll_train: 14372.8690999349 kl_train: -0.1313976143 mse_train: 0.0756466791 acc_train: 0.5416666667 nll_val: 14330.8603515625 kl_val: -0.2076367488 mse_val: 0.0754255826 acc_val: 0.3333333333 time: 0.4988s
Epoch: 0179 nll_train: 14387.5822347005 kl_train: -0.1317606087 mse_train: 0.0757241162 acc_train: 0.5416666667 nll_val: 14673.5615234375 kl_val: -0.2281292876 mse_val: 0.0772292688 acc_val: 0.5000000000 time: 0.4988s
Epoch: 0180 nll_train: 14390.6225992839 kl_train: -0.1440627077 mse_train: 0.0757401188 acc_train: 0.5208333333 nll_val: 14425.3004557292 kl_val: -0.2313186626 mse_val: 0.0759226332 acc_val: 0.5000000000 time: 0.5001s
Epoch: 0181 nll_train: 14348.8784993490 kl_train: -0.1420824795 mse_train: 0.0755204139 acc_train: 0.5000000000 nll_val: 14329.2330729167 kl_val: -0.2364649971 mse_val: 0.0754170169 acc_val: 0.3333333333 time: 0.4960s
Epoch: 0182 nll_train: 14349.7469075521 kl_train: -0.1440715985 mse_train: 0.0755249833 acc_train: 0.5208333333 nll_val: 14316.9638671875 kl_val: -0.1894008120 mse_val: 0.0753524378 acc_val: 0.5000000000 time: 0.5009s
Epoch: 0183 nll_train: 14309.5759277344 kl_train: -0.1374527945 mse_train: 0.0753135576 acc_train: 0.5208333333 nll_val: 14274.9388020833 kl_val: -0.2159699301 mse_val: 0.0751312549 acc_val: 0.5000000000 time: 0.4988s
Epoch: 0184 nll_train: 14335.4371744792 kl_train: -0.1234690615 mse_train: 0.0754496705 acc_train: 0.5208333333 nll_val: 14272.7607421875 kl_val: -0.1563277890 mse_val: 0.0751197922 acc_val: 0.5000000000 time: 0.5029s
Epoch: 0185 nll_train: 14277.7284342448 kl_train: -0.1337627890 mse_train: 0.0751459394 acc_train: 0.5208333333 nll_val: 14310.4055989583 kl_val: -0.1899535805 mse_val: 0.0753179242 acc_val: 0.1666666667 time: 0.5033s
Epoch: 0186 nll_train: 14280.7931722005 kl_train: -0.1295914960 mse_train: 0.0751620708 acc_train: 0.5625000000 nll_val: 14422.3046875000 kl_val: -0.1837814450 mse_val: 0.0759068703 acc_val: 0.3333333333 time: 0.5014s
Epoch: 0187 nll_train: 14256.2543131510 kl_train: -0.1377313718 mse_train: 0.0750329169 acc_train: 0.5208333333 nll_val: 14394.3873697917 kl_val: -0.1658413857 mse_val: 0.0757599349 acc_val: 0.5000000000 time: 0.5010s
Epoch: 0188 nll_train: 14270.8328857422 kl_train: -0.1366354202 mse_train: 0.0751096473 acc_train: 0.5625000000 nll_val: 14487.7994791667 kl_val: -0.1827809264 mse_val: 0.0762515763 acc_val: 0.5000000000 time: 0.5012s
Epoch: 0189 nll_train: 14235.7088216146 kl_train: -0.1417658090 mse_train: 0.0749247844 acc_train: 0.5208333333 nll_val: 14466.4908854167 kl_val: -0.1886584709 mse_val: 0.0761394277 acc_val: 0.5000000000 time: 0.4974s
Epoch: 0190 nll_train: 14247.2417805990 kl_train: -0.1425886243 mse_train: 0.0749854838 acc_train: 0.5625000000 nll_val: 14637.3147786458 kl_val: -0.1793433130 mse_val: 0.0770384992 acc_val: 0.5000000000 time: 0.4979s
Epoch: 0191 nll_train: 14240.1292317708 kl_train: -0.1365959393 mse_train: 0.0749480496 acc_train: 0.5416666667 nll_val: 14528.2545572917 kl_val: -0.1824134539 mse_val: 0.0764644966 acc_val: 0.5000000000 time: 0.4991s
Epoch: 0192 nll_train: 14239.6913248698 kl_train: -0.1429891593 mse_train: 0.0749457430 acc_train: 0.5416666667 nll_val: 14604.5091145833 kl_val: -0.2340397586 mse_val: 0.0768658370 acc_val: 0.5000000000 time: 0.5027s
Epoch: 0193 nll_train: 14241.3095296224 kl_train: -0.1641310177 mse_train: 0.0749542618 acc_train: 0.5208333333 nll_val: 14486.7584635417 kl_val: -0.1938370168 mse_val: 0.0762460977 acc_val: 0.5000000000 time: 0.5042s
Epoch: 0194 nll_train: 14238.9703369141 kl_train: -0.1569650571 mse_train: 0.0749419487 acc_train: 0.5416666667 nll_val: 14708.1578776042 kl_val: -0.2227905144 mse_val: 0.0774113586 acc_val: 0.5000000000 time: 0.4993s
Epoch: 0195 nll_train: 14237.8169352214 kl_train: -0.1477397187 mse_train: 0.0749358777 acc_train: 0.5000000000 nll_val: 14484.5784505208 kl_val: -0.1898110062 mse_val: 0.0762346188 acc_val: 0.5000000000 time: 0.4978s
Epoch: 0196 nll_train: 14226.3197835286 kl_train: -0.1470428448 mse_train: 0.0748753686 acc_train: 0.5416666667 nll_val: 14630.0000000000 kl_val: -0.2020938496 mse_val: 0.0770000021 acc_val: 0.5000000000 time: 0.4984s
Epoch: 0197 nll_train: 14232.3642578125 kl_train: -0.1324485607 mse_train: 0.0749071818 acc_train: 0.5208333333 nll_val: 14533.5501302083 kl_val: -0.1799685558 mse_val: 0.0764923717 acc_val: 0.5000000000 time: 0.5007s
Epoch: 0198 nll_train: 14249.2362874349 kl_train: -0.1321749035 mse_train: 0.0749959811 acc_train: 0.5208333333 nll_val: 14447.0862630208 kl_val: -0.1865258863 mse_val: 0.0760372976 acc_val: 0.1666666667 time: 0.5015s
Epoch: 0199 nll_train: 14220.9351806641 kl_train: -0.1297082497 mse_train: 0.0748470294 acc_train: 0.4791666667 nll_val: 14379.3623046875 kl_val: -0.1779608577 mse_val: 0.0756808495 acc_val: 0.5000000000 time: 0.4971s
Optimization finished
Best epoch 20
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 12191.6012369792 kl_test: -0.0794812466 mse_test: 0.0641663211 acc_test: 0.5000000000
MSE: [ 0.066066026688 , 0.065102249384 , 0.065307557583 , 0.065245799720 , 0.064651988447 , 0.065283767879 , 0.064440719783 , 0.063799969852 , 0.062606789172 , 0.062152754515 , 0.062297973782 , 0.064406640828 , 0.065222017467 , 0.064817287028 , 0.064080052078 , 0.064480490983 , 0.063162177801 , 0.063673131168 , 0.063452973962 ]
Accuracy for experiment id 4 is 0.5
----------Iter = 100----------
Loss = 1.031023
Variable usage = 100.00%
----------Iter = 200----------
Loss = 1.028541
Variable usage = 100.00%
----------Iter = 300----------
Loss = 1.027067
Variable usage = 100.00%
----------Iter = 400----------
Loss = 1.025965
Variable usage = 100.00%
----------Iter = 500----------
Loss = 1.025077
Variable usage = 100.00%
----------Iter = 600----------
Loss = 1.024311
Variable usage = 100.00%
----------Iter = 700----------
Loss = 1.023666
Variable usage = 100.00%
----------Iter = 800----------
Loss = 1.023095
Variable usage = 100.00%
----------Iter = 900----------
Loss = 1.022572
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 1.022112
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 1.021691
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 1.021296
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 1.020933
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 1.020601
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 1.020284
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 1.019976
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 1.019676
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 1.019384
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 1.019094
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 1.018807
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 1.018517
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 1.018237
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 1.017967
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 1.017709
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 1.017457
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 1.017214
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 1.016975
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 1.016738
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 1.016511
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 1.016289
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 1.016067
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 1.015846
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 1.015632
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 1.015433
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 1.015245
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 1.015064
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 1.014890
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 1.014720
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 1.014546
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 1.014379
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 1.014213
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 1.014047
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 1.013888
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 1.013729
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 1.013565
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 1.013404
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 1.013238
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 1.013071
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 1.012912
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 1.012758
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 1.012608
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 1.012471
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 1.012335
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 1.012201
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 1.012075
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 1.011948
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 1.011822
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 1.011698
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 1.011573
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 1.011456
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 1.011347
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 1.011235
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 1.011126
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 1.011020
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 1.010919
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 1.010821
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 1.010724
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 1.010628
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 1.010535
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 1.010444
Variable usage = 100.00%
----------Iter = 7100----------
Loss = 1.010356
Variable usage = 100.00%
----------Iter = 7200----------
Loss = 1.010269
Variable usage = 100.00%
----------Iter = 7300----------
Loss = 1.010189
Variable usage = 100.00%
----------Iter = 7400----------
Loss = 1.010108
Variable usage = 100.00%
----------Iter = 7500----------
Loss = 1.010028
Variable usage = 100.00%
----------Iter = 7600----------
Loss = 1.009947
Variable usage = 100.00%
----------Iter = 7700----------
Loss = 1.009868
Variable usage = 100.00%
----------Iter = 7800----------
Loss = 1.009788
Variable usage = 100.00%
----------Iter = 7900----------
Loss = 1.009707
Variable usage = 100.00%
----------Iter = 8000----------
Loss = 1.009627
Variable usage = 100.00%
----------Iter = 8100----------
Loss = 1.009554
Variable usage = 100.00%
----------Iter = 8200----------
Loss = 1.009488
Variable usage = 100.00%
----------Iter = 8300----------
Loss = 1.009422
Variable usage = 100.00%
----------Iter = 8400----------
Loss = 1.009358
Variable usage = 100.00%
----------Iter = 8500----------
Loss = 1.009292
Variable usage = 100.00%
----------Iter = 8600----------
Loss = 1.009231
Variable usage = 100.00%
----------Iter = 8700----------
Loss = 1.009172
Variable usage = 100.00%
----------Iter = 8800----------
Loss = 1.009114
Variable usage = 100.00%
----------Iter = 8900----------
Loss = 1.009057
Variable usage = 100.00%
----------Iter = 9000----------
Loss = 1.009001
Variable usage = 100.00%
----------Iter = 9100----------
Loss = 1.008945
Variable usage = 100.00%
----------Iter = 9200----------
Loss = 1.008892
Variable usage = 100.00%
----------Iter = 9300----------
Loss = 1.008840
Variable usage = 100.00%
----------Iter = 9400----------
Loss = 1.008788
Variable usage = 100.00%
----------Iter = 9500----------
Loss = 1.008734
Variable usage = 100.00%
----------Iter = 9600----------
Loss = 1.008680
Variable usage = 100.00%
----------Iter = 9700----------
Loss = 1.008628
Variable usage = 100.00%
----------Iter = 9800----------
Loss = 1.008578
Variable usage = 100.00%
----------Iter = 9900----------
Loss = 1.008530
Variable usage = 100.00%
----------Iter = 10000----------
Loss = 1.008481
Variable usage = 100.00%
----------Iter = 10100----------
Loss = 1.008433
Variable usage = 100.00%
----------Iter = 10200----------
Loss = 1.008386
Variable usage = 100.00%
----------Iter = 10300----------
Loss = 1.008339
Variable usage = 100.00%
----------Iter = 10400----------
Loss = 1.008292
Variable usage = 100.00%
----------Iter = 10500----------
Loss = 1.008246
Variable usage = 100.00%
----------Iter = 10600----------
Loss = 1.008201
Variable usage = 100.00%
----------Iter = 10700----------
Loss = 1.008160
Variable usage = 100.00%
----------Iter = 10800----------
Loss = 1.008118
Variable usage = 100.00%
----------Iter = 10900----------
Loss = 1.008075
Variable usage = 100.00%
----------Iter = 11000----------
Loss = 1.008034
Variable usage = 100.00%
----------Iter = 11100----------
Loss = 1.007992
Variable usage = 100.00%
----------Iter = 11200----------
Loss = 1.007950
Variable usage = 100.00%
----------Iter = 11300----------
Loss = 1.007910
Variable usage = 100.00%
----------Iter = 11400----------
Loss = 1.007868
Variable usage = 100.00%
----------Iter = 11500----------
Loss = 1.007825
Variable usage = 100.00%
----------Iter = 11600----------
Loss = 1.007783
Variable usage = 100.00%
----------Iter = 11700----------
Loss = 1.007740
Variable usage = 100.00%
----------Iter = 11800----------
Loss = 1.007697
Variable usage = 100.00%
----------Iter = 11900----------
Loss = 1.007654
Variable usage = 100.00%
----------Iter = 12000----------
Loss = 1.007616
Variable usage = 100.00%
----------Iter = 12100----------
Loss = 1.007579
Variable usage = 100.00%
----------Iter = 12200----------
Loss = 1.007543
Variable usage = 100.00%
----------Iter = 12300----------
Loss = 1.007508
Variable usage = 100.00%
----------Iter = 12400----------
Loss = 1.007472
Variable usage = 100.00%
----------Iter = 12500----------
Loss = 1.007437
Variable usage = 100.00%
----------Iter = 12600----------
Loss = 1.007402
Variable usage = 100.00%
----------Iter = 12700----------
Loss = 1.007367
Variable usage = 100.00%
----------Iter = 12800----------
Loss = 1.007333
Variable usage = 100.00%
----------Iter = 12900----------
Loss = 1.007298
Variable usage = 100.00%
----------Iter = 13000----------
Loss = 1.007264
Variable usage = 100.00%
----------Iter = 13100----------
Loss = 1.007232
Variable usage = 100.00%
----------Iter = 13200----------
Loss = 1.007199
Variable usage = 100.00%
----------Iter = 13300----------
Loss = 1.007165
Variable usage = 100.00%
----------Iter = 13400----------
Loss = 1.007134
Variable usage = 100.00%
----------Iter = 13500----------
Loss = 1.007103
Variable usage = 100.00%
----------Iter = 13600----------
Loss = 1.007073
Variable usage = 100.00%
----------Iter = 13700----------
Loss = 1.007045
Variable usage = 100.00%
----------Iter = 13800----------
Loss = 1.007017
Variable usage = 100.00%
----------Iter = 13900----------
Loss = 1.006987
Variable usage = 100.00%
----------Iter = 14000----------
Loss = 1.006958
Variable usage = 100.00%
----------Iter = 14100----------
Loss = 1.006930
Variable usage = 100.00%
----------Iter = 14200----------
Loss = 1.006903
Variable usage = 100.00%
----------Iter = 14300----------
Loss = 1.006876
Variable usage = 100.00%
----------Iter = 14400----------
Loss = 1.006850
Variable usage = 100.00%
----------Iter = 14500----------
Loss = 1.006823
Variable usage = 100.00%
----------Iter = 14600----------
Loss = 1.006796
Variable usage = 100.00%
----------Iter = 14700----------
Loss = 1.006768
Variable usage = 100.00%
----------Iter = 14800----------
Loss = 1.006741
Variable usage = 100.00%
----------Iter = 14900----------
Loss = 1.006714
Variable usage = 100.00%
----------Iter = 15000----------
Loss = 1.006688
Variable usage = 100.00%
----------Iter = 15100----------
Loss = 1.006663
Variable usage = 100.00%
----------Iter = 15200----------
Loss = 1.006639
Variable usage = 100.00%
----------Iter = 15300----------
Loss = 1.006616
Variable usage = 100.00%
----------Iter = 15400----------
Loss = 1.006593
Variable usage = 100.00%
----------Iter = 15500----------
Loss = 1.006569
Variable usage = 100.00%
----------Iter = 15600----------
Loss = 1.006546
Variable usage = 100.00%
----------Iter = 15700----------
Loss = 1.006523
Variable usage = 100.00%
----------Iter = 15800----------
Loss = 1.006500
Variable usage = 100.00%
----------Iter = 15900----------
Loss = 1.006475
Variable usage = 100.00%
----------Iter = 16000----------
Loss = 1.006448
Variable usage = 100.00%
----------Iter = 16100----------
Loss = 1.006423
Variable usage = 100.00%
----------Iter = 16200----------
Loss = 1.006399
Variable usage = 100.00%
----------Iter = 16300----------
Loss = 1.006376
Variable usage = 100.00%
----------Iter = 16400----------
Loss = 1.006353
Variable usage = 100.00%
----------Iter = 16500----------
Loss = 1.006331
Variable usage = 100.00%
----------Iter = 16600----------
Loss = 1.006309
Variable usage = 100.00%
----------Iter = 16700----------
Loss = 1.006287
Variable usage = 100.00%
----------Iter = 16800----------
Loss = 1.006265
Variable usage = 100.00%
----------Iter = 16900----------
Loss = 1.006244
Variable usage = 100.00%
----------Iter = 17000----------
Loss = 1.006224
Variable usage = 100.00%
----------Iter = 17100----------
Loss = 1.006204
Variable usage = 100.00%
----------Iter = 17200----------
Loss = 1.006184
Variable usage = 100.00%
----------Iter = 17300----------
Loss = 1.006165
Variable usage = 100.00%
----------Iter = 17400----------
Loss = 1.006146
Variable usage = 100.00%
----------Iter = 17500----------
Loss = 1.006127
Variable usage = 100.00%
----------Iter = 17600----------
Loss = 1.006109
Variable usage = 100.00%
----------Iter = 17700----------
Loss = 1.006091
Variable usage = 100.00%
----------Iter = 17800----------
Loss = 1.006072
Variable usage = 100.00%
----------Iter = 17900----------
Loss = 1.006054
Variable usage = 100.00%
----------Iter = 18000----------
Loss = 1.006036
Variable usage = 100.00%
----------Iter = 18100----------
Loss = 1.006018
Variable usage = 100.00%
----------Iter = 18200----------
Loss = 1.006000
Variable usage = 100.00%
----------Iter = 18300----------
Loss = 1.005982
Variable usage = 100.00%
----------Iter = 18400----------
Loss = 1.005964
Variable usage = 100.00%
----------Iter = 18500----------
Loss = 1.005946
Variable usage = 100.00%
----------Iter = 18600----------
Loss = 1.005928
Variable usage = 100.00%
----------Iter = 18700----------
Loss = 1.005910
Variable usage = 100.00%
----------Iter = 18800----------
Loss = 1.005891
Variable usage = 100.00%
----------Iter = 18900----------
Loss = 1.005872
Variable usage = 100.00%
----------Iter = 19000----------
Loss = 1.005854
Variable usage = 100.00%
----------Iter = 19100----------
Loss = 1.005837
Variable usage = 100.00%
----------Iter = 19200----------
Loss = 1.005820
Variable usage = 100.00%
----------Iter = 19300----------
Loss = 1.005804
Variable usage = 100.00%
----------Iter = 19400----------
Loss = 1.005787
Variable usage = 100.00%
----------Iter = 19500----------
Loss = 1.005772
Variable usage = 100.00%
----------Iter = 19600----------
Loss = 1.005756
Variable usage = 100.00%
----------Iter = 19700----------
Loss = 1.005740
Variable usage = 100.00%
----------Iter = 19800----------
Loss = 1.005724
Variable usage = 100.00%
----------Iter = 19900----------
Loss = 1.005709
Variable usage = 100.00%
----------Iter = 20000----------
Loss = 1.005693
Variable usage = 100.00%
----------Iter = 20100----------
Loss = 1.005678
Variable usage = 100.00%
----------Iter = 20200----------
Loss = 1.005663
Variable usage = 100.00%
----------Iter = 20300----------
Loss = 1.005647
Variable usage = 100.00%
----------Iter = 20400----------
Loss = 1.005632
Variable usage = 100.00%
----------Iter = 20500----------
Loss = 1.005617
Variable usage = 100.00%
----------Iter = 20600----------
Loss = 1.005603
Variable usage = 100.00%
----------Iter = 20700----------
Loss = 1.005588
Variable usage = 100.00%
----------Iter = 20800----------
Loss = 1.005574
Variable usage = 100.00%
----------Iter = 20900----------
Loss = 1.005559
Variable usage = 100.00%
----------Iter = 21000----------
Loss = 1.005545
Variable usage = 100.00%
----------Iter = 21100----------
Loss = 1.005531
Variable usage = 100.00%
----------Iter = 21200----------
Loss = 1.005517
Variable usage = 100.00%
----------Iter = 21300----------
Loss = 1.005503
Variable usage = 100.00%
----------Iter = 21400----------
Loss = 1.005489
Variable usage = 100.00%
----------Iter = 21500----------
Loss = 1.005474
Variable usage = 100.00%
----------Iter = 21600----------
Loss = 1.005460
Variable usage = 100.00%
----------Iter = 21700----------
Loss = 1.005446
Variable usage = 100.00%
----------Iter = 21800----------
Loss = 1.005433
Variable usage = 100.00%
----------Iter = 21900----------
Loss = 1.005419
Variable usage = 100.00%
----------Iter = 22000----------
Loss = 1.005406
Variable usage = 100.00%
----------Iter = 22100----------
Loss = 1.005393
Variable usage = 100.00%
----------Iter = 22200----------
Loss = 1.005381
Variable usage = 100.00%
----------Iter = 22300----------
Loss = 1.005368
Variable usage = 100.00%
----------Iter = 22400----------
Loss = 1.005355
Variable usage = 100.00%
----------Iter = 22500----------
Loss = 1.005342
Variable usage = 100.00%
----------Iter = 22600----------
Loss = 1.005329
Variable usage = 100.00%
----------Iter = 22700----------
Loss = 1.005317
Variable usage = 100.00%
----------Iter = 22800----------
Loss = 1.005304
Variable usage = 100.00%
----------Iter = 22900----------
Loss = 1.005292
Variable usage = 100.00%
----------Iter = 23000----------
Loss = 1.005279
Variable usage = 100.00%
----------Iter = 23100----------
Loss = 1.005267
Variable usage = 100.00%
----------Iter = 23200----------
Loss = 1.005255
Variable usage = 100.00%
----------Iter = 23300----------
Loss = 1.005243
Variable usage = 100.00%
----------Iter = 23400----------
Loss = 1.005231
Variable usage = 100.00%
----------Iter = 23500----------
Loss = 1.005219
Variable usage = 100.00%
----------Iter = 23600----------
Loss = 1.005207
Variable usage = 100.00%
----------Iter = 23700----------
Loss = 1.005195
Variable usage = 100.00%
----------Iter = 23800----------
Loss = 1.005183
Variable usage = 100.00%
----------Iter = 23900----------
Loss = 1.005172
Variable usage = 100.00%
----------Iter = 24000----------
Loss = 1.005160
Variable usage = 100.00%
----------Iter = 24100----------
Loss = 1.005149
Variable usage = 100.00%
----------Iter = 24200----------
Loss = 1.005138
Variable usage = 100.00%
----------Iter = 24300----------
Loss = 1.005126
Variable usage = 100.00%
----------Iter = 24400----------
Loss = 1.005116
Variable usage = 100.00%
----------Iter = 24500----------
Loss = 1.005105
Variable usage = 100.00%
----------Iter = 24600----------
Loss = 1.005093
Variable usage = 100.00%
----------Iter = 24700----------
Loss = 1.005082
Variable usage = 100.00%
----------Iter = 24800----------
Loss = 1.005071
Variable usage = 100.00%
----------Iter = 24900----------
Loss = 1.005060
Variable usage = 100.00%
----------Iter = 25000----------
Loss = 1.005049
Variable usage = 100.00%
----------Iter = 25100----------
Loss = 1.005038
Variable usage = 100.00%
----------Iter = 25200----------
Loss = 1.005027
Variable usage = 100.00%
----------Iter = 25300----------
Loss = 1.005017
Variable usage = 100.00%
----------Iter = 25400----------
Loss = 1.005006
Variable usage = 100.00%
----------Iter = 25500----------
Loss = 1.004996
Variable usage = 100.00%
----------Iter = 25600----------
Loss = 1.004986
Variable usage = 100.00%
----------Iter = 25700----------
Loss = 1.004976
Variable usage = 100.00%
----------Iter = 25800----------
Loss = 1.004966
Variable usage = 100.00%
----------Iter = 25900----------
Loss = 1.004955
Variable usage = 100.00%
----------Iter = 26000----------
Loss = 1.004945
Variable usage = 100.00%
----------Iter = 26100----------
Loss = 1.004936
Variable usage = 100.00%
----------Iter = 26200----------
Loss = 1.004925
Variable usage = 100.00%
----------Iter = 26300----------
Loss = 1.004915
Variable usage = 100.00%
----------Iter = 26400----------
Loss = 1.004905
Variable usage = 100.00%
----------Iter = 26500----------
Loss = 1.004894
Variable usage = 100.00%
----------Iter = 26600----------
Loss = 1.004884
Variable usage = 100.00%
----------Iter = 26700----------
Loss = 1.004874
Variable usage = 100.00%
----------Iter = 26800----------
Loss = 1.004864
Variable usage = 100.00%
----------Iter = 26900----------
Loss = 1.004854
Variable usage = 100.00%
----------Iter = 27000----------
Loss = 1.004844
Variable usage = 100.00%
----------Iter = 27100----------
Loss = 1.004834
Variable usage = 100.00%
----------Iter = 27200----------
Loss = 1.004824
Variable usage = 100.00%
----------Iter = 27300----------
Loss = 1.004813
Variable usage = 100.00%
----------Iter = 27400----------
Loss = 1.004802
Variable usage = 100.00%
----------Iter = 27500----------
Loss = 1.004791
Variable usage = 100.00%
----------Iter = 27600----------
Loss = 1.004781
Variable usage = 100.00%
----------Iter = 27700----------
Loss = 1.004770
Variable usage = 100.00%
----------Iter = 27800----------
Loss = 1.004760
Variable usage = 100.00%
----------Iter = 27900----------
Loss = 1.004749
Variable usage = 100.00%
----------Iter = 28000----------
Loss = 1.004739
Variable usage = 100.00%
----------Iter = 28100----------
Loss = 1.004728
Variable usage = 100.00%
----------Iter = 28200----------
Loss = 1.004718
Variable usage = 100.00%
----------Iter = 28300----------
Loss = 1.004707
Variable usage = 100.00%
----------Iter = 28400----------
Loss = 1.004696
Variable usage = 100.00%
----------Iter = 28500----------
Loss = 1.004685
Variable usage = 100.00%
----------Iter = 28600----------
Loss = 1.004675
Variable usage = 100.00%
----------Iter = 28700----------
Loss = 1.004664
Variable usage = 100.00%
----------Iter = 28800----------
Loss = 1.004655
Variable usage = 100.00%
----------Iter = 28900----------
Loss = 1.004645
Variable usage = 100.00%
----------Iter = 29000----------
Loss = 1.004636
Variable usage = 100.00%
----------Iter = 29100----------
Loss = 1.004627
Variable usage = 100.00%
----------Iter = 29200----------
Loss = 1.004618
Variable usage = 100.00%
----------Iter = 29300----------
Loss = 1.004608
Variable usage = 100.00%
----------Iter = 29400----------
Loss = 1.004599
Variable usage = 100.00%
----------Iter = 29500----------
Loss = 1.004589
Variable usage = 100.00%
----------Iter = 29600----------
Loss = 1.004580
Variable usage = 100.00%
----------Iter = 29700----------
Loss = 1.004571
Variable usage = 100.00%
----------Iter = 29800----------
Loss = 1.004563
Variable usage = 100.00%
----------Iter = 29900----------
Loss = 1.004554
Variable usage = 100.00%
----------Iter = 30000----------
Loss = 1.004546
Variable usage = 100.00%
----------Iter = 30100----------
Loss = 1.004538
Variable usage = 100.00%
----------Iter = 30200----------
Loss = 1.004529
Variable usage = 100.00%
----------Iter = 30300----------
Loss = 1.004521
Variable usage = 100.00%
----------Iter = 30400----------
Loss = 1.004513
Variable usage = 100.00%
----------Iter = 30500----------
Loss = 1.004504
Variable usage = 100.00%
----------Iter = 30600----------
Loss = 1.004496
Variable usage = 100.00%
----------Iter = 30700----------
Loss = 1.004488
Variable usage = 100.00%
----------Iter = 30800----------
Loss = 1.004479
Variable usage = 100.00%
----------Iter = 30900----------
Loss = 1.004472
Variable usage = 100.00%
----------Iter = 31000----------
Loss = 1.004464
Variable usage = 100.00%
----------Iter = 31100----------
Loss = 1.004456
Variable usage = 100.00%
----------Iter = 31200----------
Loss = 1.004448
Variable usage = 100.00%
----------Iter = 31300----------
Loss = 1.004441
Variable usage = 100.00%
----------Iter = 31400----------
Loss = 1.004433
Variable usage = 100.00%
----------Iter = 31500----------
Loss = 1.004425
Variable usage = 100.00%
----------Iter = 31600----------
Loss = 1.004417
Variable usage = 100.00%
----------Iter = 31700----------
Loss = 1.004410
Variable usage = 100.00%
----------Iter = 31800----------
Loss = 1.004403
Variable usage = 100.00%
----------Iter = 31900----------
Loss = 1.004395
Variable usage = 100.00%
----------Iter = 32000----------
Loss = 1.004388
Variable usage = 100.00%
----------Iter = 32100----------
Loss = 1.004381
Variable usage = 100.00%
----------Iter = 32200----------
Loss = 1.004374
Variable usage = 100.00%
----------Iter = 32300----------
Loss = 1.004367
Variable usage = 100.00%
----------Iter = 32400----------
Loss = 1.004360
Variable usage = 100.00%
----------Iter = 32500----------
Loss = 1.004352
Variable usage = 100.00%
----------Iter = 32600----------
Loss = 1.004345
Variable usage = 100.00%
----------Iter = 32700----------
Loss = 1.004338
Variable usage = 100.00%
----------Iter = 32800----------
Loss = 1.004331
Variable usage = 100.00%
----------Iter = 32900----------
Loss = 1.004324
Variable usage = 100.00%
----------Iter = 33000----------
Loss = 1.004317
Variable usage = 100.00%
----------Iter = 33100----------
Loss = 1.004310
Variable usage = 100.00%
----------Iter = 33200----------
Loss = 1.004303
Variable usage = 100.00%
----------Iter = 33300----------
Loss = 1.004296
Variable usage = 100.00%
----------Iter = 33400----------
Loss = 1.004289
Variable usage = 100.00%
----------Iter = 33500----------
Loss = 1.004282
Variable usage = 100.00%
----------Iter = 33600----------
Loss = 1.004275
Variable usage = 100.00%
----------Iter = 33700----------
Loss = 1.004268
Variable usage = 100.00%
----------Iter = 33800----------
Loss = 1.004261
Variable usage = 100.00%
----------Iter = 33900----------
Loss = 1.004254
Variable usage = 100.00%
----------Iter = 34000----------
Loss = 1.004247
Variable usage = 100.00%
----------Iter = 34100----------
Loss = 1.004240
Variable usage = 100.00%
----------Iter = 34200----------
Loss = 1.004233
Variable usage = 100.00%
----------Iter = 34300----------
Loss = 1.004226
Variable usage = 100.00%
----------Iter = 34400----------
Loss = 1.004219
Variable usage = 100.00%
----------Iter = 34500----------
Loss = 1.004212
Variable usage = 100.00%
----------Iter = 34600----------
Loss = 1.004205
Variable usage = 100.00%
----------Iter = 34700----------
Loss = 1.004199
Variable usage = 100.00%
----------Iter = 34800----------
Loss = 1.004192
Variable usage = 100.00%
----------Iter = 34900----------
Loss = 1.004185
Variable usage = 100.00%
----------Iter = 35000----------
Loss = 1.004178
Variable usage = 100.00%
----------Iter = 35100----------
Loss = 1.004171
Variable usage = 100.00%
----------Iter = 35200----------
Loss = 1.004165
Variable usage = 100.00%
----------Iter = 35300----------
Loss = 1.004158
Variable usage = 100.00%
----------Iter = 35400----------
Loss = 1.004151
Variable usage = 100.00%
----------Iter = 35500----------
Loss = 1.004145
Variable usage = 100.00%
----------Iter = 35600----------
Loss = 1.004139
Variable usage = 100.00%
----------Iter = 35700----------
Loss = 1.004132
Variable usage = 100.00%
----------Iter = 35800----------
Loss = 1.004126
Variable usage = 100.00%
----------Iter = 35900----------
Loss = 1.004119
Variable usage = 100.00%
----------Iter = 36000----------
Loss = 1.004113
Variable usage = 100.00%
----------Iter = 36100----------
Loss = 1.004106
Variable usage = 100.00%
----------Iter = 36200----------
Loss = 1.004099
Variable usage = 100.00%
----------Iter = 36300----------
Loss = 1.004092
Variable usage = 100.00%
----------Iter = 36400----------
Loss = 1.004085
Variable usage = 100.00%
----------Iter = 36500----------
Loss = 1.004079
Variable usage = 100.00%
----------Iter = 36600----------
Loss = 1.004072
Variable usage = 100.00%
----------Iter = 36700----------
Loss = 1.004066
Variable usage = 100.00%
----------Iter = 36800----------
Loss = 1.004059
Variable usage = 100.00%
----------Iter = 36900----------
Loss = 1.004053
Variable usage = 100.00%
----------Iter = 37000----------
Loss = 1.004046
Variable usage = 100.00%
----------Iter = 37100----------
Loss = 1.004040
Variable usage = 100.00%
----------Iter = 37200----------
Loss = 1.004032
Variable usage = 100.00%
----------Iter = 37300----------
Loss = 1.004023
Variable usage = 100.00%
----------Iter = 37400----------
Loss = 1.004014
Variable usage = 100.00%
----------Iter = 37500----------
Loss = 1.004008
Variable usage = 100.00%
----------Iter = 37600----------
Loss = 1.004001
Variable usage = 100.00%
----------Iter = 37700----------
Loss = 1.003996
Variable usage = 100.00%
----------Iter = 37800----------
Loss = 1.003989
Variable usage = 100.00%
----------Iter = 37900----------
Loss = 1.003983
Variable usage = 100.00%
----------Iter = 38000----------
Loss = 1.003977
Variable usage = 100.00%
----------Iter = 38100----------
Loss = 1.003971
Variable usage = 100.00%
----------Iter = 38200----------
Loss = 1.003965
Variable usage = 100.00%
----------Iter = 38300----------
Loss = 1.003960
Variable usage = 100.00%
----------Iter = 38400----------
Loss = 1.003954
Variable usage = 100.00%
----------Iter = 38500----------
Loss = 1.003947
Variable usage = 100.00%
----------Iter = 38600----------
Loss = 1.003941
Variable usage = 100.00%
----------Iter = 38700----------
Loss = 1.003935
Variable usage = 100.00%
----------Iter = 38800----------
Loss = 1.003929
Variable usage = 100.00%
----------Iter = 38900----------
Loss = 1.003922
Variable usage = 100.00%
----------Iter = 39000----------
Loss = 1.003917
Variable usage = 100.00%
----------Iter = 39100----------
Loss = 1.003911
Variable usage = 100.00%
----------Iter = 39200----------
Loss = 1.003905
Variable usage = 100.00%
----------Iter = 39300----------
Loss = 1.003899
Variable usage = 100.00%
----------Iter = 39400----------
Loss = 1.003893
Variable usage = 100.00%
----------Iter = 39500----------
Loss = 1.003887
Variable usage = 100.00%
----------Iter = 39600----------
Loss = 1.003882
Variable usage = 100.00%
----------Iter = 39700----------
Loss = 1.003876
Variable usage = 100.00%
----------Iter = 39800----------
Loss = 1.003871
Variable usage = 100.00%
----------Iter = 39900----------
Loss = 1.003865
Variable usage = 100.00%
----------Iter = 40000----------
Loss = 1.003860
Variable usage = 100.00%
----------Iter = 40100----------
Loss = 1.003854
Variable usage = 100.00%
----------Iter = 40200----------
Loss = 1.003849
Variable usage = 100.00%
----------Iter = 40300----------
Loss = 1.003844
Variable usage = 100.00%
----------Iter = 40400----------
Loss = 1.003838
Variable usage = 100.00%
----------Iter = 40500----------
Loss = 1.003833
Variable usage = 100.00%
----------Iter = 40600----------
Loss = 1.003828
Variable usage = 100.00%
----------Iter = 40700----------
Loss = 1.003823
Variable usage = 100.00%
----------Iter = 40800----------
Loss = 1.003818
Variable usage = 100.00%
----------Iter = 40900----------
Loss = 1.003812
Variable usage = 100.00%
----------Iter = 41000----------
Loss = 1.003807
Variable usage = 100.00%
----------Iter = 41100----------
Loss = 1.003802
Variable usage = 100.00%
----------Iter = 41200----------
Loss = 1.003797
Variable usage = 100.00%
----------Iter = 41300----------
Loss = 1.003792
Variable usage = 100.00%
----------Iter = 41400----------
Loss = 1.003787
Variable usage = 100.00%
----------Iter = 41500----------
Loss = 1.003782
Variable usage = 100.00%
----------Iter = 41600----------
Loss = 1.003777
Variable usage = 100.00%
----------Iter = 41700----------
Loss = 1.003772
Variable usage = 100.00%
----------Iter = 41800----------
Loss = 1.003767
Variable usage = 100.00%
----------Iter = 41900----------
Loss = 1.003762
Variable usage = 100.00%
----------Iter = 42000----------
Loss = 1.003757
Variable usage = 100.00%
----------Iter = 42100----------
Loss = 1.003752
Variable usage = 100.00%
----------Iter = 42200----------
Loss = 1.003747
Variable usage = 100.00%
----------Iter = 42300----------
Loss = 1.003743
Variable usage = 100.00%
----------Iter = 42400----------
Loss = 1.003738
Variable usage = 100.00%
----------Iter = 42500----------
Loss = 1.003733
Variable usage = 100.00%
----------Iter = 42600----------
Loss = 1.003728
Variable usage = 100.00%
----------Iter = 42700----------
Loss = 1.003723
Variable usage = 100.00%
----------Iter = 42800----------
Loss = 1.003718
Variable usage = 100.00%
----------Iter = 42900----------
Loss = 1.003713
Variable usage = 100.00%
----------Iter = 43000----------
Loss = 1.003709
Variable usage = 100.00%
----------Iter = 43100----------
Loss = 1.003704
Variable usage = 100.00%
----------Iter = 43200----------
Loss = 1.003699
Variable usage = 100.00%
----------Iter = 43300----------
Loss = 1.003694
Variable usage = 100.00%
----------Iter = 43400----------
Loss = 1.003690
Variable usage = 100.00%
----------Iter = 43500----------
Loss = 1.003685
Variable usage = 100.00%
----------Iter = 43600----------
Loss = 1.003680
Variable usage = 100.00%
----------Iter = 43700----------
Loss = 1.003675
Variable usage = 100.00%
----------Iter = 43800----------
Loss = 1.003671
Variable usage = 100.00%
----------Iter = 43900----------
Loss = 1.003666
Variable usage = 100.00%
----------Iter = 44000----------
Loss = 1.003661
Variable usage = 100.00%
----------Iter = 44100----------
Loss = 1.003656
Variable usage = 100.00%
----------Iter = 44200----------
Loss = 1.003651
Variable usage = 100.00%
----------Iter = 44300----------
Loss = 1.003646
Variable usage = 100.00%
----------Iter = 44400----------
Loss = 1.003642
Variable usage = 100.00%
----------Iter = 44500----------
Loss = 1.003637
Variable usage = 100.00%
----------Iter = 44600----------
Loss = 1.003633
Variable usage = 100.00%
----------Iter = 44700----------
Loss = 1.003628
Variable usage = 100.00%
----------Iter = 44800----------
Loss = 1.003624
Variable usage = 100.00%
----------Iter = 44900----------
Loss = 1.003619
Variable usage = 100.00%
----------Iter = 45000----------
Loss = 1.003615
Variable usage = 100.00%
----------Iter = 45100----------
Loss = 1.003610
Variable usage = 100.00%
----------Iter = 45200----------
Loss = 1.003606
Variable usage = 100.00%
----------Iter = 45300----------
Loss = 1.003602
Variable usage = 100.00%
----------Iter = 45400----------
Loss = 1.003597
Variable usage = 100.00%
----------Iter = 45500----------
Loss = 1.003593
Variable usage = 100.00%
----------Iter = 45600----------
Loss = 1.003589
Variable usage = 100.00%
----------Iter = 45700----------
Loss = 1.003584
Variable usage = 100.00%
----------Iter = 45800----------
Loss = 1.003580
Variable usage = 100.00%
----------Iter = 45900----------
Loss = 1.003576
Variable usage = 100.00%
----------Iter = 46000----------
Loss = 1.003572
Variable usage = 100.00%
----------Iter = 46100----------
Loss = 1.003567
Variable usage = 100.00%
----------Iter = 46200----------
Loss = 1.003563
Variable usage = 100.00%
----------Iter = 46300----------
Loss = 1.003558
Variable usage = 100.00%
----------Iter = 46400----------
Loss = 1.003554
Variable usage = 100.00%
----------Iter = 46500----------
Loss = 1.003550
Variable usage = 100.00%
----------Iter = 46600----------
Loss = 1.003545
Variable usage = 100.00%
----------Iter = 46700----------
Loss = 1.003541
Variable usage = 100.00%
----------Iter = 46800----------
Loss = 1.003536
Variable usage = 100.00%
----------Iter = 46900----------
Loss = 1.003532
Variable usage = 100.00%
----------Iter = 47000----------
Loss = 1.003528
Variable usage = 100.00%
----------Iter = 47100----------
Loss = 1.003524
Variable usage = 100.00%
----------Iter = 47200----------
Loss = 1.003519
Variable usage = 100.00%
----------Iter = 47300----------
Loss = 1.003515
Variable usage = 100.00%
----------Iter = 47400----------
Loss = 1.003511
Variable usage = 100.00%
----------Iter = 47500----------
Loss = 1.003506
Variable usage = 100.00%
----------Iter = 47600----------
Loss = 1.003502
Variable usage = 100.00%
----------Iter = 47700----------
Loss = 1.003498
Variable usage = 100.00%
----------Iter = 47800----------
Loss = 1.003494
Variable usage = 100.00%
----------Iter = 47900----------
Loss = 1.003490
Variable usage = 100.00%
----------Iter = 48000----------
Loss = 1.003486
Variable usage = 100.00%
----------Iter = 48100----------
Loss = 1.003481
Variable usage = 100.00%
----------Iter = 48200----------
Loss = 1.003477
Variable usage = 100.00%
----------Iter = 48300----------
Loss = 1.003473
Variable usage = 100.00%
----------Iter = 48400----------
Loss = 1.003469
Variable usage = 100.00%
----------Iter = 48500----------
Loss = 1.003465
Variable usage = 100.00%
----------Iter = 48600----------
Loss = 1.003461
Variable usage = 100.00%
----------Iter = 48700----------
Loss = 1.003457
Variable usage = 100.00%
----------Iter = 48800----------
Loss = 1.003453
Variable usage = 100.00%
----------Iter = 48900----------
Loss = 1.003449
Variable usage = 100.00%
----------Iter = 49000----------
Loss = 1.003445
Variable usage = 100.00%
----------Iter = 49100----------
Loss = 1.003441
Variable usage = 100.00%
----------Iter = 49200----------
Loss = 1.003438
Variable usage = 100.00%
----------Iter = 49300----------
Loss = 1.003434
Variable usage = 100.00%
----------Iter = 49400----------
Loss = 1.003429
Variable usage = 100.00%
----------Iter = 49500----------
Loss = 1.003425
Variable usage = 100.00%
----------Iter = 49600----------
Loss = 1.003420
Variable usage = 100.00%
----------Iter = 49700----------
Loss = 1.003415
Variable usage = 100.00%
----------Iter = 49800----------
Loss = 1.003411
Variable usage = 100.00%
----------Iter = 49900----------
Loss = 1.003407
Variable usage = 100.00%
----------Iter = 50000----------
Loss = 1.003403
Variable usage = 100.00%
True variable usage = 75.00%
Estimated variable usage = 100.00%
Accuracy = 0.75%
Accuracy for experiment id 5 is 0.75
----------Iter = 100----------
Loss = 1.038498
Variable usage = 100.00%
----------Iter = 200----------
Loss = 1.036246
Variable usage = 100.00%
----------Iter = 300----------
Loss = 1.034809
Variable usage = 100.00%
----------Iter = 400----------
Loss = 1.033767
Variable usage = 100.00%
----------Iter = 500----------
Loss = 1.032958
Variable usage = 100.00%
----------Iter = 600----------
Loss = 1.032290
Variable usage = 100.00%
----------Iter = 700----------
Loss = 1.031718
Variable usage = 100.00%
----------Iter = 800----------
Loss = 1.031233
Variable usage = 100.00%
----------Iter = 900----------
Loss = 1.030802
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 1.030409
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 1.030036
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 1.029678
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 1.029338
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 1.029002
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 1.028674
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 1.028366
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 1.028072
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 1.027787
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 1.027510
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 1.027231
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 1.026950
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 1.026687
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 1.026436
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 1.026197
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 1.025971
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 1.025755
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 1.025536
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 1.025329
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 1.025128
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 1.024935
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 1.024747
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 1.024561
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 1.024380
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 1.024207
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 1.024043
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 1.023885
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 1.023723
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 1.023559
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 1.023400
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 1.023245
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 1.023093
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 1.022942
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 1.022800
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 1.022658
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 1.022516
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 1.022380
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 1.022251
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 1.022130
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 1.022009
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 1.021886
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 1.021764
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 1.021647
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 1.021528
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 1.021411
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 1.021296
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 1.021186
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 1.021074
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 1.020964
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 1.020864
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 1.020765
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 1.020668
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 1.020574
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 1.020478
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 1.020381
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 1.020281
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 1.020186
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 1.020095
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 1.020009
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 1.019924
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 1.019843
Variable usage = 100.00%
----------Iter = 7100----------
Loss = 1.019771
Variable usage = 100.00%
----------Iter = 7200----------
Loss = 1.019700
Variable usage = 100.00%
----------Iter = 7300----------
Loss = 1.019632
Variable usage = 100.00%
----------Iter = 7400----------
Loss = 1.019564
Variable usage = 100.00%
----------Iter = 7500----------
Loss = 1.019496
Variable usage = 100.00%
----------Iter = 7600----------
Loss = 1.019427
Variable usage = 100.00%
----------Iter = 7700----------
Loss = 1.019361
Variable usage = 100.00%
----------Iter = 7800----------
Loss = 1.019297
Variable usage = 100.00%
----------Iter = 7900----------
Loss = 1.019233
Variable usage = 100.00%
----------Iter = 8000----------
Loss = 1.019170
Variable usage = 100.00%
----------Iter = 8100----------
Loss = 1.019109
Variable usage = 100.00%
----------Iter = 8200----------
Loss = 1.019048
Variable usage = 100.00%
----------Iter = 8300----------
Loss = 1.018986
Variable usage = 100.00%
----------Iter = 8400----------
Loss = 1.018925
Variable usage = 100.00%
----------Iter = 8500----------
Loss = 1.018866
Variable usage = 100.00%
----------Iter = 8600----------
Loss = 1.018804
Variable usage = 100.00%
----------Iter = 8700----------
Loss = 1.018745
Variable usage = 100.00%
----------Iter = 8800----------
Loss = 1.018685
Variable usage = 100.00%
----------Iter = 8900----------
Loss = 1.018625
Variable usage = 100.00%
----------Iter = 9000----------
Loss = 1.018565
Variable usage = 100.00%
----------Iter = 9100----------
Loss = 1.018507
Variable usage = 100.00%
----------Iter = 9200----------
Loss = 1.018451
Variable usage = 100.00%
----------Iter = 9300----------
Loss = 1.018396
Variable usage = 100.00%
----------Iter = 9400----------
Loss = 1.018340
Variable usage = 100.00%
----------Iter = 9500----------
Loss = 1.018286
Variable usage = 100.00%
----------Iter = 9600----------
Loss = 1.018235
Variable usage = 100.00%
----------Iter = 9700----------
Loss = 1.018183
Variable usage = 100.00%
----------Iter = 9800----------
Loss = 1.018133
Variable usage = 100.00%
----------Iter = 9900----------
Loss = 1.018085
Variable usage = 100.00%
----------Iter = 10000----------
Loss = 1.018039
Variable usage = 100.00%
----------Iter = 10100----------
Loss = 1.017995
Variable usage = 100.00%
----------Iter = 10200----------
Loss = 1.017949
Variable usage = 100.00%
----------Iter = 10300----------
Loss = 1.017901
Variable usage = 100.00%
----------Iter = 10400----------
Loss = 1.017853
Variable usage = 100.00%
----------Iter = 10500----------
Loss = 1.017807
Variable usage = 100.00%
----------Iter = 10600----------
Loss = 1.017758
Variable usage = 100.00%
----------Iter = 10700----------
Loss = 1.017710
Variable usage = 100.00%
----------Iter = 10800----------
Loss = 1.017662
Variable usage = 100.00%
----------Iter = 10900----------
Loss = 1.017616
Variable usage = 100.00%
----------Iter = 11000----------
Loss = 1.017572
Variable usage = 100.00%
----------Iter = 11100----------
Loss = 1.017532
Variable usage = 100.00%
----------Iter = 11200----------
Loss = 1.017492
Variable usage = 100.00%
----------Iter = 11300----------
Loss = 1.017453
Variable usage = 100.00%
----------Iter = 11400----------
Loss = 1.017413
Variable usage = 100.00%
----------Iter = 11500----------
Loss = 1.017374
Variable usage = 100.00%
----------Iter = 11600----------
Loss = 1.017335
Variable usage = 100.00%
----------Iter = 11700----------
Loss = 1.017297
Variable usage = 100.00%
----------Iter = 11800----------
Loss = 1.017260
Variable usage = 100.00%
----------Iter = 11900----------
Loss = 1.017223
Variable usage = 100.00%
----------Iter = 12000----------
Loss = 1.017187
Variable usage = 100.00%
----------Iter = 12100----------
Loss = 1.017152
Variable usage = 100.00%
----------Iter = 12200----------
Loss = 1.017118
Variable usage = 100.00%
----------Iter = 12300----------
Loss = 1.017086
Variable usage = 100.00%
----------Iter = 12400----------
Loss = 1.017054
Variable usage = 100.00%
----------Iter = 12500----------
Loss = 1.017023
Variable usage = 100.00%
----------Iter = 12600----------
Loss = 1.016992
Variable usage = 100.00%
----------Iter = 12700----------
Loss = 1.016961
Variable usage = 100.00%
----------Iter = 12800----------
Loss = 1.016933
Variable usage = 100.00%
----------Iter = 12900----------
Loss = 1.016904
Variable usage = 100.00%
----------Iter = 13000----------
Loss = 1.016876
Variable usage = 100.00%
----------Iter = 13100----------
Loss = 1.016848
Variable usage = 100.00%
----------Iter = 13200----------
Loss = 1.016820
Variable usage = 100.00%
----------Iter = 13300----------
Loss = 1.016791
Variable usage = 100.00%
----------Iter = 13400----------
Loss = 1.016763
Variable usage = 100.00%
----------Iter = 13500----------
Loss = 1.016734
Variable usage = 100.00%
----------Iter = 13600----------
Loss = 1.016707
Variable usage = 100.00%
----------Iter = 13700----------
Loss = 1.016680
Variable usage = 100.00%
----------Iter = 13800----------
Loss = 1.016653
Variable usage = 100.00%
----------Iter = 13900----------
Loss = 1.016625
Variable usage = 100.00%
----------Iter = 14000----------
Loss = 1.016598
Variable usage = 100.00%
----------Iter = 14100----------
Loss = 1.016569
Variable usage = 100.00%
----------Iter = 14200----------
Loss = 1.016540
Variable usage = 100.00%
----------Iter = 14300----------
Loss = 1.016513
Variable usage = 100.00%
----------Iter = 14400----------
Loss = 1.016485
Variable usage = 100.00%
----------Iter = 14500----------
Loss = 1.016459
Variable usage = 100.00%
----------Iter = 14600----------
Loss = 1.016433
Variable usage = 100.00%
----------Iter = 14700----------
Loss = 1.016407
Variable usage = 100.00%
----------Iter = 14800----------
Loss = 1.016381
Variable usage = 100.00%
----------Iter = 14900----------
Loss = 1.016355
Variable usage = 100.00%
----------Iter = 15000----------
Loss = 1.016330
Variable usage = 100.00%
----------Iter = 15100----------
Loss = 1.016304
Variable usage = 100.00%
----------Iter = 15200----------
Loss = 1.016278
Variable usage = 100.00%
----------Iter = 15300----------
Loss = 1.016253
Variable usage = 100.00%
----------Iter = 15400----------
Loss = 1.016228
Variable usage = 100.00%
----------Iter = 15500----------
Loss = 1.016204
Variable usage = 100.00%
----------Iter = 15600----------
Loss = 1.016181
Variable usage = 100.00%
----------Iter = 15700----------
Loss = 1.016156
Variable usage = 100.00%
----------Iter = 15800----------
Loss = 1.016131
Variable usage = 100.00%
----------Iter = 15900----------
Loss = 1.016108
Variable usage = 100.00%
----------Iter = 16000----------
Loss = 1.016086
Variable usage = 100.00%
----------Iter = 16100----------
Loss = 1.016064
Variable usage = 100.00%
----------Iter = 16200----------
Loss = 1.016043
Variable usage = 100.00%
----------Iter = 16300----------
Loss = 1.016021
Variable usage = 100.00%
----------Iter = 16400----------
Loss = 1.016001
Variable usage = 100.00%
----------Iter = 16500----------
Loss = 1.015980
Variable usage = 100.00%
----------Iter = 16600----------
Loss = 1.015958
Variable usage = 100.00%
----------Iter = 16700----------
Loss = 1.015936
Variable usage = 100.00%
----------Iter = 16800----------
Loss = 1.015914
Variable usage = 100.00%
----------Iter = 16900----------
Loss = 1.015890
Variable usage = 100.00%
----------Iter = 17000----------
Loss = 1.015865
Variable usage = 100.00%
----------Iter = 17100----------
Loss = 1.015841
Variable usage = 100.00%
----------Iter = 17200----------
Loss = 1.015818
Variable usage = 100.00%
----------Iter = 17300----------
Loss = 1.015795
Variable usage = 100.00%
----------Iter = 17400----------
Loss = 1.015775
Variable usage = 100.00%
----------Iter = 17500----------
Loss = 1.015756
Variable usage = 100.00%
----------Iter = 17600----------
Loss = 1.015737
Variable usage = 100.00%
----------Iter = 17700----------
Loss = 1.015718
Variable usage = 100.00%
----------Iter = 17800----------
Loss = 1.015699
Variable usage = 100.00%
----------Iter = 17900----------
Loss = 1.015679
Variable usage = 100.00%
----------Iter = 18000----------
Loss = 1.015660
Variable usage = 100.00%
----------Iter = 18100----------
Loss = 1.015642
Variable usage = 100.00%
----------Iter = 18200----------
Loss = 1.015624
Variable usage = 100.00%
----------Iter = 18300----------
Loss = 1.015605
Variable usage = 100.00%
----------Iter = 18400----------
Loss = 1.015586
Variable usage = 100.00%
----------Iter = 18500----------
Loss = 1.015566
Variable usage = 100.00%
----------Iter = 18600----------
Loss = 1.015546
Variable usage = 100.00%
----------Iter = 18700----------
Loss = 1.015525
Variable usage = 100.00%
----------Iter = 18800----------
Loss = 1.015505
Variable usage = 100.00%
----------Iter = 18900----------
Loss = 1.015484
Variable usage = 100.00%
----------Iter = 19000----------
Loss = 1.015463
Variable usage = 100.00%
----------Iter = 19100----------
Loss = 1.015444
Variable usage = 100.00%
----------Iter = 19200----------
Loss = 1.015427
Variable usage = 100.00%
----------Iter = 19300----------
Loss = 1.015409
Variable usage = 100.00%
----------Iter = 19400----------
Loss = 1.015392
Variable usage = 100.00%
----------Iter = 19500----------
Loss = 1.015374
Variable usage = 100.00%
----------Iter = 19600----------
Loss = 1.015357
Variable usage = 100.00%
----------Iter = 19700----------
Loss = 1.015340
Variable usage = 100.00%
----------Iter = 19800----------
Loss = 1.015324
Variable usage = 100.00%
----------Iter = 19900----------
Loss = 1.015308
Variable usage = 100.00%
----------Iter = 20000----------
Loss = 1.015292
Variable usage = 100.00%
----------Iter = 20100----------
Loss = 1.015275
Variable usage = 100.00%
----------Iter = 20200----------
Loss = 1.015259
Variable usage = 100.00%
----------Iter = 20300----------
Loss = 1.015244
Variable usage = 100.00%
----------Iter = 20400----------
Loss = 1.015228
Variable usage = 100.00%
----------Iter = 20500----------
Loss = 1.015213
Variable usage = 100.00%
----------Iter = 20600----------
Loss = 1.015198
Variable usage = 100.00%
----------Iter = 20700----------
Loss = 1.015182
Variable usage = 100.00%
----------Iter = 20800----------
Loss = 1.015167
Variable usage = 100.00%
----------Iter = 20900----------
Loss = 1.015153
Variable usage = 100.00%
----------Iter = 21000----------
Loss = 1.015140
Variable usage = 100.00%
----------Iter = 21100----------
Loss = 1.015127
Variable usage = 100.00%
----------Iter = 21200----------
Loss = 1.015114
Variable usage = 100.00%
----------Iter = 21300----------
Loss = 1.015101
Variable usage = 100.00%
----------Iter = 21400----------
Loss = 1.015088
Variable usage = 100.00%
----------Iter = 21500----------
Loss = 1.015076
Variable usage = 100.00%
----------Iter = 21600----------
Loss = 1.015063
Variable usage = 100.00%
----------Iter = 21700----------
Loss = 1.015050
Variable usage = 100.00%
----------Iter = 21800----------
Loss = 1.015037
Variable usage = 100.00%
----------Iter = 21900----------
Loss = 1.015025
Variable usage = 100.00%
----------Iter = 22000----------
Loss = 1.015012
Variable usage = 100.00%
----------Iter = 22100----------
Loss = 1.014999
Variable usage = 100.00%
----------Iter = 22200----------
Loss = 1.014987
Variable usage = 100.00%
----------Iter = 22300----------
Loss = 1.014973
Variable usage = 100.00%
----------Iter = 22400----------
Loss = 1.014961
Variable usage = 100.00%
----------Iter = 22500----------
Loss = 1.014949
Variable usage = 100.00%
----------Iter = 22600----------
Loss = 1.014937
Variable usage = 100.00%
----------Iter = 22700----------
Loss = 1.014925
Variable usage = 100.00%
----------Iter = 22800----------
Loss = 1.014912
Variable usage = 100.00%
----------Iter = 22900----------
Loss = 1.014900
Variable usage = 100.00%
----------Iter = 23000----------
Loss = 1.014888
Variable usage = 100.00%
----------Iter = 23100----------
Loss = 1.014875
Variable usage = 100.00%
----------Iter = 23200----------
Loss = 1.014864
Variable usage = 100.00%
----------Iter = 23300----------
Loss = 1.014853
Variable usage = 100.00%
----------Iter = 23400----------
Loss = 1.014841
Variable usage = 100.00%
----------Iter = 23500----------
Loss = 1.014830
Variable usage = 100.00%
----------Iter = 23600----------
Loss = 1.014818
Variable usage = 100.00%
----------Iter = 23700----------
Loss = 1.014807
Variable usage = 100.00%
----------Iter = 23800----------
Loss = 1.014797
Variable usage = 100.00%
----------Iter = 23900----------
Loss = 1.014786
Variable usage = 100.00%
----------Iter = 24000----------
Loss = 1.014776
Variable usage = 100.00%
----------Iter = 24100----------
Loss = 1.014766
Variable usage = 100.00%
----------Iter = 24200----------
Loss = 1.014755
Variable usage = 100.00%
----------Iter = 24300----------
Loss = 1.014744
Variable usage = 100.00%
----------Iter = 24400----------
Loss = 1.014733
Variable usage = 100.00%
----------Iter = 24500----------
Loss = 1.014722
Variable usage = 100.00%
----------Iter = 24600----------
Loss = 1.014711
Variable usage = 100.00%
----------Iter = 24700----------
Loss = 1.014699
Variable usage = 100.00%
----------Iter = 24800----------
Loss = 1.014686
Variable usage = 100.00%
----------Iter = 24900----------
Loss = 1.014673
Variable usage = 100.00%
----------Iter = 25000----------
Loss = 1.014661
Variable usage = 100.00%
----------Iter = 25100----------
Loss = 1.014648
Variable usage = 100.00%
----------Iter = 25200----------
Loss = 1.014636
Variable usage = 100.00%
----------Iter = 25300----------
Loss = 1.014624
Variable usage = 100.00%
----------Iter = 25400----------
Loss = 1.014612
Variable usage = 100.00%
----------Iter = 25500----------
Loss = 1.014600
Variable usage = 100.00%
----------Iter = 25600----------
Loss = 1.014589
Variable usage = 100.00%
----------Iter = 25700----------
Loss = 1.014578
Variable usage = 100.00%
----------Iter = 25800----------
Loss = 1.014565
Variable usage = 100.00%
----------Iter = 25900----------
Loss = 1.014554
Variable usage = 100.00%
----------Iter = 26000----------
Loss = 1.014543
Variable usage = 100.00%
----------Iter = 26100----------
Loss = 1.014533
Variable usage = 100.00%
----------Iter = 26200----------
Loss = 1.014523
Variable usage = 100.00%
----------Iter = 26300----------
Loss = 1.014513
Variable usage = 100.00%
----------Iter = 26400----------
Loss = 1.014503
Variable usage = 100.00%
----------Iter = 26500----------
Loss = 1.014492
Variable usage = 100.00%
----------Iter = 26600----------
Loss = 1.014482
Variable usage = 100.00%
----------Iter = 26700----------
Loss = 1.014472
Variable usage = 100.00%
----------Iter = 26800----------
Loss = 1.014462
Variable usage = 100.00%
----------Iter = 26900----------
Loss = 1.014452
Variable usage = 100.00%
----------Iter = 27000----------
Loss = 1.014441
Variable usage = 100.00%
----------Iter = 27100----------
Loss = 1.014429
Variable usage = 100.00%
----------Iter = 27200----------
Loss = 1.014418
Variable usage = 100.00%
----------Iter = 27300----------
Loss = 1.014407
Variable usage = 100.00%
----------Iter = 27400----------
Loss = 1.014395
Variable usage = 100.00%
----------Iter = 27500----------
Loss = 1.014384
Variable usage = 100.00%
----------Iter = 27600----------
Loss = 1.014372
Variable usage = 100.00%
----------Iter = 27700----------
Loss = 1.014361
Variable usage = 100.00%
----------Iter = 27800----------
Loss = 1.014350
Variable usage = 100.00%
----------Iter = 27900----------
Loss = 1.014339
Variable usage = 100.00%
----------Iter = 28000----------
Loss = 1.014328
Variable usage = 100.00%
----------Iter = 28100----------
Loss = 1.014318
Variable usage = 100.00%
----------Iter = 28200----------
Loss = 1.014308
Variable usage = 100.00%
----------Iter = 28300----------
Loss = 1.014299
Variable usage = 100.00%
----------Iter = 28400----------
Loss = 1.014289
Variable usage = 100.00%
----------Iter = 28500----------
Loss = 1.014279
Variable usage = 100.00%
----------Iter = 28600----------
Loss = 1.014269
Variable usage = 100.00%
----------Iter = 28700----------
Loss = 1.014258
Variable usage = 100.00%
----------Iter = 28800----------
Loss = 1.014248
Variable usage = 100.00%
----------Iter = 28900----------
Loss = 1.014238
Variable usage = 100.00%
----------Iter = 29000----------
Loss = 1.014229
Variable usage = 100.00%
----------Iter = 29100----------
Loss = 1.014220
Variable usage = 100.00%
----------Iter = 29200----------
Loss = 1.014212
Variable usage = 100.00%
----------Iter = 29300----------
Loss = 1.014203
Variable usage = 100.00%
----------Iter = 29400----------
Loss = 1.014194
Variable usage = 100.00%
----------Iter = 29500----------
Loss = 1.014185
Variable usage = 100.00%
----------Iter = 29600----------
Loss = 1.014176
Variable usage = 100.00%
----------Iter = 29700----------
Loss = 1.014166
Variable usage = 100.00%
----------Iter = 29800----------
Loss = 1.014157
Variable usage = 100.00%
----------Iter = 29900----------
Loss = 1.014148
Variable usage = 100.00%
----------Iter = 30000----------
Loss = 1.014139
Variable usage = 100.00%
----------Iter = 30100----------
Loss = 1.014131
Variable usage = 100.00%
----------Iter = 30200----------
Loss = 1.014123
Variable usage = 100.00%
----------Iter = 30300----------
Loss = 1.014115
Variable usage = 100.00%
----------Iter = 30400----------
Loss = 1.014107
Variable usage = 100.00%
----------Iter = 30500----------
Loss = 1.014099
Variable usage = 100.00%
----------Iter = 30600----------
Loss = 1.014091
Variable usage = 100.00%
----------Iter = 30700----------
Loss = 1.014083
Variable usage = 100.00%
----------Iter = 30800----------
Loss = 1.014074
Variable usage = 100.00%
----------Iter = 30900----------
Loss = 1.014066
Variable usage = 100.00%
----------Iter = 31000----------
Loss = 1.014058
Variable usage = 100.00%
----------Iter = 31100----------
Loss = 1.014050
Variable usage = 100.00%
----------Iter = 31200----------
Loss = 1.014042
Variable usage = 100.00%
----------Iter = 31300----------
Loss = 1.014033
Variable usage = 100.00%
----------Iter = 31400----------
Loss = 1.014024
Variable usage = 100.00%
----------Iter = 31500----------
Loss = 1.014015
Variable usage = 100.00%
----------Iter = 31600----------
Loss = 1.014007
Variable usage = 100.00%
----------Iter = 31700----------
Loss = 1.013998
Variable usage = 100.00%
----------Iter = 31800----------
Loss = 1.013990
Variable usage = 100.00%
----------Iter = 31900----------
Loss = 1.013982
Variable usage = 100.00%
----------Iter = 32000----------
Loss = 1.013974
Variable usage = 100.00%
----------Iter = 32100----------
Loss = 1.013967
Variable usage = 100.00%
----------Iter = 32200----------
Loss = 1.013959
Variable usage = 100.00%
----------Iter = 32300----------
Loss = 1.013952
Variable usage = 100.00%
----------Iter = 32400----------
Loss = 1.013945
Variable usage = 100.00%
----------Iter = 32500----------
Loss = 1.013937
Variable usage = 100.00%
----------Iter = 32600----------
Loss = 1.013929
Variable usage = 100.00%
----------Iter = 32700----------
Loss = 1.013922
Variable usage = 100.00%
----------Iter = 32800----------
Loss = 1.013915
Variable usage = 100.00%
----------Iter = 32900----------
Loss = 1.013908
Variable usage = 100.00%
----------Iter = 33000----------
Loss = 1.013902
Variable usage = 100.00%
----------Iter = 33100----------
Loss = 1.013895
Variable usage = 100.00%
----------Iter = 33200----------
Loss = 1.013889
Variable usage = 100.00%
----------Iter = 33300----------
Loss = 1.013882
Variable usage = 100.00%
----------Iter = 33400----------
Loss = 1.013875
Variable usage = 100.00%
----------Iter = 33500----------
Loss = 1.013869
Variable usage = 100.00%
----------Iter = 33600----------
Loss = 1.013863
Variable usage = 100.00%
----------Iter = 33700----------
Loss = 1.013857
Variable usage = 100.00%
----------Iter = 33800----------
Loss = 1.013851
Variable usage = 100.00%
----------Iter = 33900----------
Loss = 1.013846
Variable usage = 100.00%
----------Iter = 34000----------
Loss = 1.013840
Variable usage = 100.00%
----------Iter = 34100----------
Loss = 1.013834
Variable usage = 100.00%
----------Iter = 34200----------
Loss = 1.013828
Variable usage = 100.00%
----------Iter = 34300----------
Loss = 1.013823
Variable usage = 100.00%
----------Iter = 34400----------
Loss = 1.013817
Variable usage = 100.00%
----------Iter = 34500----------
Loss = 1.013810
Variable usage = 100.00%
----------Iter = 34600----------
Loss = 1.013804
Variable usage = 100.00%
----------Iter = 34700----------
Loss = 1.013797
Variable usage = 100.00%
----------Iter = 34800----------
Loss = 1.013791
Variable usage = 100.00%
----------Iter = 34900----------
Loss = 1.013786
Variable usage = 100.00%
----------Iter = 35000----------
Loss = 1.013780
Variable usage = 100.00%
----------Iter = 35100----------
Loss = 1.013775
Variable usage = 100.00%
----------Iter = 35200----------
Loss = 1.013769
Variable usage = 100.00%
----------Iter = 35300----------
Loss = 1.013764
Variable usage = 100.00%
----------Iter = 35400----------
Loss = 1.013759
Variable usage = 100.00%
----------Iter = 35500----------
Loss = 1.013754
Variable usage = 100.00%
----------Iter = 35600----------
Loss = 1.013749
Variable usage = 100.00%
----------Iter = 35700----------
Loss = 1.013744
Variable usage = 100.00%
----------Iter = 35800----------
Loss = 1.013739
Variable usage = 100.00%
----------Iter = 35900----------
Loss = 1.013734
Variable usage = 100.00%
----------Iter = 36000----------
Loss = 1.013729
Variable usage = 100.00%
----------Iter = 36100----------
Loss = 1.013724
Variable usage = 100.00%
----------Iter = 36200----------
Loss = 1.013719
Variable usage = 100.00%
----------Iter = 36300----------
Loss = 1.013714
Variable usage = 100.00%
----------Iter = 36400----------
Loss = 1.013709
Variable usage = 100.00%
----------Iter = 36500----------
Loss = 1.013704
Variable usage = 100.00%
----------Iter = 36600----------
Loss = 1.013699
Variable usage = 100.00%
----------Iter = 36700----------
Loss = 1.013694
Variable usage = 100.00%
----------Iter = 36800----------
Loss = 1.013689
Variable usage = 100.00%
----------Iter = 36900----------
Loss = 1.013685
Variable usage = 100.00%
----------Iter = 37000----------
Loss = 1.013680
Variable usage = 100.00%
----------Iter = 37100----------
Loss = 1.013676
Variable usage = 100.00%
----------Iter = 37200----------
Loss = 1.013671
Variable usage = 100.00%
----------Iter = 37300----------
Loss = 1.013667
Variable usage = 100.00%
----------Iter = 37400----------
Loss = 1.013662
Variable usage = 100.00%
----------Iter = 37500----------
Loss = 1.013658
Variable usage = 100.00%
----------Iter = 37600----------
Loss = 1.013654
Variable usage = 100.00%
----------Iter = 37700----------
Loss = 1.013649
Variable usage = 100.00%
----------Iter = 37800----------
Loss = 1.013645
Variable usage = 100.00%
----------Iter = 37900----------
Loss = 1.013641
Variable usage = 100.00%
----------Iter = 38000----------
Loss = 1.013637
Variable usage = 100.00%
----------Iter = 38100----------
Loss = 1.013633
Variable usage = 100.00%
----------Iter = 38200----------
Loss = 1.013629
Variable usage = 100.00%
----------Iter = 38300----------
Loss = 1.013625
Variable usage = 100.00%
----------Iter = 38400----------
Loss = 1.013621
Variable usage = 100.00%
----------Iter = 38500----------
Loss = 1.013617
Variable usage = 100.00%
----------Iter = 38600----------
Loss = 1.013613
Variable usage = 100.00%
----------Iter = 38700----------
Loss = 1.013609
Variable usage = 100.00%
----------Iter = 38800----------
Loss = 1.013605
Variable usage = 100.00%
----------Iter = 38900----------
Loss = 1.013601
Variable usage = 100.00%
----------Iter = 39000----------
Loss = 1.013598
Variable usage = 100.00%
----------Iter = 39100----------
Loss = 1.013594
Variable usage = 100.00%
----------Iter = 39200----------
Loss = 1.013590
Variable usage = 100.00%
----------Iter = 39300----------
Loss = 1.013586
Variable usage = 100.00%
----------Iter = 39400----------
Loss = 1.013583
Variable usage = 100.00%
----------Iter = 39500----------
Loss = 1.013579
Variable usage = 100.00%
----------Iter = 39600----------
Loss = 1.013576
Variable usage = 100.00%
----------Iter = 39700----------
Loss = 1.013572
Variable usage = 100.00%
----------Iter = 39800----------
Loss = 1.013568
Variable usage = 100.00%
----------Iter = 39900----------
Loss = 1.013565
Variable usage = 100.00%
----------Iter = 40000----------
Loss = 1.013561
Variable usage = 100.00%
----------Iter = 40100----------
Loss = 1.013558
Variable usage = 100.00%
----------Iter = 40200----------
Loss = 1.013554
Variable usage = 100.00%
----------Iter = 40300----------
Loss = 1.013551
Variable usage = 100.00%
----------Iter = 40400----------
Loss = 1.013548
Variable usage = 100.00%
----------Iter = 40500----------
Loss = 1.013544
Variable usage = 100.00%
----------Iter = 40600----------
Loss = 1.013541
Variable usage = 100.00%
----------Iter = 40700----------
Loss = 1.013537
Variable usage = 100.00%
----------Iter = 40800----------
Loss = 1.013534
Variable usage = 100.00%
----------Iter = 40900----------
Loss = 1.013530
Variable usage = 100.00%
----------Iter = 41000----------
Loss = 1.013526
Variable usage = 100.00%
----------Iter = 41100----------
Loss = 1.013522
Variable usage = 100.00%
----------Iter = 41200----------
Loss = 1.013519
Variable usage = 100.00%
----------Iter = 41300----------
Loss = 1.013515
Variable usage = 100.00%
----------Iter = 41400----------
Loss = 1.013512
Variable usage = 100.00%
----------Iter = 41500----------
Loss = 1.013508
Variable usage = 100.00%
----------Iter = 41600----------
Loss = 1.013505
Variable usage = 100.00%
----------Iter = 41700----------
Loss = 1.013502
Variable usage = 100.00%
----------Iter = 41800----------
Loss = 1.013499
Variable usage = 100.00%
----------Iter = 41900----------
Loss = 1.013496
Variable usage = 100.00%
----------Iter = 42000----------
Loss = 1.013492
Variable usage = 100.00%
----------Iter = 42100----------
Loss = 1.013489
Variable usage = 100.00%
----------Iter = 42200----------
Loss = 1.013486
Variable usage = 100.00%
----------Iter = 42300----------
Loss = 1.013483
Variable usage = 100.00%
----------Iter = 42400----------
Loss = 1.013480
Variable usage = 100.00%
----------Iter = 42500----------
Loss = 1.013477
Variable usage = 100.00%
----------Iter = 42600----------
Loss = 1.013473
Variable usage = 100.00%
----------Iter = 42700----------
Loss = 1.013470
Variable usage = 100.00%
----------Iter = 42800----------
Loss = 1.013467
Variable usage = 100.00%
----------Iter = 42900----------
Loss = 1.013464
Variable usage = 100.00%
----------Iter = 43000----------
Loss = 1.013461
Variable usage = 100.00%
----------Iter = 43100----------
Loss = 1.013458
Variable usage = 100.00%
----------Iter = 43200----------
Loss = 1.013456
Variable usage = 100.00%
----------Iter = 43300----------
Loss = 1.013453
Variable usage = 100.00%
----------Iter = 43400----------
Loss = 1.013450
Variable usage = 100.00%
----------Iter = 43500----------
Loss = 1.013447
Variable usage = 100.00%
----------Iter = 43600----------
Loss = 1.013444
Variable usage = 100.00%
----------Iter = 43700----------
Loss = 1.013441
Variable usage = 100.00%
----------Iter = 43800----------
Loss = 1.013439
Variable usage = 100.00%
----------Iter = 43900----------
Loss = 1.013436
Variable usage = 100.00%
----------Iter = 44000----------
Loss = 1.013433
Variable usage = 100.00%
----------Iter = 44100----------
Loss = 1.013430
Variable usage = 100.00%
----------Iter = 44200----------
Loss = 1.013427
Variable usage = 100.00%
----------Iter = 44300----------
Loss = 1.013425
Variable usage = 100.00%
----------Iter = 44400----------
Loss = 1.013422
Variable usage = 100.00%
----------Iter = 44500----------
Loss = 1.013420
Variable usage = 100.00%
----------Iter = 44600----------
Loss = 1.013417
Variable usage = 100.00%
----------Iter = 44700----------
Loss = 1.013414
Variable usage = 100.00%
----------Iter = 44800----------
Loss = 1.013412
Variable usage = 100.00%
----------Iter = 44900----------
Loss = 1.013409
Variable usage = 100.00%
----------Iter = 45000----------
Loss = 1.013407
Variable usage = 100.00%
----------Iter = 45100----------
Loss = 1.013404
Variable usage = 100.00%
----------Iter = 45200----------
Loss = 1.013401
Variable usage = 100.00%
----------Iter = 45300----------
Loss = 1.013399
Variable usage = 100.00%
----------Iter = 45400----------
Loss = 1.013396
Variable usage = 100.00%
----------Iter = 45500----------
Loss = 1.013394
Variable usage = 100.00%
----------Iter = 45600----------
Loss = 1.013391
Variable usage = 100.00%
----------Iter = 45700----------
Loss = 1.013389
Variable usage = 100.00%
----------Iter = 45800----------
Loss = 1.013386
Variable usage = 100.00%
----------Iter = 45900----------
Loss = 1.013384
Variable usage = 100.00%
----------Iter = 46000----------
Loss = 1.013381
Variable usage = 100.00%
----------Iter = 46100----------
Loss = 1.013379
Variable usage = 100.00%
----------Iter = 46200----------
Loss = 1.013376
Variable usage = 100.00%
----------Iter = 46300----------
Loss = 1.013374
Variable usage = 100.00%
----------Iter = 46400----------
Loss = 1.013372
Variable usage = 100.00%
----------Iter = 46500----------
Loss = 1.013369
Variable usage = 100.00%
----------Iter = 46600----------
Loss = 1.013367
Variable usage = 100.00%
----------Iter = 46700----------
Loss = 1.013364
Variable usage = 100.00%
----------Iter = 46800----------
Loss = 1.013362
Variable usage = 100.00%
----------Iter = 46900----------
Loss = 1.013359
Variable usage = 100.00%
----------Iter = 47000----------
Loss = 1.013357
Variable usage = 100.00%
----------Iter = 47100----------
Loss = 1.013354
Variable usage = 100.00%
----------Iter = 47200----------
Loss = 1.013351
Variable usage = 100.00%
----------Iter = 47300----------
Loss = 1.013349
Variable usage = 100.00%
----------Iter = 47400----------
Loss = 1.013347
Variable usage = 100.00%
----------Iter = 47500----------
Loss = 1.013344
Variable usage = 100.00%
----------Iter = 47600----------
Loss = 1.013342
Variable usage = 100.00%
----------Iter = 47700----------
Loss = 1.013340
Variable usage = 100.00%
----------Iter = 47800----------
Loss = 1.013337
Variable usage = 100.00%
----------Iter = 47900----------
Loss = 1.013335
Variable usage = 100.00%
----------Iter = 48000----------
Loss = 1.013333
Variable usage = 100.00%
----------Iter = 48100----------
Loss = 1.013331
Variable usage = 100.00%
----------Iter = 48200----------
Loss = 1.013328
Variable usage = 100.00%
----------Iter = 48300----------
Loss = 1.013326
Variable usage = 100.00%
----------Iter = 48400----------
Loss = 1.013324
Variable usage = 100.00%
----------Iter = 48500----------
Loss = 1.013322
Variable usage = 100.00%
----------Iter = 48600----------
Loss = 1.013319
Variable usage = 100.00%
----------Iter = 48700----------
Loss = 1.013317
Variable usage = 100.00%
----------Iter = 48800----------
Loss = 1.013314
Variable usage = 100.00%
----------Iter = 48900----------
Loss = 1.013312
Variable usage = 100.00%
----------Iter = 49000----------
Loss = 1.013310
Variable usage = 100.00%
----------Iter = 49100----------
Loss = 1.013307
Variable usage = 100.00%
----------Iter = 49200----------
Loss = 1.013305
Variable usage = 100.00%
----------Iter = 49300----------
Loss = 1.013303
Variable usage = 100.00%
----------Iter = 49400----------
Loss = 1.013300
Variable usage = 100.00%
----------Iter = 49500----------
Loss = 1.013298
Variable usage = 100.00%
----------Iter = 49600----------
Loss = 1.013295
Variable usage = 100.00%
----------Iter = 49700----------
Loss = 1.013293
Variable usage = 100.00%
----------Iter = 49800----------
Loss = 1.013290
Variable usage = 100.00%
----------Iter = 49900----------
Loss = 1.013287
Variable usage = 100.00%
----------Iter = 50000----------
Loss = 1.013285
Variable usage = 100.00%
True variable usage = 75.00%
Estimated variable usage = 100.00%
Accuracy = 0.75%
Accuracy for experiment id 6 is 0.75
----------Iter = 100----------
Loss = 1.046895
Variable usage = 100.00%
----------Iter = 200----------
Loss = 1.044821
Variable usage = 100.00%
----------Iter = 300----------
Loss = 1.043444
Variable usage = 100.00%
----------Iter = 400----------
Loss = 1.042423
Variable usage = 100.00%
----------Iter = 500----------
Loss = 1.041623
Variable usage = 100.00%
----------Iter = 600----------
Loss = 1.040955
Variable usage = 100.00%
----------Iter = 700----------
Loss = 1.040378
Variable usage = 100.00%
----------Iter = 800----------
Loss = 1.039882
Variable usage = 100.00%
----------Iter = 900----------
Loss = 1.039430
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 1.039010
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 1.038617
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 1.038251
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 1.037907
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 1.037567
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 1.037251
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 1.036958
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 1.036686
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 1.036423
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 1.036173
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 1.035937
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 1.035707
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 1.035484
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 1.035267
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 1.035058
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 1.034856
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 1.034655
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 1.034458
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 1.034272
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 1.034092
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 1.033918
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 1.033745
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 1.033575
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 1.033412
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 1.033246
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 1.033081
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 1.032910
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 1.032746
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 1.032592
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 1.032449
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 1.032307
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 1.032174
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 1.032042
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 1.031912
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 1.031781
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 1.031650
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 1.031522
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 1.031393
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 1.031263
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 1.031132
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 1.031001
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 1.030876
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 1.030757
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 1.030639
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 1.030524
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 1.030414
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 1.030309
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 1.030204
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 1.030102
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 1.030006
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 1.029915
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 1.029821
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 1.029727
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 1.029635
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 1.029543
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 1.029451
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 1.029359
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 1.029270
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 1.029184
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 1.029101
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 1.029022
Variable usage = 100.00%
----------Iter = 7100----------
Loss = 1.028943
Variable usage = 100.00%
----------Iter = 7200----------
Loss = 1.028864
Variable usage = 100.00%
----------Iter = 7300----------
Loss = 1.028786
Variable usage = 100.00%
----------Iter = 7400----------
Loss = 1.028712
Variable usage = 100.00%
----------Iter = 7500----------
Loss = 1.028643
Variable usage = 100.00%
----------Iter = 7600----------
Loss = 1.028577
Variable usage = 100.00%
----------Iter = 7700----------
Loss = 1.028510
Variable usage = 100.00%
----------Iter = 7800----------
Loss = 1.028445
Variable usage = 100.00%
----------Iter = 7900----------
Loss = 1.028383
Variable usage = 100.00%
----------Iter = 8000----------
Loss = 1.028319
Variable usage = 100.00%
----------Iter = 8100----------
Loss = 1.028256
Variable usage = 100.00%
----------Iter = 8200----------
Loss = 1.028196
Variable usage = 100.00%
----------Iter = 8300----------
Loss = 1.028138
Variable usage = 100.00%
----------Iter = 8400----------
Loss = 1.028081
Variable usage = 100.00%
----------Iter = 8500----------
Loss = 1.028023
Variable usage = 100.00%
----------Iter = 8600----------
Loss = 1.027969
Variable usage = 100.00%
----------Iter = 8700----------
Loss = 1.027915
Variable usage = 100.00%
----------Iter = 8800----------
Loss = 1.027859
Variable usage = 100.00%
----------Iter = 8900----------
Loss = 1.027803
Variable usage = 100.00%
----------Iter = 9000----------
Loss = 1.027748
Variable usage = 100.00%
----------Iter = 9100----------
Loss = 1.027696
Variable usage = 100.00%
----------Iter = 9200----------
Loss = 1.027645
Variable usage = 100.00%
----------Iter = 9300----------
Loss = 1.027592
Variable usage = 100.00%
----------Iter = 9400----------
Loss = 1.027541
Variable usage = 100.00%
----------Iter = 9500----------
Loss = 1.027488
Variable usage = 100.00%
----------Iter = 9600----------
Loss = 1.027433
Variable usage = 100.00%
----------Iter = 9700----------
Loss = 1.027376
Variable usage = 100.00%
----------Iter = 9800----------
Loss = 1.027320
Variable usage = 100.00%
----------Iter = 9900----------
Loss = 1.027267
Variable usage = 100.00%
----------Iter = 10000----------
Loss = 1.027216
Variable usage = 100.00%
----------Iter = 10100----------
Loss = 1.027162
Variable usage = 100.00%
----------Iter = 10200----------
Loss = 1.027112
Variable usage = 100.00%
----------Iter = 10300----------
Loss = 1.027062
Variable usage = 100.00%
----------Iter = 10400----------
Loss = 1.027006
Variable usage = 100.00%
----------Iter = 10500----------
Loss = 1.026952
Variable usage = 100.00%
----------Iter = 10600----------
Loss = 1.026904
Variable usage = 100.00%
----------Iter = 10700----------
Loss = 1.026859
Variable usage = 100.00%
----------Iter = 10800----------
Loss = 1.026812
Variable usage = 100.00%
----------Iter = 10900----------
Loss = 1.026762
Variable usage = 100.00%
----------Iter = 11000----------
Loss = 1.026716
Variable usage = 100.00%
----------Iter = 11100----------
Loss = 1.026673
Variable usage = 100.00%
----------Iter = 11200----------
Loss = 1.026632
Variable usage = 100.00%
----------Iter = 11300----------
Loss = 1.026590
Variable usage = 100.00%
----------Iter = 11400----------
Loss = 1.026546
Variable usage = 100.00%
----------Iter = 11500----------
Loss = 1.026505
Variable usage = 100.00%
----------Iter = 11600----------
Loss = 1.026465
Variable usage = 100.00%
----------Iter = 11700----------
Loss = 1.026427
Variable usage = 100.00%
----------Iter = 11800----------
Loss = 1.026390
Variable usage = 100.00%
----------Iter = 11900----------
Loss = 1.026354
Variable usage = 100.00%
----------Iter = 12000----------
Loss = 1.026320
Variable usage = 100.00%
----------Iter = 12100----------
Loss = 1.026288
Variable usage = 100.00%
----------Iter = 12200----------
Loss = 1.026257
Variable usage = 100.00%
----------Iter = 12300----------
Loss = 1.026227
Variable usage = 100.00%
----------Iter = 12400----------
Loss = 1.026198
Variable usage = 100.00%
----------Iter = 12500----------
Loss = 1.026169
Variable usage = 100.00%
----------Iter = 12600----------
Loss = 1.026139
Variable usage = 100.00%
----------Iter = 12700----------
Loss = 1.026110
Variable usage = 100.00%
----------Iter = 12800----------
Loss = 1.026081
Variable usage = 100.00%
----------Iter = 12900----------
Loss = 1.026052
Variable usage = 100.00%
----------Iter = 13000----------
Loss = 1.026023
Variable usage = 100.00%
----------Iter = 13100----------
Loss = 1.025995
Variable usage = 100.00%
----------Iter = 13200----------
Loss = 1.025968
Variable usage = 100.00%
----------Iter = 13300----------
Loss = 1.025939
Variable usage = 100.00%
----------Iter = 13400----------
Loss = 1.025910
Variable usage = 100.00%
----------Iter = 13500----------
Loss = 1.025882
Variable usage = 100.00%
----------Iter = 13600----------
Loss = 1.025854
Variable usage = 100.00%
----------Iter = 13700----------
Loss = 1.025826
Variable usage = 100.00%
----------Iter = 13800----------
Loss = 1.025797
Variable usage = 100.00%
----------Iter = 13900----------
Loss = 1.025768
Variable usage = 100.00%
----------Iter = 14000----------
Loss = 1.025741
Variable usage = 100.00%
----------Iter = 14100----------
Loss = 1.025713
Variable usage = 100.00%
----------Iter = 14200----------
Loss = 1.025687
Variable usage = 100.00%
----------Iter = 14300----------
Loss = 1.025662
Variable usage = 100.00%
----------Iter = 14400----------
Loss = 1.025635
Variable usage = 100.00%
----------Iter = 14500----------
Loss = 1.025608
Variable usage = 100.00%
----------Iter = 14600----------
Loss = 1.025583
Variable usage = 100.00%
----------Iter = 14700----------
Loss = 1.025558
Variable usage = 100.00%
----------Iter = 14800----------
Loss = 1.025536
Variable usage = 100.00%
----------Iter = 14900----------
Loss = 1.025514
Variable usage = 100.00%
----------Iter = 15000----------
Loss = 1.025492
Variable usage = 100.00%
----------Iter = 15100----------
Loss = 1.025470
Variable usage = 100.00%
----------Iter = 15200----------
Loss = 1.025448
Variable usage = 100.00%
----------Iter = 15300----------
Loss = 1.025427
Variable usage = 100.00%
----------Iter = 15400----------
Loss = 1.025407
Variable usage = 100.00%
----------Iter = 15500----------
Loss = 1.025387
Variable usage = 100.00%
----------Iter = 15600----------
Loss = 1.025367
Variable usage = 100.00%
----------Iter = 15700----------
Loss = 1.025347
Variable usage = 100.00%
----------Iter = 15800----------
Loss = 1.025325
Variable usage = 100.00%
----------Iter = 15900----------
Loss = 1.025304
Variable usage = 100.00%
----------Iter = 16000----------
Loss = 1.025283
Variable usage = 100.00%
----------Iter = 16100----------
Loss = 1.025262
Variable usage = 100.00%
----------Iter = 16200----------
Loss = 1.025243
Variable usage = 100.00%
----------Iter = 16300----------
Loss = 1.025223
Variable usage = 100.00%
----------Iter = 16400----------
Loss = 1.025203
Variable usage = 100.00%
----------Iter = 16500----------
Loss = 1.025181
Variable usage = 100.00%
----------Iter = 16600----------
Loss = 1.025162
Variable usage = 100.00%
----------Iter = 16700----------
Loss = 1.025143
Variable usage = 100.00%
----------Iter = 16800----------
Loss = 1.025125
Variable usage = 100.00%
----------Iter = 16900----------
Loss = 1.025108
Variable usage = 100.00%
----------Iter = 17000----------
Loss = 1.025091
Variable usage = 100.00%
----------Iter = 17100----------
Loss = 1.025074
Variable usage = 100.00%
----------Iter = 17200----------
Loss = 1.025058
Variable usage = 100.00%
----------Iter = 17300----------
Loss = 1.025042
Variable usage = 100.00%
----------Iter = 17400----------
Loss = 1.025025
Variable usage = 100.00%
----------Iter = 17500----------
Loss = 1.025009
Variable usage = 100.00%
----------Iter = 17600----------
Loss = 1.024992
Variable usage = 100.00%
----------Iter = 17700----------
Loss = 1.024975
Variable usage = 100.00%
----------Iter = 17800----------
Loss = 1.024958
Variable usage = 100.00%
----------Iter = 17900----------
Loss = 1.024941
Variable usage = 100.00%
----------Iter = 18000----------
Loss = 1.024924
Variable usage = 100.00%
----------Iter = 18100----------
Loss = 1.024908
Variable usage = 100.00%
----------Iter = 18200----------
Loss = 1.024892
Variable usage = 100.00%
----------Iter = 18300----------
Loss = 1.024875
Variable usage = 100.00%
----------Iter = 18400----------
Loss = 1.024858
Variable usage = 100.00%
----------Iter = 18500----------
Loss = 1.024841
Variable usage = 100.00%
----------Iter = 18600----------
Loss = 1.024825
Variable usage = 100.00%
----------Iter = 18700----------
Loss = 1.024807
Variable usage = 100.00%
----------Iter = 18800----------
Loss = 1.024790
Variable usage = 100.00%
----------Iter = 18900----------
Loss = 1.024772
Variable usage = 100.00%
----------Iter = 19000----------
Loss = 1.024755
Variable usage = 100.00%
----------Iter = 19100----------
Loss = 1.024738
Variable usage = 100.00%
----------Iter = 19200----------
Loss = 1.024722
Variable usage = 100.00%
----------Iter = 19300----------
Loss = 1.024706
Variable usage = 100.00%
----------Iter = 19400----------
Loss = 1.024690
Variable usage = 100.00%
----------Iter = 19500----------
Loss = 1.024674
Variable usage = 100.00%
----------Iter = 19600----------
Loss = 1.024659
Variable usage = 100.00%
----------Iter = 19700----------
Loss = 1.024645
Variable usage = 100.00%
----------Iter = 19800----------
Loss = 1.024629
Variable usage = 100.00%
----------Iter = 19900----------
Loss = 1.024615
Variable usage = 100.00%
----------Iter = 20000----------
Loss = 1.024599
Variable usage = 100.00%
----------Iter = 20100----------
Loss = 1.024583
Variable usage = 100.00%
----------Iter = 20200----------
Loss = 1.024569
Variable usage = 100.00%
----------Iter = 20300----------
Loss = 1.024554
Variable usage = 100.00%
----------Iter = 20400----------
Loss = 1.024541
Variable usage = 100.00%
----------Iter = 20500----------
Loss = 1.024528
Variable usage = 100.00%
----------Iter = 20600----------
Loss = 1.024515
Variable usage = 100.00%
----------Iter = 20700----------
Loss = 1.024502
Variable usage = 100.00%
----------Iter = 20800----------
Loss = 1.024489
Variable usage = 100.00%
----------Iter = 20900----------
Loss = 1.024476
Variable usage = 100.00%
----------Iter = 21000----------
Loss = 1.024464
Variable usage = 100.00%
----------Iter = 21100----------
Loss = 1.024451
Variable usage = 100.00%
----------Iter = 21200----------
Loss = 1.024439
Variable usage = 100.00%
----------Iter = 21300----------
Loss = 1.024427
Variable usage = 100.00%
----------Iter = 21400----------
Loss = 1.024414
Variable usage = 100.00%
----------Iter = 21500----------
Loss = 1.024402
Variable usage = 100.00%
----------Iter = 21600----------
Loss = 1.024390
Variable usage = 100.00%
----------Iter = 21700----------
Loss = 1.024377
Variable usage = 100.00%
----------Iter = 21800----------
Loss = 1.024363
Variable usage = 100.00%
----------Iter = 21900----------
Loss = 1.024350
Variable usage = 100.00%
----------Iter = 22000----------
Loss = 1.024337
Variable usage = 100.00%
----------Iter = 22100----------
Loss = 1.024323
Variable usage = 100.00%
----------Iter = 22200----------
Loss = 1.024310
Variable usage = 100.00%
----------Iter = 22300----------
Loss = 1.024296
Variable usage = 100.00%
----------Iter = 22400----------
Loss = 1.024283
Variable usage = 100.00%
----------Iter = 22500----------
Loss = 1.024271
Variable usage = 100.00%
----------Iter = 22600----------
Loss = 1.024259
Variable usage = 100.00%
----------Iter = 22700----------
Loss = 1.024246
Variable usage = 100.00%
----------Iter = 22800----------
Loss = 1.024233
Variable usage = 100.00%
----------Iter = 22900----------
Loss = 1.024221
Variable usage = 100.00%
----------Iter = 23000----------
Loss = 1.024209
Variable usage = 100.00%
----------Iter = 23100----------
Loss = 1.024198
Variable usage = 100.00%
----------Iter = 23200----------
Loss = 1.024187
Variable usage = 100.00%
----------Iter = 23300----------
Loss = 1.024176
Variable usage = 100.00%
----------Iter = 23400----------
Loss = 1.024165
Variable usage = 100.00%
----------Iter = 23500----------
Loss = 1.024154
Variable usage = 100.00%
----------Iter = 23600----------
Loss = 1.024143
Variable usage = 100.00%
----------Iter = 23700----------
Loss = 1.024132
Variable usage = 100.00%
----------Iter = 23800----------
Loss = 1.024122
Variable usage = 100.00%
----------Iter = 23900----------
Loss = 1.024112
Variable usage = 100.00%
----------Iter = 24000----------
Loss = 1.024103
Variable usage = 100.00%
----------Iter = 24100----------
Loss = 1.024092
Variable usage = 100.00%
----------Iter = 24200----------
Loss = 1.024082
Variable usage = 100.00%
----------Iter = 24300----------
Loss = 1.024072
Variable usage = 100.00%
----------Iter = 24400----------
Loss = 1.024062
Variable usage = 100.00%
----------Iter = 24500----------
Loss = 1.024053
Variable usage = 100.00%
----------Iter = 24600----------
Loss = 1.024043
Variable usage = 100.00%
----------Iter = 24700----------
Loss = 1.024033
Variable usage = 100.00%
----------Iter = 24800----------
Loss = 1.024023
Variable usage = 100.00%
----------Iter = 24900----------
Loss = 1.024013
Variable usage = 100.00%
----------Iter = 25000----------
Loss = 1.024004
Variable usage = 100.00%
----------Iter = 25100----------
Loss = 1.023995
Variable usage = 100.00%
----------Iter = 25200----------
Loss = 1.023985
Variable usage = 100.00%
----------Iter = 25300----------
Loss = 1.023976
Variable usage = 100.00%
----------Iter = 25400----------
Loss = 1.023967
Variable usage = 100.00%
----------Iter = 25500----------
Loss = 1.023958
Variable usage = 100.00%
----------Iter = 25600----------
Loss = 1.023949
Variable usage = 100.00%
----------Iter = 25700----------
Loss = 1.023940
Variable usage = 100.00%
----------Iter = 25800----------
Loss = 1.023931
Variable usage = 100.00%
----------Iter = 25900----------
Loss = 1.023922
Variable usage = 100.00%
----------Iter = 26000----------
Loss = 1.023913
Variable usage = 100.00%
----------Iter = 26100----------
Loss = 1.023904
Variable usage = 100.00%
----------Iter = 26200----------
Loss = 1.023895
Variable usage = 100.00%
----------Iter = 26300----------
Loss = 1.023887
Variable usage = 100.00%
----------Iter = 26400----------
Loss = 1.023878
Variable usage = 100.00%
----------Iter = 26500----------
Loss = 1.023869
Variable usage = 100.00%
----------Iter = 26600----------
Loss = 1.023861
Variable usage = 100.00%
----------Iter = 26700----------
Loss = 1.023852
Variable usage = 100.00%
----------Iter = 26800----------
Loss = 1.023844
Variable usage = 100.00%
----------Iter = 26900----------
Loss = 1.023836
Variable usage = 100.00%
----------Iter = 27000----------
Loss = 1.023827
Variable usage = 100.00%
----------Iter = 27100----------
Loss = 1.023819
Variable usage = 100.00%
----------Iter = 27200----------
Loss = 1.023811
Variable usage = 100.00%
----------Iter = 27300----------
Loss = 1.023803
Variable usage = 100.00%
----------Iter = 27400----------
Loss = 1.023794
Variable usage = 100.00%
----------Iter = 27500----------
Loss = 1.023785
Variable usage = 100.00%
----------Iter = 27600----------
Loss = 1.023777
Variable usage = 100.00%
----------Iter = 27700----------
Loss = 1.023767
Variable usage = 100.00%
----------Iter = 27800----------
Loss = 1.023758
Variable usage = 100.00%
----------Iter = 27900----------
Loss = 1.023748
Variable usage = 100.00%
----------Iter = 28000----------
Loss = 1.023739
Variable usage = 100.00%
----------Iter = 28100----------
Loss = 1.023730
Variable usage = 100.00%
----------Iter = 28200----------
Loss = 1.023722
Variable usage = 100.00%
----------Iter = 28300----------
Loss = 1.023713
Variable usage = 100.00%
----------Iter = 28400----------
Loss = 1.023706
Variable usage = 100.00%
----------Iter = 28500----------
Loss = 1.023697
Variable usage = 100.00%
----------Iter = 28600----------
Loss = 1.023690
Variable usage = 100.00%
----------Iter = 28700----------
Loss = 1.023682
Variable usage = 100.00%
----------Iter = 28800----------
Loss = 1.023674
Variable usage = 100.00%
----------Iter = 28900----------
Loss = 1.023665
Variable usage = 100.00%
----------Iter = 29000----------
Loss = 1.023657
Variable usage = 100.00%
----------Iter = 29100----------
Loss = 1.023649
Variable usage = 100.00%
----------Iter = 29200----------
Loss = 1.023641
Variable usage = 100.00%
----------Iter = 29300----------
Loss = 1.023633
Variable usage = 100.00%
----------Iter = 29400----------
Loss = 1.023625
Variable usage = 100.00%
----------Iter = 29500----------
Loss = 1.023618
Variable usage = 100.00%
----------Iter = 29600----------
Loss = 1.023610
Variable usage = 100.00%
----------Iter = 29700----------
Loss = 1.023602
Variable usage = 100.00%
----------Iter = 29800----------
Loss = 1.023594
Variable usage = 100.00%
----------Iter = 29900----------
Loss = 1.023586
Variable usage = 100.00%
----------Iter = 30000----------
Loss = 1.023578
Variable usage = 100.00%
----------Iter = 30100----------
Loss = 1.023571
Variable usage = 100.00%
----------Iter = 30200----------
Loss = 1.023563
Variable usage = 100.00%
----------Iter = 30300----------
Loss = 1.023555
Variable usage = 100.00%
----------Iter = 30400----------
Loss = 1.023547
Variable usage = 100.00%
----------Iter = 30500----------
Loss = 1.023540
Variable usage = 100.00%
----------Iter = 30600----------
Loss = 1.023532
Variable usage = 100.00%
----------Iter = 30700----------
Loss = 1.023525
Variable usage = 100.00%
----------Iter = 30800----------
Loss = 1.023518
Variable usage = 100.00%
----------Iter = 30900----------
Loss = 1.023511
Variable usage = 100.00%
----------Iter = 31000----------
Loss = 1.023504
Variable usage = 100.00%
----------Iter = 31100----------
Loss = 1.023497
Variable usage = 100.00%
----------Iter = 31200----------
Loss = 1.023490
Variable usage = 100.00%
----------Iter = 31300----------
Loss = 1.023482
Variable usage = 100.00%
----------Iter = 31400----------
Loss = 1.023475
Variable usage = 100.00%
----------Iter = 31500----------
Loss = 1.023467
Variable usage = 100.00%
----------Iter = 31600----------
Loss = 1.023459
Variable usage = 100.00%
----------Iter = 31700----------
Loss = 1.023451
Variable usage = 100.00%
----------Iter = 31800----------
Loss = 1.023444
Variable usage = 100.00%
----------Iter = 31900----------
Loss = 1.023437
Variable usage = 100.00%
----------Iter = 32000----------
Loss = 1.023430
Variable usage = 100.00%
----------Iter = 32100----------
Loss = 1.023423
Variable usage = 100.00%
----------Iter = 32200----------
Loss = 1.023416
Variable usage = 100.00%
----------Iter = 32300----------
Loss = 1.023410
Variable usage = 100.00%
----------Iter = 32400----------
Loss = 1.023404
Variable usage = 100.00%
----------Iter = 32500----------
Loss = 1.023397
Variable usage = 100.00%
----------Iter = 32600----------
Loss = 1.023391
Variable usage = 100.00%
----------Iter = 32700----------
Loss = 1.023384
Variable usage = 100.00%
----------Iter = 32800----------
Loss = 1.023377
Variable usage = 100.00%
----------Iter = 32900----------
Loss = 1.023371
Variable usage = 100.00%
----------Iter = 33000----------
Loss = 1.023364
Variable usage = 100.00%
----------Iter = 33100----------
Loss = 1.023358
Variable usage = 100.00%
----------Iter = 33200----------
Loss = 1.023351
Variable usage = 100.00%
----------Iter = 33300----------
Loss = 1.023344
Variable usage = 100.00%
----------Iter = 33400----------
Loss = 1.023337
Variable usage = 100.00%
----------Iter = 33500----------
Loss = 1.023331
Variable usage = 100.00%
----------Iter = 33600----------
Loss = 1.023324
Variable usage = 100.00%
----------Iter = 33700----------
Loss = 1.023318
Variable usage = 100.00%
----------Iter = 33800----------
Loss = 1.023311
Variable usage = 100.00%
----------Iter = 33900----------
Loss = 1.023305
Variable usage = 100.00%
----------Iter = 34000----------
Loss = 1.023298
Variable usage = 100.00%
----------Iter = 34100----------
Loss = 1.023291
Variable usage = 100.00%
----------Iter = 34200----------
Loss = 1.023283
Variable usage = 100.00%
----------Iter = 34300----------
Loss = 1.023276
Variable usage = 100.00%
----------Iter = 34400----------
Loss = 1.023269
Variable usage = 100.00%
----------Iter = 34500----------
Loss = 1.023262
Variable usage = 100.00%
----------Iter = 34600----------
Loss = 1.023256
Variable usage = 100.00%
----------Iter = 34700----------
Loss = 1.023250
Variable usage = 100.00%
----------Iter = 34800----------
Loss = 1.023243
Variable usage = 100.00%
----------Iter = 34900----------
Loss = 1.023236
Variable usage = 100.00%
----------Iter = 35000----------
Loss = 1.023229
Variable usage = 100.00%
----------Iter = 35100----------
Loss = 1.023221
Variable usage = 100.00%
----------Iter = 35200----------
Loss = 1.023214
Variable usage = 100.00%
----------Iter = 35300----------
Loss = 1.023208
Variable usage = 100.00%
----------Iter = 35400----------
Loss = 1.023201
Variable usage = 100.00%
----------Iter = 35500----------
Loss = 1.023196
Variable usage = 100.00%
----------Iter = 35600----------
Loss = 1.023189
Variable usage = 100.00%
----------Iter = 35700----------
Loss = 1.023184
Variable usage = 100.00%
----------Iter = 35800----------
Loss = 1.023178
Variable usage = 100.00%
----------Iter = 35900----------
Loss = 1.023172
Variable usage = 100.00%
----------Iter = 36000----------
Loss = 1.023167
Variable usage = 100.00%
----------Iter = 36100----------
Loss = 1.023161
Variable usage = 100.00%
----------Iter = 36200----------
Loss = 1.023155
Variable usage = 100.00%
----------Iter = 36300----------
Loss = 1.023150
Variable usage = 100.00%
----------Iter = 36400----------
Loss = 1.023144
Variable usage = 100.00%
----------Iter = 36500----------
Loss = 1.023138
Variable usage = 100.00%
----------Iter = 36600----------
Loss = 1.023133
Variable usage = 100.00%
----------Iter = 36700----------
Loss = 1.023127
Variable usage = 100.00%
----------Iter = 36800----------
Loss = 1.023122
Variable usage = 100.00%
----------Iter = 36900----------
Loss = 1.023116
Variable usage = 100.00%
----------Iter = 37000----------
Loss = 1.023111
Variable usage = 100.00%
----------Iter = 37100----------
Loss = 1.023105
Variable usage = 100.00%
----------Iter = 37200----------
Loss = 1.023099
Variable usage = 100.00%
----------Iter = 37300----------
Loss = 1.023093
Variable usage = 100.00%
----------Iter = 37400----------
Loss = 1.023087
Variable usage = 100.00%
----------Iter = 37500----------
Loss = 1.023081
Variable usage = 100.00%
----------Iter = 37600----------
Loss = 1.023075
Variable usage = 100.00%
----------Iter = 37700----------
Loss = 1.023069
Variable usage = 100.00%
----------Iter = 37800----------
Loss = 1.023063
Variable usage = 100.00%
----------Iter = 37900----------
Loss = 1.023058
Variable usage = 100.00%
----------Iter = 38000----------
Loss = 1.023052
Variable usage = 100.00%
----------Iter = 38100----------
Loss = 1.023047
Variable usage = 100.00%
----------Iter = 38200----------
Loss = 1.023042
Variable usage = 100.00%
----------Iter = 38300----------
Loss = 1.023037
Variable usage = 100.00%
----------Iter = 38400----------
Loss = 1.023032
Variable usage = 100.00%
----------Iter = 38500----------
Loss = 1.023027
Variable usage = 100.00%
----------Iter = 38600----------
Loss = 1.023022
Variable usage = 100.00%
----------Iter = 38700----------
Loss = 1.023017
Variable usage = 100.00%
----------Iter = 38800----------
Loss = 1.023012
Variable usage = 100.00%
----------Iter = 38900----------
Loss = 1.023007
Variable usage = 100.00%
----------Iter = 39000----------
Loss = 1.023003
Variable usage = 100.00%
----------Iter = 39100----------
Loss = 1.022997
Variable usage = 100.00%
----------Iter = 39200----------
Loss = 1.022993
Variable usage = 100.00%
----------Iter = 39300----------
Loss = 1.022988
Variable usage = 100.00%
----------Iter = 39400----------
Loss = 1.022983
Variable usage = 100.00%
----------Iter = 39500----------
Loss = 1.022978
Variable usage = 100.00%
----------Iter = 39600----------
Loss = 1.022973
Variable usage = 100.00%
----------Iter = 39700----------
Loss = 1.022968
Variable usage = 100.00%
----------Iter = 39800----------
Loss = 1.022962
Variable usage = 100.00%
----------Iter = 39900----------
Loss = 1.022957
Variable usage = 100.00%
----------Iter = 40000----------
Loss = 1.022952
Variable usage = 100.00%
----------Iter = 40100----------
Loss = 1.022946
Variable usage = 100.00%
----------Iter = 40200----------
Loss = 1.022940
Variable usage = 100.00%
----------Iter = 40300----------
Loss = 1.022935
Variable usage = 100.00%
----------Iter = 40400----------
Loss = 1.022929
Variable usage = 100.00%
----------Iter = 40500----------
Loss = 1.022923
Variable usage = 100.00%
----------Iter = 40600----------
Loss = 1.022917
Variable usage = 100.00%
----------Iter = 40700----------
Loss = 1.022911
Variable usage = 100.00%
----------Iter = 40800----------
Loss = 1.022903
Variable usage = 100.00%
----------Iter = 40900----------
Loss = 1.022897
Variable usage = 100.00%
----------Iter = 41000----------
Loss = 1.022892
Variable usage = 100.00%
----------Iter = 41100----------
Loss = 1.022885
Variable usage = 100.00%
----------Iter = 41200----------
Loss = 1.022879
Variable usage = 100.00%
----------Iter = 41300----------
Loss = 1.022872
Variable usage = 100.00%
----------Iter = 41400----------
Loss = 1.022864
Variable usage = 100.00%
----------Iter = 41500----------
Loss = 1.022858
Variable usage = 100.00%
----------Iter = 41600----------
Loss = 1.022850
Variable usage = 100.00%
----------Iter = 41700----------
Loss = 1.022843
Variable usage = 100.00%
----------Iter = 41800----------
Loss = 1.022838
Variable usage = 100.00%
----------Iter = 41900----------
Loss = 1.022832
Variable usage = 100.00%
----------Iter = 42000----------
Loss = 1.022827
Variable usage = 100.00%
----------Iter = 42100----------
Loss = 1.022821
Variable usage = 100.00%
----------Iter = 42200----------
Loss = 1.022815
Variable usage = 100.00%
----------Iter = 42300----------
Loss = 1.022808
Variable usage = 100.00%
----------Iter = 42400----------
Loss = 1.022803
Variable usage = 100.00%
----------Iter = 42500----------
Loss = 1.022798
Variable usage = 100.00%
----------Iter = 42600----------
Loss = 1.022792
Variable usage = 100.00%
----------Iter = 42700----------
Loss = 1.022787
Variable usage = 100.00%
----------Iter = 42800----------
Loss = 1.022783
Variable usage = 100.00%
----------Iter = 42900----------
Loss = 1.022778
Variable usage = 100.00%
----------Iter = 43000----------
Loss = 1.022773
Variable usage = 100.00%
----------Iter = 43100----------
Loss = 1.022768
Variable usage = 100.00%
----------Iter = 43200----------
Loss = 1.022764
Variable usage = 100.00%
----------Iter = 43300----------
Loss = 1.022759
Variable usage = 100.00%
----------Iter = 43400----------
Loss = 1.022754
Variable usage = 100.00%
----------Iter = 43500----------
Loss = 1.022750
Variable usage = 100.00%
----------Iter = 43600----------
Loss = 1.022745
Variable usage = 100.00%
----------Iter = 43700----------
Loss = 1.022741
Variable usage = 100.00%
----------Iter = 43800----------
Loss = 1.022736
Variable usage = 100.00%
----------Iter = 43900----------
Loss = 1.022732
Variable usage = 100.00%
----------Iter = 44000----------
Loss = 1.022726
Variable usage = 100.00%
----------Iter = 44100----------
Loss = 1.022722
Variable usage = 100.00%
----------Iter = 44200----------
Loss = 1.022717
Variable usage = 100.00%
----------Iter = 44300----------
Loss = 1.022712
Variable usage = 100.00%
----------Iter = 44400----------
Loss = 1.022708
Variable usage = 100.00%
----------Iter = 44500----------
Loss = 1.022703
Variable usage = 100.00%
----------Iter = 44600----------
Loss = 1.022699
Variable usage = 100.00%
----------Iter = 44700----------
Loss = 1.022694
Variable usage = 100.00%
----------Iter = 44800----------
Loss = 1.022690
Variable usage = 100.00%
----------Iter = 44900----------
Loss = 1.022685
Variable usage = 100.00%
----------Iter = 45000----------
Loss = 1.022680
Variable usage = 100.00%
----------Iter = 45100----------
Loss = 1.022676
Variable usage = 100.00%
----------Iter = 45200----------
Loss = 1.022671
Variable usage = 100.00%
----------Iter = 45300----------
Loss = 1.022667
Variable usage = 100.00%
----------Iter = 45400----------
Loss = 1.022662
Variable usage = 100.00%
----------Iter = 45500----------
Loss = 1.022658
Variable usage = 100.00%
----------Iter = 45600----------
Loss = 1.022654
Variable usage = 100.00%
----------Iter = 45700----------
Loss = 1.022650
Variable usage = 100.00%
----------Iter = 45800----------
Loss = 1.022646
Variable usage = 100.00%
----------Iter = 45900----------
Loss = 1.022641
Variable usage = 100.00%
----------Iter = 46000----------
Loss = 1.022637
Variable usage = 100.00%
----------Iter = 46100----------
Loss = 1.022633
Variable usage = 100.00%
----------Iter = 46200----------
Loss = 1.022628
Variable usage = 100.00%
----------Iter = 46300----------
Loss = 1.022624
Variable usage = 100.00%
----------Iter = 46400----------
Loss = 1.022619
Variable usage = 100.00%
----------Iter = 46500----------
Loss = 1.022615
Variable usage = 100.00%
----------Iter = 46600----------
Loss = 1.022611
Variable usage = 100.00%
----------Iter = 46700----------
Loss = 1.022607
Variable usage = 100.00%
----------Iter = 46800----------
Loss = 1.022603
Variable usage = 100.00%
----------Iter = 46900----------
Loss = 1.022599
Variable usage = 100.00%
----------Iter = 47000----------
Loss = 1.022595
Variable usage = 100.00%
----------Iter = 47100----------
Loss = 1.022591
Variable usage = 100.00%
----------Iter = 47200----------
Loss = 1.022587
Variable usage = 100.00%
----------Iter = 47300----------
Loss = 1.022583
Variable usage = 100.00%
----------Iter = 47400----------
Loss = 1.022579
Variable usage = 100.00%
----------Iter = 47500----------
Loss = 1.022575
Variable usage = 100.00%
----------Iter = 47600----------
Loss = 1.022570
Variable usage = 100.00%
----------Iter = 47700----------
Loss = 1.022566
Variable usage = 100.00%
----------Iter = 47800----------
Loss = 1.022562
Variable usage = 100.00%
----------Iter = 47900----------
Loss = 1.022558
Variable usage = 100.00%
----------Iter = 48000----------
Loss = 1.022554
Variable usage = 100.00%
----------Iter = 48100----------
Loss = 1.022550
Variable usage = 100.00%
----------Iter = 48200----------
Loss = 1.022546
Variable usage = 100.00%
----------Iter = 48300----------
Loss = 1.022542
Variable usage = 100.00%
----------Iter = 48400----------
Loss = 1.022538
Variable usage = 100.00%
----------Iter = 48500----------
Loss = 1.022534
Variable usage = 100.00%
----------Iter = 48600----------
Loss = 1.022530
Variable usage = 100.00%
----------Iter = 48700----------
Loss = 1.022526
Variable usage = 100.00%
----------Iter = 48800----------
Loss = 1.022522
Variable usage = 100.00%
----------Iter = 48900----------
Loss = 1.022517
Variable usage = 100.00%
----------Iter = 49000----------
Loss = 1.022513
Variable usage = 100.00%
----------Iter = 49100----------
Loss = 1.022509
Variable usage = 100.00%
----------Iter = 49200----------
Loss = 1.022504
Variable usage = 100.00%
----------Iter = 49300----------
Loss = 1.022500
Variable usage = 100.00%
----------Iter = 49400----------
Loss = 1.022497
Variable usage = 100.00%
----------Iter = 49500----------
Loss = 1.022493
Variable usage = 100.00%
----------Iter = 49600----------
Loss = 1.022489
Variable usage = 100.00%
----------Iter = 49700----------
Loss = 1.022484
Variable usage = 100.00%
----------Iter = 49800----------
Loss = 1.022480
Variable usage = 100.00%
----------Iter = 49900----------
Loss = 1.022476
Variable usage = 100.00%
----------Iter = 50000----------
Loss = 1.022472
Variable usage = 100.00%
True variable usage = 75.00%
Estimated variable usage = 100.00%
Accuracy = 0.75%
Accuracy for experiment id 7 is 0.75
----------Iter = 100----------
Loss = 1.018390
Variable usage = 100.00%
----------Iter = 200----------
Loss = 1.016487
Variable usage = 100.00%
----------Iter = 300----------
Loss = 1.015220
Variable usage = 100.00%
----------Iter = 400----------
Loss = 1.014305
Variable usage = 100.00%
----------Iter = 500----------
Loss = 1.013600
Variable usage = 100.00%
----------Iter = 600----------
Loss = 1.013033
Variable usage = 100.00%
----------Iter = 700----------
Loss = 1.012558
Variable usage = 100.00%
----------Iter = 800----------
Loss = 1.012149
Variable usage = 100.00%
----------Iter = 900----------
Loss = 1.011797
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 1.011478
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 1.011187
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 1.010912
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 1.010652
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 1.010406
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 1.010164
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 1.009931
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 1.009712
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 1.009501
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 1.009295
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 1.009098
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 1.008908
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 1.008719
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 1.008539
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 1.008362
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 1.008190
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 1.008023
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 1.007860
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 1.007707
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 1.007556
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 1.007404
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 1.007258
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 1.007116
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 1.006979
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 1.006844
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 1.006708
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 1.006575
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 1.006446
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 1.006321
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 1.006198
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 1.006081
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 1.005967
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 1.005852
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 1.005739
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 1.005625
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 1.005515
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 1.005403
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 1.005296
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 1.005189
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 1.005081
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 1.004976
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 1.004875
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 1.004773
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 1.004670
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 1.004571
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 1.004475
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 1.004379
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 1.004283
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 1.004188
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 1.004095
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 1.004006
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 1.003917
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 1.003829
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 1.003742
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 1.003655
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 1.003566
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 1.003476
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 1.003389
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 1.003304
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 1.003217
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 1.003128
Variable usage = 100.00%
----------Iter = 7100----------
Loss = 1.003040
Variable usage = 100.00%
----------Iter = 7200----------
Loss = 1.002956
Variable usage = 100.00%
----------Iter = 7300----------
Loss = 1.002873
Variable usage = 100.00%
----------Iter = 7400----------
Loss = 1.002792
Variable usage = 100.00%
----------Iter = 7500----------
Loss = 1.002712
Variable usage = 100.00%
----------Iter = 7600----------
Loss = 1.002631
Variable usage = 100.00%
----------Iter = 7700----------
Loss = 1.002550
Variable usage = 100.00%
----------Iter = 7800----------
Loss = 1.002472
Variable usage = 100.00%
----------Iter = 7900----------
Loss = 1.002397
Variable usage = 100.00%
----------Iter = 8000----------
Loss = 1.002323
Variable usage = 100.00%
----------Iter = 8100----------
Loss = 1.002250
Variable usage = 100.00%
----------Iter = 8200----------
Loss = 1.002178
Variable usage = 100.00%
----------Iter = 8300----------
Loss = 1.002108
Variable usage = 100.00%
----------Iter = 8400----------
Loss = 1.002040
Variable usage = 100.00%
----------Iter = 8500----------
Loss = 1.001973
Variable usage = 100.00%
----------Iter = 8600----------
Loss = 1.001909
Variable usage = 100.00%
----------Iter = 8700----------
Loss = 1.001844
Variable usage = 100.00%
----------Iter = 8800----------
Loss = 1.001780
Variable usage = 100.00%
----------Iter = 8900----------
Loss = 1.001717
Variable usage = 100.00%
----------Iter = 9000----------
Loss = 1.001658
Variable usage = 100.00%
----------Iter = 9100----------
Loss = 1.001601
Variable usage = 100.00%
----------Iter = 9200----------
Loss = 1.001545
Variable usage = 100.00%
----------Iter = 9300----------
Loss = 1.001489
Variable usage = 100.00%
----------Iter = 9400----------
Loss = 1.001434
Variable usage = 100.00%
----------Iter = 9500----------
Loss = 1.001377
Variable usage = 100.00%
----------Iter = 9600----------
Loss = 1.001322
Variable usage = 100.00%
----------Iter = 9700----------
Loss = 1.001268
Variable usage = 100.00%
----------Iter = 9800----------
Loss = 1.001213
Variable usage = 100.00%
----------Iter = 9900----------
Loss = 1.001160
Variable usage = 100.00%
----------Iter = 10000----------
Loss = 1.001110
Variable usage = 100.00%
----------Iter = 10100----------
Loss = 1.001061
Variable usage = 100.00%
----------Iter = 10200----------
Loss = 1.001012
Variable usage = 100.00%
----------Iter = 10300----------
Loss = 1.000964
Variable usage = 100.00%
----------Iter = 10400----------
Loss = 1.000918
Variable usage = 100.00%
----------Iter = 10500----------
Loss = 1.000873
Variable usage = 100.00%
----------Iter = 10600----------
Loss = 1.000829
Variable usage = 100.00%
----------Iter = 10700----------
Loss = 1.000786
Variable usage = 100.00%
----------Iter = 10800----------
Loss = 1.000744
Variable usage = 100.00%
----------Iter = 10900----------
Loss = 1.000704
Variable usage = 100.00%
----------Iter = 11000----------
Loss = 1.000665
Variable usage = 100.00%
----------Iter = 11100----------
Loss = 1.000625
Variable usage = 100.00%
----------Iter = 11200----------
Loss = 1.000586
Variable usage = 100.00%
----------Iter = 11300----------
Loss = 1.000547
Variable usage = 100.00%
----------Iter = 11400----------
Loss = 1.000507
Variable usage = 100.00%
----------Iter = 11500----------
Loss = 1.000468
Variable usage = 100.00%
----------Iter = 11600----------
Loss = 1.000430
Variable usage = 100.00%
----------Iter = 11700----------
Loss = 1.000392
Variable usage = 100.00%
----------Iter = 11800----------
Loss = 1.000355
Variable usage = 100.00%
----------Iter = 11900----------
Loss = 1.000317
Variable usage = 100.00%
----------Iter = 12000----------
Loss = 1.000279
Variable usage = 100.00%
----------Iter = 12100----------
Loss = 1.000242
Variable usage = 100.00%
----------Iter = 12200----------
Loss = 1.000206
Variable usage = 100.00%
----------Iter = 12300----------
Loss = 1.000170
Variable usage = 100.00%
----------Iter = 12400----------
Loss = 1.000134
Variable usage = 100.00%
----------Iter = 12500----------
Loss = 1.000098
Variable usage = 100.00%
----------Iter = 12600----------
Loss = 1.000063
Variable usage = 100.00%
----------Iter = 12700----------
Loss = 1.000029
Variable usage = 100.00%
----------Iter = 12800----------
Loss = 0.999995
Variable usage = 100.00%
----------Iter = 12900----------
Loss = 0.999962
Variable usage = 100.00%
----------Iter = 13000----------
Loss = 0.999928
Variable usage = 100.00%
----------Iter = 13100----------
Loss = 0.999895
Variable usage = 100.00%
----------Iter = 13200----------
Loss = 0.999861
Variable usage = 100.00%
----------Iter = 13300----------
Loss = 0.999827
Variable usage = 100.00%
----------Iter = 13400----------
Loss = 0.999794
Variable usage = 100.00%
----------Iter = 13500----------
Loss = 0.999759
Variable usage = 100.00%
----------Iter = 13600----------
Loss = 0.999724
Variable usage = 100.00%
----------Iter = 13700----------
Loss = 0.999690
Variable usage = 100.00%
----------Iter = 13800----------
Loss = 0.999657
Variable usage = 100.00%
----------Iter = 13900----------
Loss = 0.999623
Variable usage = 100.00%
----------Iter = 14000----------
Loss = 0.999588
Variable usage = 100.00%
----------Iter = 14100----------
Loss = 0.999555
Variable usage = 100.00%
----------Iter = 14200----------
Loss = 0.999520
Variable usage = 100.00%
----------Iter = 14300----------
Loss = 0.999487
Variable usage = 100.00%
----------Iter = 14400----------
Loss = 0.999454
Variable usage = 100.00%
----------Iter = 14500----------
Loss = 0.999420
Variable usage = 100.00%
----------Iter = 14600----------
Loss = 0.999387
Variable usage = 100.00%
----------Iter = 14700----------
Loss = 0.999353
Variable usage = 100.00%
----------Iter = 14800----------
Loss = 0.999317
Variable usage = 100.00%
----------Iter = 14900----------
Loss = 0.999283
Variable usage = 100.00%
----------Iter = 15000----------
Loss = 0.999249
Variable usage = 100.00%
----------Iter = 15100----------
Loss = 0.999216
Variable usage = 100.00%
----------Iter = 15200----------
Loss = 0.999183
Variable usage = 100.00%
----------Iter = 15300----------
Loss = 0.999149
Variable usage = 100.00%
----------Iter = 15400----------
Loss = 0.999116
Variable usage = 100.00%
----------Iter = 15500----------
Loss = 0.999083
Variable usage = 100.00%
----------Iter = 15600----------
Loss = 0.999050
Variable usage = 100.00%
----------Iter = 15700----------
Loss = 0.999019
Variable usage = 100.00%
----------Iter = 15800----------
Loss = 0.998988
Variable usage = 100.00%
----------Iter = 15900----------
Loss = 0.998959
Variable usage = 100.00%
----------Iter = 16000----------
Loss = 0.998930
Variable usage = 100.00%
----------Iter = 16100----------
Loss = 0.998902
Variable usage = 100.00%
----------Iter = 16200----------
Loss = 0.998875
Variable usage = 100.00%
----------Iter = 16300----------
Loss = 0.998848
Variable usage = 100.00%
----------Iter = 16400----------
Loss = 0.998822
Variable usage = 100.00%
----------Iter = 16500----------
Loss = 0.998795
Variable usage = 100.00%
----------Iter = 16600----------
Loss = 0.998769
Variable usage = 100.00%
----------Iter = 16700----------
Loss = 0.998742
Variable usage = 100.00%
----------Iter = 16800----------
Loss = 0.998716
Variable usage = 100.00%
----------Iter = 16900----------
Loss = 0.998690
Variable usage = 100.00%
----------Iter = 17000----------
Loss = 0.998662
Variable usage = 100.00%
----------Iter = 17100----------
Loss = 0.998635
Variable usage = 100.00%
----------Iter = 17200----------
Loss = 0.998607
Variable usage = 100.00%
----------Iter = 17300----------
Loss = 0.998579
Variable usage = 100.00%
----------Iter = 17400----------
Loss = 0.998552
Variable usage = 100.00%
----------Iter = 17500----------
Loss = 0.998525
Variable usage = 100.00%
----------Iter = 17600----------
Loss = 0.998498
Variable usage = 100.00%
----------Iter = 17700----------
Loss = 0.998470
Variable usage = 100.00%
----------Iter = 17800----------
Loss = 0.998442
Variable usage = 100.00%
----------Iter = 17900----------
Loss = 0.998415
Variable usage = 100.00%
----------Iter = 18000----------
Loss = 0.998390
Variable usage = 100.00%
----------Iter = 18100----------
Loss = 0.998366
Variable usage = 100.00%
----------Iter = 18200----------
Loss = 0.998342
Variable usage = 100.00%
----------Iter = 18300----------
Loss = 0.998318
Variable usage = 100.00%
----------Iter = 18400----------
Loss = 0.998294
Variable usage = 100.00%
----------Iter = 18500----------
Loss = 0.998271
Variable usage = 100.00%
----------Iter = 18600----------
Loss = 0.998247
Variable usage = 100.00%
----------Iter = 18700----------
Loss = 0.998223
Variable usage = 100.00%
----------Iter = 18800----------
Loss = 0.998200
Variable usage = 100.00%
----------Iter = 18900----------
Loss = 0.998177
Variable usage = 100.00%
----------Iter = 19000----------
Loss = 0.998155
Variable usage = 100.00%
----------Iter = 19100----------
Loss = 0.998133
Variable usage = 100.00%
----------Iter = 19200----------
Loss = 0.998111
Variable usage = 100.00%
----------Iter = 19300----------
Loss = 0.998088
Variable usage = 100.00%
----------Iter = 19400----------
Loss = 0.998064
Variable usage = 100.00%
----------Iter = 19500----------
Loss = 0.998041
Variable usage = 100.00%
----------Iter = 19600----------
Loss = 0.998018
Variable usage = 100.00%
----------Iter = 19700----------
Loss = 0.997996
Variable usage = 100.00%
----------Iter = 19800----------
Loss = 0.997974
Variable usage = 100.00%
----------Iter = 19900----------
Loss = 0.997952
Variable usage = 100.00%
----------Iter = 20000----------
Loss = 0.997930
Variable usage = 100.00%
----------Iter = 20100----------
Loss = 0.997908
Variable usage = 100.00%
----------Iter = 20200----------
Loss = 0.997887
Variable usage = 100.00%
----------Iter = 20300----------
Loss = 0.997865
Variable usage = 100.00%
----------Iter = 20400----------
Loss = 0.997844
Variable usage = 100.00%
----------Iter = 20500----------
Loss = 0.997823
Variable usage = 100.00%
----------Iter = 20600----------
Loss = 0.997803
Variable usage = 100.00%
----------Iter = 20700----------
Loss = 0.997784
Variable usage = 100.00%
----------Iter = 20800----------
Loss = 0.997764
Variable usage = 100.00%
----------Iter = 20900----------
Loss = 0.997745
Variable usage = 100.00%
----------Iter = 21000----------
Loss = 0.997725
Variable usage = 100.00%
----------Iter = 21100----------
Loss = 0.997706
Variable usage = 100.00%
----------Iter = 21200----------
Loss = 0.997687
Variable usage = 100.00%
----------Iter = 21300----------
Loss = 0.997668
Variable usage = 100.00%
----------Iter = 21400----------
Loss = 0.997649
Variable usage = 100.00%
----------Iter = 21500----------
Loss = 0.997630
Variable usage = 100.00%
----------Iter = 21600----------
Loss = 0.997611
Variable usage = 100.00%
----------Iter = 21700----------
Loss = 0.997592
Variable usage = 100.00%
----------Iter = 21800----------
Loss = 0.997573
Variable usage = 100.00%
----------Iter = 21900----------
Loss = 0.997554
Variable usage = 100.00%
----------Iter = 22000----------
Loss = 0.997535
Variable usage = 100.00%
----------Iter = 22100----------
Loss = 0.997516
Variable usage = 100.00%
----------Iter = 22200----------
Loss = 0.997498
Variable usage = 100.00%
----------Iter = 22300----------
Loss = 0.997480
Variable usage = 100.00%
----------Iter = 22400----------
Loss = 0.997462
Variable usage = 100.00%
----------Iter = 22500----------
Loss = 0.997444
Variable usage = 100.00%
----------Iter = 22600----------
Loss = 0.997427
Variable usage = 100.00%
----------Iter = 22700----------
Loss = 0.997409
Variable usage = 100.00%
----------Iter = 22800----------
Loss = 0.997391
Variable usage = 100.00%
----------Iter = 22900----------
Loss = 0.997374
Variable usage = 100.00%
----------Iter = 23000----------
Loss = 0.997357
Variable usage = 100.00%
----------Iter = 23100----------
Loss = 0.997340
Variable usage = 100.00%
----------Iter = 23200----------
Loss = 0.997323
Variable usage = 100.00%
----------Iter = 23300----------
Loss = 0.997306
Variable usage = 100.00%
----------Iter = 23400----------
Loss = 0.997289
Variable usage = 100.00%
----------Iter = 23500----------
Loss = 0.997273
Variable usage = 100.00%
----------Iter = 23600----------
Loss = 0.997256
Variable usage = 100.00%
----------Iter = 23700----------
Loss = 0.997239
Variable usage = 100.00%
----------Iter = 23800----------
Loss = 0.997223
Variable usage = 100.00%
----------Iter = 23900----------
Loss = 0.997207
Variable usage = 100.00%
----------Iter = 24000----------
Loss = 0.997190
Variable usage = 100.00%
----------Iter = 24100----------
Loss = 0.997174
Variable usage = 100.00%
----------Iter = 24200----------
Loss = 0.997158
Variable usage = 100.00%
----------Iter = 24300----------
Loss = 0.997141
Variable usage = 100.00%
----------Iter = 24400----------
Loss = 0.997124
Variable usage = 100.00%
----------Iter = 24500----------
Loss = 0.997108
Variable usage = 100.00%
----------Iter = 24600----------
Loss = 0.997093
Variable usage = 100.00%
----------Iter = 24700----------
Loss = 0.997076
Variable usage = 100.00%
----------Iter = 24800----------
Loss = 0.997059
Variable usage = 100.00%
----------Iter = 24900----------
Loss = 0.997042
Variable usage = 100.00%
----------Iter = 25000----------
Loss = 0.997025
Variable usage = 100.00%
----------Iter = 25100----------
Loss = 0.997009
Variable usage = 100.00%
----------Iter = 25200----------
Loss = 0.996993
Variable usage = 100.00%
----------Iter = 25300----------
Loss = 0.996978
Variable usage = 100.00%
----------Iter = 25400----------
Loss = 0.996962
Variable usage = 100.00%
----------Iter = 25500----------
Loss = 0.996946
Variable usage = 100.00%
----------Iter = 25600----------
Loss = 0.996931
Variable usage = 100.00%
----------Iter = 25700----------
Loss = 0.996915
Variable usage = 100.00%
----------Iter = 25800----------
Loss = 0.996900
Variable usage = 100.00%
----------Iter = 25900----------
Loss = 0.996884
Variable usage = 100.00%
----------Iter = 26000----------
Loss = 0.996869
Variable usage = 100.00%
----------Iter = 26100----------
Loss = 0.996853
Variable usage = 100.00%
----------Iter = 26200----------
Loss = 0.996838
Variable usage = 100.00%
----------Iter = 26300----------
Loss = 0.996822
Variable usage = 100.00%
----------Iter = 26400----------
Loss = 0.996807
Variable usage = 100.00%
----------Iter = 26500----------
Loss = 0.996791
Variable usage = 100.00%
----------Iter = 26600----------
Loss = 0.996775
Variable usage = 100.00%
----------Iter = 26700----------
Loss = 0.996759
Variable usage = 100.00%
----------Iter = 26800----------
Loss = 0.996743
Variable usage = 100.00%
----------Iter = 26900----------
Loss = 0.996727
Variable usage = 100.00%
----------Iter = 27000----------
Loss = 0.996712
Variable usage = 100.00%
----------Iter = 27100----------
Loss = 0.996696
Variable usage = 100.00%
----------Iter = 27200----------
Loss = 0.996681
Variable usage = 100.00%
----------Iter = 27300----------
Loss = 0.996665
Variable usage = 100.00%
----------Iter = 27400----------
Loss = 0.996649
Variable usage = 100.00%
----------Iter = 27500----------
Loss = 0.996633
Variable usage = 100.00%
----------Iter = 27600----------
Loss = 0.996618
Variable usage = 100.00%
----------Iter = 27700----------
Loss = 0.996603
Variable usage = 100.00%
----------Iter = 27800----------
Loss = 0.996587
Variable usage = 100.00%
----------Iter = 27900----------
Loss = 0.996572
Variable usage = 100.00%
----------Iter = 28000----------
Loss = 0.996557
Variable usage = 100.00%
----------Iter = 28100----------
Loss = 0.996540
Variable usage = 100.00%
----------Iter = 28200----------
Loss = 0.996524
Variable usage = 100.00%
----------Iter = 28300----------
Loss = 0.996508
Variable usage = 100.00%
----------Iter = 28400----------
Loss = 0.996493
Variable usage = 100.00%
----------Iter = 28500----------
Loss = 0.996479
Variable usage = 100.00%
----------Iter = 28600----------
Loss = 0.996464
Variable usage = 100.00%
----------Iter = 28700----------
Loss = 0.996449
Variable usage = 100.00%
----------Iter = 28800----------
Loss = 0.996434
Variable usage = 100.00%
----------Iter = 28900----------
Loss = 0.996419
Variable usage = 100.00%
----------Iter = 29000----------
Loss = 0.996404
Variable usage = 100.00%
----------Iter = 29100----------
Loss = 0.996389
Variable usage = 100.00%
----------Iter = 29200----------
Loss = 0.996374
Variable usage = 100.00%
----------Iter = 29300----------
Loss = 0.996359
Variable usage = 100.00%
----------Iter = 29400----------
Loss = 0.996345
Variable usage = 100.00%
----------Iter = 29500----------
Loss = 0.996330
Variable usage = 100.00%
----------Iter = 29600----------
Loss = 0.996316
Variable usage = 100.00%
----------Iter = 29700----------
Loss = 0.996301
Variable usage = 100.00%
----------Iter = 29800----------
Loss = 0.996287
Variable usage = 100.00%
----------Iter = 29900----------
Loss = 0.996272
Variable usage = 100.00%
----------Iter = 30000----------
Loss = 0.996258
Variable usage = 100.00%
----------Iter = 30100----------
Loss = 0.996244
Variable usage = 100.00%
----------Iter = 30200----------
Loss = 0.996230
Variable usage = 100.00%
----------Iter = 30300----------
Loss = 0.996216
Variable usage = 100.00%
----------Iter = 30400----------
Loss = 0.996201
Variable usage = 100.00%
----------Iter = 30500----------
Loss = 0.996187
Variable usage = 100.00%
----------Iter = 30600----------
Loss = 0.996173
Variable usage = 100.00%
----------Iter = 30700----------
Loss = 0.996160
Variable usage = 100.00%
----------Iter = 30800----------
Loss = 0.996146
Variable usage = 100.00%
----------Iter = 30900----------
Loss = 0.996132
Variable usage = 100.00%
----------Iter = 31000----------
Loss = 0.996118
Variable usage = 100.00%
----------Iter = 31100----------
Loss = 0.996103
Variable usage = 100.00%
----------Iter = 31200----------
Loss = 0.996089
Variable usage = 100.00%
----------Iter = 31300----------
Loss = 0.996075
Variable usage = 100.00%
----------Iter = 31400----------
Loss = 0.996060
Variable usage = 100.00%
----------Iter = 31500----------
Loss = 0.996046
Variable usage = 100.00%
----------Iter = 31600----------
Loss = 0.996031
Variable usage = 100.00%
----------Iter = 31700----------
Loss = 0.996017
Variable usage = 100.00%
----------Iter = 31800----------
Loss = 0.996004
Variable usage = 100.00%
----------Iter = 31900----------
Loss = 0.995989
Variable usage = 100.00%
----------Iter = 32000----------
Loss = 0.995974
Variable usage = 100.00%
----------Iter = 32100----------
Loss = 0.995960
Variable usage = 100.00%
----------Iter = 32200----------
Loss = 0.995946
Variable usage = 100.00%
----------Iter = 32300----------
Loss = 0.995932
Variable usage = 100.00%
----------Iter = 32400----------
Loss = 0.995918
Variable usage = 100.00%
----------Iter = 32500----------
Loss = 0.995904
Variable usage = 100.00%
----------Iter = 32600----------
Loss = 0.995892
Variable usage = 100.00%
----------Iter = 32700----------
Loss = 0.995879
Variable usage = 100.00%
----------Iter = 32800----------
Loss = 0.995866
Variable usage = 100.00%
----------Iter = 32900----------
Loss = 0.995853
Variable usage = 100.00%
----------Iter = 33000----------
Loss = 0.995839
Variable usage = 100.00%
----------Iter = 33100----------
Loss = 0.995826
Variable usage = 100.00%
----------Iter = 33200----------
Loss = 0.995812
Variable usage = 100.00%
----------Iter = 33300----------
Loss = 0.995797
Variable usage = 100.00%
----------Iter = 33400----------
Loss = 0.995783
Variable usage = 100.00%
----------Iter = 33500----------
Loss = 0.995770
Variable usage = 100.00%
----------Iter = 33600----------
Loss = 0.995757
Variable usage = 100.00%
----------Iter = 33700----------
Loss = 0.995744
Variable usage = 100.00%
----------Iter = 33800----------
Loss = 0.995731
Variable usage = 100.00%
----------Iter = 33900----------
Loss = 0.995718
Variable usage = 100.00%
----------Iter = 34000----------
Loss = 0.995705
Variable usage = 100.00%
----------Iter = 34100----------
Loss = 0.995693
Variable usage = 100.00%
----------Iter = 34200----------
Loss = 0.995681
Variable usage = 100.00%
----------Iter = 34300----------
Loss = 0.995668
Variable usage = 100.00%
----------Iter = 34400----------
Loss = 0.995655
Variable usage = 100.00%
----------Iter = 34500----------
Loss = 0.995643
Variable usage = 100.00%
----------Iter = 34600----------
Loss = 0.995630
Variable usage = 100.00%
----------Iter = 34700----------
Loss = 0.995618
Variable usage = 100.00%
----------Iter = 34800----------
Loss = 0.995605
Variable usage = 100.00%
----------Iter = 34900----------
Loss = 0.995592
Variable usage = 100.00%
----------Iter = 35000----------
Loss = 0.995580
Variable usage = 100.00%
----------Iter = 35100----------
Loss = 0.995567
Variable usage = 100.00%
----------Iter = 35200----------
Loss = 0.995554
Variable usage = 100.00%
----------Iter = 35300----------
Loss = 0.995542
Variable usage = 100.00%
----------Iter = 35400----------
Loss = 0.995528
Variable usage = 100.00%
----------Iter = 35500----------
Loss = 0.995516
Variable usage = 100.00%
----------Iter = 35600----------
Loss = 0.995503
Variable usage = 100.00%
----------Iter = 35700----------
Loss = 0.995490
Variable usage = 100.00%
----------Iter = 35800----------
Loss = 0.995478
Variable usage = 100.00%
----------Iter = 35900----------
Loss = 0.995465
Variable usage = 100.00%
----------Iter = 36000----------
Loss = 0.995452
Variable usage = 100.00%
----------Iter = 36100----------
Loss = 0.995438
Variable usage = 100.00%
----------Iter = 36200----------
Loss = 0.995425
Variable usage = 100.00%
----------Iter = 36300----------
Loss = 0.995412
Variable usage = 100.00%
----------Iter = 36400----------
Loss = 0.995399
Variable usage = 100.00%
----------Iter = 36500----------
Loss = 0.995386
Variable usage = 100.00%
----------Iter = 36600----------
Loss = 0.995373
Variable usage = 100.00%
----------Iter = 36700----------
Loss = 0.995360
Variable usage = 100.00%
----------Iter = 36800----------
Loss = 0.995347
Variable usage = 100.00%
----------Iter = 36900----------
Loss = 0.995334
Variable usage = 100.00%
----------Iter = 37000----------
Loss = 0.995321
Variable usage = 100.00%
----------Iter = 37100----------
Loss = 0.995309
Variable usage = 100.00%
----------Iter = 37200----------
Loss = 0.995296
Variable usage = 100.00%
----------Iter = 37300----------
Loss = 0.995283
Variable usage = 100.00%
----------Iter = 37400----------
Loss = 0.995270
Variable usage = 100.00%
----------Iter = 37500----------
Loss = 0.995256
Variable usage = 100.00%
----------Iter = 37600----------
Loss = 0.995242
Variable usage = 100.00%
----------Iter = 37700----------
Loss = 0.995228
Variable usage = 100.00%
----------Iter = 37800----------
Loss = 0.995214
Variable usage = 100.00%
----------Iter = 37900----------
Loss = 0.995202
Variable usage = 100.00%
----------Iter = 38000----------
Loss = 0.995189
Variable usage = 100.00%
----------Iter = 38100----------
Loss = 0.995176
Variable usage = 100.00%
----------Iter = 38200----------
Loss = 0.995163
Variable usage = 100.00%
----------Iter = 38300----------
Loss = 0.995151
Variable usage = 100.00%
----------Iter = 38400----------
Loss = 0.995138
Variable usage = 100.00%
----------Iter = 38500----------
Loss = 0.995126
Variable usage = 100.00%
----------Iter = 38600----------
Loss = 0.995113
Variable usage = 100.00%
----------Iter = 38700----------
Loss = 0.995100
Variable usage = 100.00%
----------Iter = 38800----------
Loss = 0.995088
Variable usage = 100.00%
----------Iter = 38900----------
Loss = 0.995076
Variable usage = 100.00%
----------Iter = 39000----------
Loss = 0.995063
Variable usage = 100.00%
----------Iter = 39100----------
Loss = 0.995051
Variable usage = 100.00%
----------Iter = 39200----------
Loss = 0.995039
Variable usage = 100.00%
----------Iter = 39300----------
Loss = 0.995027
Variable usage = 100.00%
----------Iter = 39400----------
Loss = 0.995015
Variable usage = 100.00%
----------Iter = 39500----------
Loss = 0.995003
Variable usage = 100.00%
----------Iter = 39600----------
Loss = 0.994991
Variable usage = 100.00%
----------Iter = 39700----------
Loss = 0.994979
Variable usage = 100.00%
----------Iter = 39800----------
Loss = 0.994967
Variable usage = 100.00%
----------Iter = 39900----------
Loss = 0.994955
Variable usage = 100.00%
----------Iter = 40000----------
Loss = 0.994943
Variable usage = 100.00%
----------Iter = 40100----------
Loss = 0.994931
Variable usage = 100.00%
----------Iter = 40200----------
Loss = 0.994919
Variable usage = 100.00%
----------Iter = 40300----------
Loss = 0.994907
Variable usage = 100.00%
----------Iter = 40400----------
Loss = 0.994895
Variable usage = 100.00%
----------Iter = 40500----------
Loss = 0.994883
Variable usage = 100.00%
----------Iter = 40600----------
Loss = 0.994871
Variable usage = 100.00%
----------Iter = 40700----------
Loss = 0.994859
Variable usage = 100.00%
----------Iter = 40800----------
Loss = 0.994846
Variable usage = 100.00%
----------Iter = 40900----------
Loss = 0.994834
Variable usage = 100.00%
----------Iter = 41000----------
Loss = 0.994823
Variable usage = 100.00%
----------Iter = 41100----------
Loss = 0.994811
Variable usage = 100.00%
----------Iter = 41200----------
Loss = 0.994799
Variable usage = 100.00%
----------Iter = 41300----------
Loss = 0.994787
Variable usage = 100.00%
----------Iter = 41400----------
Loss = 0.994776
Variable usage = 100.00%
----------Iter = 41500----------
Loss = 0.994764
Variable usage = 100.00%
----------Iter = 41600----------
Loss = 0.994753
Variable usage = 100.00%
----------Iter = 41700----------
Loss = 0.994741
Variable usage = 100.00%
----------Iter = 41800----------
Loss = 0.994730
Variable usage = 100.00%
----------Iter = 41900----------
Loss = 0.994719
Variable usage = 100.00%
----------Iter = 42000----------
Loss = 0.994707
Variable usage = 100.00%
----------Iter = 42100----------
Loss = 0.994696
Variable usage = 100.00%
----------Iter = 42200----------
Loss = 0.994684
Variable usage = 100.00%
----------Iter = 42300----------
Loss = 0.994673
Variable usage = 100.00%
----------Iter = 42400----------
Loss = 0.994662
Variable usage = 100.00%
----------Iter = 42500----------
Loss = 0.994650
Variable usage = 100.00%
----------Iter = 42600----------
Loss = 0.994640
Variable usage = 100.00%
----------Iter = 42700----------
Loss = 0.994628
Variable usage = 100.00%
----------Iter = 42800----------
Loss = 0.994617
Variable usage = 100.00%
----------Iter = 42900----------
Loss = 0.994606
Variable usage = 100.00%
----------Iter = 43000----------
Loss = 0.994595
Variable usage = 100.00%
----------Iter = 43100----------
Loss = 0.994584
Variable usage = 100.00%
----------Iter = 43200----------
Loss = 0.994573
Variable usage = 100.00%
----------Iter = 43300----------
Loss = 0.994562
Variable usage = 100.00%
----------Iter = 43400----------
Loss = 0.994551
Variable usage = 100.00%
----------Iter = 43500----------
Loss = 0.994540
Variable usage = 100.00%
----------Iter = 43600----------
Loss = 0.994529
Variable usage = 100.00%
----------Iter = 43700----------
Loss = 0.994518
Variable usage = 100.00%
----------Iter = 43800----------
Loss = 0.994507
Variable usage = 100.00%
----------Iter = 43900----------
Loss = 0.994496
Variable usage = 100.00%
----------Iter = 44000----------
Loss = 0.994485
Variable usage = 100.00%
----------Iter = 44100----------
Loss = 0.994474
Variable usage = 100.00%
----------Iter = 44200----------
Loss = 0.994463
Variable usage = 100.00%
----------Iter = 44300----------
Loss = 0.994452
Variable usage = 100.00%
----------Iter = 44400----------
Loss = 0.994441
Variable usage = 100.00%
----------Iter = 44500----------
Loss = 0.994430
Variable usage = 100.00%
----------Iter = 44600----------
Loss = 0.994420
Variable usage = 100.00%
----------Iter = 44700----------
Loss = 0.994408
Variable usage = 100.00%
----------Iter = 44800----------
Loss = 0.994398
Variable usage = 100.00%
----------Iter = 44900----------
Loss = 0.994387
Variable usage = 100.00%
----------Iter = 45000----------
Loss = 0.994376
Variable usage = 100.00%
----------Iter = 45100----------
Loss = 0.994365
Variable usage = 100.00%
----------Iter = 45200----------
Loss = 0.994354
Variable usage = 100.00%
----------Iter = 45300----------
Loss = 0.994342
Variable usage = 100.00%
----------Iter = 45400----------
Loss = 0.994331
Variable usage = 100.00%
----------Iter = 45500----------
Loss = 0.994320
Variable usage = 100.00%
----------Iter = 45600----------
Loss = 0.994309
Variable usage = 100.00%
----------Iter = 45700----------
Loss = 0.994298
Variable usage = 100.00%
----------Iter = 45800----------
Loss = 0.994287
Variable usage = 100.00%
----------Iter = 45900----------
Loss = 0.994276
Variable usage = 100.00%
----------Iter = 46000----------
Loss = 0.994265
Variable usage = 100.00%
----------Iter = 46100----------
Loss = 0.994253
Variable usage = 100.00%
----------Iter = 46200----------
Loss = 0.994242
Variable usage = 100.00%
----------Iter = 46300----------
Loss = 0.994230
Variable usage = 100.00%
----------Iter = 46400----------
Loss = 0.994219
Variable usage = 100.00%
----------Iter = 46500----------
Loss = 0.994208
Variable usage = 100.00%
----------Iter = 46600----------
Loss = 0.994196
Variable usage = 100.00%
----------Iter = 46700----------
Loss = 0.994184
Variable usage = 100.00%
----------Iter = 46800----------
Loss = 0.994172
Variable usage = 100.00%
----------Iter = 46900----------
Loss = 0.994161
Variable usage = 100.00%
----------Iter = 47000----------
Loss = 0.994149
Variable usage = 100.00%
----------Iter = 47100----------
Loss = 0.994137
Variable usage = 100.00%
----------Iter = 47200----------
Loss = 0.994126
Variable usage = 100.00%
----------Iter = 47300----------
Loss = 0.994115
Variable usage = 100.00%
----------Iter = 47400----------
Loss = 0.994103
Variable usage = 100.00%
----------Iter = 47500----------
Loss = 0.994092
Variable usage = 100.00%
----------Iter = 47600----------
Loss = 0.994081
Variable usage = 100.00%
----------Iter = 47700----------
Loss = 0.994070
Variable usage = 100.00%
----------Iter = 47800----------
Loss = 0.994059
Variable usage = 100.00%
----------Iter = 47900----------
Loss = 0.994047
Variable usage = 100.00%
----------Iter = 48000----------
Loss = 0.994036
Variable usage = 100.00%
----------Iter = 48100----------
Loss = 0.994025
Variable usage = 100.00%
----------Iter = 48200----------
Loss = 0.994014
Variable usage = 100.00%
----------Iter = 48300----------
Loss = 0.994004
Variable usage = 100.00%
----------Iter = 48400----------
Loss = 0.993993
Variable usage = 100.00%
----------Iter = 48500----------
Loss = 0.993982
Variable usage = 100.00%
----------Iter = 48600----------
Loss = 0.993971
Variable usage = 100.00%
----------Iter = 48700----------
Loss = 0.993960
Variable usage = 100.00%
----------Iter = 48800----------
Loss = 0.993950
Variable usage = 100.00%
----------Iter = 48900----------
Loss = 0.993939
Variable usage = 100.00%
----------Iter = 49000----------
Loss = 0.993928
Variable usage = 100.00%
----------Iter = 49100----------
Loss = 0.993918
Variable usage = 100.00%
----------Iter = 49200----------
Loss = 0.993907
Variable usage = 100.00%
----------Iter = 49300----------
Loss = 0.993897
Variable usage = 100.00%
----------Iter = 49400----------
Loss = 0.993886
Variable usage = 100.00%
----------Iter = 49500----------
Loss = 0.993876
Variable usage = 100.00%
----------Iter = 49600----------
Loss = 0.993865
Variable usage = 100.00%
----------Iter = 49700----------
Loss = 0.993855
Variable usage = 100.00%
----------Iter = 49800----------
Loss = 0.993844
Variable usage = 100.00%
----------Iter = 49900----------
Loss = 0.993834
Variable usage = 100.00%
----------Iter = 50000----------
Loss = 0.993823
Variable usage = 100.00%
True variable usage = 75.00%
Estimated variable usage = 100.00%
Accuracy = 0.75%
Accuracy for experiment id 8 is 0.75
----------Iter = 100----------
Loss = 1.012833
Variable usage = 100.00%
----------Iter = 200----------
Loss = 1.010769
Variable usage = 100.00%
----------Iter = 300----------
Loss = 1.009395
Variable usage = 100.00%
----------Iter = 400----------
Loss = 1.008361
Variable usage = 100.00%
----------Iter = 500----------
Loss = 1.007532
Variable usage = 100.00%
----------Iter = 600----------
Loss = 1.006846
Variable usage = 100.00%
----------Iter = 700----------
Loss = 1.006252
Variable usage = 100.00%
----------Iter = 800----------
Loss = 1.005717
Variable usage = 100.00%
----------Iter = 900----------
Loss = 1.005226
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 1.004766
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 1.004347
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 1.003952
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 1.003579
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 1.003236
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 1.002921
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 1.002622
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 1.002348
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 1.002085
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 1.001831
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 1.001592
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 1.001363
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 1.001135
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 1.000907
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 1.000688
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 1.000478
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 1.000278
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 1.000074
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 0.999869
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 0.999670
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 0.999480
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 0.999291
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 0.999108
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 0.998931
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 0.998756
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 0.998583
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 0.998420
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 0.998262
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 0.998104
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 0.997949
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 0.997798
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 0.997655
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 0.997515
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 0.997381
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 0.997254
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 0.997131
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 0.997016
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 0.996901
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 0.996784
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 0.996666
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 0.996551
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 0.996441
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 0.996332
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 0.996221
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 0.996109
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 0.996001
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 0.995896
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 0.995794
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 0.995691
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 0.995592
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 0.995498
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 0.995404
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 0.995317
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 0.995232
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 0.995147
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 0.995060
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 0.994973
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 0.994889
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 0.994806
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 0.994723
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 0.994640
Variable usage = 100.00%
----------Iter = 7100----------
Loss = 0.994556
Variable usage = 100.00%
----------Iter = 7200----------
Loss = 0.994470
Variable usage = 100.00%
----------Iter = 7300----------
Loss = 0.994384
Variable usage = 100.00%
----------Iter = 7400----------
Loss = 0.994300
Variable usage = 100.00%
----------Iter = 7500----------
Loss = 0.994219
Variable usage = 100.00%
----------Iter = 7600----------
Loss = 0.994138
Variable usage = 100.00%
----------Iter = 7700----------
Loss = 0.994054
Variable usage = 100.00%
----------Iter = 7800----------
Loss = 0.993967
Variable usage = 100.00%
----------Iter = 7900----------
Loss = 0.993881
Variable usage = 100.00%
----------Iter = 8000----------
Loss = 0.993801
Variable usage = 100.00%
----------Iter = 8100----------
Loss = 0.993723
Variable usage = 100.00%
----------Iter = 8200----------
Loss = 0.993646
Variable usage = 100.00%
----------Iter = 8300----------
Loss = 0.993574
Variable usage = 100.00%
----------Iter = 8400----------
Loss = 0.993501
Variable usage = 100.00%
----------Iter = 8500----------
Loss = 0.993430
Variable usage = 100.00%
----------Iter = 8600----------
Loss = 0.993361
Variable usage = 100.00%
----------Iter = 8700----------
Loss = 0.993295
Variable usage = 100.00%
----------Iter = 8800----------
Loss = 0.993234
Variable usage = 100.00%
----------Iter = 8900----------
Loss = 0.993173
Variable usage = 100.00%
----------Iter = 9000----------
Loss = 0.993113
Variable usage = 100.00%
----------Iter = 9100----------
Loss = 0.993056
Variable usage = 100.00%
----------Iter = 9200----------
Loss = 0.992996
Variable usage = 100.00%
----------Iter = 9300----------
Loss = 0.992936
Variable usage = 100.00%
----------Iter = 9400----------
Loss = 0.992882
Variable usage = 100.00%
----------Iter = 9500----------
Loss = 0.992829
Variable usage = 100.00%
----------Iter = 9600----------
Loss = 0.992778
Variable usage = 100.00%
----------Iter = 9700----------
Loss = 0.992724
Variable usage = 100.00%
----------Iter = 9800----------
Loss = 0.992668
Variable usage = 100.00%
----------Iter = 9900----------
Loss = 0.992613
Variable usage = 100.00%
----------Iter = 10000----------
Loss = 0.992559
Variable usage = 100.00%
----------Iter = 10100----------
Loss = 0.992504
Variable usage = 100.00%
----------Iter = 10200----------
Loss = 0.992452
Variable usage = 100.00%
----------Iter = 10300----------
Loss = 0.992402
Variable usage = 100.00%
----------Iter = 10400----------
Loss = 0.992351
Variable usage = 100.00%
----------Iter = 10500----------
Loss = 0.992302
Variable usage = 100.00%
----------Iter = 10600----------
Loss = 0.992255
Variable usage = 100.00%
----------Iter = 10700----------
Loss = 0.992207
Variable usage = 100.00%
----------Iter = 10800----------
Loss = 0.992159
Variable usage = 100.00%
----------Iter = 10900----------
Loss = 0.992112
Variable usage = 100.00%
----------Iter = 11000----------
Loss = 0.992065
Variable usage = 100.00%
----------Iter = 11100----------
Loss = 0.992017
Variable usage = 100.00%
----------Iter = 11200----------
Loss = 0.991970
Variable usage = 100.00%
----------Iter = 11300----------
Loss = 0.991924
Variable usage = 100.00%
----------Iter = 11400----------
Loss = 0.991881
Variable usage = 100.00%
----------Iter = 11500----------
Loss = 0.991836
Variable usage = 100.00%
----------Iter = 11600----------
Loss = 0.991792
Variable usage = 100.00%
----------Iter = 11700----------
Loss = 0.991748
Variable usage = 100.00%
----------Iter = 11800----------
Loss = 0.991708
Variable usage = 100.00%
----------Iter = 11900----------
Loss = 0.991668
Variable usage = 100.00%
----------Iter = 12000----------
Loss = 0.991629
Variable usage = 100.00%
----------Iter = 12100----------
Loss = 0.991592
Variable usage = 100.00%
----------Iter = 12200----------
Loss = 0.991553
Variable usage = 100.00%
----------Iter = 12300----------
Loss = 0.991515
Variable usage = 100.00%
----------Iter = 12400----------
Loss = 0.991479
Variable usage = 100.00%
----------Iter = 12500----------
Loss = 0.991442
Variable usage = 100.00%
----------Iter = 12600----------
Loss = 0.991407
Variable usage = 100.00%
----------Iter = 12700----------
Loss = 0.991372
Variable usage = 100.00%
----------Iter = 12800----------
Loss = 0.991338
Variable usage = 100.00%
----------Iter = 12900----------
Loss = 0.991304
Variable usage = 100.00%
----------Iter = 13000----------
Loss = 0.991271
Variable usage = 100.00%
----------Iter = 13100----------
Loss = 0.991238
Variable usage = 100.00%
----------Iter = 13200----------
Loss = 0.991205
Variable usage = 100.00%
----------Iter = 13300----------
Loss = 0.991174
Variable usage = 100.00%
----------Iter = 13400----------
Loss = 0.991141
Variable usage = 100.00%
----------Iter = 13500----------
Loss = 0.991110
Variable usage = 100.00%
----------Iter = 13600----------
Loss = 0.991080
Variable usage = 100.00%
----------Iter = 13700----------
Loss = 0.991049
Variable usage = 100.00%
----------Iter = 13800----------
Loss = 0.991019
Variable usage = 100.00%
----------Iter = 13900----------
Loss = 0.990988
Variable usage = 100.00%
----------Iter = 14000----------
Loss = 0.990957
Variable usage = 100.00%
----------Iter = 14100----------
Loss = 0.990928
Variable usage = 100.00%
----------Iter = 14200----------
Loss = 0.990900
Variable usage = 100.00%
----------Iter = 14300----------
Loss = 0.990873
Variable usage = 100.00%
----------Iter = 14400----------
Loss = 0.990847
Variable usage = 100.00%
----------Iter = 14500----------
Loss = 0.990820
Variable usage = 100.00%
----------Iter = 14600----------
Loss = 0.990795
Variable usage = 100.00%
----------Iter = 14700----------
Loss = 0.990770
Variable usage = 100.00%
----------Iter = 14800----------
Loss = 0.990744
Variable usage = 100.00%
----------Iter = 14900----------
Loss = 0.990720
Variable usage = 100.00%
----------Iter = 15000----------
Loss = 0.990696
Variable usage = 100.00%
----------Iter = 15100----------
Loss = 0.990672
Variable usage = 100.00%
----------Iter = 15200----------
Loss = 0.990647
Variable usage = 100.00%
----------Iter = 15300----------
Loss = 0.990622
Variable usage = 100.00%
----------Iter = 15400----------
Loss = 0.990598
Variable usage = 100.00%
----------Iter = 15500----------
Loss = 0.990573
Variable usage = 100.00%
----------Iter = 15600----------
Loss = 0.990549
Variable usage = 100.00%
----------Iter = 15700----------
Loss = 0.990526
Variable usage = 100.00%
----------Iter = 15800----------
Loss = 0.990503
Variable usage = 100.00%
----------Iter = 15900----------
Loss = 0.990480
Variable usage = 100.00%
----------Iter = 16000----------
Loss = 0.990457
Variable usage = 100.00%
----------Iter = 16100----------
Loss = 0.990435
Variable usage = 100.00%
----------Iter = 16200----------
Loss = 0.990413
Variable usage = 100.00%
----------Iter = 16300----------
Loss = 0.990391
Variable usage = 100.00%
----------Iter = 16400----------
Loss = 0.990370
Variable usage = 100.00%
----------Iter = 16500----------
Loss = 0.990349
Variable usage = 100.00%
----------Iter = 16600----------
Loss = 0.990328
Variable usage = 100.00%
----------Iter = 16700----------
Loss = 0.990307
Variable usage = 100.00%
----------Iter = 16800----------
Loss = 0.990286
Variable usage = 100.00%
----------Iter = 16900----------
Loss = 0.990265
Variable usage = 100.00%
----------Iter = 17000----------
Loss = 0.990245
Variable usage = 100.00%
----------Iter = 17100----------
Loss = 0.990224
Variable usage = 100.00%
----------Iter = 17200----------
Loss = 0.990203
Variable usage = 100.00%
----------Iter = 17300----------
Loss = 0.990183
Variable usage = 100.00%
----------Iter = 17400----------
Loss = 0.990164
Variable usage = 100.00%
----------Iter = 17500----------
Loss = 0.990144
Variable usage = 100.00%
----------Iter = 17600----------
Loss = 0.990124
Variable usage = 100.00%
----------Iter = 17700----------
Loss = 0.990104
Variable usage = 100.00%
----------Iter = 17800----------
Loss = 0.990084
Variable usage = 100.00%
----------Iter = 17900----------
Loss = 0.990064
Variable usage = 100.00%
----------Iter = 18000----------
Loss = 0.990044
Variable usage = 100.00%
----------Iter = 18100----------
Loss = 0.990025
Variable usage = 100.00%
----------Iter = 18200----------
Loss = 0.990006
Variable usage = 100.00%
----------Iter = 18300----------
Loss = 0.989987
Variable usage = 100.00%
----------Iter = 18400----------
Loss = 0.989968
Variable usage = 100.00%
----------Iter = 18500----------
Loss = 0.989949
Variable usage = 100.00%
----------Iter = 18600----------
Loss = 0.989929
Variable usage = 100.00%
----------Iter = 18700----------
Loss = 0.989909
Variable usage = 100.00%
----------Iter = 18800----------
Loss = 0.989888
Variable usage = 100.00%
----------Iter = 18900----------
Loss = 0.989869
Variable usage = 100.00%
----------Iter = 19000----------
Loss = 0.989850
Variable usage = 100.00%
----------Iter = 19100----------
Loss = 0.989831
Variable usage = 100.00%
----------Iter = 19200----------
Loss = 0.989813
Variable usage = 100.00%
----------Iter = 19300----------
Loss = 0.989795
Variable usage = 100.00%
----------Iter = 19400----------
Loss = 0.989777
Variable usage = 100.00%
----------Iter = 19500----------
Loss = 0.989760
Variable usage = 100.00%
----------Iter = 19600----------
Loss = 0.989743
Variable usage = 100.00%
----------Iter = 19700----------
Loss = 0.989726
Variable usage = 100.00%
----------Iter = 19800----------
Loss = 0.989710
Variable usage = 100.00%
----------Iter = 19900----------
Loss = 0.989693
Variable usage = 100.00%
----------Iter = 20000----------
Loss = 0.989675
Variable usage = 100.00%
----------Iter = 20100----------
Loss = 0.989658
Variable usage = 100.00%
----------Iter = 20200----------
Loss = 0.989640
Variable usage = 100.00%
----------Iter = 20300----------
Loss = 0.989623
Variable usage = 100.00%
----------Iter = 20400----------
Loss = 0.989605
Variable usage = 100.00%
----------Iter = 20500----------
Loss = 0.989587
Variable usage = 100.00%
----------Iter = 20600----------
Loss = 0.989570
Variable usage = 100.00%
----------Iter = 20700----------
Loss = 0.989553
Variable usage = 100.00%
----------Iter = 20800----------
Loss = 0.989536
Variable usage = 100.00%
----------Iter = 20900----------
Loss = 0.989518
Variable usage = 100.00%
----------Iter = 21000----------
Loss = 0.989500
Variable usage = 100.00%
----------Iter = 21100----------
Loss = 0.989481
Variable usage = 100.00%
----------Iter = 21200----------
Loss = 0.989463
Variable usage = 100.00%
----------Iter = 21300----------
Loss = 0.989445
Variable usage = 100.00%
----------Iter = 21400----------
Loss = 0.989428
Variable usage = 100.00%
----------Iter = 21500----------
Loss = 0.989410
Variable usage = 100.00%
----------Iter = 21600----------
Loss = 0.989393
Variable usage = 100.00%
----------Iter = 21700----------
Loss = 0.989375
Variable usage = 100.00%
----------Iter = 21800----------
Loss = 0.989358
Variable usage = 100.00%
----------Iter = 21900----------
Loss = 0.989342
Variable usage = 100.00%
----------Iter = 22000----------
Loss = 0.989326
Variable usage = 100.00%
----------Iter = 22100----------
Loss = 0.989311
Variable usage = 100.00%
----------Iter = 22200----------
Loss = 0.989296
Variable usage = 100.00%
----------Iter = 22300----------
Loss = 0.989282
Variable usage = 100.00%
----------Iter = 22400----------
Loss = 0.989268
Variable usage = 100.00%
----------Iter = 22500----------
Loss = 0.989255
Variable usage = 100.00%
----------Iter = 22600----------
Loss = 0.989240
Variable usage = 100.00%
----------Iter = 22700----------
Loss = 0.989227
Variable usage = 100.00%
----------Iter = 22800----------
Loss = 0.989213
Variable usage = 100.00%
----------Iter = 22900----------
Loss = 0.989199
Variable usage = 100.00%
----------Iter = 23000----------
Loss = 0.989185
Variable usage = 100.00%
----------Iter = 23100----------
Loss = 0.989171
Variable usage = 100.00%
----------Iter = 23200----------
Loss = 0.989156
Variable usage = 100.00%
----------Iter = 23300----------
Loss = 0.989142
Variable usage = 100.00%
----------Iter = 23400----------
Loss = 0.989127
Variable usage = 100.00%
----------Iter = 23500----------
Loss = 0.989113
Variable usage = 100.00%
----------Iter = 23600----------
Loss = 0.989099
Variable usage = 100.00%
----------Iter = 23700----------
Loss = 0.989085
Variable usage = 100.00%
----------Iter = 23800----------
Loss = 0.989072
Variable usage = 100.00%
----------Iter = 23900----------
Loss = 0.989059
Variable usage = 100.00%
----------Iter = 24000----------
Loss = 0.989046
Variable usage = 100.00%
----------Iter = 24100----------
Loss = 0.989033
Variable usage = 100.00%
----------Iter = 24200----------
Loss = 0.989021
Variable usage = 100.00%
----------Iter = 24300----------
Loss = 0.989009
Variable usage = 100.00%
----------Iter = 24400----------
Loss = 0.988997
Variable usage = 100.00%
----------Iter = 24500----------
Loss = 0.988985
Variable usage = 100.00%
----------Iter = 24600----------
Loss = 0.988974
Variable usage = 100.00%
----------Iter = 24700----------
Loss = 0.988963
Variable usage = 100.00%
----------Iter = 24800----------
Loss = 0.988951
Variable usage = 100.00%
----------Iter = 24900----------
Loss = 0.988940
Variable usage = 100.00%
----------Iter = 25000----------
Loss = 0.988928
Variable usage = 100.00%
----------Iter = 25100----------
Loss = 0.988917
Variable usage = 100.00%
----------Iter = 25200----------
Loss = 0.988904
Variable usage = 100.00%
----------Iter = 25300----------
Loss = 0.988892
Variable usage = 100.00%
----------Iter = 25400----------
Loss = 0.988881
Variable usage = 100.00%
----------Iter = 25500----------
Loss = 0.988869
Variable usage = 100.00%
----------Iter = 25600----------
Loss = 0.988859
Variable usage = 100.00%
----------Iter = 25700----------
Loss = 0.988848
Variable usage = 100.00%
----------Iter = 25800----------
Loss = 0.988837
Variable usage = 100.00%
----------Iter = 25900----------
Loss = 0.988827
Variable usage = 100.00%
----------Iter = 26000----------
Loss = 0.988817
Variable usage = 100.00%
----------Iter = 26100----------
Loss = 0.988807
Variable usage = 100.00%
----------Iter = 26200----------
Loss = 0.988798
Variable usage = 100.00%
----------Iter = 26300----------
Loss = 0.988788
Variable usage = 100.00%
----------Iter = 26400----------
Loss = 0.988778
Variable usage = 100.00%
----------Iter = 26500----------
Loss = 0.988768
Variable usage = 100.00%
----------Iter = 26600----------
Loss = 0.988758
Variable usage = 100.00%
----------Iter = 26700----------
Loss = 0.988748
Variable usage = 100.00%
----------Iter = 26800----------
Loss = 0.988738
Variable usage = 100.00%
----------Iter = 26900----------
Loss = 0.988728
Variable usage = 100.00%
----------Iter = 27000----------
Loss = 0.988718
Variable usage = 100.00%
----------Iter = 27100----------
Loss = 0.988707
Variable usage = 100.00%
----------Iter = 27200----------
Loss = 0.988697
Variable usage = 100.00%
----------Iter = 27300----------
Loss = 0.988688
Variable usage = 100.00%
----------Iter = 27400----------
Loss = 0.988679
Variable usage = 100.00%
----------Iter = 27500----------
Loss = 0.988669
Variable usage = 100.00%
----------Iter = 27600----------
Loss = 0.988659
Variable usage = 100.00%
----------Iter = 27700----------
Loss = 0.988649
Variable usage = 100.00%
----------Iter = 27800----------
Loss = 0.988639
Variable usage = 100.00%
----------Iter = 27900----------
Loss = 0.988629
Variable usage = 100.00%
----------Iter = 28000----------
Loss = 0.988619
Variable usage = 100.00%
----------Iter = 28100----------
Loss = 0.988610
Variable usage = 100.00%
----------Iter = 28200----------
Loss = 0.988601
Variable usage = 100.00%
----------Iter = 28300----------
Loss = 0.988592
Variable usage = 100.00%
----------Iter = 28400----------
Loss = 0.988584
Variable usage = 100.00%
----------Iter = 28500----------
Loss = 0.988575
Variable usage = 100.00%
----------Iter = 28600----------
Loss = 0.988567
Variable usage = 100.00%
----------Iter = 28700----------
Loss = 0.988558
Variable usage = 100.00%
----------Iter = 28800----------
Loss = 0.988550
Variable usage = 100.00%
----------Iter = 28900----------
Loss = 0.988542
Variable usage = 100.00%
----------Iter = 29000----------
Loss = 0.988534
Variable usage = 100.00%
----------Iter = 29100----------
Loss = 0.988526
Variable usage = 100.00%
----------Iter = 29200----------
Loss = 0.988518
Variable usage = 100.00%
----------Iter = 29300----------
Loss = 0.988511
Variable usage = 100.00%
----------Iter = 29400----------
Loss = 0.988503
Variable usage = 100.00%
----------Iter = 29500----------
Loss = 0.988495
Variable usage = 100.00%
----------Iter = 29600----------
Loss = 0.988487
Variable usage = 100.00%
----------Iter = 29700----------
Loss = 0.988480
Variable usage = 100.00%
----------Iter = 29800----------
Loss = 0.988472
Variable usage = 100.00%
----------Iter = 29900----------
Loss = 0.988465
Variable usage = 100.00%
----------Iter = 30000----------
Loss = 0.988458
Variable usage = 100.00%
----------Iter = 30100----------
Loss = 0.988451
Variable usage = 100.00%
----------Iter = 30200----------
Loss = 0.988444
Variable usage = 100.00%
----------Iter = 30300----------
Loss = 0.988437
Variable usage = 100.00%
----------Iter = 30400----------
Loss = 0.988429
Variable usage = 100.00%
----------Iter = 30500----------
Loss = 0.988422
Variable usage = 100.00%
----------Iter = 30600----------
Loss = 0.988415
Variable usage = 100.00%
----------Iter = 30700----------
Loss = 0.988408
Variable usage = 100.00%
----------Iter = 30800----------
Loss = 0.988401
Variable usage = 100.00%
----------Iter = 30900----------
Loss = 0.988394
Variable usage = 100.00%
----------Iter = 31000----------
Loss = 0.988387
Variable usage = 100.00%
----------Iter = 31100----------
Loss = 0.988380
Variable usage = 100.00%
----------Iter = 31200----------
Loss = 0.988372
Variable usage = 100.00%
----------Iter = 31300----------
Loss = 0.988364
Variable usage = 100.00%
----------Iter = 31400----------
Loss = 0.988357
Variable usage = 100.00%
----------Iter = 31500----------
Loss = 0.988349
Variable usage = 100.00%
----------Iter = 31600----------
Loss = 0.988342
Variable usage = 100.00%
----------Iter = 31700----------
Loss = 0.988335
Variable usage = 100.00%
----------Iter = 31800----------
Loss = 0.988328
Variable usage = 100.00%
----------Iter = 31900----------
Loss = 0.988321
Variable usage = 100.00%
----------Iter = 32000----------
Loss = 0.988314
Variable usage = 100.00%
----------Iter = 32100----------
Loss = 0.988306
Variable usage = 100.00%
----------Iter = 32200----------
Loss = 0.988299
Variable usage = 100.00%
----------Iter = 32300----------
Loss = 0.988291
Variable usage = 100.00%
----------Iter = 32400----------
Loss = 0.988284
Variable usage = 100.00%
----------Iter = 32500----------
Loss = 0.988277
Variable usage = 100.00%
----------Iter = 32600----------
Loss = 0.988270
Variable usage = 100.00%
----------Iter = 32700----------
Loss = 0.988263
Variable usage = 100.00%
----------Iter = 32800----------
Loss = 0.988256
Variable usage = 100.00%
----------Iter = 32900----------
Loss = 0.988248
Variable usage = 100.00%
----------Iter = 33000----------
Loss = 0.988240
Variable usage = 100.00%
----------Iter = 33100----------
Loss = 0.988232
Variable usage = 100.00%
----------Iter = 33200----------
Loss = 0.988225
Variable usage = 100.00%
----------Iter = 33300----------
Loss = 0.988217
Variable usage = 100.00%
----------Iter = 33400----------
Loss = 0.988209
Variable usage = 100.00%
----------Iter = 33500----------
Loss = 0.988201
Variable usage = 100.00%
----------Iter = 33600----------
Loss = 0.988194
Variable usage = 100.00%
----------Iter = 33700----------
Loss = 0.988187
Variable usage = 100.00%
----------Iter = 33800----------
Loss = 0.988180
Variable usage = 100.00%
----------Iter = 33900----------
Loss = 0.988173
Variable usage = 100.00%
----------Iter = 34000----------
Loss = 0.988166
Variable usage = 100.00%
----------Iter = 34100----------
Loss = 0.988160
Variable usage = 100.00%
----------Iter = 34200----------
Loss = 0.988153
Variable usage = 100.00%
----------Iter = 34300----------
Loss = 0.988146
Variable usage = 100.00%
----------Iter = 34400----------
Loss = 0.988139
Variable usage = 100.00%
----------Iter = 34500----------
Loss = 0.988133
Variable usage = 100.00%
----------Iter = 34600----------
Loss = 0.988127
Variable usage = 100.00%
----------Iter = 34700----------
Loss = 0.988121
Variable usage = 100.00%
----------Iter = 34800----------
Loss = 0.988115
Variable usage = 100.00%
----------Iter = 34900----------
Loss = 0.988109
Variable usage = 100.00%
----------Iter = 35000----------
Loss = 0.988103
Variable usage = 100.00%
----------Iter = 35100----------
Loss = 0.988097
Variable usage = 100.00%
----------Iter = 35200----------
Loss = 0.988092
Variable usage = 100.00%
----------Iter = 35300----------
Loss = 0.988086
Variable usage = 100.00%
----------Iter = 35400----------
Loss = 0.988081
Variable usage = 100.00%
----------Iter = 35500----------
Loss = 0.988075
Variable usage = 100.00%
----------Iter = 35600----------
Loss = 0.988069
Variable usage = 100.00%
----------Iter = 35700----------
Loss = 0.988064
Variable usage = 100.00%
----------Iter = 35800----------
Loss = 0.988058
Variable usage = 100.00%
----------Iter = 35900----------
Loss = 0.988052
Variable usage = 100.00%
----------Iter = 36000----------
Loss = 0.988047
Variable usage = 100.00%
----------Iter = 36100----------
Loss = 0.988042
Variable usage = 100.00%
----------Iter = 36200----------
Loss = 0.988036
Variable usage = 100.00%
----------Iter = 36300----------
Loss = 0.988031
Variable usage = 100.00%
----------Iter = 36400----------
Loss = 0.988025
Variable usage = 100.00%
----------Iter = 36500----------
Loss = 0.988020
Variable usage = 100.00%
----------Iter = 36600----------
Loss = 0.988015
Variable usage = 100.00%
----------Iter = 36700----------
Loss = 0.988009
Variable usage = 100.00%
----------Iter = 36800----------
Loss = 0.988004
Variable usage = 100.00%
----------Iter = 36900----------
Loss = 0.987999
Variable usage = 100.00%
----------Iter = 37000----------
Loss = 0.987993
Variable usage = 100.00%
----------Iter = 37100----------
Loss = 0.987988
Variable usage = 100.00%
----------Iter = 37200----------
Loss = 0.987983
Variable usage = 100.00%
----------Iter = 37300----------
Loss = 0.987978
Variable usage = 100.00%
----------Iter = 37400----------
Loss = 0.987972
Variable usage = 100.00%
----------Iter = 37500----------
Loss = 0.987967
Variable usage = 100.00%
----------Iter = 37600----------
Loss = 0.987962
Variable usage = 100.00%
----------Iter = 37700----------
Loss = 0.987957
Variable usage = 100.00%
----------Iter = 37800----------
Loss = 0.987952
Variable usage = 100.00%
----------Iter = 37900----------
Loss = 0.987947
Variable usage = 100.00%
----------Iter = 38000----------
Loss = 0.987942
Variable usage = 100.00%
----------Iter = 38100----------
Loss = 0.987937
Variable usage = 100.00%
----------Iter = 38200----------
Loss = 0.987932
Variable usage = 100.00%
----------Iter = 38300----------
Loss = 0.987927
Variable usage = 100.00%
----------Iter = 38400----------
Loss = 0.987922
Variable usage = 100.00%
----------Iter = 38500----------
Loss = 0.987917
Variable usage = 100.00%
----------Iter = 38600----------
Loss = 0.987912
Variable usage = 100.00%
----------Iter = 38700----------
Loss = 0.987907
Variable usage = 100.00%
----------Iter = 38800----------
Loss = 0.987903
Variable usage = 100.00%
----------Iter = 38900----------
Loss = 0.987898
Variable usage = 100.00%
----------Iter = 39000----------
Loss = 0.987893
Variable usage = 100.00%
----------Iter = 39100----------
Loss = 0.987888
Variable usage = 100.00%
----------Iter = 39200----------
Loss = 0.987883
Variable usage = 100.00%
----------Iter = 39300----------
Loss = 0.987879
Variable usage = 100.00%
----------Iter = 39400----------
Loss = 0.987874
Variable usage = 100.00%
----------Iter = 39500----------
Loss = 0.987869
Variable usage = 100.00%
----------Iter = 39600----------
Loss = 0.987864
Variable usage = 100.00%
----------Iter = 39700----------
Loss = 0.987860
Variable usage = 100.00%
----------Iter = 39800----------
Loss = 0.987855
Variable usage = 100.00%
----------Iter = 39900----------
Loss = 0.987850
Variable usage = 100.00%
----------Iter = 40000----------
Loss = 0.987845
Variable usage = 100.00%
----------Iter = 40100----------
Loss = 0.987840
Variable usage = 100.00%
----------Iter = 40200----------
Loss = 0.987835
Variable usage = 100.00%
----------Iter = 40300----------
Loss = 0.987831
Variable usage = 100.00%
----------Iter = 40400----------
Loss = 0.987826
Variable usage = 100.00%
----------Iter = 40500----------
Loss = 0.987821
Variable usage = 100.00%
----------Iter = 40600----------
Loss = 0.987816
Variable usage = 100.00%
----------Iter = 40700----------
Loss = 0.987811
Variable usage = 100.00%
----------Iter = 40800----------
Loss = 0.987807
Variable usage = 100.00%
----------Iter = 40900----------
Loss = 0.987803
Variable usage = 100.00%
----------Iter = 41000----------
Loss = 0.987798
Variable usage = 100.00%
----------Iter = 41100----------
Loss = 0.987794
Variable usage = 100.00%
----------Iter = 41200----------
Loss = 0.987789
Variable usage = 100.00%
----------Iter = 41300----------
Loss = 0.987785
Variable usage = 100.00%
----------Iter = 41400----------
Loss = 0.987780
Variable usage = 100.00%
----------Iter = 41500----------
Loss = 0.987776
Variable usage = 100.00%
----------Iter = 41600----------
Loss = 0.987771
Variable usage = 100.00%
----------Iter = 41700----------
Loss = 0.987767
Variable usage = 100.00%
----------Iter = 41800----------
Loss = 0.987763
Variable usage = 100.00%
----------Iter = 41900----------
Loss = 0.987758
Variable usage = 100.00%
----------Iter = 42000----------
Loss = 0.987754
Variable usage = 100.00%
----------Iter = 42100----------
Loss = 0.987750
Variable usage = 100.00%
----------Iter = 42200----------
Loss = 0.987746
Variable usage = 100.00%
----------Iter = 42300----------
Loss = 0.987742
Variable usage = 100.00%
----------Iter = 42400----------
Loss = 0.987737
Variable usage = 100.00%
----------Iter = 42500----------
Loss = 0.987733
Variable usage = 100.00%
----------Iter = 42600----------
Loss = 0.987729
Variable usage = 100.00%
----------Iter = 42700----------
Loss = 0.987725
Variable usage = 100.00%
----------Iter = 42800----------
Loss = 0.987721
Variable usage = 100.00%
----------Iter = 42900----------
Loss = 0.987717
Variable usage = 100.00%
----------Iter = 43000----------
Loss = 0.987713
Variable usage = 100.00%
----------Iter = 43100----------
Loss = 0.987709
Variable usage = 100.00%
----------Iter = 43200----------
Loss = 0.987705
Variable usage = 100.00%
----------Iter = 43300----------
Loss = 0.987701
Variable usage = 100.00%
----------Iter = 43400----------
Loss = 0.987696
Variable usage = 100.00%
----------Iter = 43500----------
Loss = 0.987692
Variable usage = 100.00%
----------Iter = 43600----------
Loss = 0.987688
Variable usage = 100.00%
----------Iter = 43700----------
Loss = 0.987683
Variable usage = 100.00%
----------Iter = 43800----------
Loss = 0.987678
Variable usage = 100.00%
----------Iter = 43900----------
Loss = 0.987673
Variable usage = 100.00%
----------Iter = 44000----------
Loss = 0.987668
Variable usage = 100.00%
----------Iter = 44100----------
Loss = 0.987663
Variable usage = 100.00%
----------Iter = 44200----------
Loss = 0.987658
Variable usage = 100.00%
----------Iter = 44300----------
Loss = 0.987653
Variable usage = 100.00%
----------Iter = 44400----------
Loss = 0.987648
Variable usage = 100.00%
----------Iter = 44500----------
Loss = 0.987643
Variable usage = 100.00%
----------Iter = 44600----------
Loss = 0.987638
Variable usage = 100.00%
----------Iter = 44700----------
Loss = 0.987633
Variable usage = 100.00%
----------Iter = 44800----------
Loss = 0.987628
Variable usage = 100.00%
----------Iter = 44900----------
Loss = 0.987623
Variable usage = 100.00%
----------Iter = 45000----------
Loss = 0.987618
Variable usage = 100.00%
----------Iter = 45100----------
Loss = 0.987613
Variable usage = 100.00%
----------Iter = 45200----------
Loss = 0.987609
Variable usage = 100.00%
----------Iter = 45300----------
Loss = 0.987605
Variable usage = 100.00%
----------Iter = 45400----------
Loss = 0.987600
Variable usage = 100.00%
----------Iter = 45500----------
Loss = 0.987596
Variable usage = 100.00%
----------Iter = 45600----------
Loss = 0.987591
Variable usage = 100.00%
----------Iter = 45700----------
Loss = 0.987587
Variable usage = 100.00%
----------Iter = 45800----------
Loss = 0.987583
Variable usage = 100.00%
----------Iter = 45900----------
Loss = 0.987579
Variable usage = 100.00%
----------Iter = 46000----------
Loss = 0.987575
Variable usage = 100.00%
----------Iter = 46100----------
Loss = 0.987571
Variable usage = 100.00%
----------Iter = 46200----------
Loss = 0.987567
Variable usage = 100.00%
----------Iter = 46300----------
Loss = 0.987563
Variable usage = 100.00%
----------Iter = 46400----------
Loss = 0.987559
Variable usage = 100.00%
----------Iter = 46500----------
Loss = 0.987556
Variable usage = 100.00%
----------Iter = 46600----------
Loss = 0.987552
Variable usage = 100.00%
----------Iter = 46700----------
Loss = 0.987548
Variable usage = 100.00%
----------Iter = 46800----------
Loss = 0.987545
Variable usage = 100.00%
----------Iter = 46900----------
Loss = 0.987541
Variable usage = 100.00%
----------Iter = 47000----------
Loss = 0.987538
Variable usage = 100.00%
----------Iter = 47100----------
Loss = 0.987534
Variable usage = 100.00%
----------Iter = 47200----------
Loss = 0.987530
Variable usage = 100.00%
----------Iter = 47300----------
Loss = 0.987527
Variable usage = 100.00%
----------Iter = 47400----------
Loss = 0.987523
Variable usage = 100.00%
----------Iter = 47500----------
Loss = 0.987519
Variable usage = 100.00%
----------Iter = 47600----------
Loss = 0.987516
Variable usage = 100.00%
----------Iter = 47700----------
Loss = 0.987512
Variable usage = 100.00%
----------Iter = 47800----------
Loss = 0.987509
Variable usage = 100.00%
----------Iter = 47900----------
Loss = 0.987505
Variable usage = 100.00%
----------Iter = 48000----------
Loss = 0.987502
Variable usage = 100.00%
----------Iter = 48100----------
Loss = 0.987499
Variable usage = 100.00%
----------Iter = 48200----------
Loss = 0.987495
Variable usage = 100.00%
----------Iter = 48300----------
Loss = 0.987492
Variable usage = 100.00%
----------Iter = 48400----------
Loss = 0.987489
Variable usage = 100.00%
----------Iter = 48500----------
Loss = 0.987485
Variable usage = 100.00%
----------Iter = 48600----------
Loss = 0.987482
Variable usage = 100.00%
----------Iter = 48700----------
Loss = 0.987479
Variable usage = 100.00%
----------Iter = 48800----------
Loss = 0.987475
Variable usage = 100.00%
----------Iter = 48900----------
Loss = 0.987472
Variable usage = 100.00%
----------Iter = 49000----------
Loss = 0.987469
Variable usage = 100.00%
----------Iter = 49100----------
Loss = 0.987466
Variable usage = 100.00%
----------Iter = 49200----------
Loss = 0.987462
Variable usage = 100.00%
----------Iter = 49300----------
Loss = 0.987459
Variable usage = 100.00%
----------Iter = 49400----------
Loss = 0.987456
Variable usage = 100.00%
----------Iter = 49500----------
Loss = 0.987452
Variable usage = 100.00%
----------Iter = 49600----------
Loss = 0.987449
Variable usage = 100.00%
----------Iter = 49700----------
Loss = 0.987446
Variable usage = 100.00%
----------Iter = 49800----------
Loss = 0.987443
Variable usage = 100.00%
----------Iter = 49900----------
Loss = 0.987440
Variable usage = 100.00%
----------Iter = 50000----------
Loss = 0.987437
Variable usage = 100.00%
True variable usage = 75.00%
Estimated variable usage = 100.00%
Accuracy = 0.75%
Accuracy for experiment id 9 is 0.75
----------Iter = 100----------
Loss = 1.002672
Variable usage = 100.00%
----------Iter = 200----------
Loss = 1.002663
Variable usage = 100.00%
----------Iter = 300----------
Loss = 1.002663
Variable usage = 100.00%
----------Iter = 400----------
Loss = 1.002663
Variable usage = 100.00%
----------Iter = 500----------
Loss = 1.002663
Variable usage = 100.00%
----------Iter = 600----------
Loss = 1.002663
Variable usage = 100.00%
----------Iter = 700----------
Loss = 1.002663
Variable usage = 100.00%
Stopping early
True variable usage = 75.00%
Estimated variable usage = 100.00%
Accuracy = 0.75%
Accuracy for experiment id 10 is 0.75
----------Iter = 100----------
Loss = 1.012114
Variable usage = 75.00%
----------Iter = 200----------
Loss = 1.012106
Variable usage = 75.00%
----------Iter = 300----------
Loss = 1.012106
Variable usage = 75.00%
----------Iter = 400----------
Loss = 1.012106
Variable usage = 75.00%
----------Iter = 500----------
Loss = 1.012106
Variable usage = 75.00%
----------Iter = 600----------
Loss = 1.012106
Variable usage = 75.00%
----------Iter = 700----------
Loss = 1.012106
Variable usage = 75.00%
Stopping early
True variable usage = 75.00%
Estimated variable usage = 75.00%
Accuracy = 1.00%
Accuracy for experiment id 11 is 1.0
----------Iter = 100----------
Loss = 1.019691
Variable usage = 75.00%
----------Iter = 200----------
Loss = 1.019682
Variable usage = 75.00%
----------Iter = 300----------
Loss = 1.019682
Variable usage = 75.00%
----------Iter = 400----------
Loss = 1.019682
Variable usage = 75.00%
----------Iter = 500----------
Loss = 1.019682
Variable usage = 75.00%
----------Iter = 600----------
Loss = 1.019682
Variable usage = 75.00%
----------Iter = 700----------
Loss = 1.019682
Variable usage = 75.00%
Stopping early
True variable usage = 75.00%
Estimated variable usage = 75.00%
Accuracy = 1.00%
Accuracy for experiment id 12 is 1.0
----------Iter = 100----------
Loss = 0.990431
Variable usage = 100.00%
----------Iter = 200----------
Loss = 0.990423
Variable usage = 100.00%
----------Iter = 300----------
Loss = 0.990423
Variable usage = 100.00%
----------Iter = 400----------
Loss = 0.990423
Variable usage = 100.00%
----------Iter = 500----------
Loss = 0.990423
Variable usage = 100.00%
----------Iter = 600----------
Loss = 0.990423
Variable usage = 100.00%
----------Iter = 700----------
Loss = 0.990423
Variable usage = 100.00%
Stopping early
True variable usage = 75.00%
Estimated variable usage = 100.00%
Accuracy = 0.75%
Accuracy for experiment id 13 is 0.75
----------Iter = 100----------
Loss = 0.987083
Variable usage = 75.00%
----------Iter = 200----------
Loss = 0.987071
Variable usage = 75.00%
----------Iter = 300----------
Loss = 0.987071
Variable usage = 75.00%
----------Iter = 400----------
Loss = 0.987071
Variable usage = 75.00%
----------Iter = 500----------
Loss = 0.987071
Variable usage = 75.00%
----------Iter = 600----------
Loss = 0.987071
Variable usage = 75.00%
----------Iter = 700----------
Loss = 0.987071
Variable usage = 75.00%
Stopping early
True variable usage = 75.00%
Estimated variable usage = 75.00%
Accuracy = 0.50%
Accuracy for experiment id 14 is 0.5


###############################################################################
Science Cluster
Job 2547016 for user 'bjerkovic'
Finished at: Mon Dec 12 06:25:12 CET 2022

Job details:
============

Name                : exp5c
User                : bjerkovic
Partition           : csedu
Nodes               : cn47
Cores               : 2
State               : COMPLETED
Submit              : 2022-12-12T06:01:01
Start               : 2022-12-12T06:01:01
End                 : 2022-12-12T06:25:12
Reserved walltime   : 1-06:00:00
Used walltime       :   00:24:11
Used CPU time       :   00:28:31 (efficiency: 58.98%)
% User (Computation): 99.40%
% System (I/O)      :  0.60%
Mem reserved        : 2G/core
Max Mem used        : 2.29G (cn47)
Max Disk Write      : 0.00  (cn47)
Max Disk Read       : 839.68K (cn47)


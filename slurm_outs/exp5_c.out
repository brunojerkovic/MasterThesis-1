/home/bjerkovic/thesis/models/nri/sourcecode/modules.py:27: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.
  nn.init.xavier_normal(m.weight.data)
/home/bjerkovic/thesis/models/nri/sourcecode/modules.py:110: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.
  nn.init.xavier_normal(m.weight.data)
/home/bjerkovic/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/bjerkovic/thesis/models/nri/sourcecode/utils.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  soft_max_1d = F.softmax(trans_input) # dim=1
/home/bjerkovic/thesis/models/nri/nri_train_test.py:117: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data, relations = Variable(data, volatile=True), Variable(
/home/bjerkovic/thesis/models/nri/nri_train_test.py:191: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data, relations = Variable(data, volatile=True), Variable(
----------Iter = 100----------
Loss = 0.999381
Variable usage = 75.00%
----------Iter = 200----------
Loss = 0.999374
Variable usage = 25.00%
----------Iter = 300----------
Loss = 0.999374
Variable usage = 25.00%
----------Iter = 400----------
Loss = 0.999374
Variable usage = 25.00%
----------Iter = 500----------
Loss = 0.999374
Variable usage = 25.00%
----------Iter = 600----------
Loss = 0.999374
Variable usage = 25.00%
----------Iter = 700----------
Loss = 0.999374
Variable usage = 25.00%
Stopping early
True variable usage = 50.00%
Estimated variable usage = 25.00%
Accuracy = 0.75%
Accuracy for experiment id 0 is 0.75
----------Iter = 100----------
Loss = 0.997121
Variable usage = 100.00%
----------Iter = 200----------
Loss = 0.997106
Variable usage = 100.00%
----------Iter = 300----------
Loss = 0.997106
Variable usage = 100.00%
----------Iter = 400----------
Loss = 0.997106
Variable usage = 100.00%
----------Iter = 500----------
Loss = 0.997106
Variable usage = 100.00%
----------Iter = 600----------
Loss = 0.997106
Variable usage = 100.00%
----------Iter = 700----------
Loss = 0.997106
Variable usage = 100.00%
----------Iter = 800----------
Loss = 0.997106
Variable usage = 100.00%
Stopping early
True variable usage = 50.00%
Estimated variable usage = 100.00%
Accuracy = 0.50%
Accuracy for experiment id 1 is 0.5
----------Iter = 100----------
Loss = 1.006153
Variable usage = 100.00%
----------Iter = 200----------
Loss = 1.006146
Variable usage = 100.00%
----------Iter = 300----------
Loss = 1.006146
Variable usage = 100.00%
----------Iter = 400----------
Loss = 1.006146
Variable usage = 100.00%
----------Iter = 500----------
Loss = 1.006146
Variable usage = 100.00%
----------Iter = 600----------
Loss = 1.006146
Variable usage = 100.00%
----------Iter = 700----------
Loss = 1.006146
Variable usage = 100.00%
----------Iter = 800----------
Loss = 1.006146
Variable usage = 100.00%
Stopping early
True variable usage = 50.00%
Estimated variable usage = 100.00%
Accuracy = 0.50%
Accuracy for experiment id 2 is 0.5
----------Iter = 100----------
Loss = 1.005304
Variable usage = 25.00%
----------Iter = 200----------
Loss = 1.005303
Variable usage = 25.00%
----------Iter = 300----------
Loss = 1.005303
Variable usage = 25.00%
----------Iter = 400----------
Loss = 1.005303
Variable usage = 25.00%
----------Iter = 500----------
Loss = 1.005303
Variable usage = 25.00%
----------Iter = 600----------
Loss = 1.005303
Variable usage = 25.00%
----------Iter = 700----------
Loss = 1.005303
Variable usage = 25.00%
Stopping early
True variable usage = 50.00%
Estimated variable usage = 25.00%
Accuracy = 0.25%
Accuracy for experiment id 3 is 0.25
----------Iter = 100----------
Loss = 1.012043
Variable usage = 50.00%
----------Iter = 200----------
Loss = 1.012033
Variable usage = 50.00%
----------Iter = 300----------
Loss = 1.012033
Variable usage = 50.00%
----------Iter = 400----------
Loss = 1.012033
Variable usage = 50.00%
----------Iter = 500----------
Loss = 1.012033
Variable usage = 50.00%
----------Iter = 600----------
Loss = 1.012033
Variable usage = 50.00%
----------Iter = 700----------
Loss = 1.012033
Variable usage = 50.00%
Stopping early
True variable usage = 50.00%
Estimated variable usage = 50.00%
Accuracy = 0.50%
Accuracy for experiment id 4 is 0.5
----------Iter = 100----------
Loss = 1.029429
Variable usage = 100.00%
----------Iter = 200----------
Loss = 1.027701
Variable usage = 100.00%
----------Iter = 300----------
Loss = 1.026732
Variable usage = 100.00%
----------Iter = 400----------
Loss = 1.026059
Variable usage = 100.00%
----------Iter = 500----------
Loss = 1.025546
Variable usage = 100.00%
----------Iter = 600----------
Loss = 1.025136
Variable usage = 100.00%
----------Iter = 700----------
Loss = 1.024794
Variable usage = 100.00%
----------Iter = 800----------
Loss = 1.024500
Variable usage = 100.00%
----------Iter = 900----------
Loss = 1.024243
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 1.024012
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 1.023797
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 1.023597
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 1.023407
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 1.023228
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 1.023056
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 1.022891
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 1.022728
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 1.022568
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 1.022408
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 1.022254
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 1.022104
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 1.021956
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 1.021810
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 1.021665
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 1.021521
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 1.021378
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 1.021235
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 1.021091
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 1.020947
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 1.020803
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 1.020661
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 1.020520
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 1.020379
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 1.020239
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 1.020100
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 1.019962
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 1.019823
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 1.019685
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 1.019548
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 1.019413
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 1.019281
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 1.019149
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 1.019020
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 1.018892
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 1.018766
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 1.018641
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 1.018518
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 1.018395
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 1.018272
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 1.018150
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 1.018028
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 1.017908
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 1.017788
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 1.017669
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 1.017551
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 1.017434
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 1.017315
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 1.017198
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 1.017080
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 1.016962
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 1.016845
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 1.016729
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 1.016615
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 1.016502
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 1.016392
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 1.016281
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 1.016171
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 1.016063
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 1.015956
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 1.015850
Variable usage = 100.00%
----------Iter = 7100----------
Loss = 1.015744
Variable usage = 100.00%
----------Iter = 7200----------
Loss = 1.015637
Variable usage = 100.00%
----------Iter = 7300----------
Loss = 1.015532
Variable usage = 100.00%
----------Iter = 7400----------
Loss = 1.015427
Variable usage = 100.00%
----------Iter = 7500----------
Loss = 1.015325
Variable usage = 100.00%
----------Iter = 7600----------
Loss = 1.015222
Variable usage = 100.00%
----------Iter = 7700----------
Loss = 1.015120
Variable usage = 100.00%
----------Iter = 7800----------
Loss = 1.015020
Variable usage = 100.00%
----------Iter = 7900----------
Loss = 1.014920
Variable usage = 100.00%
----------Iter = 8000----------
Loss = 1.014820
Variable usage = 100.00%
----------Iter = 8100----------
Loss = 1.014722
Variable usage = 100.00%
----------Iter = 8200----------
Loss = 1.014625
Variable usage = 100.00%
----------Iter = 8300----------
Loss = 1.014529
Variable usage = 100.00%
----------Iter = 8400----------
Loss = 1.014431
Variable usage = 100.00%
----------Iter = 8500----------
Loss = 1.014334
Variable usage = 100.00%
----------Iter = 8600----------
Loss = 1.014237
Variable usage = 100.00%
----------Iter = 8700----------
Loss = 1.014141
Variable usage = 100.00%
----------Iter = 8800----------
Loss = 1.014046
Variable usage = 100.00%
----------Iter = 8900----------
Loss = 1.013949
Variable usage = 100.00%
----------Iter = 9000----------
Loss = 1.013854
Variable usage = 100.00%
----------Iter = 9100----------
Loss = 1.013758
Variable usage = 100.00%
----------Iter = 9200----------
Loss = 1.013665
Variable usage = 100.00%
----------Iter = 9300----------
Loss = 1.013573
Variable usage = 100.00%
----------Iter = 9400----------
Loss = 1.013482
Variable usage = 100.00%
----------Iter = 9500----------
Loss = 1.013392
Variable usage = 100.00%
----------Iter = 9600----------
Loss = 1.013303
Variable usage = 100.00%
----------Iter = 9700----------
Loss = 1.013214
Variable usage = 100.00%
----------Iter = 9800----------
Loss = 1.013126
Variable usage = 100.00%
----------Iter = 9900----------
Loss = 1.013040
Variable usage = 100.00%
----------Iter = 10000----------
Loss = 1.012955
Variable usage = 100.00%
----------Iter = 10100----------
Loss = 1.012871
Variable usage = 100.00%
----------Iter = 10200----------
Loss = 1.012787
Variable usage = 100.00%
----------Iter = 10300----------
Loss = 1.012703
Variable usage = 100.00%
----------Iter = 10400----------
Loss = 1.012621
Variable usage = 100.00%
----------Iter = 10500----------
Loss = 1.012539
Variable usage = 100.00%
----------Iter = 10600----------
Loss = 1.012457
Variable usage = 100.00%
----------Iter = 10700----------
Loss = 1.012376
Variable usage = 100.00%
----------Iter = 10800----------
Loss = 1.012295
Variable usage = 100.00%
----------Iter = 10900----------
Loss = 1.012215
Variable usage = 100.00%
----------Iter = 11000----------
Loss = 1.012136
Variable usage = 100.00%
----------Iter = 11100----------
Loss = 1.012057
Variable usage = 100.00%
----------Iter = 11200----------
Loss = 1.011979
Variable usage = 100.00%
----------Iter = 11300----------
Loss = 1.011903
Variable usage = 100.00%
----------Iter = 11400----------
Loss = 1.011826
Variable usage = 100.00%
----------Iter = 11500----------
Loss = 1.011750
Variable usage = 100.00%
----------Iter = 11600----------
Loss = 1.011675
Variable usage = 100.00%
----------Iter = 11700----------
Loss = 1.011601
Variable usage = 100.00%
----------Iter = 11800----------
Loss = 1.011529
Variable usage = 100.00%
----------Iter = 11900----------
Loss = 1.011456
Variable usage = 100.00%
----------Iter = 12000----------
Loss = 1.011384
Variable usage = 100.00%
----------Iter = 12100----------
Loss = 1.011313
Variable usage = 100.00%
----------Iter = 12200----------
Loss = 1.011243
Variable usage = 100.00%
----------Iter = 12300----------
Loss = 1.011173
Variable usage = 100.00%
----------Iter = 12400----------
Loss = 1.011103
Variable usage = 100.00%
----------Iter = 12500----------
Loss = 1.011033
Variable usage = 100.00%
----------Iter = 12600----------
Loss = 1.010965
Variable usage = 100.00%
----------Iter = 12700----------
Loss = 1.010898
Variable usage = 100.00%
----------Iter = 12800----------
Loss = 1.010831
Variable usage = 100.00%
----------Iter = 12900----------
Loss = 1.010764
Variable usage = 100.00%
----------Iter = 13000----------
Loss = 1.010696
Variable usage = 100.00%
----------Iter = 13100----------
Loss = 1.010629
Variable usage = 100.00%
----------Iter = 13200----------
Loss = 1.010562
Variable usage = 100.00%
----------Iter = 13300----------
Loss = 1.010497
Variable usage = 100.00%
----------Iter = 13400----------
Loss = 1.010430
Variable usage = 100.00%
----------Iter = 13500----------
Loss = 1.010364
Variable usage = 100.00%
----------Iter = 13600----------
Loss = 1.010299
Variable usage = 100.00%
----------Iter = 13700----------
Loss = 1.010234
Variable usage = 100.00%
----------Iter = 13800----------
Loss = 1.010171
Variable usage = 100.00%
----------Iter = 13900----------
Loss = 1.010109
Variable usage = 100.00%
----------Iter = 14000----------
Loss = 1.010048
Variable usage = 100.00%
----------Iter = 14100----------
Loss = 1.009987
Variable usage = 100.00%
----------Iter = 14200----------
Loss = 1.009927
Variable usage = 100.00%
----------Iter = 14300----------
Loss = 1.009868
Variable usage = 100.00%
----------Iter = 14400----------
Loss = 1.009809
Variable usage = 100.00%
----------Iter = 14500----------
Loss = 1.009751
Variable usage = 100.00%
----------Iter = 14600----------
Loss = 1.009692
Variable usage = 100.00%
----------Iter = 14700----------
Loss = 1.009635
Variable usage = 100.00%
----------Iter = 14800----------
Loss = 1.009577
Variable usage = 100.00%
----------Iter = 14900----------
Loss = 1.009520
Variable usage = 100.00%
----------Iter = 15000----------
Loss = 1.009463
Variable usage = 100.00%
----------Iter = 15100----------
Loss = 1.009406
Variable usage = 100.00%
----------Iter = 15200----------
Loss = 1.009351
Variable usage = 100.00%
----------Iter = 15300----------
Loss = 1.009296
Variable usage = 100.00%
----------Iter = 15400----------
Loss = 1.009242
Variable usage = 100.00%
----------Iter = 15500----------
Loss = 1.009189
Variable usage = 100.00%
----------Iter = 15600----------
Loss = 1.009136
Variable usage = 100.00%
----------Iter = 15700----------
Loss = 1.009084
Variable usage = 100.00%
----------Iter = 15800----------
Loss = 1.009032
Variable usage = 100.00%
----------Iter = 15900----------
Loss = 1.008981
Variable usage = 100.00%
----------Iter = 16000----------
Loss = 1.008930
Variable usage = 100.00%
----------Iter = 16100----------
Loss = 1.008880
Variable usage = 100.00%
----------Iter = 16200----------
Loss = 1.008830
Variable usage = 100.00%
----------Iter = 16300----------
Loss = 1.008781
Variable usage = 100.00%
----------Iter = 16400----------
Loss = 1.008732
Variable usage = 100.00%
----------Iter = 16500----------
Loss = 1.008683
Variable usage = 100.00%
----------Iter = 16600----------
Loss = 1.008634
Variable usage = 100.00%
----------Iter = 16700----------
Loss = 1.008586
Variable usage = 100.00%
----------Iter = 16800----------
Loss = 1.008539
Variable usage = 100.00%
----------Iter = 16900----------
Loss = 1.008492
Variable usage = 100.00%
----------Iter = 17000----------
Loss = 1.008446
Variable usage = 100.00%
----------Iter = 17100----------
Loss = 1.008399
Variable usage = 100.00%
----------Iter = 17200----------
Loss = 1.008352
Variable usage = 100.00%
----------Iter = 17300----------
Loss = 1.008305
Variable usage = 100.00%
----------Iter = 17400----------
Loss = 1.008259
Variable usage = 100.00%
----------Iter = 17500----------
Loss = 1.008212
Variable usage = 100.00%
----------Iter = 17600----------
Loss = 1.008166
Variable usage = 100.00%
----------Iter = 17700----------
Loss = 1.008120
Variable usage = 100.00%
----------Iter = 17800----------
Loss = 1.008074
Variable usage = 100.00%
----------Iter = 17900----------
Loss = 1.008028
Variable usage = 100.00%
----------Iter = 18000----------
Loss = 1.007983
Variable usage = 100.00%
----------Iter = 18100----------
Loss = 1.007938
Variable usage = 100.00%
----------Iter = 18200----------
Loss = 1.007892
Variable usage = 100.00%
----------Iter = 18300----------
Loss = 1.007847
Variable usage = 100.00%
----------Iter = 18400----------
Loss = 1.007802
Variable usage = 100.00%
----------Iter = 18500----------
Loss = 1.007757
Variable usage = 100.00%
----------Iter = 18600----------
Loss = 1.007713
Variable usage = 100.00%
----------Iter = 18700----------
Loss = 1.007670
Variable usage = 100.00%
----------Iter = 18800----------
Loss = 1.007627
Variable usage = 100.00%
----------Iter = 18900----------
Loss = 1.007584
Variable usage = 100.00%
----------Iter = 19000----------
Loss = 1.007541
Variable usage = 100.00%
----------Iter = 19100----------
Loss = 1.007498
Variable usage = 100.00%
----------Iter = 19200----------
Loss = 1.007456
Variable usage = 100.00%
----------Iter = 19300----------
Loss = 1.007415
Variable usage = 100.00%
----------Iter = 19400----------
Loss = 1.007373
Variable usage = 100.00%
----------Iter = 19500----------
Loss = 1.007332
Variable usage = 100.00%
----------Iter = 19600----------
Loss = 1.007292
Variable usage = 100.00%
----------Iter = 19700----------
Loss = 1.007252
Variable usage = 100.00%
----------Iter = 19800----------
Loss = 1.007212
Variable usage = 100.00%
----------Iter = 19900----------
Loss = 1.007172
Variable usage = 100.00%
----------Iter = 20000----------
Loss = 1.007132
Variable usage = 100.00%
----------Iter = 20100----------
Loss = 1.007092
Variable usage = 100.00%
----------Iter = 20200----------
Loss = 1.007053
Variable usage = 100.00%
----------Iter = 20300----------
Loss = 1.007014
Variable usage = 100.00%
----------Iter = 20400----------
Loss = 1.006976
Variable usage = 100.00%
----------Iter = 20500----------
Loss = 1.006938
Variable usage = 100.00%
----------Iter = 20600----------
Loss = 1.006901
Variable usage = 100.00%
----------Iter = 20700----------
Loss = 1.006863
Variable usage = 100.00%
----------Iter = 20800----------
Loss = 1.006825
Variable usage = 100.00%
----------Iter = 20900----------
Loss = 1.006787
Variable usage = 100.00%
----------Iter = 21000----------
Loss = 1.006750
Variable usage = 100.00%
----------Iter = 21100----------
Loss = 1.006712
Variable usage = 100.00%
----------Iter = 21200----------
Loss = 1.006674
Variable usage = 100.00%
----------Iter = 21300----------
Loss = 1.006637
Variable usage = 100.00%
----------Iter = 21400----------
Loss = 1.006601
Variable usage = 100.00%
----------Iter = 21500----------
Loss = 1.006566
Variable usage = 100.00%
----------Iter = 21600----------
Loss = 1.006530
Variable usage = 100.00%
----------Iter = 21700----------
Loss = 1.006494
Variable usage = 100.00%
----------Iter = 21800----------
Loss = 1.006459
Variable usage = 100.00%
----------Iter = 21900----------
Loss = 1.006423
Variable usage = 100.00%
----------Iter = 22000----------
Loss = 1.006387
Variable usage = 100.00%
----------Iter = 22100----------
Loss = 1.006351
Variable usage = 100.00%
----------Iter = 22200----------
Loss = 1.006316
Variable usage = 100.00%
----------Iter = 22300----------
Loss = 1.006281
Variable usage = 100.00%
----------Iter = 22400----------
Loss = 1.006246
Variable usage = 100.00%
----------Iter = 22500----------
Loss = 1.006212
Variable usage = 100.00%
----------Iter = 22600----------
Loss = 1.006178
Variable usage = 100.00%
----------Iter = 22700----------
Loss = 1.006145
Variable usage = 100.00%
----------Iter = 22800----------
Loss = 1.006111
Variable usage = 100.00%
----------Iter = 22900----------
Loss = 1.006078
Variable usage = 100.00%
----------Iter = 23000----------
Loss = 1.006045
Variable usage = 100.00%
----------Iter = 23100----------
Loss = 1.006012
Variable usage = 100.00%
----------Iter = 23200----------
Loss = 1.005979
Variable usage = 100.00%
----------Iter = 23300----------
Loss = 1.005947
Variable usage = 100.00%
----------Iter = 23400----------
Loss = 1.005915
Variable usage = 100.00%
----------Iter = 23500----------
Loss = 1.005883
Variable usage = 100.00%
----------Iter = 23600----------
Loss = 1.005851
Variable usage = 100.00%
----------Iter = 23700----------
Loss = 1.005820
Variable usage = 100.00%
----------Iter = 23800----------
Loss = 1.005788
Variable usage = 100.00%
----------Iter = 23900----------
Loss = 1.005757
Variable usage = 100.00%
----------Iter = 24000----------
Loss = 1.005726
Variable usage = 100.00%
----------Iter = 24100----------
Loss = 1.005695
Variable usage = 100.00%
----------Iter = 24200----------
Loss = 1.005664
Variable usage = 100.00%
----------Iter = 24300----------
Loss = 1.005634
Variable usage = 100.00%
----------Iter = 24400----------
Loss = 1.005604
Variable usage = 100.00%
----------Iter = 24500----------
Loss = 1.005574
Variable usage = 100.00%
----------Iter = 24600----------
Loss = 1.005544
Variable usage = 100.00%
----------Iter = 24700----------
Loss = 1.005514
Variable usage = 100.00%
----------Iter = 24800----------
Loss = 1.005485
Variable usage = 100.00%
----------Iter = 24900----------
Loss = 1.005456
Variable usage = 100.00%
----------Iter = 25000----------
Loss = 1.005426
Variable usage = 100.00%
----------Iter = 25100----------
Loss = 1.005397
Variable usage = 100.00%
----------Iter = 25200----------
Loss = 1.005368
Variable usage = 100.00%
----------Iter = 25300----------
Loss = 1.005338
Variable usage = 100.00%
----------Iter = 25400----------
Loss = 1.005308
Variable usage = 100.00%
----------Iter = 25500----------
Loss = 1.005278
Variable usage = 100.00%
----------Iter = 25600----------
Loss = 1.005247
Variable usage = 100.00%
----------Iter = 25700----------
Loss = 1.005216
Variable usage = 100.00%
----------Iter = 25800----------
Loss = 1.005185
Variable usage = 100.00%
----------Iter = 25900----------
Loss = 1.005154
Variable usage = 100.00%
----------Iter = 26000----------
Loss = 1.005123
Variable usage = 100.00%
----------Iter = 26100----------
Loss = 1.005093
Variable usage = 100.00%
----------Iter = 26200----------
Loss = 1.005062
Variable usage = 100.00%
----------Iter = 26300----------
Loss = 1.005030
Variable usage = 100.00%
----------Iter = 26400----------
Loss = 1.004999
Variable usage = 100.00%
----------Iter = 26500----------
Loss = 1.004968
Variable usage = 100.00%
----------Iter = 26600----------
Loss = 1.004938
Variable usage = 100.00%
----------Iter = 26700----------
Loss = 1.004908
Variable usage = 100.00%
----------Iter = 26800----------
Loss = 1.004878
Variable usage = 100.00%
----------Iter = 26900----------
Loss = 1.004849
Variable usage = 100.00%
----------Iter = 27000----------
Loss = 1.004820
Variable usage = 100.00%
----------Iter = 27100----------
Loss = 1.004791
Variable usage = 100.00%
----------Iter = 27200----------
Loss = 1.004762
Variable usage = 100.00%
----------Iter = 27300----------
Loss = 1.004732
Variable usage = 100.00%
----------Iter = 27400----------
Loss = 1.004703
Variable usage = 100.00%
----------Iter = 27500----------
Loss = 1.004674
Variable usage = 100.00%
----------Iter = 27600----------
Loss = 1.004646
Variable usage = 100.00%
----------Iter = 27700----------
Loss = 1.004617
Variable usage = 100.00%
----------Iter = 27800----------
Loss = 1.004589
Variable usage = 100.00%
----------Iter = 27900----------
Loss = 1.004562
Variable usage = 100.00%
----------Iter = 28000----------
Loss = 1.004534
Variable usage = 100.00%
----------Iter = 28100----------
Loss = 1.004506
Variable usage = 100.00%
----------Iter = 28200----------
Loss = 1.004479
Variable usage = 100.00%
----------Iter = 28300----------
Loss = 1.004452
Variable usage = 100.00%
----------Iter = 28400----------
Loss = 1.004425
Variable usage = 100.00%
----------Iter = 28500----------
Loss = 1.004398
Variable usage = 100.00%
----------Iter = 28600----------
Loss = 1.004371
Variable usage = 100.00%
----------Iter = 28700----------
Loss = 1.004344
Variable usage = 100.00%
----------Iter = 28800----------
Loss = 1.004318
Variable usage = 100.00%
----------Iter = 28900----------
Loss = 1.004291
Variable usage = 100.00%
----------Iter = 29000----------
Loss = 1.004265
Variable usage = 100.00%
----------Iter = 29100----------
Loss = 1.004239
Variable usage = 100.00%
----------Iter = 29200----------
Loss = 1.004213
Variable usage = 100.00%
----------Iter = 29300----------
Loss = 1.004187
Variable usage = 100.00%
----------Iter = 29400----------
Loss = 1.004161
Variable usage = 100.00%
----------Iter = 29500----------
Loss = 1.004136
Variable usage = 100.00%
----------Iter = 29600----------
Loss = 1.004110
Variable usage = 100.00%
----------Iter = 29700----------
Loss = 1.004084
Variable usage = 100.00%
----------Iter = 29800----------
Loss = 1.004058
Variable usage = 100.00%
----------Iter = 29900----------
Loss = 1.004032
Variable usage = 100.00%
----------Iter = 30000----------
Loss = 1.004007
Variable usage = 100.00%
----------Iter = 30100----------
Loss = 1.003981
Variable usage = 100.00%
----------Iter = 30200----------
Loss = 1.003956
Variable usage = 100.00%
----------Iter = 30300----------
Loss = 1.003930
Variable usage = 100.00%
----------Iter = 30400----------
Loss = 1.003905
Variable usage = 100.00%
----------Iter = 30500----------
Loss = 1.003880
Variable usage = 100.00%
----------Iter = 30600----------
Loss = 1.003854
Variable usage = 100.00%
----------Iter = 30700----------
Loss = 1.003828
Variable usage = 100.00%
----------Iter = 30800----------
Loss = 1.003802
Variable usage = 100.00%
----------Iter = 30900----------
Loss = 1.003777
Variable usage = 100.00%
----------Iter = 31000----------
Loss = 1.003751
Variable usage = 100.00%
----------Iter = 31100----------
Loss = 1.003725
Variable usage = 100.00%
----------Iter = 31200----------
Loss = 1.003700
Variable usage = 100.00%
----------Iter = 31300----------
Loss = 1.003675
Variable usage = 100.00%
----------Iter = 31400----------
Loss = 1.003649
Variable usage = 100.00%
----------Iter = 31500----------
Loss = 1.003624
Variable usage = 100.00%
----------Iter = 31600----------
Loss = 1.003599
Variable usage = 100.00%
----------Iter = 31700----------
Loss = 1.003574
Variable usage = 100.00%
----------Iter = 31800----------
Loss = 1.003548
Variable usage = 100.00%
----------Iter = 31900----------
Loss = 1.003524
Variable usage = 100.00%
----------Iter = 32000----------
Loss = 1.003499
Variable usage = 100.00%
----------Iter = 32100----------
Loss = 1.003474
Variable usage = 100.00%
----------Iter = 32200----------
Loss = 1.003449
Variable usage = 100.00%
----------Iter = 32300----------
Loss = 1.003424
Variable usage = 100.00%
----------Iter = 32400----------
Loss = 1.003399
Variable usage = 100.00%
----------Iter = 32500----------
Loss = 1.003373
Variable usage = 100.00%
----------Iter = 32600----------
Loss = 1.003348
Variable usage = 100.00%
----------Iter = 32700----------
Loss = 1.003322
Variable usage = 100.00%
----------Iter = 32800----------
Loss = 1.003298
Variable usage = 100.00%
----------Iter = 32900----------
Loss = 1.003273
Variable usage = 100.00%
----------Iter = 33000----------
Loss = 1.003247
Variable usage = 100.00%
----------Iter = 33100----------
Loss = 1.003222
Variable usage = 100.00%
----------Iter = 33200----------
Loss = 1.003197
Variable usage = 100.00%
----------Iter = 33300----------
Loss = 1.003172
Variable usage = 100.00%
----------Iter = 33400----------
Loss = 1.003147
Variable usage = 100.00%
----------Iter = 33500----------
Loss = 1.003123
Variable usage = 100.00%
----------Iter = 33600----------
Loss = 1.003098
Variable usage = 100.00%
----------Iter = 33700----------
Loss = 1.003074
Variable usage = 100.00%
----------Iter = 33800----------
Loss = 1.003049
Variable usage = 100.00%
----------Iter = 33900----------
Loss = 1.003025
Variable usage = 100.00%
----------Iter = 34000----------
Loss = 1.003000
Variable usage = 100.00%
----------Iter = 34100----------
Loss = 1.002976
Variable usage = 100.00%
----------Iter = 34200----------
Loss = 1.002952
Variable usage = 100.00%
----------Iter = 34300----------
Loss = 1.002928
Variable usage = 100.00%
----------Iter = 34400----------
Loss = 1.002903
Variable usage = 100.00%
----------Iter = 34500----------
Loss = 1.002878
Variable usage = 100.00%
----------Iter = 34600----------
Loss = 1.002854
Variable usage = 100.00%
----------Iter = 34700----------
Loss = 1.002829
Variable usage = 100.00%
----------Iter = 34800----------
Loss = 1.002804
Variable usage = 100.00%
----------Iter = 34900----------
Loss = 1.002780
Variable usage = 100.00%
----------Iter = 35000----------
Loss = 1.002755
Variable usage = 100.00%
----------Iter = 35100----------
Loss = 1.002730
Variable usage = 100.00%
----------Iter = 35200----------
Loss = 1.002706
Variable usage = 100.00%
----------Iter = 35300----------
Loss = 1.002681
Variable usage = 100.00%
----------Iter = 35400----------
Loss = 1.002656
Variable usage = 100.00%
----------Iter = 35500----------
Loss = 1.002632
Variable usage = 100.00%
----------Iter = 35600----------
Loss = 1.002607
Variable usage = 100.00%
----------Iter = 35700----------
Loss = 1.002582
Variable usage = 100.00%
----------Iter = 35800----------
Loss = 1.002556
Variable usage = 100.00%
----------Iter = 35900----------
Loss = 1.002530
Variable usage = 100.00%
----------Iter = 36000----------
Loss = 1.002504
Variable usage = 100.00%
----------Iter = 36100----------
Loss = 1.002479
Variable usage = 100.00%
----------Iter = 36200----------
Loss = 1.002453
Variable usage = 100.00%
----------Iter = 36300----------
Loss = 1.002427
Variable usage = 100.00%
----------Iter = 36400----------
Loss = 1.002402
Variable usage = 100.00%
----------Iter = 36500----------
Loss = 1.002378
Variable usage = 100.00%
----------Iter = 36600----------
Loss = 1.002353
Variable usage = 100.00%
----------Iter = 36700----------
Loss = 1.002329
Variable usage = 100.00%
----------Iter = 36800----------
Loss = 1.002304
Variable usage = 100.00%
----------Iter = 36900----------
Loss = 1.002280
Variable usage = 100.00%
----------Iter = 37000----------
Loss = 1.002256
Variable usage = 100.00%
----------Iter = 37100----------
Loss = 1.002231
Variable usage = 100.00%
----------Iter = 37200----------
Loss = 1.002207
Variable usage = 100.00%
----------Iter = 37300----------
Loss = 1.002182
Variable usage = 100.00%
----------Iter = 37400----------
Loss = 1.002159
Variable usage = 100.00%
----------Iter = 37500----------
Loss = 1.002135
Variable usage = 100.00%
----------Iter = 37600----------
Loss = 1.002112
Variable usage = 100.00%
----------Iter = 37700----------
Loss = 1.002088
Variable usage = 100.00%
----------Iter = 37800----------
Loss = 1.002065
Variable usage = 100.00%
----------Iter = 37900----------
Loss = 1.002041
Variable usage = 100.00%
----------Iter = 38000----------
Loss = 1.002018
Variable usage = 100.00%
----------Iter = 38100----------
Loss = 1.001995
Variable usage = 100.00%
----------Iter = 38200----------
Loss = 1.001971
Variable usage = 100.00%
----------Iter = 38300----------
Loss = 1.001947
Variable usage = 100.00%
----------Iter = 38400----------
Loss = 1.001923
Variable usage = 100.00%
----------Iter = 38500----------
Loss = 1.001899
Variable usage = 100.00%
----------Iter = 38600----------
Loss = 1.001875
Variable usage = 100.00%
----------Iter = 38700----------
Loss = 1.001851
Variable usage = 100.00%
----------Iter = 38800----------
Loss = 1.001827
Variable usage = 100.00%
----------Iter = 38900----------
Loss = 1.001803
Variable usage = 100.00%
----------Iter = 39000----------
Loss = 1.001779
Variable usage = 100.00%
----------Iter = 39100----------
Loss = 1.001754
Variable usage = 100.00%
----------Iter = 39200----------
Loss = 1.001729
Variable usage = 100.00%
----------Iter = 39300----------
Loss = 1.001704
Variable usage = 100.00%
----------Iter = 39400----------
Loss = 1.001678
Variable usage = 100.00%
----------Iter = 39500----------
Loss = 1.001654
Variable usage = 100.00%
----------Iter = 39600----------
Loss = 1.001630
Variable usage = 100.00%
----------Iter = 39700----------
Loss = 1.001605
Variable usage = 100.00%
----------Iter = 39800----------
Loss = 1.001581
Variable usage = 100.00%
----------Iter = 39900----------
Loss = 1.001558
Variable usage = 100.00%
----------Iter = 40000----------
Loss = 1.001535
Variable usage = 100.00%
----------Iter = 40100----------
Loss = 1.001513
Variable usage = 100.00%
----------Iter = 40200----------
Loss = 1.001492
Variable usage = 100.00%
----------Iter = 40300----------
Loss = 1.001471
Variable usage = 100.00%
----------Iter = 40400----------
Loss = 1.001449
Variable usage = 100.00%
----------Iter = 40500----------
Loss = 1.001428
Variable usage = 100.00%
----------Iter = 40600----------
Loss = 1.001406
Variable usage = 100.00%
----------Iter = 40700----------
Loss = 1.001385
Variable usage = 100.00%
----------Iter = 40800----------
Loss = 1.001364
Variable usage = 100.00%
----------Iter = 40900----------
Loss = 1.001343
Variable usage = 100.00%
----------Iter = 41000----------
Loss = 1.001322
Variable usage = 100.00%
----------Iter = 41100----------
Loss = 1.001301
Variable usage = 100.00%
----------Iter = 41200----------
Loss = 1.001280
Variable usage = 100.00%
----------Iter = 41300----------
Loss = 1.001259
Variable usage = 100.00%
----------Iter = 41400----------
Loss = 1.001238
Variable usage = 100.00%
----------Iter = 41500----------
Loss = 1.001218
Variable usage = 100.00%
----------Iter = 41600----------
Loss = 1.001198
Variable usage = 100.00%
----------Iter = 41700----------
Loss = 1.001178
Variable usage = 100.00%
----------Iter = 41800----------
Loss = 1.001158
Variable usage = 100.00%
----------Iter = 41900----------
Loss = 1.001138
Variable usage = 100.00%
----------Iter = 42000----------
Loss = 1.001118
Variable usage = 100.00%
----------Iter = 42100----------
Loss = 1.001099
Variable usage = 100.00%
----------Iter = 42200----------
Loss = 1.001079
Variable usage = 100.00%
----------Iter = 42300----------
Loss = 1.001060
Variable usage = 100.00%
----------Iter = 42400----------
Loss = 1.001041
Variable usage = 100.00%
----------Iter = 42500----------
Loss = 1.001022
Variable usage = 100.00%
----------Iter = 42600----------
Loss = 1.001004
Variable usage = 100.00%
----------Iter = 42700----------
Loss = 1.000986
Variable usage = 100.00%
----------Iter = 42800----------
Loss = 1.000967
Variable usage = 100.00%
----------Iter = 42900----------
Loss = 1.000949
Variable usage = 100.00%
----------Iter = 43000----------
Loss = 1.000930
Variable usage = 100.00%
----------Iter = 43100----------
Loss = 1.000912
Variable usage = 100.00%
----------Iter = 43200----------
Loss = 1.000893
Variable usage = 100.00%
----------Iter = 43300----------
Loss = 1.000876
Variable usage = 100.00%
----------Iter = 43400----------
Loss = 1.000858
Variable usage = 100.00%
----------Iter = 43500----------
Loss = 1.000840
Variable usage = 100.00%
----------Iter = 43600----------
Loss = 1.000823
Variable usage = 100.00%
----------Iter = 43700----------
Loss = 1.000806
Variable usage = 100.00%
----------Iter = 43800----------
Loss = 1.000788
Variable usage = 100.00%
----------Iter = 43900----------
Loss = 1.000771
Variable usage = 100.00%
----------Iter = 44000----------
Loss = 1.000753
Variable usage = 100.00%
----------Iter = 44100----------
Loss = 1.000735
Variable usage = 100.00%
----------Iter = 44200----------
Loss = 1.000717
Variable usage = 100.00%
----------Iter = 44300----------
Loss = 1.000699
Variable usage = 100.00%
----------Iter = 44400----------
Loss = 1.000682
Variable usage = 100.00%
----------Iter = 44500----------
Loss = 1.000665
Variable usage = 100.00%
----------Iter = 44600----------
Loss = 1.000647
Variable usage = 100.00%
----------Iter = 44700----------
Loss = 1.000629
Variable usage = 100.00%
----------Iter = 44800----------
Loss = 1.000611
Variable usage = 100.00%
----------Iter = 44900----------
Loss = 1.000594
Variable usage = 100.00%
----------Iter = 45000----------
Loss = 1.000577
Variable usage = 100.00%
----------Iter = 45100----------
Loss = 1.000560
Variable usage = 100.00%
----------Iter = 45200----------
Loss = 1.000543
Variable usage = 100.00%
----------Iter = 45300----------
Loss = 1.000526
Variable usage = 100.00%
----------Iter = 45400----------
Loss = 1.000508
Variable usage = 100.00%
----------Iter = 45500----------
Loss = 1.000490
Variable usage = 100.00%
----------Iter = 45600----------
Loss = 1.000472
Variable usage = 100.00%
----------Iter = 45700----------
Loss = 1.000453
Variable usage = 100.00%
----------Iter = 45800----------
Loss = 1.000433
Variable usage = 100.00%
----------Iter = 45900----------
Loss = 1.000415
Variable usage = 100.00%
----------Iter = 46000----------
Loss = 1.000396
Variable usage = 100.00%
----------Iter = 46100----------
Loss = 1.000377
Variable usage = 100.00%
----------Iter = 46200----------
Loss = 1.000359
Variable usage = 100.00%
----------Iter = 46300----------
Loss = 1.000341
Variable usage = 100.00%
----------Iter = 46400----------
Loss = 1.000323
Variable usage = 100.00%
----------Iter = 46500----------
Loss = 1.000304
Variable usage = 100.00%
----------Iter = 46600----------
Loss = 1.000286
Variable usage = 100.00%
----------Iter = 46700----------
Loss = 1.000268
Variable usage = 100.00%
----------Iter = 46800----------
Loss = 1.000250
Variable usage = 100.00%
----------Iter = 46900----------
Loss = 1.000233
Variable usage = 100.00%
----------Iter = 47000----------
Loss = 1.000215
Variable usage = 75.00%
----------Iter = 47100----------
Loss = 1.000199
Variable usage = 75.00%
----------Iter = 47200----------
Loss = 1.000184
Variable usage = 75.00%
----------Iter = 47300----------
Loss = 1.000172
Variable usage = 75.00%
----------Iter = 47400----------
Loss = 1.000161
Variable usage = 75.00%
----------Iter = 47500----------
Loss = 1.000149
Variable usage = 75.00%
----------Iter = 47600----------
Loss = 1.000138
Variable usage = 75.00%
----------Iter = 47700----------
Loss = 1.000127
Variable usage = 75.00%
----------Iter = 47800----------
Loss = 1.000116
Variable usage = 75.00%
----------Iter = 47900----------
Loss = 1.000105
Variable usage = 75.00%
----------Iter = 48000----------
Loss = 1.000094
Variable usage = 75.00%
----------Iter = 48100----------
Loss = 1.000084
Variable usage = 75.00%
----------Iter = 48200----------
Loss = 1.000073
Variable usage = 75.00%
----------Iter = 48300----------
Loss = 1.000063
Variable usage = 75.00%
----------Iter = 48400----------
Loss = 1.000053
Variable usage = 75.00%
----------Iter = 48500----------
Loss = 1.000043
Variable usage = 75.00%
----------Iter = 48600----------
Loss = 1.000032
Variable usage = 75.00%
----------Iter = 48700----------
Loss = 1.000022
Variable usage = 75.00%
----------Iter = 48800----------
Loss = 1.000013
Variable usage = 50.00%
----------Iter = 48900----------
Loss = 1.000003
Variable usage = 50.00%
----------Iter = 49000----------
Loss = 0.999996
Variable usage = 50.00%
----------Iter = 49100----------
Loss = 0.999990
Variable usage = 50.00%
----------Iter = 49200----------
Loss = 0.999984
Variable usage = 50.00%
----------Iter = 49300----------
Loss = 0.999978
Variable usage = 50.00%
----------Iter = 49400----------
Loss = 0.999972
Variable usage = 50.00%
----------Iter = 49500----------
Loss = 0.999967
Variable usage = 50.00%
----------Iter = 49600----------
Loss = 0.999961
Variable usage = 50.00%
----------Iter = 49700----------
Loss = 0.999956
Variable usage = 50.00%
----------Iter = 49800----------
Loss = 0.999951
Variable usage = 50.00%
----------Iter = 49900----------
Loss = 0.999945
Variable usage = 50.00%
----------Iter = 50000----------
Loss = 0.999940
Variable usage = 50.00%
True variable usage = 50.00%
Estimated variable usage = 50.00%
Accuracy = 1.00%
Accuracy for experiment id 5 is 1.0
----------Iter = 100----------
Loss = 1.026063
Variable usage = 100.00%
----------Iter = 200----------
Loss = 1.024536
Variable usage = 100.00%
----------Iter = 300----------
Loss = 1.023562
Variable usage = 100.00%
----------Iter = 400----------
Loss = 1.022858
Variable usage = 100.00%
----------Iter = 500----------
Loss = 1.022308
Variable usage = 100.00%
----------Iter = 600----------
Loss = 1.021867
Variable usage = 100.00%
----------Iter = 700----------
Loss = 1.021492
Variable usage = 100.00%
----------Iter = 800----------
Loss = 1.021168
Variable usage = 100.00%
----------Iter = 900----------
Loss = 1.020883
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 1.020626
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 1.020387
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 1.020162
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 1.019947
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 1.019742
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 1.019548
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 1.019358
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 1.019172
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 1.018987
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 1.018810
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 1.018638
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 1.018468
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 1.018301
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 1.018137
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 1.017974
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 1.017813
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 1.017652
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 1.017494
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 1.017336
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 1.017178
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 1.017026
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 1.016878
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 1.016731
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 1.016589
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 1.016450
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 1.016312
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 1.016179
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 1.016049
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 1.015923
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 1.015799
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 1.015677
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 1.015557
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 1.015438
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 1.015321
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 1.015206
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 1.015094
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 1.014982
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 1.014871
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 1.014761
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 1.014652
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 1.014544
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 1.014434
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 1.014326
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 1.014219
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 1.014113
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 1.014010
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 1.013907
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 1.013804
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 1.013700
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 1.013597
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 1.013496
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 1.013394
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 1.013294
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 1.013195
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 1.013098
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 1.013003
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 1.012908
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 1.012814
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 1.012720
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 1.012628
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 1.012537
Variable usage = 100.00%
----------Iter = 7100----------
Loss = 1.012447
Variable usage = 100.00%
----------Iter = 7200----------
Loss = 1.012358
Variable usage = 100.00%
----------Iter = 7300----------
Loss = 1.012268
Variable usage = 100.00%
----------Iter = 7400----------
Loss = 1.012178
Variable usage = 100.00%
----------Iter = 7500----------
Loss = 1.012090
Variable usage = 100.00%
----------Iter = 7600----------
Loss = 1.012003
Variable usage = 100.00%
----------Iter = 7700----------
Loss = 1.011916
Variable usage = 100.00%
----------Iter = 7800----------
Loss = 1.011831
Variable usage = 100.00%
----------Iter = 7900----------
Loss = 1.011746
Variable usage = 100.00%
----------Iter = 8000----------
Loss = 1.011662
Variable usage = 100.00%
----------Iter = 8100----------
Loss = 1.011578
Variable usage = 100.00%
----------Iter = 8200----------
Loss = 1.011494
Variable usage = 100.00%
----------Iter = 8300----------
Loss = 1.011411
Variable usage = 100.00%
----------Iter = 8400----------
Loss = 1.011327
Variable usage = 100.00%
----------Iter = 8500----------
Loss = 1.011245
Variable usage = 100.00%
----------Iter = 8600----------
Loss = 1.011163
Variable usage = 100.00%
----------Iter = 8700----------
Loss = 1.011081
Variable usage = 100.00%
----------Iter = 8800----------
Loss = 1.011001
Variable usage = 100.00%
----------Iter = 8900----------
Loss = 1.010921
Variable usage = 100.00%
----------Iter = 9000----------
Loss = 1.010843
Variable usage = 100.00%
----------Iter = 9100----------
Loss = 1.010766
Variable usage = 100.00%
----------Iter = 9200----------
Loss = 1.010690
Variable usage = 100.00%
----------Iter = 9300----------
Loss = 1.010614
Variable usage = 100.00%
----------Iter = 9400----------
Loss = 1.010540
Variable usage = 100.00%
----------Iter = 9500----------
Loss = 1.010467
Variable usage = 100.00%
----------Iter = 9600----------
Loss = 1.010395
Variable usage = 100.00%
----------Iter = 9700----------
Loss = 1.010324
Variable usage = 100.00%
----------Iter = 9800----------
Loss = 1.010252
Variable usage = 100.00%
----------Iter = 9900----------
Loss = 1.010182
Variable usage = 100.00%
----------Iter = 10000----------
Loss = 1.010112
Variable usage = 100.00%
----------Iter = 10100----------
Loss = 1.010044
Variable usage = 100.00%
----------Iter = 10200----------
Loss = 1.009976
Variable usage = 100.00%
----------Iter = 10300----------
Loss = 1.009908
Variable usage = 100.00%
----------Iter = 10400----------
Loss = 1.009840
Variable usage = 100.00%
----------Iter = 10500----------
Loss = 1.009772
Variable usage = 100.00%
----------Iter = 10600----------
Loss = 1.009704
Variable usage = 100.00%
----------Iter = 10700----------
Loss = 1.009636
Variable usage = 100.00%
----------Iter = 10800----------
Loss = 1.009569
Variable usage = 100.00%
----------Iter = 10900----------
Loss = 1.009503
Variable usage = 100.00%
----------Iter = 11000----------
Loss = 1.009437
Variable usage = 100.00%
----------Iter = 11100----------
Loss = 1.009371
Variable usage = 100.00%
----------Iter = 11200----------
Loss = 1.009305
Variable usage = 100.00%
----------Iter = 11300----------
Loss = 1.009240
Variable usage = 100.00%
----------Iter = 11400----------
Loss = 1.009175
Variable usage = 100.00%
----------Iter = 11500----------
Loss = 1.009111
Variable usage = 100.00%
----------Iter = 11600----------
Loss = 1.009048
Variable usage = 100.00%
----------Iter = 11700----------
Loss = 1.008985
Variable usage = 100.00%
----------Iter = 11800----------
Loss = 1.008923
Variable usage = 100.00%
----------Iter = 11900----------
Loss = 1.008860
Variable usage = 100.00%
----------Iter = 12000----------
Loss = 1.008797
Variable usage = 100.00%
----------Iter = 12100----------
Loss = 1.008735
Variable usage = 100.00%
----------Iter = 12200----------
Loss = 1.008673
Variable usage = 100.00%
----------Iter = 12300----------
Loss = 1.008613
Variable usage = 100.00%
----------Iter = 12400----------
Loss = 1.008554
Variable usage = 100.00%
----------Iter = 12500----------
Loss = 1.008494
Variable usage = 100.00%
----------Iter = 12600----------
Loss = 1.008436
Variable usage = 100.00%
----------Iter = 12700----------
Loss = 1.008378
Variable usage = 100.00%
----------Iter = 12800----------
Loss = 1.008321
Variable usage = 100.00%
----------Iter = 12900----------
Loss = 1.008264
Variable usage = 100.00%
----------Iter = 13000----------
Loss = 1.008207
Variable usage = 100.00%
----------Iter = 13100----------
Loss = 1.008149
Variable usage = 100.00%
----------Iter = 13200----------
Loss = 1.008093
Variable usage = 100.00%
----------Iter = 13300----------
Loss = 1.008037
Variable usage = 100.00%
----------Iter = 13400----------
Loss = 1.007981
Variable usage = 100.00%
----------Iter = 13500----------
Loss = 1.007927
Variable usage = 100.00%
----------Iter = 13600----------
Loss = 1.007873
Variable usage = 100.00%
----------Iter = 13700----------
Loss = 1.007819
Variable usage = 100.00%
----------Iter = 13800----------
Loss = 1.007767
Variable usage = 100.00%
----------Iter = 13900----------
Loss = 1.007715
Variable usage = 100.00%
----------Iter = 14000----------
Loss = 1.007663
Variable usage = 100.00%
----------Iter = 14100----------
Loss = 1.007611
Variable usage = 100.00%
----------Iter = 14200----------
Loss = 1.007561
Variable usage = 100.00%
----------Iter = 14300----------
Loss = 1.007511
Variable usage = 100.00%
----------Iter = 14400----------
Loss = 1.007462
Variable usage = 100.00%
----------Iter = 14500----------
Loss = 1.007413
Variable usage = 100.00%
----------Iter = 14600----------
Loss = 1.007366
Variable usage = 100.00%
----------Iter = 14700----------
Loss = 1.007319
Variable usage = 100.00%
----------Iter = 14800----------
Loss = 1.007272
Variable usage = 100.00%
----------Iter = 14900----------
Loss = 1.007225
Variable usage = 100.00%
----------Iter = 15000----------
Loss = 1.007179
Variable usage = 100.00%
----------Iter = 15100----------
Loss = 1.007133
Variable usage = 100.00%
----------Iter = 15200----------
Loss = 1.007088
Variable usage = 100.00%
----------Iter = 15300----------
Loss = 1.007044
Variable usage = 100.00%
----------Iter = 15400----------
Loss = 1.006999
Variable usage = 100.00%
----------Iter = 15500----------
Loss = 1.006955
Variable usage = 100.00%
----------Iter = 15600----------
Loss = 1.006911
Variable usage = 100.00%
----------Iter = 15700----------
Loss = 1.006868
Variable usage = 100.00%
----------Iter = 15800----------
Loss = 1.006823
Variable usage = 100.00%
----------Iter = 15900----------
Loss = 1.006779
Variable usage = 100.00%
----------Iter = 16000----------
Loss = 1.006735
Variable usage = 100.00%
----------Iter = 16100----------
Loss = 1.006691
Variable usage = 100.00%
----------Iter = 16200----------
Loss = 1.006647
Variable usage = 100.00%
----------Iter = 16300----------
Loss = 1.006603
Variable usage = 100.00%
----------Iter = 16400----------
Loss = 1.006561
Variable usage = 100.00%
----------Iter = 16500----------
Loss = 1.006517
Variable usage = 100.00%
----------Iter = 16600----------
Loss = 1.006475
Variable usage = 100.00%
----------Iter = 16700----------
Loss = 1.006434
Variable usage = 100.00%
----------Iter = 16800----------
Loss = 1.006393
Variable usage = 100.00%
----------Iter = 16900----------
Loss = 1.006353
Variable usage = 100.00%
----------Iter = 17000----------
Loss = 1.006312
Variable usage = 100.00%
----------Iter = 17100----------
Loss = 1.006272
Variable usage = 100.00%
----------Iter = 17200----------
Loss = 1.006232
Variable usage = 100.00%
----------Iter = 17300----------
Loss = 1.006193
Variable usage = 100.00%
----------Iter = 17400----------
Loss = 1.006153
Variable usage = 100.00%
----------Iter = 17500----------
Loss = 1.006114
Variable usage = 100.00%
----------Iter = 17600----------
Loss = 1.006074
Variable usage = 100.00%
----------Iter = 17700----------
Loss = 1.006035
Variable usage = 100.00%
----------Iter = 17800----------
Loss = 1.005996
Variable usage = 100.00%
----------Iter = 17900----------
Loss = 1.005958
Variable usage = 100.00%
----------Iter = 18000----------
Loss = 1.005920
Variable usage = 100.00%
----------Iter = 18100----------
Loss = 1.005883
Variable usage = 100.00%
----------Iter = 18200----------
Loss = 1.005846
Variable usage = 100.00%
----------Iter = 18300----------
Loss = 1.005809
Variable usage = 100.00%
----------Iter = 18400----------
Loss = 1.005772
Variable usage = 100.00%
----------Iter = 18500----------
Loss = 1.005736
Variable usage = 100.00%
----------Iter = 18600----------
Loss = 1.005699
Variable usage = 100.00%
----------Iter = 18700----------
Loss = 1.005663
Variable usage = 100.00%
----------Iter = 18800----------
Loss = 1.005627
Variable usage = 100.00%
----------Iter = 18900----------
Loss = 1.005591
Variable usage = 100.00%
----------Iter = 19000----------
Loss = 1.005556
Variable usage = 100.00%
----------Iter = 19100----------
Loss = 1.005520
Variable usage = 100.00%
----------Iter = 19200----------
Loss = 1.005484
Variable usage = 100.00%
----------Iter = 19300----------
Loss = 1.005449
Variable usage = 100.00%
----------Iter = 19400----------
Loss = 1.005414
Variable usage = 100.00%
----------Iter = 19500----------
Loss = 1.005379
Variable usage = 100.00%
----------Iter = 19600----------
Loss = 1.005344
Variable usage = 100.00%
----------Iter = 19700----------
Loss = 1.005309
Variable usage = 100.00%
----------Iter = 19800----------
Loss = 1.005275
Variable usage = 100.00%
----------Iter = 19900----------
Loss = 1.005242
Variable usage = 100.00%
----------Iter = 20000----------
Loss = 1.005207
Variable usage = 100.00%
----------Iter = 20100----------
Loss = 1.005173
Variable usage = 100.00%
----------Iter = 20200----------
Loss = 1.005139
Variable usage = 100.00%
----------Iter = 20300----------
Loss = 1.005105
Variable usage = 100.00%
----------Iter = 20400----------
Loss = 1.005070
Variable usage = 100.00%
----------Iter = 20500----------
Loss = 1.005036
Variable usage = 100.00%
----------Iter = 20600----------
Loss = 1.005002
Variable usage = 100.00%
----------Iter = 20700----------
Loss = 1.004968
Variable usage = 100.00%
----------Iter = 20800----------
Loss = 1.004933
Variable usage = 100.00%
----------Iter = 20900----------
Loss = 1.004899
Variable usage = 100.00%
----------Iter = 21000----------
Loss = 1.004865
Variable usage = 100.00%
----------Iter = 21100----------
Loss = 1.004830
Variable usage = 100.00%
----------Iter = 21200----------
Loss = 1.004794
Variable usage = 100.00%
----------Iter = 21300----------
Loss = 1.004758
Variable usage = 100.00%
----------Iter = 21400----------
Loss = 1.004723
Variable usage = 100.00%
----------Iter = 21500----------
Loss = 1.004687
Variable usage = 100.00%
----------Iter = 21600----------
Loss = 1.004652
Variable usage = 100.00%
----------Iter = 21700----------
Loss = 1.004616
Variable usage = 100.00%
----------Iter = 21800----------
Loss = 1.004581
Variable usage = 100.00%
----------Iter = 21900----------
Loss = 1.004546
Variable usage = 100.00%
----------Iter = 22000----------
Loss = 1.004510
Variable usage = 100.00%
----------Iter = 22100----------
Loss = 1.004476
Variable usage = 100.00%
----------Iter = 22200----------
Loss = 1.004443
Variable usage = 100.00%
----------Iter = 22300----------
Loss = 1.004410
Variable usage = 100.00%
----------Iter = 22400----------
Loss = 1.004378
Variable usage = 100.00%
----------Iter = 22500----------
Loss = 1.004346
Variable usage = 100.00%
----------Iter = 22600----------
Loss = 1.004314
Variable usage = 100.00%
----------Iter = 22700----------
Loss = 1.004282
Variable usage = 100.00%
----------Iter = 22800----------
Loss = 1.004250
Variable usage = 100.00%
----------Iter = 22900----------
Loss = 1.004219
Variable usage = 100.00%
----------Iter = 23000----------
Loss = 1.004189
Variable usage = 100.00%
----------Iter = 23100----------
Loss = 1.004158
Variable usage = 100.00%
----------Iter = 23200----------
Loss = 1.004128
Variable usage = 100.00%
----------Iter = 23300----------
Loss = 1.004098
Variable usage = 100.00%
----------Iter = 23400----------
Loss = 1.004068
Variable usage = 100.00%
----------Iter = 23500----------
Loss = 1.004037
Variable usage = 100.00%
----------Iter = 23600----------
Loss = 1.004007
Variable usage = 100.00%
----------Iter = 23700----------
Loss = 1.003978
Variable usage = 100.00%
----------Iter = 23800----------
Loss = 1.003948
Variable usage = 100.00%
----------Iter = 23900----------
Loss = 1.003919
Variable usage = 100.00%
----------Iter = 24000----------
Loss = 1.003890
Variable usage = 100.00%
----------Iter = 24100----------
Loss = 1.003860
Variable usage = 100.00%
----------Iter = 24200----------
Loss = 1.003831
Variable usage = 100.00%
----------Iter = 24300----------
Loss = 1.003801
Variable usage = 100.00%
----------Iter = 24400----------
Loss = 1.003771
Variable usage = 100.00%
----------Iter = 24500----------
Loss = 1.003742
Variable usage = 100.00%
----------Iter = 24600----------
Loss = 1.003711
Variable usage = 100.00%
----------Iter = 24700----------
Loss = 1.003681
Variable usage = 100.00%
----------Iter = 24800----------
Loss = 1.003652
Variable usage = 100.00%
----------Iter = 24900----------
Loss = 1.003623
Variable usage = 100.00%
----------Iter = 25000----------
Loss = 1.003594
Variable usage = 100.00%
----------Iter = 25100----------
Loss = 1.003566
Variable usage = 100.00%
----------Iter = 25200----------
Loss = 1.003538
Variable usage = 100.00%
----------Iter = 25300----------
Loss = 1.003510
Variable usage = 100.00%
----------Iter = 25400----------
Loss = 1.003482
Variable usage = 100.00%
----------Iter = 25500----------
Loss = 1.003455
Variable usage = 100.00%
----------Iter = 25600----------
Loss = 1.003427
Variable usage = 100.00%
----------Iter = 25700----------
Loss = 1.003400
Variable usage = 100.00%
----------Iter = 25800----------
Loss = 1.003374
Variable usage = 100.00%
----------Iter = 25900----------
Loss = 1.003347
Variable usage = 100.00%
----------Iter = 26000----------
Loss = 1.003321
Variable usage = 100.00%
----------Iter = 26100----------
Loss = 1.003295
Variable usage = 100.00%
----------Iter = 26200----------
Loss = 1.003268
Variable usage = 100.00%
----------Iter = 26300----------
Loss = 1.003242
Variable usage = 100.00%
----------Iter = 26400----------
Loss = 1.003216
Variable usage = 100.00%
----------Iter = 26500----------
Loss = 1.003190
Variable usage = 100.00%
----------Iter = 26600----------
Loss = 1.003165
Variable usage = 100.00%
----------Iter = 26700----------
Loss = 1.003139
Variable usage = 100.00%
----------Iter = 26800----------
Loss = 1.003113
Variable usage = 100.00%
----------Iter = 26900----------
Loss = 1.003086
Variable usage = 100.00%
----------Iter = 27000----------
Loss = 1.003060
Variable usage = 100.00%
----------Iter = 27100----------
Loss = 1.003034
Variable usage = 100.00%
----------Iter = 27200----------
Loss = 1.003008
Variable usage = 100.00%
----------Iter = 27300----------
Loss = 1.002982
Variable usage = 100.00%
----------Iter = 27400----------
Loss = 1.002956
Variable usage = 100.00%
----------Iter = 27500----------
Loss = 1.002931
Variable usage = 100.00%
----------Iter = 27600----------
Loss = 1.002905
Variable usage = 100.00%
----------Iter = 27700----------
Loss = 1.002880
Variable usage = 100.00%
----------Iter = 27800----------
Loss = 1.002855
Variable usage = 100.00%
----------Iter = 27900----------
Loss = 1.002830
Variable usage = 100.00%
----------Iter = 28000----------
Loss = 1.002805
Variable usage = 100.00%
----------Iter = 28100----------
Loss = 1.002781
Variable usage = 100.00%
----------Iter = 28200----------
Loss = 1.002756
Variable usage = 100.00%
----------Iter = 28300----------
Loss = 1.002731
Variable usage = 100.00%
----------Iter = 28400----------
Loss = 1.002707
Variable usage = 100.00%
----------Iter = 28500----------
Loss = 1.002681
Variable usage = 100.00%
----------Iter = 28600----------
Loss = 1.002656
Variable usage = 100.00%
----------Iter = 28700----------
Loss = 1.002631
Variable usage = 100.00%
----------Iter = 28800----------
Loss = 1.002606
Variable usage = 100.00%
----------Iter = 28900----------
Loss = 1.002581
Variable usage = 100.00%
----------Iter = 29000----------
Loss = 1.002556
Variable usage = 100.00%
----------Iter = 29100----------
Loss = 1.002530
Variable usage = 100.00%
----------Iter = 29200----------
Loss = 1.002504
Variable usage = 100.00%
----------Iter = 29300----------
Loss = 1.002478
Variable usage = 100.00%
----------Iter = 29400----------
Loss = 1.002452
Variable usage = 100.00%
----------Iter = 29500----------
Loss = 1.002426
Variable usage = 100.00%
----------Iter = 29600----------
Loss = 1.002401
Variable usage = 100.00%
----------Iter = 29700----------
Loss = 1.002377
Variable usage = 100.00%
----------Iter = 29800----------
Loss = 1.002353
Variable usage = 100.00%
----------Iter = 29900----------
Loss = 1.002329
Variable usage = 100.00%
----------Iter = 30000----------
Loss = 1.002305
Variable usage = 100.00%
----------Iter = 30100----------
Loss = 1.002281
Variable usage = 100.00%
----------Iter = 30200----------
Loss = 1.002258
Variable usage = 100.00%
----------Iter = 30300----------
Loss = 1.002234
Variable usage = 100.00%
----------Iter = 30400----------
Loss = 1.002211
Variable usage = 100.00%
----------Iter = 30500----------
Loss = 1.002189
Variable usage = 100.00%
----------Iter = 30600----------
Loss = 1.002166
Variable usage = 100.00%
----------Iter = 30700----------
Loss = 1.002143
Variable usage = 100.00%
----------Iter = 30800----------
Loss = 1.002120
Variable usage = 100.00%
----------Iter = 30900----------
Loss = 1.002097
Variable usage = 100.00%
----------Iter = 31000----------
Loss = 1.002074
Variable usage = 100.00%
----------Iter = 31100----------
Loss = 1.002052
Variable usage = 100.00%
----------Iter = 31200----------
Loss = 1.002030
Variable usage = 100.00%
----------Iter = 31300----------
Loss = 1.002008
Variable usage = 100.00%
----------Iter = 31400----------
Loss = 1.001986
Variable usage = 100.00%
----------Iter = 31500----------
Loss = 1.001964
Variable usage = 100.00%
----------Iter = 31600----------
Loss = 1.001942
Variable usage = 100.00%
----------Iter = 31700----------
Loss = 1.001920
Variable usage = 100.00%
----------Iter = 31800----------
Loss = 1.001898
Variable usage = 100.00%
----------Iter = 31900----------
Loss = 1.001877
Variable usage = 100.00%
----------Iter = 32000----------
Loss = 1.001855
Variable usage = 100.00%
----------Iter = 32100----------
Loss = 1.001833
Variable usage = 100.00%
----------Iter = 32200----------
Loss = 1.001812
Variable usage = 100.00%
----------Iter = 32300----------
Loss = 1.001791
Variable usage = 100.00%
----------Iter = 32400----------
Loss = 1.001770
Variable usage = 100.00%
----------Iter = 32500----------
Loss = 1.001749
Variable usage = 100.00%
----------Iter = 32600----------
Loss = 1.001728
Variable usage = 100.00%
----------Iter = 32700----------
Loss = 1.001707
Variable usage = 100.00%
----------Iter = 32800----------
Loss = 1.001686
Variable usage = 100.00%
----------Iter = 32900----------
Loss = 1.001665
Variable usage = 100.00%
----------Iter = 33000----------
Loss = 1.001644
Variable usage = 100.00%
----------Iter = 33100----------
Loss = 1.001622
Variable usage = 100.00%
----------Iter = 33200----------
Loss = 1.001601
Variable usage = 100.00%
----------Iter = 33300----------
Loss = 1.001580
Variable usage = 100.00%
----------Iter = 33400----------
Loss = 1.001559
Variable usage = 100.00%
----------Iter = 33500----------
Loss = 1.001538
Variable usage = 100.00%
----------Iter = 33600----------
Loss = 1.001516
Variable usage = 100.00%
----------Iter = 33700----------
Loss = 1.001495
Variable usage = 100.00%
----------Iter = 33800----------
Loss = 1.001474
Variable usage = 100.00%
----------Iter = 33900----------
Loss = 1.001453
Variable usage = 100.00%
----------Iter = 34000----------
Loss = 1.001432
Variable usage = 100.00%
----------Iter = 34100----------
Loss = 1.001411
Variable usage = 100.00%
----------Iter = 34200----------
Loss = 1.001390
Variable usage = 100.00%
----------Iter = 34300----------
Loss = 1.001370
Variable usage = 100.00%
----------Iter = 34400----------
Loss = 1.001349
Variable usage = 100.00%
----------Iter = 34500----------
Loss = 1.001329
Variable usage = 100.00%
----------Iter = 34600----------
Loss = 1.001309
Variable usage = 100.00%
----------Iter = 34700----------
Loss = 1.001289
Variable usage = 100.00%
----------Iter = 34800----------
Loss = 1.001269
Variable usage = 100.00%
----------Iter = 34900----------
Loss = 1.001249
Variable usage = 100.00%
----------Iter = 35000----------
Loss = 1.001229
Variable usage = 100.00%
----------Iter = 35100----------
Loss = 1.001210
Variable usage = 100.00%
----------Iter = 35200----------
Loss = 1.001190
Variable usage = 100.00%
----------Iter = 35300----------
Loss = 1.001171
Variable usage = 100.00%
----------Iter = 35400----------
Loss = 1.001152
Variable usage = 100.00%
----------Iter = 35500----------
Loss = 1.001132
Variable usage = 100.00%
----------Iter = 35600----------
Loss = 1.001114
Variable usage = 100.00%
----------Iter = 35700----------
Loss = 1.001095
Variable usage = 100.00%
----------Iter = 35800----------
Loss = 1.001076
Variable usage = 100.00%
----------Iter = 35900----------
Loss = 1.001057
Variable usage = 100.00%
----------Iter = 36000----------
Loss = 1.001038
Variable usage = 100.00%
----------Iter = 36100----------
Loss = 1.001020
Variable usage = 100.00%
----------Iter = 36200----------
Loss = 1.001001
Variable usage = 100.00%
----------Iter = 36300----------
Loss = 1.000982
Variable usage = 100.00%
----------Iter = 36400----------
Loss = 1.000963
Variable usage = 100.00%
----------Iter = 36500----------
Loss = 1.000945
Variable usage = 100.00%
----------Iter = 36600----------
Loss = 1.000926
Variable usage = 100.00%
----------Iter = 36700----------
Loss = 1.000907
Variable usage = 100.00%
----------Iter = 36800----------
Loss = 1.000889
Variable usage = 100.00%
----------Iter = 36900----------
Loss = 1.000870
Variable usage = 100.00%
----------Iter = 37000----------
Loss = 1.000851
Variable usage = 100.00%
----------Iter = 37100----------
Loss = 1.000833
Variable usage = 100.00%
----------Iter = 37200----------
Loss = 1.000814
Variable usage = 100.00%
----------Iter = 37300----------
Loss = 1.000796
Variable usage = 100.00%
----------Iter = 37400----------
Loss = 1.000777
Variable usage = 100.00%
----------Iter = 37500----------
Loss = 1.000758
Variable usage = 100.00%
----------Iter = 37600----------
Loss = 1.000739
Variable usage = 100.00%
----------Iter = 37700----------
Loss = 1.000720
Variable usage = 100.00%
----------Iter = 37800----------
Loss = 1.000701
Variable usage = 100.00%
----------Iter = 37900----------
Loss = 1.000682
Variable usage = 100.00%
----------Iter = 38000----------
Loss = 1.000663
Variable usage = 100.00%
----------Iter = 38100----------
Loss = 1.000644
Variable usage = 100.00%
----------Iter = 38200----------
Loss = 1.000625
Variable usage = 100.00%
----------Iter = 38300----------
Loss = 1.000606
Variable usage = 100.00%
----------Iter = 38400----------
Loss = 1.000587
Variable usage = 100.00%
----------Iter = 38500----------
Loss = 1.000569
Variable usage = 100.00%
----------Iter = 38600----------
Loss = 1.000550
Variable usage = 100.00%
----------Iter = 38700----------
Loss = 1.000531
Variable usage = 100.00%
----------Iter = 38800----------
Loss = 1.000513
Variable usage = 100.00%
----------Iter = 38900----------
Loss = 1.000495
Variable usage = 100.00%
----------Iter = 39000----------
Loss = 1.000477
Variable usage = 100.00%
----------Iter = 39100----------
Loss = 1.000459
Variable usage = 100.00%
----------Iter = 39200----------
Loss = 1.000441
Variable usage = 100.00%
----------Iter = 39300----------
Loss = 1.000423
Variable usage = 100.00%
----------Iter = 39400----------
Loss = 1.000406
Variable usage = 100.00%
----------Iter = 39500----------
Loss = 1.000388
Variable usage = 100.00%
----------Iter = 39600----------
Loss = 1.000370
Variable usage = 100.00%
----------Iter = 39700----------
Loss = 1.000353
Variable usage = 100.00%
----------Iter = 39800----------
Loss = 1.000335
Variable usage = 100.00%
----------Iter = 39900----------
Loss = 1.000317
Variable usage = 100.00%
----------Iter = 40000----------
Loss = 1.000300
Variable usage = 100.00%
----------Iter = 40100----------
Loss = 1.000282
Variable usage = 100.00%
----------Iter = 40200----------
Loss = 1.000264
Variable usage = 100.00%
----------Iter = 40300----------
Loss = 1.000245
Variable usage = 100.00%
----------Iter = 40400----------
Loss = 1.000227
Variable usage = 100.00%
----------Iter = 40500----------
Loss = 1.000210
Variable usage = 100.00%
----------Iter = 40600----------
Loss = 1.000192
Variable usage = 100.00%
----------Iter = 40700----------
Loss = 1.000174
Variable usage = 100.00%
----------Iter = 40800----------
Loss = 1.000157
Variable usage = 100.00%
----------Iter = 40900----------
Loss = 1.000140
Variable usage = 100.00%
----------Iter = 41000----------
Loss = 1.000123
Variable usage = 100.00%
----------Iter = 41100----------
Loss = 1.000106
Variable usage = 100.00%
----------Iter = 41200----------
Loss = 1.000089
Variable usage = 100.00%
----------Iter = 41300----------
Loss = 1.000072
Variable usage = 100.00%
----------Iter = 41400----------
Loss = 1.000054
Variable usage = 100.00%
----------Iter = 41500----------
Loss = 1.000037
Variable usage = 100.00%
----------Iter = 41600----------
Loss = 1.000020
Variable usage = 100.00%
----------Iter = 41700----------
Loss = 1.000003
Variable usage = 100.00%
----------Iter = 41800----------
Loss = 0.999986
Variable usage = 100.00%
----------Iter = 41900----------
Loss = 0.999969
Variable usage = 100.00%
----------Iter = 42000----------
Loss = 0.999951
Variable usage = 100.00%
----------Iter = 42100----------
Loss = 0.999934
Variable usage = 100.00%
----------Iter = 42200----------
Loss = 0.999916
Variable usage = 100.00%
----------Iter = 42300----------
Loss = 0.999899
Variable usage = 100.00%
----------Iter = 42400----------
Loss = 0.999881
Variable usage = 100.00%
----------Iter = 42500----------
Loss = 0.999864
Variable usage = 100.00%
----------Iter = 42600----------
Loss = 0.999846
Variable usage = 100.00%
----------Iter = 42700----------
Loss = 0.999828
Variable usage = 100.00%
----------Iter = 42800----------
Loss = 0.999811
Variable usage = 100.00%
----------Iter = 42900----------
Loss = 0.999794
Variable usage = 100.00%
----------Iter = 43000----------
Loss = 0.999776
Variable usage = 100.00%
----------Iter = 43100----------
Loss = 0.999759
Variable usage = 100.00%
----------Iter = 43200----------
Loss = 0.999743
Variable usage = 100.00%
----------Iter = 43300----------
Loss = 0.999726
Variable usage = 100.00%
----------Iter = 43400----------
Loss = 0.999709
Variable usage = 100.00%
----------Iter = 43500----------
Loss = 0.999692
Variable usage = 100.00%
----------Iter = 43600----------
Loss = 0.999676
Variable usage = 100.00%
----------Iter = 43700----------
Loss = 0.999659
Variable usage = 100.00%
----------Iter = 43800----------
Loss = 0.999642
Variable usage = 100.00%
----------Iter = 43900----------
Loss = 0.999624
Variable usage = 100.00%
----------Iter = 44000----------
Loss = 0.999607
Variable usage = 100.00%
----------Iter = 44100----------
Loss = 0.999590
Variable usage = 100.00%
----------Iter = 44200----------
Loss = 0.999573
Variable usage = 100.00%
----------Iter = 44300----------
Loss = 0.999556
Variable usage = 100.00%
----------Iter = 44400----------
Loss = 0.999539
Variable usage = 100.00%
----------Iter = 44500----------
Loss = 0.999523
Variable usage = 100.00%
----------Iter = 44600----------
Loss = 0.999507
Variable usage = 100.00%
----------Iter = 44700----------
Loss = 0.999491
Variable usage = 100.00%
----------Iter = 44800----------
Loss = 0.999474
Variable usage = 100.00%
----------Iter = 44900----------
Loss = 0.999458
Variable usage = 100.00%
----------Iter = 45000----------
Loss = 0.999442
Variable usage = 100.00%
----------Iter = 45100----------
Loss = 0.999426
Variable usage = 100.00%
----------Iter = 45200----------
Loss = 0.999409
Variable usage = 100.00%
----------Iter = 45300----------
Loss = 0.999393
Variable usage = 100.00%
----------Iter = 45400----------
Loss = 0.999377
Variable usage = 100.00%
----------Iter = 45500----------
Loss = 0.999359
Variable usage = 100.00%
----------Iter = 45600----------
Loss = 0.999343
Variable usage = 100.00%
----------Iter = 45700----------
Loss = 0.999327
Variable usage = 100.00%
----------Iter = 45800----------
Loss = 0.999311
Variable usage = 100.00%
----------Iter = 45900----------
Loss = 0.999295
Variable usage = 100.00%
----------Iter = 46000----------
Loss = 0.999278
Variable usage = 100.00%
----------Iter = 46100----------
Loss = 0.999261
Variable usage = 100.00%
----------Iter = 46200----------
Loss = 0.999244
Variable usage = 100.00%
----------Iter = 46300----------
Loss = 0.999226
Variable usage = 100.00%
----------Iter = 46400----------
Loss = 0.999209
Variable usage = 100.00%
----------Iter = 46500----------
Loss = 0.999191
Variable usage = 100.00%
----------Iter = 46600----------
Loss = 0.999174
Variable usage = 100.00%
----------Iter = 46700----------
Loss = 0.999157
Variable usage = 100.00%
----------Iter = 46800----------
Loss = 0.999141
Variable usage = 100.00%
----------Iter = 46900----------
Loss = 0.999124
Variable usage = 100.00%
----------Iter = 47000----------
Loss = 0.999107
Variable usage = 100.00%
----------Iter = 47100----------
Loss = 0.999089
Variable usage = 100.00%
----------Iter = 47200----------
Loss = 0.999070
Variable usage = 100.00%
----------Iter = 47300----------
Loss = 0.999051
Variable usage = 100.00%
----------Iter = 47400----------
Loss = 0.999033
Variable usage = 100.00%
----------Iter = 47500----------
Loss = 0.999015
Variable usage = 100.00%
----------Iter = 47600----------
Loss = 0.998997
Variable usage = 100.00%
----------Iter = 47700----------
Loss = 0.998980
Variable usage = 100.00%
----------Iter = 47800----------
Loss = 0.998962
Variable usage = 100.00%
----------Iter = 47900----------
Loss = 0.998944
Variable usage = 100.00%
----------Iter = 48000----------
Loss = 0.998926
Variable usage = 100.00%
----------Iter = 48100----------
Loss = 0.998907
Variable usage = 100.00%
----------Iter = 48200----------
Loss = 0.998886
Variable usage = 100.00%
----------Iter = 48300----------
Loss = 0.998865
Variable usage = 100.00%
----------Iter = 48400----------
Loss = 0.998845
Variable usage = 100.00%
----------Iter = 48500----------
Loss = 0.998825
Variable usage = 100.00%
----------Iter = 48600----------
Loss = 0.998805
Variable usage = 100.00%
----------Iter = 48700----------
Loss = 0.998785
Variable usage = 100.00%
----------Iter = 48800----------
Loss = 0.998766
Variable usage = 100.00%
----------Iter = 48900----------
Loss = 0.998746
Variable usage = 100.00%
----------Iter = 49000----------
Loss = 0.998726
Variable usage = 100.00%
----------Iter = 49100----------
Loss = 0.998707
Variable usage = 100.00%
----------Iter = 49200----------
Loss = 0.998688
Variable usage = 100.00%
----------Iter = 49300----------
Loss = 0.998669
Variable usage = 100.00%
----------Iter = 49400----------
Loss = 0.998652
Variable usage = 100.00%
----------Iter = 49500----------
Loss = 0.998635
Variable usage = 100.00%
----------Iter = 49600----------
Loss = 0.998618
Variable usage = 100.00%
----------Iter = 49700----------
Loss = 0.998601
Variable usage = 100.00%
----------Iter = 49800----------
Loss = 0.998586
Variable usage = 100.00%
----------Iter = 49900----------
Loss = 0.998571
Variable usage = 100.00%
----------Iter = 50000----------
Loss = 0.998556
Variable usage = 100.00%
True variable usage = 50.00%
Estimated variable usage = 100.00%
Accuracy = 0.50%
Accuracy for experiment id 6 is 0.5
----------Iter = 100----------
Loss = 1.033912
Variable usage = 100.00%
----------Iter = 200----------
Loss = 1.031998
Variable usage = 100.00%
----------Iter = 300----------
Loss = 1.030787
Variable usage = 100.00%
----------Iter = 400----------
Loss = 1.029931
Variable usage = 100.00%
----------Iter = 500----------
Loss = 1.029282
Variable usage = 100.00%
----------Iter = 600----------
Loss = 1.028768
Variable usage = 100.00%
----------Iter = 700----------
Loss = 1.028347
Variable usage = 100.00%
----------Iter = 800----------
Loss = 1.027990
Variable usage = 100.00%
----------Iter = 900----------
Loss = 1.027681
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 1.027407
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 1.027158
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 1.026929
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 1.026714
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 1.026513
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 1.026320
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 1.026134
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 1.025953
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 1.025777
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 1.025608
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 1.025440
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 1.025274
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 1.025114
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 1.024959
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 1.024803
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 1.024650
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 1.024502
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 1.024357
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 1.024211
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 1.024068
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 1.023925
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 1.023781
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 1.023637
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 1.023497
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 1.023358
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 1.023223
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 1.023086
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 1.022951
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 1.022818
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 1.022689
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 1.022564
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 1.022440
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 1.022314
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 1.022191
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 1.022074
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 1.021960
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 1.021848
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 1.021737
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 1.021628
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 1.021519
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 1.021410
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 1.021303
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 1.021198
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 1.021094
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 1.020991
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 1.020890
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 1.020790
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 1.020693
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 1.020597
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 1.020503
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 1.020411
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 1.020321
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 1.020232
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 1.020145
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 1.020057
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 1.019970
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 1.019883
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 1.019797
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 1.019713
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 1.019631
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 1.019549
Variable usage = 100.00%
----------Iter = 7100----------
Loss = 1.019467
Variable usage = 100.00%
----------Iter = 7200----------
Loss = 1.019387
Variable usage = 100.00%
----------Iter = 7300----------
Loss = 1.019306
Variable usage = 100.00%
----------Iter = 7400----------
Loss = 1.019226
Variable usage = 100.00%
----------Iter = 7500----------
Loss = 1.019145
Variable usage = 100.00%
----------Iter = 7600----------
Loss = 1.019066
Variable usage = 100.00%
----------Iter = 7700----------
Loss = 1.018986
Variable usage = 100.00%
----------Iter = 7800----------
Loss = 1.018907
Variable usage = 100.00%
----------Iter = 7900----------
Loss = 1.018829
Variable usage = 100.00%
----------Iter = 8000----------
Loss = 1.018754
Variable usage = 100.00%
----------Iter = 8100----------
Loss = 1.018680
Variable usage = 100.00%
----------Iter = 8200----------
Loss = 1.018607
Variable usage = 100.00%
----------Iter = 8300----------
Loss = 1.018536
Variable usage = 100.00%
----------Iter = 8400----------
Loss = 1.018466
Variable usage = 100.00%
----------Iter = 8500----------
Loss = 1.018396
Variable usage = 100.00%
----------Iter = 8600----------
Loss = 1.018327
Variable usage = 100.00%
----------Iter = 8700----------
Loss = 1.018257
Variable usage = 100.00%
----------Iter = 8800----------
Loss = 1.018188
Variable usage = 100.00%
----------Iter = 8900----------
Loss = 1.018121
Variable usage = 100.00%
----------Iter = 9000----------
Loss = 1.018055
Variable usage = 100.00%
----------Iter = 9100----------
Loss = 1.017990
Variable usage = 100.00%
----------Iter = 9200----------
Loss = 1.017923
Variable usage = 100.00%
----------Iter = 9300----------
Loss = 1.017858
Variable usage = 100.00%
----------Iter = 9400----------
Loss = 1.017793
Variable usage = 100.00%
----------Iter = 9500----------
Loss = 1.017727
Variable usage = 100.00%
----------Iter = 9600----------
Loss = 1.017661
Variable usage = 100.00%
----------Iter = 9700----------
Loss = 1.017594
Variable usage = 100.00%
----------Iter = 9800----------
Loss = 1.017527
Variable usage = 100.00%
----------Iter = 9900----------
Loss = 1.017461
Variable usage = 100.00%
----------Iter = 10000----------
Loss = 1.017398
Variable usage = 100.00%
----------Iter = 10100----------
Loss = 1.017334
Variable usage = 100.00%
----------Iter = 10200----------
Loss = 1.017271
Variable usage = 100.00%
----------Iter = 10300----------
Loss = 1.017208
Variable usage = 100.00%
----------Iter = 10400----------
Loss = 1.017146
Variable usage = 100.00%
----------Iter = 10500----------
Loss = 1.017084
Variable usage = 100.00%
----------Iter = 10600----------
Loss = 1.017022
Variable usage = 100.00%
----------Iter = 10700----------
Loss = 1.016960
Variable usage = 100.00%
----------Iter = 10800----------
Loss = 1.016899
Variable usage = 100.00%
----------Iter = 10900----------
Loss = 1.016837
Variable usage = 100.00%
----------Iter = 11000----------
Loss = 1.016776
Variable usage = 100.00%
----------Iter = 11100----------
Loss = 1.016716
Variable usage = 100.00%
----------Iter = 11200----------
Loss = 1.016658
Variable usage = 100.00%
----------Iter = 11300----------
Loss = 1.016600
Variable usage = 100.00%
----------Iter = 11400----------
Loss = 1.016542
Variable usage = 100.00%
----------Iter = 11500----------
Loss = 1.016482
Variable usage = 100.00%
----------Iter = 11600----------
Loss = 1.016423
Variable usage = 100.00%
----------Iter = 11700----------
Loss = 1.016367
Variable usage = 100.00%
----------Iter = 11800----------
Loss = 1.016310
Variable usage = 100.00%
----------Iter = 11900----------
Loss = 1.016255
Variable usage = 100.00%
----------Iter = 12000----------
Loss = 1.016200
Variable usage = 100.00%
----------Iter = 12100----------
Loss = 1.016146
Variable usage = 100.00%
----------Iter = 12200----------
Loss = 1.016093
Variable usage = 100.00%
----------Iter = 12300----------
Loss = 1.016039
Variable usage = 100.00%
----------Iter = 12400----------
Loss = 1.015985
Variable usage = 100.00%
----------Iter = 12500----------
Loss = 1.015931
Variable usage = 100.00%
----------Iter = 12600----------
Loss = 1.015876
Variable usage = 100.00%
----------Iter = 12700----------
Loss = 1.015823
Variable usage = 100.00%
----------Iter = 12800----------
Loss = 1.015770
Variable usage = 100.00%
----------Iter = 12900----------
Loss = 1.015719
Variable usage = 100.00%
----------Iter = 13000----------
Loss = 1.015668
Variable usage = 100.00%
----------Iter = 13100----------
Loss = 1.015618
Variable usage = 100.00%
----------Iter = 13200----------
Loss = 1.015568
Variable usage = 100.00%
----------Iter = 13300----------
Loss = 1.015518
Variable usage = 100.00%
----------Iter = 13400----------
Loss = 1.015468
Variable usage = 100.00%
----------Iter = 13500----------
Loss = 1.015419
Variable usage = 100.00%
----------Iter = 13600----------
Loss = 1.015371
Variable usage = 100.00%
----------Iter = 13700----------
Loss = 1.015323
Variable usage = 100.00%
----------Iter = 13800----------
Loss = 1.015276
Variable usage = 100.00%
----------Iter = 13900----------
Loss = 1.015229
Variable usage = 100.00%
----------Iter = 14000----------
Loss = 1.015182
Variable usage = 100.00%
----------Iter = 14100----------
Loss = 1.015135
Variable usage = 100.00%
----------Iter = 14200----------
Loss = 1.015088
Variable usage = 100.00%
----------Iter = 14300----------
Loss = 1.015042
Variable usage = 100.00%
----------Iter = 14400----------
Loss = 1.014995
Variable usage = 100.00%
----------Iter = 14500----------
Loss = 1.014948
Variable usage = 100.00%
----------Iter = 14600----------
Loss = 1.014903
Variable usage = 100.00%
----------Iter = 14700----------
Loss = 1.014857
Variable usage = 100.00%
----------Iter = 14800----------
Loss = 1.014812
Variable usage = 100.00%
----------Iter = 14900----------
Loss = 1.014767
Variable usage = 100.00%
----------Iter = 15000----------
Loss = 1.014722
Variable usage = 100.00%
----------Iter = 15100----------
Loss = 1.014677
Variable usage = 100.00%
----------Iter = 15200----------
Loss = 1.014632
Variable usage = 100.00%
----------Iter = 15300----------
Loss = 1.014589
Variable usage = 100.00%
----------Iter = 15400----------
Loss = 1.014545
Variable usage = 100.00%
----------Iter = 15500----------
Loss = 1.014501
Variable usage = 100.00%
----------Iter = 15600----------
Loss = 1.014456
Variable usage = 100.00%
----------Iter = 15700----------
Loss = 1.014413
Variable usage = 100.00%
----------Iter = 15800----------
Loss = 1.014370
Variable usage = 100.00%
----------Iter = 15900----------
Loss = 1.014328
Variable usage = 100.00%
----------Iter = 16000----------
Loss = 1.014286
Variable usage = 100.00%
----------Iter = 16100----------
Loss = 1.014245
Variable usage = 100.00%
----------Iter = 16200----------
Loss = 1.014204
Variable usage = 100.00%
----------Iter = 16300----------
Loss = 1.014163
Variable usage = 100.00%
----------Iter = 16400----------
Loss = 1.014122
Variable usage = 100.00%
----------Iter = 16500----------
Loss = 1.014083
Variable usage = 100.00%
----------Iter = 16600----------
Loss = 1.014043
Variable usage = 100.00%
----------Iter = 16700----------
Loss = 1.014004
Variable usage = 100.00%
----------Iter = 16800----------
Loss = 1.013966
Variable usage = 100.00%
----------Iter = 16900----------
Loss = 1.013927
Variable usage = 100.00%
----------Iter = 17000----------
Loss = 1.013888
Variable usage = 100.00%
----------Iter = 17100----------
Loss = 1.013850
Variable usage = 100.00%
----------Iter = 17200----------
Loss = 1.013812
Variable usage = 100.00%
----------Iter = 17300----------
Loss = 1.013774
Variable usage = 100.00%
----------Iter = 17400----------
Loss = 1.013737
Variable usage = 100.00%
----------Iter = 17500----------
Loss = 1.013700
Variable usage = 100.00%
----------Iter = 17600----------
Loss = 1.013663
Variable usage = 100.00%
----------Iter = 17700----------
Loss = 1.013627
Variable usage = 100.00%
----------Iter = 17800----------
Loss = 1.013591
Variable usage = 100.00%
----------Iter = 17900----------
Loss = 1.013555
Variable usage = 100.00%
----------Iter = 18000----------
Loss = 1.013518
Variable usage = 100.00%
----------Iter = 18100----------
Loss = 1.013483
Variable usage = 100.00%
----------Iter = 18200----------
Loss = 1.013447
Variable usage = 100.00%
----------Iter = 18300----------
Loss = 1.013412
Variable usage = 100.00%
----------Iter = 18400----------
Loss = 1.013378
Variable usage = 100.00%
----------Iter = 18500----------
Loss = 1.013343
Variable usage = 100.00%
----------Iter = 18600----------
Loss = 1.013309
Variable usage = 100.00%
----------Iter = 18700----------
Loss = 1.013275
Variable usage = 100.00%
----------Iter = 18800----------
Loss = 1.013241
Variable usage = 100.00%
----------Iter = 18900----------
Loss = 1.013208
Variable usage = 100.00%
----------Iter = 19000----------
Loss = 1.013175
Variable usage = 100.00%
----------Iter = 19100----------
Loss = 1.013142
Variable usage = 100.00%
----------Iter = 19200----------
Loss = 1.013109
Variable usage = 100.00%
----------Iter = 19300----------
Loss = 1.013076
Variable usage = 100.00%
----------Iter = 19400----------
Loss = 1.013044
Variable usage = 100.00%
----------Iter = 19500----------
Loss = 1.013011
Variable usage = 100.00%
----------Iter = 19600----------
Loss = 1.012977
Variable usage = 100.00%
----------Iter = 19700----------
Loss = 1.012943
Variable usage = 100.00%
----------Iter = 19800----------
Loss = 1.012910
Variable usage = 100.00%
----------Iter = 19900----------
Loss = 1.012877
Variable usage = 100.00%
----------Iter = 20000----------
Loss = 1.012845
Variable usage = 100.00%
----------Iter = 20100----------
Loss = 1.012813
Variable usage = 100.00%
----------Iter = 20200----------
Loss = 1.012782
Variable usage = 100.00%
----------Iter = 20300----------
Loss = 1.012751
Variable usage = 100.00%
----------Iter = 20400----------
Loss = 1.012720
Variable usage = 100.00%
----------Iter = 20500----------
Loss = 1.012690
Variable usage = 100.00%
----------Iter = 20600----------
Loss = 1.012660
Variable usage = 100.00%
----------Iter = 20700----------
Loss = 1.012630
Variable usage = 100.00%
----------Iter = 20800----------
Loss = 1.012600
Variable usage = 100.00%
----------Iter = 20900----------
Loss = 1.012570
Variable usage = 100.00%
----------Iter = 21000----------
Loss = 1.012540
Variable usage = 100.00%
----------Iter = 21100----------
Loss = 1.012511
Variable usage = 100.00%
----------Iter = 21200----------
Loss = 1.012482
Variable usage = 100.00%
----------Iter = 21300----------
Loss = 1.012452
Variable usage = 100.00%
----------Iter = 21400----------
Loss = 1.012424
Variable usage = 100.00%
----------Iter = 21500----------
Loss = 1.012396
Variable usage = 100.00%
----------Iter = 21600----------
Loss = 1.012367
Variable usage = 100.00%
----------Iter = 21700----------
Loss = 1.012339
Variable usage = 100.00%
----------Iter = 21800----------
Loss = 1.012313
Variable usage = 100.00%
----------Iter = 21900----------
Loss = 1.012286
Variable usage = 100.00%
----------Iter = 22000----------
Loss = 1.012259
Variable usage = 100.00%
----------Iter = 22100----------
Loss = 1.012233
Variable usage = 100.00%
----------Iter = 22200----------
Loss = 1.012207
Variable usage = 100.00%
----------Iter = 22300----------
Loss = 1.012181
Variable usage = 100.00%
----------Iter = 22400----------
Loss = 1.012155
Variable usage = 100.00%
----------Iter = 22500----------
Loss = 1.012129
Variable usage = 100.00%
----------Iter = 22600----------
Loss = 1.012103
Variable usage = 100.00%
----------Iter = 22700----------
Loss = 1.012076
Variable usage = 100.00%
----------Iter = 22800----------
Loss = 1.012051
Variable usage = 100.00%
----------Iter = 22900----------
Loss = 1.012025
Variable usage = 100.00%
----------Iter = 23000----------
Loss = 1.011999
Variable usage = 100.00%
----------Iter = 23100----------
Loss = 1.011974
Variable usage = 100.00%
----------Iter = 23200----------
Loss = 1.011948
Variable usage = 100.00%
----------Iter = 23300----------
Loss = 1.011923
Variable usage = 100.00%
----------Iter = 23400----------
Loss = 1.011898
Variable usage = 100.00%
----------Iter = 23500----------
Loss = 1.011873
Variable usage = 100.00%
----------Iter = 23600----------
Loss = 1.011847
Variable usage = 100.00%
----------Iter = 23700----------
Loss = 1.011822
Variable usage = 100.00%
----------Iter = 23800----------
Loss = 1.011797
Variable usage = 100.00%
----------Iter = 23900----------
Loss = 1.011772
Variable usage = 100.00%
----------Iter = 24000----------
Loss = 1.011747
Variable usage = 100.00%
----------Iter = 24100----------
Loss = 1.011722
Variable usage = 100.00%
----------Iter = 24200----------
Loss = 1.011697
Variable usage = 100.00%
----------Iter = 24300----------
Loss = 1.011672
Variable usage = 100.00%
----------Iter = 24400----------
Loss = 1.011647
Variable usage = 100.00%
----------Iter = 24500----------
Loss = 1.011622
Variable usage = 100.00%
----------Iter = 24600----------
Loss = 1.011597
Variable usage = 100.00%
----------Iter = 24700----------
Loss = 1.011572
Variable usage = 100.00%
----------Iter = 24800----------
Loss = 1.011547
Variable usage = 100.00%
----------Iter = 24900----------
Loss = 1.011522
Variable usage = 100.00%
----------Iter = 25000----------
Loss = 1.011497
Variable usage = 100.00%
----------Iter = 25100----------
Loss = 1.011473
Variable usage = 100.00%
----------Iter = 25200----------
Loss = 1.011449
Variable usage = 100.00%
----------Iter = 25300----------
Loss = 1.011425
Variable usage = 100.00%
----------Iter = 25400----------
Loss = 1.011401
Variable usage = 100.00%
----------Iter = 25500----------
Loss = 1.011378
Variable usage = 100.00%
----------Iter = 25600----------
Loss = 1.011355
Variable usage = 100.00%
----------Iter = 25700----------
Loss = 1.011331
Variable usage = 100.00%
----------Iter = 25800----------
Loss = 1.011308
Variable usage = 100.00%
----------Iter = 25900----------
Loss = 1.011285
Variable usage = 100.00%
----------Iter = 26000----------
Loss = 1.011262
Variable usage = 100.00%
----------Iter = 26100----------
Loss = 1.011239
Variable usage = 100.00%
----------Iter = 26200----------
Loss = 1.011217
Variable usage = 100.00%
----------Iter = 26300----------
Loss = 1.011194
Variable usage = 100.00%
----------Iter = 26400----------
Loss = 1.011171
Variable usage = 100.00%
----------Iter = 26500----------
Loss = 1.011148
Variable usage = 100.00%
----------Iter = 26600----------
Loss = 1.011126
Variable usage = 100.00%
----------Iter = 26700----------
Loss = 1.011103
Variable usage = 100.00%
----------Iter = 26800----------
Loss = 1.011081
Variable usage = 100.00%
----------Iter = 26900----------
Loss = 1.011059
Variable usage = 100.00%
----------Iter = 27000----------
Loss = 1.011038
Variable usage = 100.00%
----------Iter = 27100----------
Loss = 1.011017
Variable usage = 100.00%
----------Iter = 27200----------
Loss = 1.010996
Variable usage = 100.00%
----------Iter = 27300----------
Loss = 1.010975
Variable usage = 100.00%
----------Iter = 27400----------
Loss = 1.010954
Variable usage = 100.00%
----------Iter = 27500----------
Loss = 1.010934
Variable usage = 100.00%
----------Iter = 27600----------
Loss = 1.010913
Variable usage = 100.00%
----------Iter = 27700----------
Loss = 1.010893
Variable usage = 100.00%
----------Iter = 27800----------
Loss = 1.010873
Variable usage = 100.00%
----------Iter = 27900----------
Loss = 1.010852
Variable usage = 100.00%
----------Iter = 28000----------
Loss = 1.010832
Variable usage = 100.00%
----------Iter = 28100----------
Loss = 1.010811
Variable usage = 100.00%
----------Iter = 28200----------
Loss = 1.010791
Variable usage = 100.00%
----------Iter = 28300----------
Loss = 1.010771
Variable usage = 100.00%
----------Iter = 28400----------
Loss = 1.010750
Variable usage = 100.00%
----------Iter = 28500----------
Loss = 1.010730
Variable usage = 100.00%
----------Iter = 28600----------
Loss = 1.010709
Variable usage = 100.00%
----------Iter = 28700----------
Loss = 1.010689
Variable usage = 100.00%
----------Iter = 28800----------
Loss = 1.010669
Variable usage = 100.00%
----------Iter = 28900----------
Loss = 1.010649
Variable usage = 100.00%
----------Iter = 29000----------
Loss = 1.010629
Variable usage = 100.00%
----------Iter = 29100----------
Loss = 1.010610
Variable usage = 100.00%
----------Iter = 29200----------
Loss = 1.010590
Variable usage = 100.00%
----------Iter = 29300----------
Loss = 1.010571
Variable usage = 100.00%
----------Iter = 29400----------
Loss = 1.010552
Variable usage = 100.00%
----------Iter = 29500----------
Loss = 1.010533
Variable usage = 100.00%
----------Iter = 29600----------
Loss = 1.010514
Variable usage = 100.00%
----------Iter = 29700----------
Loss = 1.010495
Variable usage = 100.00%
----------Iter = 29800----------
Loss = 1.010476
Variable usage = 100.00%
----------Iter = 29900----------
Loss = 1.010457
Variable usage = 100.00%
----------Iter = 30000----------
Loss = 1.010438
Variable usage = 100.00%
----------Iter = 30100----------
Loss = 1.010419
Variable usage = 100.00%
----------Iter = 30200----------
Loss = 1.010401
Variable usage = 100.00%
----------Iter = 30300----------
Loss = 1.010383
Variable usage = 100.00%
----------Iter = 30400----------
Loss = 1.010365
Variable usage = 100.00%
----------Iter = 30500----------
Loss = 1.010347
Variable usage = 100.00%
----------Iter = 30600----------
Loss = 1.010329
Variable usage = 100.00%
----------Iter = 30700----------
Loss = 1.010311
Variable usage = 100.00%
----------Iter = 30800----------
Loss = 1.010293
Variable usage = 100.00%
----------Iter = 30900----------
Loss = 1.010275
Variable usage = 100.00%
----------Iter = 31000----------
Loss = 1.010257
Variable usage = 100.00%
----------Iter = 31100----------
Loss = 1.010240
Variable usage = 100.00%
----------Iter = 31200----------
Loss = 1.010222
Variable usage = 100.00%
----------Iter = 31300----------
Loss = 1.010204
Variable usage = 100.00%
----------Iter = 31400----------
Loss = 1.010186
Variable usage = 100.00%
----------Iter = 31500----------
Loss = 1.010169
Variable usage = 100.00%
----------Iter = 31600----------
Loss = 1.010151
Variable usage = 100.00%
----------Iter = 31700----------
Loss = 1.010133
Variable usage = 100.00%
----------Iter = 31800----------
Loss = 1.010116
Variable usage = 100.00%
----------Iter = 31900----------
Loss = 1.010098
Variable usage = 100.00%
----------Iter = 32000----------
Loss = 1.010081
Variable usage = 100.00%
----------Iter = 32100----------
Loss = 1.010063
Variable usage = 100.00%
----------Iter = 32200----------
Loss = 1.010046
Variable usage = 100.00%
----------Iter = 32300----------
Loss = 1.010028
Variable usage = 100.00%
----------Iter = 32400----------
Loss = 1.010011
Variable usage = 100.00%
----------Iter = 32500----------
Loss = 1.009994
Variable usage = 100.00%
----------Iter = 32600----------
Loss = 1.009978
Variable usage = 100.00%
----------Iter = 32700----------
Loss = 1.009961
Variable usage = 100.00%
----------Iter = 32800----------
Loss = 1.009945
Variable usage = 100.00%
----------Iter = 32900----------
Loss = 1.009928
Variable usage = 100.00%
----------Iter = 33000----------
Loss = 1.009912
Variable usage = 100.00%
----------Iter = 33100----------
Loss = 1.009896
Variable usage = 100.00%
----------Iter = 33200----------
Loss = 1.009880
Variable usage = 100.00%
----------Iter = 33300----------
Loss = 1.009864
Variable usage = 100.00%
----------Iter = 33400----------
Loss = 1.009847
Variable usage = 100.00%
----------Iter = 33500----------
Loss = 1.009831
Variable usage = 100.00%
----------Iter = 33600----------
Loss = 1.009815
Variable usage = 100.00%
----------Iter = 33700----------
Loss = 1.009799
Variable usage = 100.00%
----------Iter = 33800----------
Loss = 1.009783
Variable usage = 100.00%
----------Iter = 33900----------
Loss = 1.009767
Variable usage = 100.00%
----------Iter = 34000----------
Loss = 1.009751
Variable usage = 100.00%
----------Iter = 34100----------
Loss = 1.009734
Variable usage = 100.00%
----------Iter = 34200----------
Loss = 1.009718
Variable usage = 100.00%
----------Iter = 34300----------
Loss = 1.009703
Variable usage = 100.00%
----------Iter = 34400----------
Loss = 1.009687
Variable usage = 100.00%
----------Iter = 34500----------
Loss = 1.009671
Variable usage = 100.00%
----------Iter = 34600----------
Loss = 1.009655
Variable usage = 100.00%
----------Iter = 34700----------
Loss = 1.009640
Variable usage = 100.00%
----------Iter = 34800----------
Loss = 1.009624
Variable usage = 100.00%
----------Iter = 34900----------
Loss = 1.009609
Variable usage = 100.00%
----------Iter = 35000----------
Loss = 1.009594
Variable usage = 100.00%
----------Iter = 35100----------
Loss = 1.009579
Variable usage = 100.00%
----------Iter = 35200----------
Loss = 1.009564
Variable usage = 100.00%
----------Iter = 35300----------
Loss = 1.009549
Variable usage = 100.00%
----------Iter = 35400----------
Loss = 1.009534
Variable usage = 100.00%
----------Iter = 35500----------
Loss = 1.009519
Variable usage = 100.00%
----------Iter = 35600----------
Loss = 1.009503
Variable usage = 100.00%
----------Iter = 35700----------
Loss = 1.009488
Variable usage = 100.00%
----------Iter = 35800----------
Loss = 1.009473
Variable usage = 100.00%
----------Iter = 35900----------
Loss = 1.009458
Variable usage = 100.00%
----------Iter = 36000----------
Loss = 1.009443
Variable usage = 100.00%
----------Iter = 36100----------
Loss = 1.009428
Variable usage = 100.00%
----------Iter = 36200----------
Loss = 1.009413
Variable usage = 100.00%
----------Iter = 36300----------
Loss = 1.009399
Variable usage = 100.00%
----------Iter = 36400----------
Loss = 1.009384
Variable usage = 100.00%
----------Iter = 36500----------
Loss = 1.009370
Variable usage = 100.00%
----------Iter = 36600----------
Loss = 1.009355
Variable usage = 100.00%
----------Iter = 36700----------
Loss = 1.009341
Variable usage = 100.00%
----------Iter = 36800----------
Loss = 1.009327
Variable usage = 100.00%
----------Iter = 36900----------
Loss = 1.009313
Variable usage = 100.00%
----------Iter = 37000----------
Loss = 1.009298
Variable usage = 100.00%
----------Iter = 37100----------
Loss = 1.009284
Variable usage = 100.00%
----------Iter = 37200----------
Loss = 1.009269
Variable usage = 100.00%
----------Iter = 37300----------
Loss = 1.009254
Variable usage = 100.00%
----------Iter = 37400----------
Loss = 1.009239
Variable usage = 100.00%
----------Iter = 37500----------
Loss = 1.009224
Variable usage = 100.00%
----------Iter = 37600----------
Loss = 1.009208
Variable usage = 100.00%
----------Iter = 37700----------
Loss = 1.009192
Variable usage = 100.00%
----------Iter = 37800----------
Loss = 1.009176
Variable usage = 100.00%
----------Iter = 37900----------
Loss = 1.009160
Variable usage = 100.00%
----------Iter = 38000----------
Loss = 1.009145
Variable usage = 100.00%
----------Iter = 38100----------
Loss = 1.009130
Variable usage = 100.00%
----------Iter = 38200----------
Loss = 1.009116
Variable usage = 100.00%
----------Iter = 38300----------
Loss = 1.009102
Variable usage = 100.00%
----------Iter = 38400----------
Loss = 1.009088
Variable usage = 100.00%
----------Iter = 38500----------
Loss = 1.009074
Variable usage = 100.00%
----------Iter = 38600----------
Loss = 1.009060
Variable usage = 100.00%
----------Iter = 38700----------
Loss = 1.009047
Variable usage = 100.00%
----------Iter = 38800----------
Loss = 1.009034
Variable usage = 100.00%
----------Iter = 38900----------
Loss = 1.009020
Variable usage = 100.00%
----------Iter = 39000----------
Loss = 1.009007
Variable usage = 100.00%
----------Iter = 39100----------
Loss = 1.008993
Variable usage = 100.00%
----------Iter = 39200----------
Loss = 1.008980
Variable usage = 100.00%
----------Iter = 39300----------
Loss = 1.008967
Variable usage = 100.00%
----------Iter = 39400----------
Loss = 1.008952
Variable usage = 100.00%
----------Iter = 39500----------
Loss = 1.008939
Variable usage = 100.00%
----------Iter = 39600----------
Loss = 1.008925
Variable usage = 100.00%
----------Iter = 39700----------
Loss = 1.008912
Variable usage = 100.00%
----------Iter = 39800----------
Loss = 1.008899
Variable usage = 100.00%
----------Iter = 39900----------
Loss = 1.008886
Variable usage = 100.00%
----------Iter = 40000----------
Loss = 1.008874
Variable usage = 100.00%
----------Iter = 40100----------
Loss = 1.008861
Variable usage = 100.00%
----------Iter = 40200----------
Loss = 1.008849
Variable usage = 100.00%
----------Iter = 40300----------
Loss = 1.008836
Variable usage = 100.00%
----------Iter = 40400----------
Loss = 1.008824
Variable usage = 100.00%
----------Iter = 40500----------
Loss = 1.008812
Variable usage = 100.00%
----------Iter = 40600----------
Loss = 1.008800
Variable usage = 100.00%
----------Iter = 40700----------
Loss = 1.008788
Variable usage = 100.00%
----------Iter = 40800----------
Loss = 1.008775
Variable usage = 100.00%
----------Iter = 40900----------
Loss = 1.008763
Variable usage = 100.00%
----------Iter = 41000----------
Loss = 1.008750
Variable usage = 100.00%
----------Iter = 41100----------
Loss = 1.008737
Variable usage = 100.00%
----------Iter = 41200----------
Loss = 1.008725
Variable usage = 100.00%
----------Iter = 41300----------
Loss = 1.008712
Variable usage = 100.00%
----------Iter = 41400----------
Loss = 1.008698
Variable usage = 100.00%
----------Iter = 41500----------
Loss = 1.008685
Variable usage = 100.00%
----------Iter = 41600----------
Loss = 1.008671
Variable usage = 100.00%
----------Iter = 41700----------
Loss = 1.008657
Variable usage = 100.00%
----------Iter = 41800----------
Loss = 1.008640
Variable usage = 100.00%
----------Iter = 41900----------
Loss = 1.008620
Variable usage = 100.00%
----------Iter = 42000----------
Loss = 1.008604
Variable usage = 100.00%
----------Iter = 42100----------
Loss = 1.008591
Variable usage = 100.00%
----------Iter = 42200----------
Loss = 1.008578
Variable usage = 100.00%
----------Iter = 42300----------
Loss = 1.008565
Variable usage = 100.00%
----------Iter = 42400----------
Loss = 1.008553
Variable usage = 100.00%
----------Iter = 42500----------
Loss = 1.008541
Variable usage = 100.00%
----------Iter = 42600----------
Loss = 1.008529
Variable usage = 100.00%
----------Iter = 42700----------
Loss = 1.008517
Variable usage = 100.00%
----------Iter = 42800----------
Loss = 1.008505
Variable usage = 100.00%
----------Iter = 42900----------
Loss = 1.008493
Variable usage = 100.00%
----------Iter = 43000----------
Loss = 1.008481
Variable usage = 100.00%
----------Iter = 43100----------
Loss = 1.008469
Variable usage = 100.00%
----------Iter = 43200----------
Loss = 1.008457
Variable usage = 100.00%
----------Iter = 43300----------
Loss = 1.008445
Variable usage = 100.00%
----------Iter = 43400----------
Loss = 1.008434
Variable usage = 100.00%
----------Iter = 43500----------
Loss = 1.008422
Variable usage = 100.00%
----------Iter = 43600----------
Loss = 1.008410
Variable usage = 100.00%
----------Iter = 43700----------
Loss = 1.008397
Variable usage = 100.00%
----------Iter = 43800----------
Loss = 1.008384
Variable usage = 100.00%
----------Iter = 43900----------
Loss = 1.008371
Variable usage = 100.00%
----------Iter = 44000----------
Loss = 1.008358
Variable usage = 100.00%
----------Iter = 44100----------
Loss = 1.008346
Variable usage = 100.00%
----------Iter = 44200----------
Loss = 1.008335
Variable usage = 100.00%
----------Iter = 44300----------
Loss = 1.008324
Variable usage = 100.00%
----------Iter = 44400----------
Loss = 1.008312
Variable usage = 100.00%
----------Iter = 44500----------
Loss = 1.008300
Variable usage = 100.00%
----------Iter = 44600----------
Loss = 1.008289
Variable usage = 100.00%
----------Iter = 44700----------
Loss = 1.008278
Variable usage = 100.00%
----------Iter = 44800----------
Loss = 1.008268
Variable usage = 100.00%
----------Iter = 44900----------
Loss = 1.008257
Variable usage = 100.00%
----------Iter = 45000----------
Loss = 1.008247
Variable usage = 100.00%
----------Iter = 45100----------
Loss = 1.008237
Variable usage = 100.00%
----------Iter = 45200----------
Loss = 1.008227
Variable usage = 100.00%
----------Iter = 45300----------
Loss = 1.008217
Variable usage = 100.00%
----------Iter = 45400----------
Loss = 1.008207
Variable usage = 100.00%
----------Iter = 45500----------
Loss = 1.008197
Variable usage = 100.00%
----------Iter = 45600----------
Loss = 1.008187
Variable usage = 100.00%
----------Iter = 45700----------
Loss = 1.008177
Variable usage = 100.00%
----------Iter = 45800----------
Loss = 1.008167
Variable usage = 100.00%
----------Iter = 45900----------
Loss = 1.008158
Variable usage = 100.00%
----------Iter = 46000----------
Loss = 1.008148
Variable usage = 100.00%
----------Iter = 46100----------
Loss = 1.008138
Variable usage = 100.00%
----------Iter = 46200----------
Loss = 1.008129
Variable usage = 100.00%
----------Iter = 46300----------
Loss = 1.008120
Variable usage = 100.00%
----------Iter = 46400----------
Loss = 1.008111
Variable usage = 100.00%
----------Iter = 46500----------
Loss = 1.008102
Variable usage = 100.00%
----------Iter = 46600----------
Loss = 1.008093
Variable usage = 100.00%
----------Iter = 46700----------
Loss = 1.008084
Variable usage = 100.00%
----------Iter = 46800----------
Loss = 1.008076
Variable usage = 100.00%
----------Iter = 46900----------
Loss = 1.008068
Variable usage = 100.00%
----------Iter = 47000----------
Loss = 1.008059
Variable usage = 100.00%
----------Iter = 47100----------
Loss = 1.008051
Variable usage = 100.00%
----------Iter = 47200----------
Loss = 1.008042
Variable usage = 100.00%
----------Iter = 47300----------
Loss = 1.008034
Variable usage = 100.00%
----------Iter = 47400----------
Loss = 1.008026
Variable usage = 100.00%
----------Iter = 47500----------
Loss = 1.008018
Variable usage = 100.00%
----------Iter = 47600----------
Loss = 1.008009
Variable usage = 100.00%
----------Iter = 47700----------
Loss = 1.008001
Variable usage = 100.00%
----------Iter = 47800----------
Loss = 1.007992
Variable usage = 100.00%
----------Iter = 47900----------
Loss = 1.007984
Variable usage = 100.00%
----------Iter = 48000----------
Loss = 1.007976
Variable usage = 100.00%
----------Iter = 48100----------
Loss = 1.007967
Variable usage = 100.00%
----------Iter = 48200----------
Loss = 1.007959
Variable usage = 100.00%
----------Iter = 48300----------
Loss = 1.007951
Variable usage = 100.00%
----------Iter = 48400----------
Loss = 1.007943
Variable usage = 100.00%
----------Iter = 48500----------
Loss = 1.007935
Variable usage = 100.00%
----------Iter = 48600----------
Loss = 1.007926
Variable usage = 100.00%
----------Iter = 48700----------
Loss = 1.007918
Variable usage = 100.00%
----------Iter = 48800----------
Loss = 1.007910
Variable usage = 100.00%
----------Iter = 48900----------
Loss = 1.007902
Variable usage = 100.00%
----------Iter = 49000----------
Loss = 1.007894
Variable usage = 100.00%
----------Iter = 49100----------
Loss = 1.007887
Variable usage = 100.00%
----------Iter = 49200----------
Loss = 1.007880
Variable usage = 100.00%
----------Iter = 49300----------
Loss = 1.007873
Variable usage = 100.00%
----------Iter = 49400----------
Loss = 1.007866
Variable usage = 100.00%
----------Iter = 49500----------
Loss = 1.007859
Variable usage = 100.00%
----------Iter = 49600----------
Loss = 1.007852
Variable usage = 100.00%
----------Iter = 49700----------
Loss = 1.007846
Variable usage = 100.00%
----------Iter = 49800----------
Loss = 1.007839
Variable usage = 100.00%
----------Iter = 49900----------
Loss = 1.007833
Variable usage = 100.00%
----------Iter = 50000----------
Loss = 1.007827
Variable usage = 100.00%
True variable usage = 50.00%
Estimated variable usage = 100.00%
Accuracy = 0.50%
Accuracy for experiment id 7 is 0.5
----------Iter = 100----------
Loss = 1.033953
Variable usage = 100.00%
----------Iter = 200----------
Loss = 1.032567
Variable usage = 100.00%
----------Iter = 300----------
Loss = 1.031634
Variable usage = 100.00%
----------Iter = 400----------
Loss = 1.030940
Variable usage = 100.00%
----------Iter = 500----------
Loss = 1.030389
Variable usage = 100.00%
----------Iter = 600----------
Loss = 1.029941
Variable usage = 100.00%
----------Iter = 700----------
Loss = 1.029566
Variable usage = 100.00%
----------Iter = 800----------
Loss = 1.029242
Variable usage = 100.00%
----------Iter = 900----------
Loss = 1.028958
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 1.028702
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 1.028469
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 1.028253
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 1.028049
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 1.027854
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 1.027666
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 1.027485
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 1.027309
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 1.027140
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 1.026975
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 1.026816
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 1.026659
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 1.026502
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 1.026348
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 1.026196
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 1.026044
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 1.025894
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 1.025749
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 1.025604
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 1.025464
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 1.025328
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 1.025194
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 1.025061
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 1.024929
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 1.024796
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 1.024664
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 1.024532
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 1.024402
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 1.024271
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 1.024142
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 1.024014
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 1.023888
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 1.023761
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 1.023638
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 1.023517
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 1.023398
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 1.023281
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 1.023164
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 1.023048
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 1.022934
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 1.022819
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 1.022704
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 1.022589
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 1.022475
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 1.022361
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 1.022247
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 1.022133
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 1.022019
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 1.021906
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 1.021793
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 1.021682
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 1.021572
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 1.021463
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 1.021356
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 1.021248
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 1.021141
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 1.021034
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 1.020929
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 1.020823
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 1.020718
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 1.020614
Variable usage = 100.00%
----------Iter = 7100----------
Loss = 1.020511
Variable usage = 100.00%
----------Iter = 7200----------
Loss = 1.020409
Variable usage = 100.00%
----------Iter = 7300----------
Loss = 1.020309
Variable usage = 100.00%
----------Iter = 7400----------
Loss = 1.020210
Variable usage = 100.00%
----------Iter = 7500----------
Loss = 1.020112
Variable usage = 100.00%
----------Iter = 7600----------
Loss = 1.020015
Variable usage = 100.00%
----------Iter = 7700----------
Loss = 1.019917
Variable usage = 100.00%
----------Iter = 7800----------
Loss = 1.019820
Variable usage = 100.00%
----------Iter = 7900----------
Loss = 1.019723
Variable usage = 100.00%
----------Iter = 8000----------
Loss = 1.019626
Variable usage = 100.00%
----------Iter = 8100----------
Loss = 1.019530
Variable usage = 100.00%
----------Iter = 8200----------
Loss = 1.019435
Variable usage = 100.00%
----------Iter = 8300----------
Loss = 1.019340
Variable usage = 100.00%
----------Iter = 8400----------
Loss = 1.019246
Variable usage = 100.00%
----------Iter = 8500----------
Loss = 1.019150
Variable usage = 100.00%
----------Iter = 8600----------
Loss = 1.019056
Variable usage = 100.00%
----------Iter = 8700----------
Loss = 1.018962
Variable usage = 100.00%
----------Iter = 8800----------
Loss = 1.018869
Variable usage = 100.00%
----------Iter = 8900----------
Loss = 1.018779
Variable usage = 100.00%
----------Iter = 9000----------
Loss = 1.018691
Variable usage = 100.00%
----------Iter = 9100----------
Loss = 1.018604
Variable usage = 100.00%
----------Iter = 9200----------
Loss = 1.018516
Variable usage = 100.00%
----------Iter = 9300----------
Loss = 1.018430
Variable usage = 100.00%
----------Iter = 9400----------
Loss = 1.018345
Variable usage = 100.00%
----------Iter = 9500----------
Loss = 1.018261
Variable usage = 100.00%
----------Iter = 9600----------
Loss = 1.018178
Variable usage = 100.00%
----------Iter = 9700----------
Loss = 1.018095
Variable usage = 100.00%
----------Iter = 9800----------
Loss = 1.018014
Variable usage = 100.00%
----------Iter = 9900----------
Loss = 1.017933
Variable usage = 100.00%
----------Iter = 10000----------
Loss = 1.017851
Variable usage = 100.00%
----------Iter = 10100----------
Loss = 1.017769
Variable usage = 100.00%
----------Iter = 10200----------
Loss = 1.017686
Variable usage = 100.00%
----------Iter = 10300----------
Loss = 1.017605
Variable usage = 100.00%
----------Iter = 10400----------
Loss = 1.017524
Variable usage = 100.00%
----------Iter = 10500----------
Loss = 1.017443
Variable usage = 100.00%
----------Iter = 10600----------
Loss = 1.017361
Variable usage = 100.00%
----------Iter = 10700----------
Loss = 1.017281
Variable usage = 100.00%
----------Iter = 10800----------
Loss = 1.017202
Variable usage = 100.00%
----------Iter = 10900----------
Loss = 1.017125
Variable usage = 100.00%
----------Iter = 11000----------
Loss = 1.017048
Variable usage = 100.00%
----------Iter = 11100----------
Loss = 1.016972
Variable usage = 100.00%
----------Iter = 11200----------
Loss = 1.016898
Variable usage = 100.00%
----------Iter = 11300----------
Loss = 1.016824
Variable usage = 100.00%
----------Iter = 11400----------
Loss = 1.016752
Variable usage = 100.00%
----------Iter = 11500----------
Loss = 1.016681
Variable usage = 100.00%
----------Iter = 11600----------
Loss = 1.016612
Variable usage = 100.00%
----------Iter = 11700----------
Loss = 1.016543
Variable usage = 100.00%
----------Iter = 11800----------
Loss = 1.016475
Variable usage = 100.00%
----------Iter = 11900----------
Loss = 1.016407
Variable usage = 100.00%
----------Iter = 12000----------
Loss = 1.016338
Variable usage = 100.00%
----------Iter = 12100----------
Loss = 1.016271
Variable usage = 100.00%
----------Iter = 12200----------
Loss = 1.016204
Variable usage = 100.00%
----------Iter = 12300----------
Loss = 1.016137
Variable usage = 100.00%
----------Iter = 12400----------
Loss = 1.016071
Variable usage = 100.00%
----------Iter = 12500----------
Loss = 1.016005
Variable usage = 100.00%
----------Iter = 12600----------
Loss = 1.015938
Variable usage = 100.00%
----------Iter = 12700----------
Loss = 1.015873
Variable usage = 100.00%
----------Iter = 12800----------
Loss = 1.015808
Variable usage = 100.00%
----------Iter = 12900----------
Loss = 1.015743
Variable usage = 100.00%
----------Iter = 13000----------
Loss = 1.015680
Variable usage = 100.00%
----------Iter = 13100----------
Loss = 1.015616
Variable usage = 100.00%
----------Iter = 13200----------
Loss = 1.015553
Variable usage = 100.00%
----------Iter = 13300----------
Loss = 1.015490
Variable usage = 100.00%
----------Iter = 13400----------
Loss = 1.015428
Variable usage = 100.00%
----------Iter = 13500----------
Loss = 1.015367
Variable usage = 100.00%
----------Iter = 13600----------
Loss = 1.015306
Variable usage = 100.00%
----------Iter = 13700----------
Loss = 1.015247
Variable usage = 100.00%
----------Iter = 13800----------
Loss = 1.015187
Variable usage = 100.00%
----------Iter = 13900----------
Loss = 1.015128
Variable usage = 100.00%
----------Iter = 14000----------
Loss = 1.015069
Variable usage = 100.00%
----------Iter = 14100----------
Loss = 1.015010
Variable usage = 100.00%
----------Iter = 14200----------
Loss = 1.014952
Variable usage = 100.00%
----------Iter = 14300----------
Loss = 1.014894
Variable usage = 100.00%
----------Iter = 14400----------
Loss = 1.014837
Variable usage = 100.00%
----------Iter = 14500----------
Loss = 1.014780
Variable usage = 100.00%
----------Iter = 14600----------
Loss = 1.014724
Variable usage = 100.00%
----------Iter = 14700----------
Loss = 1.014667
Variable usage = 100.00%
----------Iter = 14800----------
Loss = 1.014609
Variable usage = 100.00%
----------Iter = 14900----------
Loss = 1.014552
Variable usage = 100.00%
----------Iter = 15000----------
Loss = 1.014493
Variable usage = 100.00%
----------Iter = 15100----------
Loss = 1.014434
Variable usage = 100.00%
----------Iter = 15200----------
Loss = 1.014374
Variable usage = 100.00%
----------Iter = 15300----------
Loss = 1.014316
Variable usage = 100.00%
----------Iter = 15400----------
Loss = 1.014258
Variable usage = 100.00%
----------Iter = 15500----------
Loss = 1.014202
Variable usage = 100.00%
----------Iter = 15600----------
Loss = 1.014145
Variable usage = 100.00%
----------Iter = 15700----------
Loss = 1.014089
Variable usage = 100.00%
----------Iter = 15800----------
Loss = 1.014033
Variable usage = 100.00%
----------Iter = 15900----------
Loss = 1.013978
Variable usage = 100.00%
----------Iter = 16000----------
Loss = 1.013924
Variable usage = 100.00%
----------Iter = 16100----------
Loss = 1.013870
Variable usage = 100.00%
----------Iter = 16200----------
Loss = 1.013817
Variable usage = 100.00%
----------Iter = 16300----------
Loss = 1.013766
Variable usage = 100.00%
----------Iter = 16400----------
Loss = 1.013715
Variable usage = 100.00%
----------Iter = 16500----------
Loss = 1.013664
Variable usage = 100.00%
----------Iter = 16600----------
Loss = 1.013614
Variable usage = 100.00%
----------Iter = 16700----------
Loss = 1.013564
Variable usage = 100.00%
----------Iter = 16800----------
Loss = 1.013515
Variable usage = 100.00%
----------Iter = 16900----------
Loss = 1.013465
Variable usage = 100.00%
----------Iter = 17000----------
Loss = 1.013416
Variable usage = 100.00%
----------Iter = 17100----------
Loss = 1.013366
Variable usage = 100.00%
----------Iter = 17200----------
Loss = 1.013318
Variable usage = 100.00%
----------Iter = 17300----------
Loss = 1.013269
Variable usage = 100.00%
----------Iter = 17400----------
Loss = 1.013220
Variable usage = 100.00%
----------Iter = 17500----------
Loss = 1.013172
Variable usage = 100.00%
----------Iter = 17600----------
Loss = 1.013123
Variable usage = 100.00%
----------Iter = 17700----------
Loss = 1.013075
Variable usage = 100.00%
----------Iter = 17800----------
Loss = 1.013027
Variable usage = 100.00%
----------Iter = 17900----------
Loss = 1.012981
Variable usage = 100.00%
----------Iter = 18000----------
Loss = 1.012934
Variable usage = 100.00%
----------Iter = 18100----------
Loss = 1.012888
Variable usage = 100.00%
----------Iter = 18200----------
Loss = 1.012842
Variable usage = 100.00%
----------Iter = 18300----------
Loss = 1.012796
Variable usage = 100.00%
----------Iter = 18400----------
Loss = 1.012751
Variable usage = 100.00%
----------Iter = 18500----------
Loss = 1.012707
Variable usage = 100.00%
----------Iter = 18600----------
Loss = 1.012663
Variable usage = 100.00%
----------Iter = 18700----------
Loss = 1.012620
Variable usage = 100.00%
----------Iter = 18800----------
Loss = 1.012578
Variable usage = 100.00%
----------Iter = 18900----------
Loss = 1.012536
Variable usage = 100.00%
----------Iter = 19000----------
Loss = 1.012494
Variable usage = 100.00%
----------Iter = 19100----------
Loss = 1.012453
Variable usage = 100.00%
----------Iter = 19200----------
Loss = 1.012412
Variable usage = 100.00%
----------Iter = 19300----------
Loss = 1.012371
Variable usage = 100.00%
----------Iter = 19400----------
Loss = 1.012330
Variable usage = 100.00%
----------Iter = 19500----------
Loss = 1.012289
Variable usage = 100.00%
----------Iter = 19600----------
Loss = 1.012248
Variable usage = 100.00%
----------Iter = 19700----------
Loss = 1.012207
Variable usage = 100.00%
----------Iter = 19800----------
Loss = 1.012166
Variable usage = 100.00%
----------Iter = 19900----------
Loss = 1.012125
Variable usage = 100.00%
----------Iter = 20000----------
Loss = 1.012085
Variable usage = 100.00%
----------Iter = 20100----------
Loss = 1.012046
Variable usage = 100.00%
----------Iter = 20200----------
Loss = 1.012007
Variable usage = 100.00%
----------Iter = 20300----------
Loss = 1.011969
Variable usage = 100.00%
----------Iter = 20400----------
Loss = 1.011930
Variable usage = 100.00%
----------Iter = 20500----------
Loss = 1.011892
Variable usage = 100.00%
----------Iter = 20600----------
Loss = 1.011853
Variable usage = 100.00%
----------Iter = 20700----------
Loss = 1.011815
Variable usage = 100.00%
----------Iter = 20800----------
Loss = 1.011778
Variable usage = 100.00%
----------Iter = 20900----------
Loss = 1.011741
Variable usage = 100.00%
----------Iter = 21000----------
Loss = 1.011703
Variable usage = 100.00%
----------Iter = 21100----------
Loss = 1.011667
Variable usage = 100.00%
----------Iter = 21200----------
Loss = 1.011631
Variable usage = 100.00%
----------Iter = 21300----------
Loss = 1.011594
Variable usage = 100.00%
----------Iter = 21400----------
Loss = 1.011558
Variable usage = 100.00%
----------Iter = 21500----------
Loss = 1.011522
Variable usage = 100.00%
----------Iter = 21600----------
Loss = 1.011486
Variable usage = 100.00%
----------Iter = 21700----------
Loss = 1.011450
Variable usage = 100.00%
----------Iter = 21800----------
Loss = 1.011415
Variable usage = 100.00%
----------Iter = 21900----------
Loss = 1.011380
Variable usage = 100.00%
----------Iter = 22000----------
Loss = 1.011346
Variable usage = 100.00%
----------Iter = 22100----------
Loss = 1.011311
Variable usage = 100.00%
----------Iter = 22200----------
Loss = 1.011277
Variable usage = 100.00%
----------Iter = 22300----------
Loss = 1.011244
Variable usage = 100.00%
----------Iter = 22400----------
Loss = 1.011210
Variable usage = 100.00%
----------Iter = 22500----------
Loss = 1.011177
Variable usage = 100.00%
----------Iter = 22600----------
Loss = 1.011144
Variable usage = 100.00%
----------Iter = 22700----------
Loss = 1.011110
Variable usage = 100.00%
----------Iter = 22800----------
Loss = 1.011077
Variable usage = 100.00%
----------Iter = 22900----------
Loss = 1.011045
Variable usage = 100.00%
----------Iter = 23000----------
Loss = 1.011012
Variable usage = 100.00%
----------Iter = 23100----------
Loss = 1.010980
Variable usage = 100.00%
----------Iter = 23200----------
Loss = 1.010949
Variable usage = 100.00%
----------Iter = 23300----------
Loss = 1.010918
Variable usage = 100.00%
----------Iter = 23400----------
Loss = 1.010887
Variable usage = 100.00%
----------Iter = 23500----------
Loss = 1.010856
Variable usage = 100.00%
----------Iter = 23600----------
Loss = 1.010825
Variable usage = 100.00%
----------Iter = 23700----------
Loss = 1.010795
Variable usage = 100.00%
----------Iter = 23800----------
Loss = 1.010765
Variable usage = 100.00%
----------Iter = 23900----------
Loss = 1.010736
Variable usage = 100.00%
----------Iter = 24000----------
Loss = 1.010706
Variable usage = 100.00%
----------Iter = 24100----------
Loss = 1.010676
Variable usage = 100.00%
----------Iter = 24200----------
Loss = 1.010647
Variable usage = 100.00%
----------Iter = 24300----------
Loss = 1.010619
Variable usage = 100.00%
----------Iter = 24400----------
Loss = 1.010591
Variable usage = 100.00%
----------Iter = 24500----------
Loss = 1.010562
Variable usage = 100.00%
----------Iter = 24600----------
Loss = 1.010534
Variable usage = 100.00%
----------Iter = 24700----------
Loss = 1.010505
Variable usage = 100.00%
----------Iter = 24800----------
Loss = 1.010477
Variable usage = 100.00%
----------Iter = 24900----------
Loss = 1.010450
Variable usage = 100.00%
----------Iter = 25000----------
Loss = 1.010422
Variable usage = 100.00%
----------Iter = 25100----------
Loss = 1.010394
Variable usage = 100.00%
----------Iter = 25200----------
Loss = 1.010367
Variable usage = 100.00%
----------Iter = 25300----------
Loss = 1.010340
Variable usage = 100.00%
----------Iter = 25400----------
Loss = 1.010313
Variable usage = 100.00%
----------Iter = 25500----------
Loss = 1.010287
Variable usage = 100.00%
----------Iter = 25600----------
Loss = 1.010261
Variable usage = 100.00%
----------Iter = 25700----------
Loss = 1.010235
Variable usage = 100.00%
----------Iter = 25800----------
Loss = 1.010209
Variable usage = 100.00%
----------Iter = 25900----------
Loss = 1.010183
Variable usage = 100.00%
----------Iter = 26000----------
Loss = 1.010158
Variable usage = 100.00%
----------Iter = 26100----------
Loss = 1.010132
Variable usage = 100.00%
----------Iter = 26200----------
Loss = 1.010107
Variable usage = 100.00%
----------Iter = 26300----------
Loss = 1.010081
Variable usage = 100.00%
----------Iter = 26400----------
Loss = 1.010056
Variable usage = 100.00%
----------Iter = 26500----------
Loss = 1.010031
Variable usage = 100.00%
----------Iter = 26600----------
Loss = 1.010005
Variable usage = 100.00%
----------Iter = 26700----------
Loss = 1.009981
Variable usage = 100.00%
----------Iter = 26800----------
Loss = 1.009957
Variable usage = 100.00%
----------Iter = 26900----------
Loss = 1.009933
Variable usage = 100.00%
----------Iter = 27000----------
Loss = 1.009910
Variable usage = 100.00%
----------Iter = 27100----------
Loss = 1.009887
Variable usage = 100.00%
----------Iter = 27200----------
Loss = 1.009864
Variable usage = 100.00%
----------Iter = 27300----------
Loss = 1.009841
Variable usage = 100.00%
----------Iter = 27400----------
Loss = 1.009818
Variable usage = 100.00%
----------Iter = 27500----------
Loss = 1.009795
Variable usage = 100.00%
----------Iter = 27600----------
Loss = 1.009773
Variable usage = 100.00%
----------Iter = 27700----------
Loss = 1.009750
Variable usage = 100.00%
----------Iter = 27800----------
Loss = 1.009728
Variable usage = 100.00%
----------Iter = 27900----------
Loss = 1.009705
Variable usage = 100.00%
----------Iter = 28000----------
Loss = 1.009682
Variable usage = 100.00%
----------Iter = 28100----------
Loss = 1.009660
Variable usage = 100.00%
----------Iter = 28200----------
Loss = 1.009637
Variable usage = 100.00%
----------Iter = 28300----------
Loss = 1.009615
Variable usage = 100.00%
----------Iter = 28400----------
Loss = 1.009593
Variable usage = 100.00%
----------Iter = 28500----------
Loss = 1.009571
Variable usage = 100.00%
----------Iter = 28600----------
Loss = 1.009549
Variable usage = 100.00%
----------Iter = 28700----------
Loss = 1.009527
Variable usage = 100.00%
----------Iter = 28800----------
Loss = 1.009506
Variable usage = 100.00%
----------Iter = 28900----------
Loss = 1.009484
Variable usage = 100.00%
----------Iter = 29000----------
Loss = 1.009462
Variable usage = 100.00%
----------Iter = 29100----------
Loss = 1.009441
Variable usage = 100.00%
----------Iter = 29200----------
Loss = 1.009420
Variable usage = 100.00%
----------Iter = 29300----------
Loss = 1.009398
Variable usage = 100.00%
----------Iter = 29400----------
Loss = 1.009377
Variable usage = 100.00%
----------Iter = 29500----------
Loss = 1.009355
Variable usage = 100.00%
----------Iter = 29600----------
Loss = 1.009334
Variable usage = 100.00%
----------Iter = 29700----------
Loss = 1.009313
Variable usage = 100.00%
----------Iter = 29800----------
Loss = 1.009292
Variable usage = 100.00%
----------Iter = 29900----------
Loss = 1.009271
Variable usage = 100.00%
----------Iter = 30000----------
Loss = 1.009251
Variable usage = 100.00%
----------Iter = 30100----------
Loss = 1.009230
Variable usage = 100.00%
----------Iter = 30200----------
Loss = 1.009209
Variable usage = 100.00%
----------Iter = 30300----------
Loss = 1.009188
Variable usage = 100.00%
----------Iter = 30400----------
Loss = 1.009167
Variable usage = 100.00%
----------Iter = 30500----------
Loss = 1.009146
Variable usage = 100.00%
----------Iter = 30600----------
Loss = 1.009125
Variable usage = 100.00%
----------Iter = 30700----------
Loss = 1.009104
Variable usage = 100.00%
----------Iter = 30800----------
Loss = 1.009084
Variable usage = 100.00%
----------Iter = 30900----------
Loss = 1.009064
Variable usage = 100.00%
----------Iter = 31000----------
Loss = 1.009044
Variable usage = 100.00%
----------Iter = 31100----------
Loss = 1.009024
Variable usage = 100.00%
----------Iter = 31200----------
Loss = 1.009005
Variable usage = 100.00%
----------Iter = 31300----------
Loss = 1.008986
Variable usage = 100.00%
----------Iter = 31400----------
Loss = 1.008966
Variable usage = 100.00%
----------Iter = 31500----------
Loss = 1.008947
Variable usage = 100.00%
----------Iter = 31600----------
Loss = 1.008928
Variable usage = 100.00%
----------Iter = 31700----------
Loss = 1.008908
Variable usage = 100.00%
----------Iter = 31800----------
Loss = 1.008889
Variable usage = 100.00%
----------Iter = 31900----------
Loss = 1.008870
Variable usage = 100.00%
----------Iter = 32000----------
Loss = 1.008852
Variable usage = 100.00%
----------Iter = 32100----------
Loss = 1.008834
Variable usage = 100.00%
----------Iter = 32200----------
Loss = 1.008816
Variable usage = 100.00%
----------Iter = 32300----------
Loss = 1.008798
Variable usage = 100.00%
----------Iter = 32400----------
Loss = 1.008780
Variable usage = 100.00%
----------Iter = 32500----------
Loss = 1.008762
Variable usage = 100.00%
----------Iter = 32600----------
Loss = 1.008744
Variable usage = 100.00%
----------Iter = 32700----------
Loss = 1.008726
Variable usage = 100.00%
----------Iter = 32800----------
Loss = 1.008708
Variable usage = 100.00%
----------Iter = 32900----------
Loss = 1.008690
Variable usage = 100.00%
----------Iter = 33000----------
Loss = 1.008672
Variable usage = 100.00%
----------Iter = 33100----------
Loss = 1.008654
Variable usage = 100.00%
----------Iter = 33200----------
Loss = 1.008636
Variable usage = 100.00%
----------Iter = 33300----------
Loss = 1.008618
Variable usage = 100.00%
----------Iter = 33400----------
Loss = 1.008600
Variable usage = 100.00%
----------Iter = 33500----------
Loss = 1.008582
Variable usage = 100.00%
----------Iter = 33600----------
Loss = 1.008564
Variable usage = 100.00%
----------Iter = 33700----------
Loss = 1.008547
Variable usage = 100.00%
----------Iter = 33800----------
Loss = 1.008528
Variable usage = 100.00%
----------Iter = 33900----------
Loss = 1.008511
Variable usage = 100.00%
----------Iter = 34000----------
Loss = 1.008493
Variable usage = 100.00%
----------Iter = 34100----------
Loss = 1.008475
Variable usage = 100.00%
----------Iter = 34200----------
Loss = 1.008457
Variable usage = 100.00%
----------Iter = 34300----------
Loss = 1.008439
Variable usage = 100.00%
----------Iter = 34400----------
Loss = 1.008421
Variable usage = 100.00%
----------Iter = 34500----------
Loss = 1.008403
Variable usage = 100.00%
----------Iter = 34600----------
Loss = 1.008386
Variable usage = 100.00%
----------Iter = 34700----------
Loss = 1.008368
Variable usage = 100.00%
----------Iter = 34800----------
Loss = 1.008351
Variable usage = 100.00%
----------Iter = 34900----------
Loss = 1.008334
Variable usage = 100.00%
----------Iter = 35000----------
Loss = 1.008317
Variable usage = 100.00%
----------Iter = 35100----------
Loss = 1.008300
Variable usage = 100.00%
----------Iter = 35200----------
Loss = 1.008283
Variable usage = 100.00%
----------Iter = 35300----------
Loss = 1.008267
Variable usage = 100.00%
----------Iter = 35400----------
Loss = 1.008250
Variable usage = 100.00%
----------Iter = 35500----------
Loss = 1.008233
Variable usage = 100.00%
----------Iter = 35600----------
Loss = 1.008216
Variable usage = 100.00%
----------Iter = 35700----------
Loss = 1.008199
Variable usage = 100.00%
----------Iter = 35800----------
Loss = 1.008183
Variable usage = 100.00%
----------Iter = 35900----------
Loss = 1.008166
Variable usage = 100.00%
----------Iter = 36000----------
Loss = 1.008150
Variable usage = 100.00%
----------Iter = 36100----------
Loss = 1.008133
Variable usage = 100.00%
----------Iter = 36200----------
Loss = 1.008117
Variable usage = 100.00%
----------Iter = 36300----------
Loss = 1.008101
Variable usage = 100.00%
----------Iter = 36400----------
Loss = 1.008085
Variable usage = 100.00%
----------Iter = 36500----------
Loss = 1.008069
Variable usage = 100.00%
----------Iter = 36600----------
Loss = 1.008054
Variable usage = 100.00%
----------Iter = 36700----------
Loss = 1.008038
Variable usage = 100.00%
----------Iter = 36800----------
Loss = 1.008022
Variable usage = 100.00%
----------Iter = 36900----------
Loss = 1.008007
Variable usage = 100.00%
----------Iter = 37000----------
Loss = 1.007991
Variable usage = 100.00%
----------Iter = 37100----------
Loss = 1.007976
Variable usage = 100.00%
----------Iter = 37200----------
Loss = 1.007960
Variable usage = 100.00%
----------Iter = 37300----------
Loss = 1.007945
Variable usage = 100.00%
----------Iter = 37400----------
Loss = 1.007930
Variable usage = 100.00%
----------Iter = 37500----------
Loss = 1.007915
Variable usage = 100.00%
----------Iter = 37600----------
Loss = 1.007900
Variable usage = 100.00%
----------Iter = 37700----------
Loss = 1.007885
Variable usage = 100.00%
----------Iter = 37800----------
Loss = 1.007870
Variable usage = 100.00%
----------Iter = 37900----------
Loss = 1.007855
Variable usage = 100.00%
----------Iter = 38000----------
Loss = 1.007840
Variable usage = 100.00%
----------Iter = 38100----------
Loss = 1.007825
Variable usage = 100.00%
----------Iter = 38200----------
Loss = 1.007811
Variable usage = 100.00%
----------Iter = 38300----------
Loss = 1.007796
Variable usage = 100.00%
----------Iter = 38400----------
Loss = 1.007781
Variable usage = 100.00%
----------Iter = 38500----------
Loss = 1.007766
Variable usage = 100.00%
----------Iter = 38600----------
Loss = 1.007751
Variable usage = 100.00%
----------Iter = 38700----------
Loss = 1.007737
Variable usage = 100.00%
----------Iter = 38800----------
Loss = 1.007722
Variable usage = 100.00%
----------Iter = 38900----------
Loss = 1.007707
Variable usage = 100.00%
----------Iter = 39000----------
Loss = 1.007693
Variable usage = 100.00%
----------Iter = 39100----------
Loss = 1.007678
Variable usage = 100.00%
----------Iter = 39200----------
Loss = 1.007664
Variable usage = 100.00%
----------Iter = 39300----------
Loss = 1.007649
Variable usage = 100.00%
----------Iter = 39400----------
Loss = 1.007635
Variable usage = 100.00%
----------Iter = 39500----------
Loss = 1.007620
Variable usage = 100.00%
----------Iter = 39600----------
Loss = 1.007605
Variable usage = 100.00%
----------Iter = 39700----------
Loss = 1.007590
Variable usage = 100.00%
----------Iter = 39800----------
Loss = 1.007576
Variable usage = 100.00%
----------Iter = 39900----------
Loss = 1.007561
Variable usage = 100.00%
----------Iter = 40000----------
Loss = 1.007547
Variable usage = 100.00%
----------Iter = 40100----------
Loss = 1.007533
Variable usage = 100.00%
----------Iter = 40200----------
Loss = 1.007520
Variable usage = 100.00%
----------Iter = 40300----------
Loss = 1.007506
Variable usage = 100.00%
----------Iter = 40400----------
Loss = 1.007492
Variable usage = 100.00%
----------Iter = 40500----------
Loss = 1.007478
Variable usage = 100.00%
----------Iter = 40600----------
Loss = 1.007465
Variable usage = 100.00%
----------Iter = 40700----------
Loss = 1.007451
Variable usage = 100.00%
----------Iter = 40800----------
Loss = 1.007437
Variable usage = 100.00%
----------Iter = 40900----------
Loss = 1.007424
Variable usage = 100.00%
----------Iter = 41000----------
Loss = 1.007411
Variable usage = 100.00%
----------Iter = 41100----------
Loss = 1.007397
Variable usage = 100.00%
----------Iter = 41200----------
Loss = 1.007384
Variable usage = 100.00%
----------Iter = 41300----------
Loss = 1.007371
Variable usage = 100.00%
----------Iter = 41400----------
Loss = 1.007358
Variable usage = 100.00%
----------Iter = 41500----------
Loss = 1.007344
Variable usage = 100.00%
----------Iter = 41600----------
Loss = 1.007331
Variable usage = 100.00%
----------Iter = 41700----------
Loss = 1.007318
Variable usage = 100.00%
----------Iter = 41800----------
Loss = 1.007305
Variable usage = 100.00%
----------Iter = 41900----------
Loss = 1.007291
Variable usage = 100.00%
----------Iter = 42000----------
Loss = 1.007278
Variable usage = 100.00%
----------Iter = 42100----------
Loss = 1.007263
Variable usage = 100.00%
----------Iter = 42200----------
Loss = 1.007248
Variable usage = 100.00%
----------Iter = 42300----------
Loss = 1.007234
Variable usage = 100.00%
----------Iter = 42400----------
Loss = 1.007219
Variable usage = 100.00%
----------Iter = 42500----------
Loss = 1.007204
Variable usage = 100.00%
----------Iter = 42600----------
Loss = 1.007190
Variable usage = 100.00%
----------Iter = 42700----------
Loss = 1.007177
Variable usage = 100.00%
----------Iter = 42800----------
Loss = 1.007163
Variable usage = 100.00%
----------Iter = 42900----------
Loss = 1.007150
Variable usage = 100.00%
----------Iter = 43000----------
Loss = 1.007137
Variable usage = 100.00%
----------Iter = 43100----------
Loss = 1.007125
Variable usage = 100.00%
----------Iter = 43200----------
Loss = 1.007112
Variable usage = 100.00%
----------Iter = 43300----------
Loss = 1.007099
Variable usage = 100.00%
----------Iter = 43400----------
Loss = 1.007086
Variable usage = 100.00%
----------Iter = 43500----------
Loss = 1.007073
Variable usage = 100.00%
----------Iter = 43600----------
Loss = 1.007060
Variable usage = 100.00%
----------Iter = 43700----------
Loss = 1.007048
Variable usage = 100.00%
----------Iter = 43800----------
Loss = 1.007035
Variable usage = 100.00%
----------Iter = 43900----------
Loss = 1.007022
Variable usage = 100.00%
----------Iter = 44000----------
Loss = 1.007010
Variable usage = 100.00%
----------Iter = 44100----------
Loss = 1.006997
Variable usage = 100.00%
----------Iter = 44200----------
Loss = 1.006984
Variable usage = 100.00%
----------Iter = 44300----------
Loss = 1.006972
Variable usage = 100.00%
----------Iter = 44400----------
Loss = 1.006959
Variable usage = 100.00%
----------Iter = 44500----------
Loss = 1.006947
Variable usage = 100.00%
----------Iter = 44600----------
Loss = 1.006934
Variable usage = 100.00%
----------Iter = 44700----------
Loss = 1.006922
Variable usage = 100.00%
----------Iter = 44800----------
Loss = 1.006910
Variable usage = 100.00%
----------Iter = 44900----------
Loss = 1.006897
Variable usage = 100.00%
----------Iter = 45000----------
Loss = 1.006885
Variable usage = 100.00%
----------Iter = 45100----------
Loss = 1.006873
Variable usage = 100.00%
----------Iter = 45200----------
Loss = 1.006860
Variable usage = 100.00%
----------Iter = 45300----------
Loss = 1.006848
Variable usage = 100.00%
----------Iter = 45400----------
Loss = 1.006836
Variable usage = 100.00%
----------Iter = 45500----------
Loss = 1.006824
Variable usage = 100.00%
----------Iter = 45600----------
Loss = 1.006812
Variable usage = 100.00%
----------Iter = 45700----------
Loss = 1.006800
Variable usage = 100.00%
----------Iter = 45800----------
Loss = 1.006787
Variable usage = 100.00%
----------Iter = 45900----------
Loss = 1.006775
Variable usage = 100.00%
----------Iter = 46000----------
Loss = 1.006763
Variable usage = 100.00%
----------Iter = 46100----------
Loss = 1.006750
Variable usage = 100.00%
----------Iter = 46200----------
Loss = 1.006737
Variable usage = 100.00%
----------Iter = 46300----------
Loss = 1.006724
Variable usage = 100.00%
----------Iter = 46400----------
Loss = 1.006709
Variable usage = 100.00%
----------Iter = 46500----------
Loss = 1.006695
Variable usage = 100.00%
----------Iter = 46600----------
Loss = 1.006681
Variable usage = 100.00%
----------Iter = 46700----------
Loss = 1.006668
Variable usage = 100.00%
----------Iter = 46800----------
Loss = 1.006656
Variable usage = 100.00%
----------Iter = 46900----------
Loss = 1.006643
Variable usage = 100.00%
----------Iter = 47000----------
Loss = 1.006631
Variable usage = 100.00%
----------Iter = 47100----------
Loss = 1.006618
Variable usage = 100.00%
----------Iter = 47200----------
Loss = 1.006606
Variable usage = 100.00%
----------Iter = 47300----------
Loss = 1.006594
Variable usage = 100.00%
----------Iter = 47400----------
Loss = 1.006582
Variable usage = 100.00%
----------Iter = 47500----------
Loss = 1.006570
Variable usage = 100.00%
----------Iter = 47600----------
Loss = 1.006557
Variable usage = 100.00%
----------Iter = 47700----------
Loss = 1.006544
Variable usage = 100.00%
----------Iter = 47800----------
Loss = 1.006531
Variable usage = 100.00%
----------Iter = 47900----------
Loss = 1.006519
Variable usage = 100.00%
----------Iter = 48000----------
Loss = 1.006505
Variable usage = 100.00%
----------Iter = 48100----------
Loss = 1.006492
Variable usage = 100.00%
----------Iter = 48200----------
Loss = 1.006479
Variable usage = 100.00%
----------Iter = 48300----------
Loss = 1.006465
Variable usage = 100.00%
----------Iter = 48400----------
Loss = 1.006452
Variable usage = 100.00%
----------Iter = 48500----------
Loss = 1.006439
Variable usage = 100.00%
----------Iter = 48600----------
Loss = 1.006425
Variable usage = 100.00%
----------Iter = 48700----------
Loss = 1.006412
Variable usage = 100.00%
----------Iter = 48800----------
Loss = 1.006399
Variable usage = 100.00%
----------Iter = 48900----------
Loss = 1.006386
Variable usage = 100.00%
----------Iter = 49000----------
Loss = 1.006372
Variable usage = 100.00%
----------Iter = 49100----------
Loss = 1.006358
Variable usage = 100.00%
----------Iter = 49200----------
Loss = 1.006344
Variable usage = 100.00%
----------Iter = 49300----------
Loss = 1.006330
Variable usage = 100.00%
----------Iter = 49400----------
Loss = 1.006317
Variable usage = 100.00%
----------Iter = 49500----------
Loss = 1.006304
Variable usage = 100.00%
----------Iter = 49600----------
Loss = 1.006291
Variable usage = 100.00%
----------Iter = 49700----------
Loss = 1.006277
Variable usage = 100.00%
----------Iter = 49800----------
Loss = 1.006263
Variable usage = 100.00%
----------Iter = 49900----------
Loss = 1.006249
Variable usage = 100.00%
----------Iter = 50000----------
Loss = 1.006235
Variable usage = 100.00%
True variable usage = 50.00%
Estimated variable usage = 100.00%
Accuracy = 0.50%
Accuracy for experiment id 8 is 0.5
----------Iter = 100----------
Loss = 1.042274
Variable usage = 100.00%
----------Iter = 200----------
Loss = 1.040279
Variable usage = 100.00%
----------Iter = 300----------
Loss = 1.039099
Variable usage = 100.00%
----------Iter = 400----------
Loss = 1.038267
Variable usage = 100.00%
----------Iter = 500----------
Loss = 1.037638
Variable usage = 100.00%
----------Iter = 600----------
Loss = 1.037131
Variable usage = 100.00%
----------Iter = 700----------
Loss = 1.036705
Variable usage = 100.00%
----------Iter = 800----------
Loss = 1.036332
Variable usage = 100.00%
----------Iter = 900----------
Loss = 1.036003
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 1.035709
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 1.035439
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 1.035190
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 1.034962
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 1.034745
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 1.034537
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 1.034336
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 1.034139
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 1.033945
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 1.033755
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 1.033568
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 1.033384
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 1.033205
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 1.033028
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 1.032853
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 1.032684
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 1.032516
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 1.032355
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 1.032195
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 1.032040
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 1.031887
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 1.031735
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 1.031585
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 1.031436
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 1.031292
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 1.031148
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 1.031006
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 1.030865
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 1.030726
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 1.030588
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 1.030452
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 1.030316
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 1.030186
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 1.030058
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 1.029932
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 1.029809
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 1.029688
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 1.029569
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 1.029450
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 1.029336
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 1.029222
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 1.029111
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 1.029000
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 1.028892
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 1.028786
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 1.028682
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 1.028578
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 1.028473
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 1.028369
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 1.028265
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 1.028162
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 1.028061
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 1.027960
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 1.027859
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 1.027759
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 1.027660
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 1.027561
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 1.027464
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 1.027367
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 1.027272
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 1.027180
Variable usage = 100.00%
----------Iter = 7100----------
Loss = 1.027088
Variable usage = 100.00%
----------Iter = 7200----------
Loss = 1.026998
Variable usage = 100.00%
----------Iter = 7300----------
Loss = 1.026908
Variable usage = 100.00%
----------Iter = 7400----------
Loss = 1.026819
Variable usage = 100.00%
----------Iter = 7500----------
Loss = 1.026732
Variable usage = 100.00%
----------Iter = 7600----------
Loss = 1.026646
Variable usage = 100.00%
----------Iter = 7700----------
Loss = 1.026561
Variable usage = 100.00%
----------Iter = 7800----------
Loss = 1.026476
Variable usage = 100.00%
----------Iter = 7900----------
Loss = 1.026392
Variable usage = 100.00%
----------Iter = 8000----------
Loss = 1.026309
Variable usage = 100.00%
----------Iter = 8100----------
Loss = 1.026227
Variable usage = 100.00%
----------Iter = 8200----------
Loss = 1.026147
Variable usage = 100.00%
----------Iter = 8300----------
Loss = 1.026068
Variable usage = 100.00%
----------Iter = 8400----------
Loss = 1.025989
Variable usage = 100.00%
----------Iter = 8500----------
Loss = 1.025913
Variable usage = 100.00%
----------Iter = 8600----------
Loss = 1.025837
Variable usage = 100.00%
----------Iter = 8700----------
Loss = 1.025761
Variable usage = 100.00%
----------Iter = 8800----------
Loss = 1.025686
Variable usage = 100.00%
----------Iter = 8900----------
Loss = 1.025611
Variable usage = 100.00%
----------Iter = 9000----------
Loss = 1.025536
Variable usage = 100.00%
----------Iter = 9100----------
Loss = 1.025461
Variable usage = 100.00%
----------Iter = 9200----------
Loss = 1.025386
Variable usage = 100.00%
----------Iter = 9300----------
Loss = 1.025312
Variable usage = 100.00%
----------Iter = 9400----------
Loss = 1.025237
Variable usage = 100.00%
----------Iter = 9500----------
Loss = 1.025164
Variable usage = 100.00%
----------Iter = 9600----------
Loss = 1.025092
Variable usage = 100.00%
----------Iter = 9700----------
Loss = 1.025019
Variable usage = 100.00%
----------Iter = 9800----------
Loss = 1.024946
Variable usage = 100.00%
----------Iter = 9900----------
Loss = 1.024873
Variable usage = 100.00%
----------Iter = 10000----------
Loss = 1.024801
Variable usage = 100.00%
----------Iter = 10100----------
Loss = 1.024730
Variable usage = 100.00%
----------Iter = 10200----------
Loss = 1.024661
Variable usage = 100.00%
----------Iter = 10300----------
Loss = 1.024592
Variable usage = 100.00%
----------Iter = 10400----------
Loss = 1.024524
Variable usage = 100.00%
----------Iter = 10500----------
Loss = 1.024457
Variable usage = 100.00%
----------Iter = 10600----------
Loss = 1.024389
Variable usage = 100.00%
----------Iter = 10700----------
Loss = 1.024322
Variable usage = 100.00%
----------Iter = 10800----------
Loss = 1.024255
Variable usage = 100.00%
----------Iter = 10900----------
Loss = 1.024189
Variable usage = 100.00%
----------Iter = 11000----------
Loss = 1.024123
Variable usage = 100.00%
----------Iter = 11100----------
Loss = 1.024057
Variable usage = 100.00%
----------Iter = 11200----------
Loss = 1.023992
Variable usage = 100.00%
----------Iter = 11300----------
Loss = 1.023927
Variable usage = 100.00%
----------Iter = 11400----------
Loss = 1.023862
Variable usage = 100.00%
----------Iter = 11500----------
Loss = 1.023797
Variable usage = 100.00%
----------Iter = 11600----------
Loss = 1.023732
Variable usage = 100.00%
----------Iter = 11700----------
Loss = 1.023668
Variable usage = 100.00%
----------Iter = 11800----------
Loss = 1.023605
Variable usage = 100.00%
----------Iter = 11900----------
Loss = 1.023542
Variable usage = 100.00%
----------Iter = 12000----------
Loss = 1.023479
Variable usage = 100.00%
----------Iter = 12100----------
Loss = 1.023417
Variable usage = 100.00%
----------Iter = 12200----------
Loss = 1.023357
Variable usage = 100.00%
----------Iter = 12300----------
Loss = 1.023297
Variable usage = 100.00%
----------Iter = 12400----------
Loss = 1.023237
Variable usage = 100.00%
----------Iter = 12500----------
Loss = 1.023179
Variable usage = 100.00%
----------Iter = 12600----------
Loss = 1.023122
Variable usage = 100.00%
----------Iter = 12700----------
Loss = 1.023065
Variable usage = 100.00%
----------Iter = 12800----------
Loss = 1.023008
Variable usage = 100.00%
----------Iter = 12900----------
Loss = 1.022951
Variable usage = 100.00%
----------Iter = 13000----------
Loss = 1.022895
Variable usage = 100.00%
----------Iter = 13100----------
Loss = 1.022840
Variable usage = 100.00%
----------Iter = 13200----------
Loss = 1.022786
Variable usage = 100.00%
----------Iter = 13300----------
Loss = 1.022732
Variable usage = 100.00%
----------Iter = 13400----------
Loss = 1.022678
Variable usage = 100.00%
----------Iter = 13500----------
Loss = 1.022624
Variable usage = 100.00%
----------Iter = 13600----------
Loss = 1.022569
Variable usage = 100.00%
----------Iter = 13700----------
Loss = 1.022514
Variable usage = 100.00%
----------Iter = 13800----------
Loss = 1.022461
Variable usage = 100.00%
----------Iter = 13900----------
Loss = 1.022407
Variable usage = 100.00%
----------Iter = 14000----------
Loss = 1.022353
Variable usage = 100.00%
----------Iter = 14100----------
Loss = 1.022300
Variable usage = 100.00%
----------Iter = 14200----------
Loss = 1.022247
Variable usage = 100.00%
----------Iter = 14300----------
Loss = 1.022195
Variable usage = 100.00%
----------Iter = 14400----------
Loss = 1.022144
Variable usage = 100.00%
----------Iter = 14500----------
Loss = 1.022094
Variable usage = 100.00%
----------Iter = 14600----------
Loss = 1.022043
Variable usage = 100.00%
----------Iter = 14700----------
Loss = 1.021992
Variable usage = 100.00%
----------Iter = 14800----------
Loss = 1.021941
Variable usage = 100.00%
----------Iter = 14900----------
Loss = 1.021891
Variable usage = 100.00%
----------Iter = 15000----------
Loss = 1.021841
Variable usage = 100.00%
----------Iter = 15100----------
Loss = 1.021791
Variable usage = 100.00%
----------Iter = 15200----------
Loss = 1.021742
Variable usage = 100.00%
----------Iter = 15300----------
Loss = 1.021692
Variable usage = 100.00%
----------Iter = 15400----------
Loss = 1.021642
Variable usage = 100.00%
----------Iter = 15500----------
Loss = 1.021594
Variable usage = 100.00%
----------Iter = 15600----------
Loss = 1.021545
Variable usage = 100.00%
----------Iter = 15700----------
Loss = 1.021496
Variable usage = 100.00%
----------Iter = 15800----------
Loss = 1.021447
Variable usage = 100.00%
----------Iter = 15900----------
Loss = 1.021399
Variable usage = 100.00%
----------Iter = 16000----------
Loss = 1.021351
Variable usage = 100.00%
----------Iter = 16100----------
Loss = 1.021304
Variable usage = 100.00%
----------Iter = 16200----------
Loss = 1.021257
Variable usage = 100.00%
----------Iter = 16300----------
Loss = 1.021209
Variable usage = 100.00%
----------Iter = 16400----------
Loss = 1.021161
Variable usage = 100.00%
----------Iter = 16500----------
Loss = 1.021114
Variable usage = 100.00%
----------Iter = 16600----------
Loss = 1.021067
Variable usage = 100.00%
----------Iter = 16700----------
Loss = 1.021021
Variable usage = 100.00%
----------Iter = 16800----------
Loss = 1.020975
Variable usage = 100.00%
----------Iter = 16900----------
Loss = 1.020930
Variable usage = 100.00%
----------Iter = 17000----------
Loss = 1.020884
Variable usage = 100.00%
----------Iter = 17100----------
Loss = 1.020839
Variable usage = 100.00%
----------Iter = 17200----------
Loss = 1.020793
Variable usage = 100.00%
----------Iter = 17300----------
Loss = 1.020747
Variable usage = 100.00%
----------Iter = 17400----------
Loss = 1.020703
Variable usage = 100.00%
----------Iter = 17500----------
Loss = 1.020659
Variable usage = 100.00%
----------Iter = 17600----------
Loss = 1.020615
Variable usage = 100.00%
----------Iter = 17700----------
Loss = 1.020572
Variable usage = 100.00%
----------Iter = 17800----------
Loss = 1.020529
Variable usage = 100.00%
----------Iter = 17900----------
Loss = 1.020487
Variable usage = 100.00%
----------Iter = 18000----------
Loss = 1.020445
Variable usage = 100.00%
----------Iter = 18100----------
Loss = 1.020404
Variable usage = 100.00%
----------Iter = 18200----------
Loss = 1.020363
Variable usage = 100.00%
----------Iter = 18300----------
Loss = 1.020323
Variable usage = 100.00%
----------Iter = 18400----------
Loss = 1.020284
Variable usage = 100.00%
----------Iter = 18500----------
Loss = 1.020245
Variable usage = 100.00%
----------Iter = 18600----------
Loss = 1.020206
Variable usage = 100.00%
----------Iter = 18700----------
Loss = 1.020167
Variable usage = 100.00%
----------Iter = 18800----------
Loss = 1.020128
Variable usage = 100.00%
----------Iter = 18900----------
Loss = 1.020090
Variable usage = 100.00%
----------Iter = 19000----------
Loss = 1.020051
Variable usage = 100.00%
----------Iter = 19100----------
Loss = 1.020013
Variable usage = 100.00%
----------Iter = 19200----------
Loss = 1.019974
Variable usage = 100.00%
----------Iter = 19300----------
Loss = 1.019936
Variable usage = 100.00%
----------Iter = 19400----------
Loss = 1.019898
Variable usage = 100.00%
----------Iter = 19500----------
Loss = 1.019860
Variable usage = 100.00%
----------Iter = 19600----------
Loss = 1.019822
Variable usage = 100.00%
----------Iter = 19700----------
Loss = 1.019785
Variable usage = 100.00%
----------Iter = 19800----------
Loss = 1.019746
Variable usage = 100.00%
----------Iter = 19900----------
Loss = 1.019708
Variable usage = 100.00%
----------Iter = 20000----------
Loss = 1.019670
Variable usage = 100.00%
----------Iter = 20100----------
Loss = 1.019631
Variable usage = 100.00%
----------Iter = 20200----------
Loss = 1.019593
Variable usage = 100.00%
----------Iter = 20300----------
Loss = 1.019556
Variable usage = 100.00%
----------Iter = 20400----------
Loss = 1.019520
Variable usage = 100.00%
----------Iter = 20500----------
Loss = 1.019484
Variable usage = 100.00%
----------Iter = 20600----------
Loss = 1.019449
Variable usage = 100.00%
----------Iter = 20700----------
Loss = 1.019413
Variable usage = 100.00%
----------Iter = 20800----------
Loss = 1.019379
Variable usage = 100.00%
----------Iter = 20900----------
Loss = 1.019343
Variable usage = 100.00%
----------Iter = 21000----------
Loss = 1.019308
Variable usage = 100.00%
----------Iter = 21100----------
Loss = 1.019273
Variable usage = 100.00%
----------Iter = 21200----------
Loss = 1.019238
Variable usage = 100.00%
----------Iter = 21300----------
Loss = 1.019203
Variable usage = 100.00%
----------Iter = 21400----------
Loss = 1.019169
Variable usage = 100.00%
----------Iter = 21500----------
Loss = 1.019135
Variable usage = 100.00%
----------Iter = 21600----------
Loss = 1.019102
Variable usage = 100.00%
----------Iter = 21700----------
Loss = 1.019069
Variable usage = 100.00%
----------Iter = 21800----------
Loss = 1.019036
Variable usage = 100.00%
----------Iter = 21900----------
Loss = 1.019005
Variable usage = 100.00%
----------Iter = 22000----------
Loss = 1.018973
Variable usage = 100.00%
----------Iter = 22100----------
Loss = 1.018942
Variable usage = 100.00%
----------Iter = 22200----------
Loss = 1.018910
Variable usage = 100.00%
----------Iter = 22300----------
Loss = 1.018879
Variable usage = 100.00%
----------Iter = 22400----------
Loss = 1.018848
Variable usage = 100.00%
----------Iter = 22500----------
Loss = 1.018816
Variable usage = 100.00%
----------Iter = 22600----------
Loss = 1.018785
Variable usage = 100.00%
----------Iter = 22700----------
Loss = 1.018754
Variable usage = 100.00%
----------Iter = 22800----------
Loss = 1.018724
Variable usage = 100.00%
----------Iter = 22900----------
Loss = 1.018694
Variable usage = 100.00%
----------Iter = 23000----------
Loss = 1.018663
Variable usage = 100.00%
----------Iter = 23100----------
Loss = 1.018633
Variable usage = 100.00%
----------Iter = 23200----------
Loss = 1.018603
Variable usage = 100.00%
----------Iter = 23300----------
Loss = 1.018573
Variable usage = 100.00%
----------Iter = 23400----------
Loss = 1.018543
Variable usage = 100.00%
----------Iter = 23500----------
Loss = 1.018514
Variable usage = 100.00%
----------Iter = 23600----------
Loss = 1.018486
Variable usage = 100.00%
----------Iter = 23700----------
Loss = 1.018457
Variable usage = 100.00%
----------Iter = 23800----------
Loss = 1.018429
Variable usage = 100.00%
----------Iter = 23900----------
Loss = 1.018401
Variable usage = 100.00%
----------Iter = 24000----------
Loss = 1.018373
Variable usage = 100.00%
----------Iter = 24100----------
Loss = 1.018345
Variable usage = 100.00%
----------Iter = 24200----------
Loss = 1.018318
Variable usage = 100.00%
----------Iter = 24300----------
Loss = 1.018290
Variable usage = 100.00%
----------Iter = 24400----------
Loss = 1.018263
Variable usage = 100.00%
----------Iter = 24500----------
Loss = 1.018237
Variable usage = 100.00%
----------Iter = 24600----------
Loss = 1.018211
Variable usage = 100.00%
----------Iter = 24700----------
Loss = 1.018185
Variable usage = 100.00%
----------Iter = 24800----------
Loss = 1.018159
Variable usage = 100.00%
----------Iter = 24900----------
Loss = 1.018134
Variable usage = 100.00%
----------Iter = 25000----------
Loss = 1.018108
Variable usage = 100.00%
----------Iter = 25100----------
Loss = 1.018082
Variable usage = 100.00%
----------Iter = 25200----------
Loss = 1.018057
Variable usage = 100.00%
----------Iter = 25300----------
Loss = 1.018030
Variable usage = 100.00%
----------Iter = 25400----------
Loss = 1.018004
Variable usage = 100.00%
----------Iter = 25500----------
Loss = 1.017977
Variable usage = 100.00%
----------Iter = 25600----------
Loss = 1.017951
Variable usage = 100.00%
----------Iter = 25700----------
Loss = 1.017925
Variable usage = 100.00%
----------Iter = 25800----------
Loss = 1.017900
Variable usage = 100.00%
----------Iter = 25900----------
Loss = 1.017875
Variable usage = 100.00%
----------Iter = 26000----------
Loss = 1.017851
Variable usage = 100.00%
----------Iter = 26100----------
Loss = 1.017826
Variable usage = 100.00%
----------Iter = 26200----------
Loss = 1.017802
Variable usage = 100.00%
----------Iter = 26300----------
Loss = 1.017777
Variable usage = 100.00%
----------Iter = 26400----------
Loss = 1.017753
Variable usage = 100.00%
----------Iter = 26500----------
Loss = 1.017730
Variable usage = 100.00%
----------Iter = 26600----------
Loss = 1.017707
Variable usage = 100.00%
----------Iter = 26700----------
Loss = 1.017683
Variable usage = 100.00%
----------Iter = 26800----------
Loss = 1.017660
Variable usage = 100.00%
----------Iter = 26900----------
Loss = 1.017638
Variable usage = 100.00%
----------Iter = 27000----------
Loss = 1.017616
Variable usage = 100.00%
----------Iter = 27100----------
Loss = 1.017593
Variable usage = 100.00%
----------Iter = 27200----------
Loss = 1.017571
Variable usage = 100.00%
----------Iter = 27300----------
Loss = 1.017549
Variable usage = 100.00%
----------Iter = 27400----------
Loss = 1.017528
Variable usage = 100.00%
----------Iter = 27500----------
Loss = 1.017506
Variable usage = 100.00%
----------Iter = 27600----------
Loss = 1.017484
Variable usage = 100.00%
----------Iter = 27700----------
Loss = 1.017462
Variable usage = 100.00%
----------Iter = 27800----------
Loss = 1.017440
Variable usage = 100.00%
----------Iter = 27900----------
Loss = 1.017418
Variable usage = 100.00%
----------Iter = 28000----------
Loss = 1.017397
Variable usage = 100.00%
----------Iter = 28100----------
Loss = 1.017375
Variable usage = 100.00%
----------Iter = 28200----------
Loss = 1.017354
Variable usage = 100.00%
----------Iter = 28300----------
Loss = 1.017333
Variable usage = 100.00%
----------Iter = 28400----------
Loss = 1.017311
Variable usage = 100.00%
----------Iter = 28500----------
Loss = 1.017290
Variable usage = 100.00%
----------Iter = 28600----------
Loss = 1.017269
Variable usage = 100.00%
----------Iter = 28700----------
Loss = 1.017248
Variable usage = 100.00%
----------Iter = 28800----------
Loss = 1.017227
Variable usage = 100.00%
----------Iter = 28900----------
Loss = 1.017206
Variable usage = 100.00%
----------Iter = 29000----------
Loss = 1.017184
Variable usage = 100.00%
----------Iter = 29100----------
Loss = 1.017164
Variable usage = 100.00%
----------Iter = 29200----------
Loss = 1.017143
Variable usage = 100.00%
----------Iter = 29300----------
Loss = 1.017123
Variable usage = 100.00%
----------Iter = 29400----------
Loss = 1.017102
Variable usage = 100.00%
----------Iter = 29500----------
Loss = 1.017082
Variable usage = 100.00%
----------Iter = 29600----------
Loss = 1.017062
Variable usage = 100.00%
----------Iter = 29700----------
Loss = 1.017042
Variable usage = 100.00%
----------Iter = 29800----------
Loss = 1.017023
Variable usage = 100.00%
----------Iter = 29900----------
Loss = 1.017003
Variable usage = 100.00%
----------Iter = 30000----------
Loss = 1.016983
Variable usage = 100.00%
----------Iter = 30100----------
Loss = 1.016964
Variable usage = 100.00%
----------Iter = 30200----------
Loss = 1.016944
Variable usage = 100.00%
----------Iter = 30300----------
Loss = 1.016925
Variable usage = 100.00%
----------Iter = 30400----------
Loss = 1.016906
Variable usage = 100.00%
----------Iter = 30500----------
Loss = 1.016887
Variable usage = 100.00%
----------Iter = 30600----------
Loss = 1.016868
Variable usage = 100.00%
----------Iter = 30700----------
Loss = 1.016849
Variable usage = 100.00%
----------Iter = 30800----------
Loss = 1.016830
Variable usage = 100.00%
----------Iter = 30900----------
Loss = 1.016811
Variable usage = 100.00%
----------Iter = 31000----------
Loss = 1.016792
Variable usage = 100.00%
----------Iter = 31100----------
Loss = 1.016773
Variable usage = 100.00%
----------Iter = 31200----------
Loss = 1.016754
Variable usage = 100.00%
----------Iter = 31300----------
Loss = 1.016736
Variable usage = 100.00%
----------Iter = 31400----------
Loss = 1.016717
Variable usage = 100.00%
----------Iter = 31500----------
Loss = 1.016699
Variable usage = 100.00%
----------Iter = 31600----------
Loss = 1.016681
Variable usage = 100.00%
----------Iter = 31700----------
Loss = 1.016663
Variable usage = 100.00%
----------Iter = 31800----------
Loss = 1.016646
Variable usage = 100.00%
----------Iter = 31900----------
Loss = 1.016628
Variable usage = 100.00%
----------Iter = 32000----------
Loss = 1.016610
Variable usage = 100.00%
----------Iter = 32100----------
Loss = 1.016592
Variable usage = 100.00%
----------Iter = 32200----------
Loss = 1.016573
Variable usage = 100.00%
----------Iter = 32300----------
Loss = 1.016556
Variable usage = 100.00%
----------Iter = 32400----------
Loss = 1.016538
Variable usage = 100.00%
----------Iter = 32500----------
Loss = 1.016521
Variable usage = 100.00%
----------Iter = 32600----------
Loss = 1.016503
Variable usage = 100.00%
----------Iter = 32700----------
Loss = 1.016486
Variable usage = 100.00%
----------Iter = 32800----------
Loss = 1.016469
Variable usage = 100.00%
----------Iter = 32900----------
Loss = 1.016452
Variable usage = 100.00%
----------Iter = 33000----------
Loss = 1.016435
Variable usage = 100.00%
----------Iter = 33100----------
Loss = 1.016418
Variable usage = 100.00%
----------Iter = 33200----------
Loss = 1.016401
Variable usage = 100.00%
----------Iter = 33300----------
Loss = 1.016384
Variable usage = 100.00%
----------Iter = 33400----------
Loss = 1.016368
Variable usage = 100.00%
----------Iter = 33500----------
Loss = 1.016351
Variable usage = 100.00%
----------Iter = 33600----------
Loss = 1.016334
Variable usage = 100.00%
----------Iter = 33700----------
Loss = 1.016318
Variable usage = 100.00%
----------Iter = 33800----------
Loss = 1.016302
Variable usage = 100.00%
----------Iter = 33900----------
Loss = 1.016285
Variable usage = 100.00%
----------Iter = 34000----------
Loss = 1.016269
Variable usage = 100.00%
----------Iter = 34100----------
Loss = 1.016253
Variable usage = 100.00%
----------Iter = 34200----------
Loss = 1.016237
Variable usage = 100.00%
----------Iter = 34300----------
Loss = 1.016221
Variable usage = 100.00%
----------Iter = 34400----------
Loss = 1.016205
Variable usage = 100.00%
----------Iter = 34500----------
Loss = 1.016190
Variable usage = 100.00%
----------Iter = 34600----------
Loss = 1.016174
Variable usage = 100.00%
----------Iter = 34700----------
Loss = 1.016159
Variable usage = 100.00%
----------Iter = 34800----------
Loss = 1.016144
Variable usage = 100.00%
----------Iter = 34900----------
Loss = 1.016128
Variable usage = 100.00%
----------Iter = 35000----------
Loss = 1.016113
Variable usage = 100.00%
----------Iter = 35100----------
Loss = 1.016098
Variable usage = 100.00%
----------Iter = 35200----------
Loss = 1.016083
Variable usage = 100.00%
----------Iter = 35300----------
Loss = 1.016068
Variable usage = 100.00%
----------Iter = 35400----------
Loss = 1.016053
Variable usage = 100.00%
----------Iter = 35500----------
Loss = 1.016037
Variable usage = 100.00%
----------Iter = 35600----------
Loss = 1.016022
Variable usage = 100.00%
----------Iter = 35700----------
Loss = 1.016007
Variable usage = 100.00%
----------Iter = 35800----------
Loss = 1.015991
Variable usage = 100.00%
----------Iter = 35900----------
Loss = 1.015976
Variable usage = 100.00%
----------Iter = 36000----------
Loss = 1.015960
Variable usage = 100.00%
----------Iter = 36100----------
Loss = 1.015945
Variable usage = 100.00%
----------Iter = 36200----------
Loss = 1.015930
Variable usage = 100.00%
----------Iter = 36300----------
Loss = 1.015914
Variable usage = 100.00%
----------Iter = 36400----------
Loss = 1.015899
Variable usage = 100.00%
----------Iter = 36500----------
Loss = 1.015883
Variable usage = 100.00%
----------Iter = 36600----------
Loss = 1.015868
Variable usage = 100.00%
----------Iter = 36700----------
Loss = 1.015852
Variable usage = 100.00%
----------Iter = 36800----------
Loss = 1.015836
Variable usage = 100.00%
----------Iter = 36900----------
Loss = 1.015821
Variable usage = 100.00%
----------Iter = 37000----------
Loss = 1.015805
Variable usage = 100.00%
----------Iter = 37100----------
Loss = 1.015790
Variable usage = 100.00%
----------Iter = 37200----------
Loss = 1.015774
Variable usage = 100.00%
----------Iter = 37300----------
Loss = 1.015758
Variable usage = 100.00%
----------Iter = 37400----------
Loss = 1.015743
Variable usage = 100.00%
----------Iter = 37500----------
Loss = 1.015727
Variable usage = 100.00%
----------Iter = 37600----------
Loss = 1.015713
Variable usage = 100.00%
----------Iter = 37700----------
Loss = 1.015698
Variable usage = 100.00%
----------Iter = 37800----------
Loss = 1.015683
Variable usage = 100.00%
----------Iter = 37900----------
Loss = 1.015668
Variable usage = 100.00%
----------Iter = 38000----------
Loss = 1.015653
Variable usage = 100.00%
----------Iter = 38100----------
Loss = 1.015638
Variable usage = 100.00%
----------Iter = 38200----------
Loss = 1.015623
Variable usage = 100.00%
----------Iter = 38300----------
Loss = 1.015608
Variable usage = 100.00%
----------Iter = 38400----------
Loss = 1.015594
Variable usage = 100.00%
----------Iter = 38500----------
Loss = 1.015580
Variable usage = 100.00%
----------Iter = 38600----------
Loss = 1.015566
Variable usage = 100.00%
----------Iter = 38700----------
Loss = 1.015552
Variable usage = 100.00%
----------Iter = 38800----------
Loss = 1.015538
Variable usage = 100.00%
----------Iter = 38900----------
Loss = 1.015524
Variable usage = 100.00%
----------Iter = 39000----------
Loss = 1.015510
Variable usage = 100.00%
----------Iter = 39100----------
Loss = 1.015496
Variable usage = 100.00%
----------Iter = 39200----------
Loss = 1.015482
Variable usage = 100.00%
----------Iter = 39300----------
Loss = 1.015469
Variable usage = 100.00%
----------Iter = 39400----------
Loss = 1.015455
Variable usage = 100.00%
----------Iter = 39500----------
Loss = 1.015441
Variable usage = 100.00%
----------Iter = 39600----------
Loss = 1.015427
Variable usage = 100.00%
----------Iter = 39700----------
Loss = 1.015414
Variable usage = 100.00%
----------Iter = 39800----------
Loss = 1.015399
Variable usage = 100.00%
----------Iter = 39900----------
Loss = 1.015386
Variable usage = 100.00%
----------Iter = 40000----------
Loss = 1.015372
Variable usage = 100.00%
----------Iter = 40100----------
Loss = 1.015357
Variable usage = 100.00%
----------Iter = 40200----------
Loss = 1.015343
Variable usage = 100.00%
----------Iter = 40300----------
Loss = 1.015329
Variable usage = 100.00%
----------Iter = 40400----------
Loss = 1.015315
Variable usage = 100.00%
----------Iter = 40500----------
Loss = 1.015301
Variable usage = 100.00%
----------Iter = 40600----------
Loss = 1.015287
Variable usage = 100.00%
----------Iter = 40700----------
Loss = 1.015273
Variable usage = 100.00%
----------Iter = 40800----------
Loss = 1.015259
Variable usage = 100.00%
----------Iter = 40900----------
Loss = 1.015245
Variable usage = 100.00%
----------Iter = 41000----------
Loss = 1.015231
Variable usage = 100.00%
----------Iter = 41100----------
Loss = 1.015218
Variable usage = 100.00%
----------Iter = 41200----------
Loss = 1.015203
Variable usage = 100.00%
----------Iter = 41300----------
Loss = 1.015189
Variable usage = 100.00%
----------Iter = 41400----------
Loss = 1.015175
Variable usage = 100.00%
----------Iter = 41500----------
Loss = 1.015161
Variable usage = 100.00%
----------Iter = 41600----------
Loss = 1.015147
Variable usage = 100.00%
----------Iter = 41700----------
Loss = 1.015134
Variable usage = 100.00%
----------Iter = 41800----------
Loss = 1.015120
Variable usage = 100.00%
----------Iter = 41900----------
Loss = 1.015107
Variable usage = 100.00%
----------Iter = 42000----------
Loss = 1.015094
Variable usage = 100.00%
----------Iter = 42100----------
Loss = 1.015080
Variable usage = 100.00%
----------Iter = 42200----------
Loss = 1.015067
Variable usage = 100.00%
----------Iter = 42300----------
Loss = 1.015054
Variable usage = 100.00%
----------Iter = 42400----------
Loss = 1.015041
Variable usage = 100.00%
----------Iter = 42500----------
Loss = 1.015028
Variable usage = 100.00%
----------Iter = 42600----------
Loss = 1.015014
Variable usage = 100.00%
----------Iter = 42700----------
Loss = 1.015001
Variable usage = 100.00%
----------Iter = 42800----------
Loss = 1.014988
Variable usage = 100.00%
----------Iter = 42900----------
Loss = 1.014975
Variable usage = 100.00%
----------Iter = 43000----------
Loss = 1.014961
Variable usage = 100.00%
----------Iter = 43100----------
Loss = 1.014948
Variable usage = 100.00%
----------Iter = 43200----------
Loss = 1.014934
Variable usage = 100.00%
----------Iter = 43300----------
Loss = 1.014921
Variable usage = 100.00%
----------Iter = 43400----------
Loss = 1.014908
Variable usage = 100.00%
----------Iter = 43500----------
Loss = 1.014895
Variable usage = 100.00%
----------Iter = 43600----------
Loss = 1.014882
Variable usage = 100.00%
----------Iter = 43700----------
Loss = 1.014870
Variable usage = 100.00%
----------Iter = 43800----------
Loss = 1.014857
Variable usage = 100.00%
----------Iter = 43900----------
Loss = 1.014845
Variable usage = 100.00%
----------Iter = 44000----------
Loss = 1.014832
Variable usage = 100.00%
----------Iter = 44100----------
Loss = 1.014820
Variable usage = 100.00%
----------Iter = 44200----------
Loss = 1.014808
Variable usage = 100.00%
----------Iter = 44300----------
Loss = 1.014796
Variable usage = 100.00%
----------Iter = 44400----------
Loss = 1.014784
Variable usage = 100.00%
----------Iter = 44500----------
Loss = 1.014772
Variable usage = 100.00%
----------Iter = 44600----------
Loss = 1.014760
Variable usage = 100.00%
----------Iter = 44700----------
Loss = 1.014748
Variable usage = 100.00%
----------Iter = 44800----------
Loss = 1.014736
Variable usage = 100.00%
----------Iter = 44900----------
Loss = 1.014724
Variable usage = 100.00%
----------Iter = 45000----------
Loss = 1.014712
Variable usage = 100.00%
----------Iter = 45100----------
Loss = 1.014700
Variable usage = 100.00%
----------Iter = 45200----------
Loss = 1.014688
Variable usage = 100.00%
----------Iter = 45300----------
Loss = 1.014675
Variable usage = 100.00%
----------Iter = 45400----------
Loss = 1.014663
Variable usage = 100.00%
----------Iter = 45500----------
Loss = 1.014652
Variable usage = 100.00%
----------Iter = 45600----------
Loss = 1.014640
Variable usage = 100.00%
----------Iter = 45700----------
Loss = 1.014628
Variable usage = 100.00%
----------Iter = 45800----------
Loss = 1.014616
Variable usage = 100.00%
----------Iter = 45900----------
Loss = 1.014603
Variable usage = 100.00%
----------Iter = 46000----------
Loss = 1.014591
Variable usage = 100.00%
----------Iter = 46100----------
Loss = 1.014579
Variable usage = 100.00%
----------Iter = 46200----------
Loss = 1.014567
Variable usage = 100.00%
----------Iter = 46300----------
Loss = 1.014554
Variable usage = 100.00%
----------Iter = 46400----------
Loss = 1.014541
Variable usage = 100.00%
----------Iter = 46500----------
Loss = 1.014528
Variable usage = 100.00%
----------Iter = 46600----------
Loss = 1.014516
Variable usage = 100.00%
----------Iter = 46700----------
Loss = 1.014503
Variable usage = 100.00%
----------Iter = 46800----------
Loss = 1.014491
Variable usage = 100.00%
----------Iter = 46900----------
Loss = 1.014479
Variable usage = 100.00%
----------Iter = 47000----------
Loss = 1.014467
Variable usage = 100.00%
----------Iter = 47100----------
Loss = 1.014455
Variable usage = 100.00%
----------Iter = 47200----------
Loss = 1.014443
Variable usage = 100.00%
----------Iter = 47300----------
Loss = 1.014430
Variable usage = 100.00%
----------Iter = 47400----------
Loss = 1.014418
Variable usage = 100.00%
----------Iter = 47500----------
Loss = 1.014405
Variable usage = 100.00%
----------Iter = 47600----------
Loss = 1.014392
Variable usage = 100.00%
----------Iter = 47700----------
Loss = 1.014379
Variable usage = 100.00%
----------Iter = 47800----------
Loss = 1.014366
Variable usage = 100.00%
----------Iter = 47900----------
Loss = 1.014353
Variable usage = 100.00%
----------Iter = 48000----------
Loss = 1.014341
Variable usage = 100.00%
----------Iter = 48100----------
Loss = 1.014328
Variable usage = 100.00%
----------Iter = 48200----------
Loss = 1.014316
Variable usage = 100.00%
----------Iter = 48300----------
Loss = 1.014304
Variable usage = 100.00%
----------Iter = 48400----------
Loss = 1.014293
Variable usage = 100.00%
----------Iter = 48500----------
Loss = 1.014281
Variable usage = 100.00%
----------Iter = 48600----------
Loss = 1.014270
Variable usage = 100.00%
----------Iter = 48700----------
Loss = 1.014258
Variable usage = 100.00%
----------Iter = 48800----------
Loss = 1.014246
Variable usage = 100.00%
----------Iter = 48900----------
Loss = 1.014233
Variable usage = 100.00%
----------Iter = 49000----------
Loss = 1.014222
Variable usage = 100.00%
----------Iter = 49100----------
Loss = 1.014210
Variable usage = 100.00%
----------Iter = 49200----------
Loss = 1.014197
Variable usage = 100.00%
----------Iter = 49300----------
Loss = 1.014185
Variable usage = 100.00%
----------Iter = 49400----------
Loss = 1.014172
Variable usage = 100.00%
----------Iter = 49500----------
Loss = 1.014158
Variable usage = 100.00%
----------Iter = 49600----------
Loss = 1.014144
Variable usage = 100.00%
----------Iter = 49700----------
Loss = 1.014131
Variable usage = 100.00%
----------Iter = 49800----------
Loss = 1.014117
Variable usage = 100.00%
----------Iter = 49900----------
Loss = 1.014103
Variable usage = 100.00%
----------Iter = 50000----------
Loss = 1.014090
Variable usage = 100.00%
True variable usage = 50.00%
Estimated variable usage = 100.00%
Accuracy = 0.50%
Accuracy for experiment id 9 is 0.5
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 20720.4529622396 kl_train: -0.1902543042 mse_train: 0.1090550168 acc_train: 0.0208333333 nll_val: 18501.0565892270 kl_val: -0.3704467228 mse_val: 0.0973739832 acc_val: 0.0000000000 time: 0.6504s
Best model so far, saving...
Epoch: 0001 nll_train: 16368.5701904297 kl_train: -0.2138515754 mse_train: 0.0861503712 acc_train: 0.0000000000 nll_val: 16389.0881476151 kl_val: -0.2055826909 mse_val: 0.0862583585 acc_val: 0.0000000000 time: 0.5426s
Best model so far, saving...
Epoch: 0002 nll_train: 15520.7417399089 kl_train: -0.1750715331 mse_train: 0.0816881141 acc_train: 0.0000000000 nll_val: 15747.4958367599 kl_val: -0.2276397504 mse_val: 0.0828815581 acc_val: 0.0000000000 time: 0.5402s
Best model so far, saving...
Epoch: 0003 nll_train: 15231.6394856771 kl_train: -0.2329719147 mse_train: 0.0801665233 acc_train: 0.0000000000 nll_val: 15418.2216796875 kl_val: -0.2081296377 mse_val: 0.0811485358 acc_val: 0.1315789474 time: 0.5388s
Best model so far, saving...
Epoch: 0004 nll_train: 14955.4370117188 kl_train: -0.1933163448 mse_train: 0.0787128269 acc_train: 0.0000000000 nll_val: 14949.7965666118 kl_val: -0.1696398388 mse_val: 0.0786831410 acc_val: 0.3421052632 time: 0.5387s
Best model so far, saving...
Epoch: 0005 nll_train: 14800.2803548177 kl_train: -0.1454555811 mse_train: 0.0778962138 acc_train: 0.0208333333 nll_val: 14627.9586245888 kl_val: -0.1147956601 mse_val: 0.0769892543 acc_val: 0.3684210526 time: 0.5411s
Best model so far, saving...
Epoch: 0006 nll_train: 14759.9643147786 kl_train: -0.1093641470 mse_train: 0.0776840250 acc_train: 0.0625000000 nll_val: 14563.0567948191 kl_val: -0.0975912778 mse_val: 0.0766476675 acc_val: 0.4473684211 time: 0.5424s
Best model so far, saving...
Epoch: 0007 nll_train: 14713.9429931641 kl_train: -0.0959416550 mse_train: 0.0774418043 acc_train: 0.1458333333 nll_val: 14529.2835629112 kl_val: -0.0851987689 mse_val: 0.0764699147 acc_val: 0.4736842105 time: 0.5382s
Best model so far, saving...
Epoch: 0008 nll_train: 14689.1394449870 kl_train: -0.0883026975 mse_train: 0.0773112606 acc_train: 0.2708333333 nll_val: 14501.6388260691 kl_val: -0.0812536281 mse_val: 0.0763244139 acc_val: 0.5000000000 time: 0.5379s
Best model so far, saving...
Epoch: 0009 nll_train: 14668.5985921224 kl_train: -0.0804290537 mse_train: 0.0772031502 acc_train: 0.3125000000 nll_val: 14494.2008120888 kl_val: -0.0763342959 mse_val: 0.0762852677 acc_val: 0.4473684211 time: 0.5394s
Best model so far, saving...
Epoch: 0010 nll_train: 14660.0006510417 kl_train: -0.0806553331 mse_train: 0.0771579004 acc_train: 0.3333333333 nll_val: 14435.1788651316 kl_val: -0.0730956152 mse_val: 0.0759746252 acc_val: 0.5263157895 time: 0.5414s
Best model so far, saving...
Epoch: 0011 nll_train: 14642.9405110677 kl_train: -0.0734974501 mse_train: 0.0770681094 acc_train: 0.4375000000 nll_val: 14444.3546977796 kl_val: -0.0710833116 mse_val: 0.0760229199 acc_val: 0.5263157895 time: 0.5408s
Epoch: 0012 nll_train: 14639.9313557943 kl_train: -0.0738574319 mse_train: 0.0770522716 acc_train: 0.4375000000 nll_val: 14430.2749280428 kl_val: -0.0697324092 mse_val: 0.0759488144 acc_val: 0.5526315789 time: 0.5373s
Best model so far, saving...
Epoch: 0013 nll_train: 14638.4898681641 kl_train: -0.0729186720 mse_train: 0.0770446848 acc_train: 0.4375000000 nll_val: 14438.8741776316 kl_val: -0.0700609547 mse_val: 0.0759940743 acc_val: 0.5263157895 time: 0.5362s
Epoch: 0014 nll_train: 14626.5268554688 kl_train: -0.0752793776 mse_train: 0.0769817208 acc_train: 0.5000000000 nll_val: 14406.2436266447 kl_val: -0.0714434266 mse_val: 0.0758223353 acc_val: 0.5789473684 time: 0.5393s
Best model so far, saving...
Epoch: 0015 nll_train: 14606.3051350911 kl_train: -0.0756379835 mse_train: 0.0768752910 acc_train: 0.5625000000 nll_val: 14404.8510485197 kl_val: -0.0727543194 mse_val: 0.0758150051 acc_val: 0.5789473684 time: 0.5377s
Best model so far, saving...
Epoch: 0016 nll_train: 14611.9171956380 kl_train: -0.0792302036 mse_train: 0.0769048266 acc_train: 0.6250000000 nll_val: 14398.0924650493 kl_val: -0.0803979024 mse_val: 0.0757794329 acc_val: 0.5789473684 time: 0.5378s
Best model so far, saving...
Epoch: 0017 nll_train: 14599.4552408854 kl_train: -0.0808381528 mse_train: 0.0768392403 acc_train: 0.5000000000 nll_val: 14389.4441303454 kl_val: -0.0830922437 mse_val: 0.0757339150 acc_val: 0.5526315789 time: 0.5377s
Best model so far, saving...
Epoch: 0018 nll_train: 14594.3435465495 kl_train: -0.0822562454 mse_train: 0.0768123361 acc_train: 0.5416666667 nll_val: 14377.0531969572 kl_val: -0.0863807337 mse_val: 0.0756687012 acc_val: 0.5789473684 time: 0.5382s
Best model so far, saving...
Epoch: 0019 nll_train: 14589.3554687500 kl_train: -0.0921937462 mse_train: 0.0767860810 acc_train: 0.5208333333 nll_val: 14374.0617804276 kl_val: -0.0927798528 mse_val: 0.0756529558 acc_val: 0.5526315789 time: 0.5414s
Best model so far, saving...
Epoch: 0020 nll_train: 14573.3333740234 kl_train: -0.0966841918 mse_train: 0.0767017549 acc_train: 0.4583333333 nll_val: 14362.5597245066 kl_val: -0.1086313179 mse_val: 0.0755924190 acc_val: 0.5526315789 time: 0.5422s
Best model so far, saving...
Epoch: 0021 nll_train: 14575.0124918620 kl_train: -0.1023934921 mse_train: 0.0767105934 acc_train: 0.4791666667 nll_val: 14370.9261924342 kl_val: -0.1037615879 mse_val: 0.0756364524 acc_val: 0.5526315789 time: 0.5388s
Epoch: 0022 nll_train: 14565.9420979818 kl_train: -0.0949808812 mse_train: 0.0766628540 acc_train: 0.4583333333 nll_val: 14346.5710834704 kl_val: -0.0948886773 mse_val: 0.0755082671 acc_val: 0.5526315789 time: 0.5379s
Best model so far, saving...
Epoch: 0023 nll_train: 14571.6862792969 kl_train: -0.0858162922 mse_train: 0.0766930864 acc_train: 0.4375000000 nll_val: 14378.0112047697 kl_val: -0.0844885754 mse_val: 0.0756737425 acc_val: 0.5263157895 time: 0.5381s
Epoch: 0024 nll_train: 14567.6686197917 kl_train: -0.0808079856 mse_train: 0.0766719397 acc_train: 0.5208333333 nll_val: 14393.5924650493 kl_val: -0.0816747730 mse_val: 0.0757557495 acc_val: 0.5789473684 time: 0.5383s
Epoch: 0025 nll_train: 14574.9529215495 kl_train: -0.0807538011 mse_train: 0.0767102786 acc_train: 0.4583333333 nll_val: 14383.4740953947 kl_val: -0.0818741898 mse_val: 0.0757024967 acc_val: 0.5789473684 time: 0.5342s
Epoch: 0026 nll_train: 14576.3942057292 kl_train: -0.0900647387 mse_train: 0.0767178641 acc_train: 0.4375000000 nll_val: 14372.8557257401 kl_val: -0.0918483762 mse_val: 0.0756466110 acc_val: 0.5263157895 time: 0.5345s
Epoch: 0027 nll_train: 14563.8868815104 kl_train: -0.0909877702 mse_train: 0.0766520364 acc_train: 0.3958333333 nll_val: 14335.3916015625 kl_val: -0.0995769016 mse_val: 0.0754494283 acc_val: 0.5000000000 time: 0.5371s
Best model so far, saving...
Epoch: 0028 nll_train: 14560.7711588542 kl_train: -0.0938190908 mse_train: 0.0766356398 acc_train: 0.4375000000 nll_val: 14368.4483963816 kl_val: -0.0925914439 mse_val: 0.0756234131 acc_val: 0.5789473684 time: 0.5383s
Epoch: 0029 nll_train: 14547.2549641927 kl_train: -0.0935339731 mse_train: 0.0765644992 acc_train: 0.5625000000 nll_val: 14374.1369243421 kl_val: -0.1035282777 mse_val: 0.0756533530 acc_val: 0.5789473684 time: 0.5367s
Epoch: 0030 nll_train: 14554.6024576823 kl_train: -0.0993287948 mse_train: 0.0766031716 acc_train: 0.6041666667 nll_val: 14378.2917351974 kl_val: -0.0990816406 mse_val: 0.0756752193 acc_val: 0.5789473684 time: 0.5345s
Epoch: 0031 nll_train: 14553.9178873698 kl_train: -0.1086356686 mse_train: 0.0765995703 acc_train: 0.5000000000 nll_val: 14378.6883223684 kl_val: -0.1141928968 mse_val: 0.0756773074 acc_val: 0.5789473684 time: 0.5368s
Epoch: 0032 nll_train: 14548.1969401042 kl_train: -0.1091598167 mse_train: 0.0765694571 acc_train: 0.5000000000 nll_val: 14369.6837993421 kl_val: -0.1204898926 mse_val: 0.0756299158 acc_val: 0.5526315789 time: 0.5379s
Epoch: 0033 nll_train: 14544.8321533203 kl_train: -0.1122180670 mse_train: 0.0765517478 acc_train: 0.5833333333 nll_val: 14355.8129111842 kl_val: -0.1092196766 mse_val: 0.0755569104 acc_val: 0.6315789474 time: 0.5360s
Epoch: 0034 nll_train: 14545.0836181641 kl_train: -0.1141502773 mse_train: 0.0765530729 acc_train: 0.6250000000 nll_val: 14328.0458470395 kl_val: -0.1133638585 mse_val: 0.0754107700 acc_val: 0.5263157895 time: 0.5383s
Best model so far, saving...
Epoch: 0035 nll_train: 14543.6418863932 kl_train: -0.1113402443 mse_train: 0.0765454844 acc_train: 0.5000000000 nll_val: 14328.2533922697 kl_val: -0.1137362573 mse_val: 0.0754118605 acc_val: 0.5526315789 time: 0.5355s
Epoch: 0036 nll_train: 14540.1682128906 kl_train: -0.1200496024 mse_train: 0.0765272009 acc_train: 0.5833333333 nll_val: 14336.3819901316 kl_val: -0.1203347966 mse_val: 0.0754546421 acc_val: 0.5263157895 time: 0.5404s
Epoch: 0037 nll_train: 14547.2265218099 kl_train: -0.1088924808 mse_train: 0.0765643494 acc_train: 0.6458333333 nll_val: 14354.6482833059 kl_val: -0.1059334251 mse_val: 0.0755507809 acc_val: 0.6578947368 time: 0.5381s
Epoch: 0038 nll_train: 14534.3404134115 kl_train: -0.1230153056 mse_train: 0.0764965294 acc_train: 0.4791666667 nll_val: 14298.7527754934 kl_val: -0.1233351266 mse_val: 0.0752565927 acc_val: 0.5263157895 time: 0.5375s
Best model so far, saving...
Epoch: 0039 nll_train: 14535.0108235677 kl_train: -0.1118856377 mse_train: 0.0765000568 acc_train: 0.4583333333 nll_val: 14347.8776212993 kl_val: -0.1102471446 mse_val: 0.0755151459 acc_val: 0.5526315789 time: 0.5372s
Epoch: 0040 nll_train: 14528.7072347005 kl_train: -0.1113235491 mse_train: 0.0764668803 acc_train: 0.6458333333 nll_val: 14335.9767166941 kl_val: -0.1047312382 mse_val: 0.0754525116 acc_val: 0.6578947368 time: 0.5365s
Epoch: 0041 nll_train: 14531.1023763021 kl_train: -0.1026592692 mse_train: 0.0764794868 acc_train: 0.6458333333 nll_val: 14329.7752364309 kl_val: -0.1022400495 mse_val: 0.0754198699 acc_val: 0.6578947368 time: 0.5365s
Epoch: 0042 nll_train: 14516.1691080729 kl_train: -0.1083407036 mse_train: 0.0764008897 acc_train: 0.6041666667 nll_val: 14306.2714329770 kl_val: -0.1116511120 mse_val: 0.0752961671 acc_val: 0.5789473684 time: 0.5359s
Epoch: 0043 nll_train: 14525.7790120443 kl_train: -0.1073558675 mse_train: 0.0764514684 acc_train: 0.6041666667 nll_val: 14381.4691097862 kl_val: -0.1167335389 mse_val: 0.0756919443 acc_val: 0.6052631579 time: 0.5357s
Epoch: 0044 nll_train: 14520.0877685547 kl_train: -0.1212147372 mse_train: 0.0764215152 acc_train: 0.6458333333 nll_val: 14338.1816406250 kl_val: -0.1266332441 mse_val: 0.0754641142 acc_val: 0.6052631579 time: 0.5422s
Epoch: 0045 nll_train: 14514.4049479167 kl_train: -0.1159310210 mse_train: 0.0763916057 acc_train: 0.6250000000 nll_val: 14358.3773643092 kl_val: -0.1125385255 mse_val: 0.0755704100 acc_val: 0.6315789474 time: 0.5384s
Epoch: 0046 nll_train: 14528.0684407552 kl_train: -0.1135272381 mse_train: 0.0764635188 acc_train: 0.7500000000 nll_val: 14397.8571134868 kl_val: -0.1166940116 mse_val: 0.0757781953 acc_val: 0.6842105263 time: 0.5376s
Epoch: 0047 nll_train: 14525.6094563802 kl_train: -0.1200507445 mse_train: 0.0764505769 acc_train: 0.6250000000 nll_val: 14307.1831825658 kl_val: -0.1103237812 mse_val: 0.0753009649 acc_val: 0.6052631579 time: 0.5370s
Epoch: 0048 nll_train: 14523.3272298177 kl_train: -0.1139882933 mse_train: 0.0764385653 acc_train: 0.6041666667 nll_val: 14354.4250102796 kl_val: -0.1054670638 mse_val: 0.0755496037 acc_val: 0.5789473684 time: 0.5357s
Epoch: 0049 nll_train: 14522.5225016276 kl_train: -0.1069934163 mse_train: 0.0764343287 acc_train: 0.6250000000 nll_val: 14305.6277754934 kl_val: -0.1020923678 mse_val: 0.0752927802 acc_val: 0.6052631579 time: 0.5386s
Epoch: 0050 nll_train: 14533.9440511068 kl_train: -0.1145570877 mse_train: 0.0764944431 acc_train: 0.6250000000 nll_val: 14353.8848170230 kl_val: -0.1127663584 mse_val: 0.0755467619 acc_val: 0.5526315789 time: 0.5359s
Epoch: 0051 nll_train: 14512.8446451823 kl_train: -0.1082487591 mse_train: 0.0763833943 acc_train: 0.7291666667 nll_val: 14316.6466385691 kl_val: -0.1024252929 mse_val: 0.0753507744 acc_val: 0.6052631579 time: 0.5368s
Epoch: 0052 nll_train: 14510.8314208984 kl_train: -0.1268452912 mse_train: 0.0763727960 acc_train: 0.5833333333 nll_val: 14337.0059621711 kl_val: -0.1364317136 mse_val: 0.0754579263 acc_val: 0.5000000000 time: 0.5351s
Epoch: 0053 nll_train: 14511.1225179036 kl_train: -0.1193117493 mse_train: 0.0763743299 acc_train: 0.7083333333 nll_val: 14344.7621299342 kl_val: -0.1201046752 mse_val: 0.0754987484 acc_val: 0.6315789474 time: 0.5365s
Epoch: 0054 nll_train: 14512.0545654297 kl_train: -0.1272655527 mse_train: 0.0763792358 acc_train: 0.6458333333 nll_val: 14364.4603207237 kl_val: -0.1415343579 mse_val: 0.0756024220 acc_val: 0.6052631579 time: 0.5365s
Epoch: 0055 nll_train: 14522.0832926432 kl_train: -0.1430630367 mse_train: 0.0764320167 acc_train: 0.5833333333 nll_val: 14483.2646998355 kl_val: -0.1371711242 mse_val: 0.0762277100 acc_val: 0.5263157895 time: 0.5404s
Epoch: 0056 nll_train: 14524.4187011719 kl_train: -0.1303114025 mse_train: 0.0764443098 acc_train: 0.7500000000 nll_val: 14370.2801192434 kl_val: -0.1006632684 mse_val: 0.0756330525 acc_val: 0.6578947368 time: 0.5355s
Epoch: 0057 nll_train: 14515.2039388021 kl_train: -0.1288511722 mse_train: 0.0763958109 acc_train: 0.5208333333 nll_val: 14333.9148334704 kl_val: -0.1355579511 mse_val: 0.0754416581 acc_val: 0.5263157895 time: 0.5343s
Epoch: 0058 nll_train: 14483.3274739583 kl_train: -0.1389171823 mse_train: 0.0762280412 acc_train: 0.7083333333 nll_val: 14327.2804790296 kl_val: -0.1282711421 mse_val: 0.0754067408 acc_val: 0.6315789474 time: 0.5367s
Epoch: 0059 nll_train: 14500.4369303385 kl_train: -0.1350524935 mse_train: 0.0763180883 acc_train: 0.8125000000 nll_val: 14509.3279194079 kl_val: -0.1478794829 mse_val: 0.0763648827 acc_val: 0.6578947368 time: 0.5388s
Epoch: 0060 nll_train: 14506.3823242188 kl_train: -0.1412356449 mse_train: 0.0763493826 acc_train: 0.6666666667 nll_val: 14378.3556229441 kl_val: -0.1278083007 mse_val: 0.0756755565 acc_val: 0.6052631579 time: 0.5357s
Epoch: 0061 nll_train: 14503.8615315755 kl_train: -0.1339619228 mse_train: 0.0763361134 acc_train: 0.7083333333 nll_val: 14451.7866981908 kl_val: -0.1250439314 mse_val: 0.0760620358 acc_val: 0.6315789474 time: 0.5382s
Epoch: 0062 nll_train: 14495.9528401693 kl_train: -0.1352958080 mse_train: 0.0762944894 acc_train: 0.6250000000 nll_val: 14426.4133429276 kl_val: -0.1466548039 mse_val: 0.0759284912 acc_val: 0.6052631579 time: 0.5382s
Epoch: 0063 nll_train: 14496.5503336589 kl_train: -0.1464568377 mse_train: 0.0762976335 acc_train: 0.7291666667 nll_val: 14396.5821340461 kl_val: -0.1374555183 mse_val: 0.0757714847 acc_val: 0.6578947368 time: 0.5366s
Epoch: 0064 nll_train: 14486.6468912760 kl_train: -0.1342880673 mse_train: 0.0762455117 acc_train: 0.7708333333 nll_val: 14365.9230057566 kl_val: -0.1299643858 mse_val: 0.0756101216 acc_val: 0.6315789474 time: 0.5357s
Epoch: 0065 nll_train: 14496.8645833333 kl_train: -0.1287807003 mse_train: 0.0762992882 acc_train: 0.8333333333 nll_val: 14450.7030222039 kl_val: -0.1357283643 mse_val: 0.0760563314 acc_val: 0.6578947368 time: 0.5357s
Epoch: 0066 nll_train: 14505.0323893229 kl_train: -0.1409450164 mse_train: 0.0763422749 acc_train: 0.7916666667 nll_val: 14514.9183285362 kl_val: -0.1345495229 mse_val: 0.0763943086 acc_val: 0.6842105263 time: 0.5359s
Epoch: 0067 nll_train: 14521.0989583333 kl_train: -0.1475173536 mse_train: 0.0764268368 acc_train: 0.8333333333 nll_val: 14486.2529296875 kl_val: -0.1181104760 mse_val: 0.0762434370 acc_val: 0.7631578947 time: 0.5358s
Epoch: 0068 nll_train: 14489.8393961589 kl_train: -0.1349748038 mse_train: 0.0762623128 acc_train: 0.8750000000 nll_val: 14475.6101973684 kl_val: -0.1484547371 mse_val: 0.0761874224 acc_val: 0.7631578947 time: 0.5361s
Epoch: 0069 nll_train: 14498.0653483073 kl_train: -0.1540047989 mse_train: 0.0763056078 acc_train: 0.7291666667 nll_val: 14421.4581620066 kl_val: -0.1298082698 mse_val: 0.0759024118 acc_val: 0.5789473684 time: 0.5360s
Epoch: 0070 nll_train: 14492.0778808594 kl_train: -0.1361684312 mse_train: 0.0762740948 acc_train: 0.8125000000 nll_val: 14425.8688836349 kl_val: -0.1440728652 mse_val: 0.0759256266 acc_val: 0.6842105263 time: 0.5366s
Epoch: 0071 nll_train: 14484.3312174479 kl_train: -0.1500517214 mse_train: 0.0762333235 acc_train: 0.8750000000 nll_val: 14441.2127364309 kl_val: -0.1305770666 mse_val: 0.0760063811 acc_val: 0.7105263158 time: 0.5368s
Epoch: 0072 nll_train: 14484.4180094401 kl_train: -0.1434373250 mse_train: 0.0762337794 acc_train: 0.8333333333 nll_val: 14493.3760279605 kl_val: -0.1577007637 mse_val: 0.0762809252 acc_val: 0.7631578947 time: 0.5375s
Epoch: 0073 nll_train: 14507.3930257161 kl_train: -0.1589697016 mse_train: 0.0763546995 acc_train: 0.8333333333 nll_val: 14465.8360916941 kl_val: -0.1391103299 mse_val: 0.0761359797 acc_val: 0.6578947368 time: 0.5380s
Epoch: 0074 nll_train: 14493.3627522786 kl_train: -0.1485424868 mse_train: 0.0762808565 acc_train: 0.8333333333 nll_val: 14455.8355263158 kl_val: -0.1275955799 mse_val: 0.0760833452 acc_val: 0.6315789474 time: 0.5371s
Epoch: 0075 nll_train: 14500.4654947917 kl_train: -0.1328975900 mse_train: 0.0763182403 acc_train: 0.7500000000 nll_val: 14486.4850431743 kl_val: -0.1257749710 mse_val: 0.0762446566 acc_val: 0.6578947368 time: 0.5376s
Epoch: 0076 nll_train: 14493.8890787760 kl_train: -0.1535356153 mse_train: 0.0762836264 acc_train: 0.6458333333 nll_val: 14461.6268503289 kl_val: -0.1405675388 mse_val: 0.0761138260 acc_val: 0.5789473684 time: 0.5372s
Epoch: 0077 nll_train: 14482.0517985026 kl_train: -0.1501749400 mse_train: 0.0762213250 acc_train: 0.7083333333 nll_val: 14492.5694387336 kl_val: -0.1556121978 mse_val: 0.0762766827 acc_val: 0.6052631579 time: 0.5354s
Epoch: 0078 nll_train: 14503.7697347005 kl_train: -0.1650232412 mse_train: 0.0763356308 acc_train: 0.8541666667 nll_val: 14524.6717208059 kl_val: -0.1481690368 mse_val: 0.0764456411 acc_val: 0.7105263158 time: 0.5348s
Epoch: 0079 nll_train: 14490.9005533854 kl_train: -0.1424150343 mse_train: 0.0762678979 acc_train: 0.8541666667 nll_val: 14497.8341899671 kl_val: -0.1222596494 mse_val: 0.0763043910 acc_val: 0.7894736842 time: 0.5373s
Epoch: 0080 nll_train: 14488.0185139974 kl_train: -0.1459601478 mse_train: 0.0762527290 acc_train: 0.8958333333 nll_val: 14496.9658203125 kl_val: -0.1273857896 mse_val: 0.0762998215 acc_val: 0.8157894737 time: 0.5367s
Epoch: 0081 nll_train: 14481.6601562500 kl_train: -0.1434190695 mse_train: 0.0762192650 acc_train: 0.7916666667 nll_val: 14477.5813116776 kl_val: -0.1445886132 mse_val: 0.0761977956 acc_val: 0.6578947368 time: 0.5361s
Epoch: 0082 nll_train: 14490.7076009115 kl_train: -0.1457576134 mse_train: 0.0762668848 acc_train: 0.8750000000 nll_val: 14539.3363486842 kl_val: -0.1455746956 mse_val: 0.0765228236 acc_val: 0.7631578947 time: 0.5368s
Epoch: 0083 nll_train: 14485.8222656250 kl_train: -0.1678440074 mse_train: 0.0762411693 acc_train: 0.8125000000 nll_val: 14632.8252981086 kl_val: -0.1465138017 mse_val: 0.0770148721 acc_val: 0.7105263158 time: 0.5350s
Epoch: 0084 nll_train: 14486.8737386068 kl_train: -0.1480909847 mse_train: 0.0762467038 acc_train: 0.8125000000 nll_val: 14515.2945106908 kl_val: -0.1353938254 mse_val: 0.0763962884 acc_val: 0.6842105263 time: 0.5369s
Epoch: 0085 nll_train: 14502.8183593750 kl_train: -0.1312429054 mse_train: 0.0763306245 acc_train: 0.8958333333 nll_val: 14526.8095189145 kl_val: -0.1255920220 mse_val: 0.0764568950 acc_val: 0.7631578947 time: 0.5394s
Epoch: 0086 nll_train: 14536.1106363932 kl_train: -0.1421679646 mse_train: 0.0765058443 acc_train: 0.9375000000 nll_val: 14457.1845703125 kl_val: -0.1138713419 mse_val: 0.0760904453 acc_val: 0.7894736842 time: 0.5367s
Epoch: 0087 nll_train: 14498.1899007161 kl_train: -0.1493112187 mse_train: 0.0763062639 acc_train: 0.8125000000 nll_val: 14458.5795127467 kl_val: -0.1732483792 mse_val: 0.0760977876 acc_val: 0.6315789474 time: 0.5348s
Epoch: 0088 nll_train: 14474.5062255859 kl_train: -0.1690464032 mse_train: 0.0761816123 acc_train: 0.8750000000 nll_val: 14510.6046463816 kl_val: -0.1451573662 mse_val: 0.0763716047 acc_val: 0.8157894737 time: 0.5365s
Epoch: 0089 nll_train: 14484.1123860677 kl_train: -0.1578029003 mse_train: 0.0762321707 acc_train: 0.9791666667 nll_val: 14608.0295024671 kl_val: -0.1575803082 mse_val: 0.0768843662 acc_val: 0.7631578947 time: 0.5376s
Epoch: 0090 nll_train: 14500.8257242839 kl_train: -0.1643039317 mse_train: 0.0763201349 acc_train: 0.8750000000 nll_val: 14608.0942125822 kl_val: -0.1473285924 mse_val: 0.0768847066 acc_val: 0.7368421053 time: 0.5440s
Epoch: 0091 nll_train: 14472.6601155599 kl_train: -0.1582115740 mse_train: 0.0761718955 acc_train: 0.8958333333 nll_val: 14567.6197060033 kl_val: -0.1628433529 mse_val: 0.0766716847 acc_val: 0.7631578947 time: 0.5359s
Epoch: 0092 nll_train: 14486.1965332031 kl_train: -0.1512746469 mse_train: 0.0762431414 acc_train: 0.9375000000 nll_val: 14537.3984375000 kl_val: -0.1413962339 mse_val: 0.0765126242 acc_val: 0.8157894737 time: 0.5395s
Epoch: 0093 nll_train: 14491.0142008464 kl_train: -0.1582293510 mse_train: 0.0762684969 acc_train: 0.9375000000 nll_val: 14624.0277035362 kl_val: -0.1566049629 mse_val: 0.0769685652 acc_val: 0.7368421053 time: 0.5345s
Epoch: 0094 nll_train: 14493.8704427083 kl_train: -0.1529380726 mse_train: 0.0762835285 acc_train: 0.9375000000 nll_val: 14656.6299342105 kl_val: -0.1334121851 mse_val: 0.0771401572 acc_val: 0.7894736842 time: 0.5406s
Epoch: 0095 nll_train: 14501.9667154948 kl_train: -0.1629028711 mse_train: 0.0763261400 acc_train: 0.7500000000 nll_val: 14674.6260793586 kl_val: -0.1490373141 mse_val: 0.0772348752 acc_val: 0.7631578947 time: 0.5475s
Epoch: 0096 nll_train: 14483.9121093750 kl_train: -0.1539992442 mse_train: 0.0762311175 acc_train: 0.9791666667 nll_val: 14574.1138980263 kl_val: -0.1467778698 mse_val: 0.0767058628 acc_val: 0.8157894737 time: 0.5508s
Epoch: 0097 nll_train: 14485.6286214193 kl_train: -0.1625810384 mse_train: 0.0762401524 acc_train: 0.9166666667 nll_val: 14695.3659539474 kl_val: -0.1547127711 mse_val: 0.0773440321 acc_val: 0.7368421053 time: 0.5389s
Epoch: 0098 nll_train: 14481.9124348958 kl_train: -0.1604562905 mse_train: 0.0762205923 acc_train: 0.9166666667 nll_val: 14481.7999074836 kl_val: -0.1407996800 mse_val: 0.0762200003 acc_val: 0.7894736842 time: 0.5364s
Epoch: 0099 nll_train: 14478.8423665365 kl_train: -0.1573535036 mse_train: 0.0762044347 acc_train: 0.9166666667 nll_val: 14613.7226048520 kl_val: -0.1509600827 mse_val: 0.0769143297 acc_val: 0.8157894737 time: 0.5360s
Epoch: 0100 nll_train: 14483.2500813802 kl_train: -0.1626063449 mse_train: 0.0762276333 acc_train: 0.9166666667 nll_val: 14584.7689144737 kl_val: -0.1436226940 mse_val: 0.0767619422 acc_val: 0.7368421053 time: 0.5356s
Epoch: 0101 nll_train: 14489.0046793620 kl_train: -0.1353898958 mse_train: 0.0762579207 acc_train: 0.9166666667 nll_val: 14508.0417865954 kl_val: -0.1158446272 mse_val: 0.0763581168 acc_val: 0.7631578947 time: 0.5368s
Epoch: 0102 nll_train: 14479.5378417969 kl_train: -0.1481727355 mse_train: 0.0762080946 acc_train: 0.8541666667 nll_val: 14477.1514185855 kl_val: -0.1477181064 mse_val: 0.0761955349 acc_val: 0.7105263158 time: 0.5358s
Epoch: 0103 nll_train: 14504.1627197266 kl_train: -0.1489437154 mse_train: 0.0763377009 acc_train: 0.9166666667 nll_val: 14644.7102179276 kl_val: -0.1295051434 mse_val: 0.0770774221 acc_val: 0.8157894737 time: 0.5363s
Epoch: 0104 nll_train: 14494.3188476562 kl_train: -0.1501809508 mse_train: 0.0762858894 acc_train: 0.9791666667 nll_val: 14507.5856291118 kl_val: -0.1506916504 mse_val: 0.0763557161 acc_val: 0.7631578947 time: 0.5358s
Epoch: 0105 nll_train: 14492.9759114583 kl_train: -0.1608481615 mse_train: 0.0762788209 acc_train: 0.9583333333 nll_val: 14593.8133223684 kl_val: -0.1557325101 mse_val: 0.0768095431 acc_val: 0.7631578947 time: 0.5347s
Epoch: 0106 nll_train: 14488.2472330729 kl_train: -0.1553673120 mse_train: 0.0762539323 acc_train: 0.8958333333 nll_val: 14510.9716796875 kl_val: -0.1408132174 mse_val: 0.0763735332 acc_val: 0.8157894737 time: 0.5348s
Epoch: 0107 nll_train: 14483.1162923177 kl_train: -0.1457290333 mse_train: 0.0762269290 acc_train: 0.9166666667 nll_val: 14543.7919921875 kl_val: -0.1363807038 mse_val: 0.0765462741 acc_val: 0.8157894737 time: 0.5363s
Epoch: 0108 nll_train: 14489.0725097656 kl_train: -0.1574400890 mse_train: 0.0762582775 acc_train: 0.7708333333 nll_val: 14477.0401932566 kl_val: -0.1506351033 mse_val: 0.0761949494 acc_val: 0.7105263158 time: 0.5375s
Epoch: 0109 nll_train: 14472.0613606771 kl_train: -0.1445254302 mse_train: 0.0761687444 acc_train: 0.9583333333 nll_val: 14516.7819695724 kl_val: -0.1468894889 mse_val: 0.0764041178 acc_val: 0.8157894737 time: 0.5382s
Epoch: 0110 nll_train: 14486.6840006510 kl_train: -0.1574664507 mse_train: 0.0762457056 acc_train: 0.8333333333 nll_val: 14543.0515522204 kl_val: -0.1471174751 mse_val: 0.0765423755 acc_val: 0.7105263158 time: 0.5349s
Epoch: 0111 nll_train: 14482.2385253906 kl_train: -0.1637192083 mse_train: 0.0762223086 acc_train: 0.7083333333 nll_val: 14507.1132812500 kl_val: -0.1681578944 mse_val: 0.0763532288 acc_val: 0.7631578947 time: 0.5359s
Epoch: 0112 nll_train: 14480.6490071615 kl_train: -0.1792289981 mse_train: 0.0762139414 acc_train: 0.8125000000 nll_val: 14393.6716180099 kl_val: -0.1636794029 mse_val: 0.0757561667 acc_val: 0.7368421053 time: 0.5341s
Epoch: 0113 nll_train: 14483.1637369792 kl_train: -0.1825257987 mse_train: 0.0762271783 acc_train: 0.9166666667 nll_val: 14471.9725020559 kl_val: -0.1770267992 mse_val: 0.0761682783 acc_val: 0.7631578947 time: 0.5372s
Epoch: 0114 nll_train: 14461.8131103516 kl_train: -0.1721462365 mse_train: 0.0761148031 acc_train: 0.8750000000 nll_val: 14509.2565275493 kl_val: -0.1497762674 mse_val: 0.0763645070 acc_val: 0.7894736842 time: 0.5376s
Epoch: 0115 nll_train: 14476.2619628906 kl_train: -0.1737265568 mse_train: 0.0761908540 acc_train: 0.9375000000 nll_val: 14447.1321957237 kl_val: -0.1625485526 mse_val: 0.0760375387 acc_val: 0.8421052632 time: 0.5359s
Epoch: 0116 nll_train: 14477.7854817708 kl_train: -0.1531468801 mse_train: 0.0761988708 acc_train: 0.9375000000 nll_val: 14432.5778166118 kl_val: -0.1584835056 mse_val: 0.0759609353 acc_val: 0.8421052632 time: 0.5375s
Epoch: 0117 nll_train: 14468.5336507161 kl_train: -0.1559309904 mse_train: 0.0761501781 acc_train: 0.9791666667 nll_val: 14421.7654194079 kl_val: -0.1437352727 mse_val: 0.0759040309 acc_val: 0.8421052632 time: 0.5357s
Epoch: 0118 nll_train: 14494.4786376953 kl_train: -0.1698893284 mse_train: 0.0762867305 acc_train: 0.9375000000 nll_val: 14540.9358038651 kl_val: -0.1617280606 mse_val: 0.0765312416 acc_val: 0.8157894737 time: 0.5377s
Epoch: 0119 nll_train: 14463.8078613281 kl_train: -0.1621691907 mse_train: 0.0761253037 acc_train: 0.8541666667 nll_val: 14604.2955386513 kl_val: -0.1524018575 mse_val: 0.0768647135 acc_val: 0.8157894737 time: 0.5366s
Epoch: 0120 nll_train: 14453.5827636719 kl_train: -0.1543644182 mse_train: 0.0760714881 acc_train: 0.9375000000 nll_val: 14560.6550164474 kl_val: -0.1425676113 mse_val: 0.0766350274 acc_val: 0.8421052632 time: 0.5343s
Epoch: 0121 nll_train: 14477.5656738281 kl_train: -0.1686063999 mse_train: 0.0761977156 acc_train: 0.8958333333 nll_val: 14607.5215871711 kl_val: -0.1459361140 mse_val: 0.0768816930 acc_val: 0.7894736842 time: 0.5361s
Epoch: 0122 nll_train: 14465.1194254557 kl_train: -0.1450324974 mse_train: 0.0761322083 acc_train: 0.8750000000 nll_val: 14511.5039576480 kl_val: -0.1385454735 mse_val: 0.0763763354 acc_val: 0.7894736842 time: 0.5343s
Epoch: 0123 nll_train: 14455.9973144531 kl_train: -0.1651444780 mse_train: 0.0760841978 acc_train: 0.8958333333 nll_val: 14547.7368421053 kl_val: -0.1779957123 mse_val: 0.0765670365 acc_val: 0.8157894737 time: 0.5357s
Epoch: 0124 nll_train: 14438.0228678385 kl_train: -0.1766441787 mse_train: 0.0759895936 acc_train: 0.9375000000 nll_val: 14526.6878083882 kl_val: -0.1646640175 mse_val: 0.0764562507 acc_val: 0.7631578947 time: 0.5361s
Epoch: 0125 nll_train: 14427.6016031901 kl_train: -0.1687982834 mse_train: 0.0759347451 acc_train: 0.9375000000 nll_val: 14398.7910156250 kl_val: -0.1582331195 mse_val: 0.0757831104 acc_val: 0.8157894737 time: 0.5394s
Epoch: 0126 nll_train: 14437.1116943359 kl_train: -0.1639242265 mse_train: 0.0759847990 acc_train: 0.9375000000 nll_val: 14449.4019839638 kl_val: -0.1606190683 mse_val: 0.0760494831 acc_val: 0.8684210526 time: 0.5363s
Epoch: 0127 nll_train: 14456.6931559245 kl_train: -0.1788924964 mse_train: 0.0760878591 acc_train: 0.9583333333 nll_val: 14497.6878597862 kl_val: -0.1667156894 mse_val: 0.0763036209 acc_val: 0.8684210526 time: 0.5360s
Epoch: 0128 nll_train: 14448.4729003906 kl_train: -0.1795388019 mse_train: 0.0760445929 acc_train: 0.8750000000 nll_val: 14489.3332134046 kl_val: -0.1813286794 mse_val: 0.0762596487 acc_val: 0.8684210526 time: 0.5376s
Epoch: 0129 nll_train: 14451.6595052083 kl_train: -0.1722289224 mse_train: 0.0760613653 acc_train: 0.8958333333 nll_val: 14412.6827713816 kl_val: -0.1452632641 mse_val: 0.0758562249 acc_val: 0.8421052632 time: 0.5359s
Epoch: 0130 nll_train: 14444.3885091146 kl_train: -0.1660095475 mse_train: 0.0760230984 acc_train: 0.9166666667 nll_val: 14558.4758429276 kl_val: -0.1683492457 mse_val: 0.0766235586 acc_val: 0.8421052632 time: 0.5342s
Epoch: 0131 nll_train: 14439.2895507812 kl_train: -0.1722539486 mse_train: 0.0759962616 acc_train: 0.8541666667 nll_val: 14484.2118112664 kl_val: -0.1547966505 mse_val: 0.0762326976 acc_val: 0.8421052632 time: 0.5366s
Epoch: 0132 nll_train: 14445.4390055339 kl_train: -0.1652276839 mse_train: 0.0760286255 acc_train: 0.8541666667 nll_val: 14445.3480674342 kl_val: -0.1709714873 mse_val: 0.0760281498 acc_val: 0.7894736842 time: 0.5362s
Epoch: 0133 nll_train: 14431.0807698568 kl_train: -0.1714798020 mse_train: 0.0759530567 acc_train: 0.8750000000 nll_val: 14468.1980879934 kl_val: -0.1671869469 mse_val: 0.0761484112 acc_val: 0.8157894737 time: 0.5365s
Epoch: 0134 nll_train: 14444.2225341797 kl_train: -0.1601362977 mse_train: 0.0760222250 acc_train: 0.8333333333 nll_val: 14514.5153680099 kl_val: -0.1358179340 mse_val: 0.0763921847 acc_val: 0.7894736842 time: 0.5350s
Epoch: 0135 nll_train: 14434.5475667318 kl_train: -0.1638716400 mse_train: 0.0759713035 acc_train: 0.8958333333 nll_val: 14520.3537726151 kl_val: -0.1789423376 mse_val: 0.0764229153 acc_val: 0.7894736842 time: 0.5347s
Epoch: 0136 nll_train: 14451.9821777344 kl_train: -0.1652566458 mse_train: 0.0760630632 acc_train: 0.8125000000 nll_val: 14434.4843236020 kl_val: -0.1472473188 mse_val: 0.0759709709 acc_val: 0.8157894737 time: 0.5365s
Epoch: 0137 nll_train: 14446.7675374349 kl_train: -0.1545437034 mse_train: 0.0760356172 acc_train: 0.8541666667 nll_val: 14371.1470497533 kl_val: -0.1708719636 mse_val: 0.0756376170 acc_val: 0.8157894737 time: 0.5378s
Epoch: 0138 nll_train: 14444.4108886719 kl_train: -0.1765413228 mse_train: 0.0760232159 acc_train: 0.8750000000 nll_val: 14520.2229646382 kl_val: -0.1638986974 mse_val: 0.0764222259 acc_val: 0.8684210526 time: 0.5371s
Epoch: 0139 nll_train: 14432.1306152344 kl_train: -0.1714710755 mse_train: 0.0759585818 acc_train: 0.9583333333 nll_val: 14458.9024979441 kl_val: -0.1887714671 mse_val: 0.0760994895 acc_val: 0.8684210526 time: 0.5366s
Epoch: 0140 nll_train: 14406.1499023438 kl_train: -0.1761429918 mse_train: 0.0758218411 acc_train: 0.9583333333 nll_val: 14487.0934416118 kl_val: -0.1571705004 mse_val: 0.0762478611 acc_val: 0.8157894737 time: 0.5372s
Epoch: 0141 nll_train: 14454.3718261719 kl_train: -0.1441233385 mse_train: 0.0760756405 acc_train: 0.9583333333 nll_val: 14387.3871299342 kl_val: -0.1115798176 mse_val: 0.0757230901 acc_val: 0.8421052632 time: 0.5435s
Epoch: 0142 nll_train: 14443.0476888021 kl_train: -0.1365639260 mse_train: 0.0760160402 acc_train: 0.9166666667 nll_val: 14394.1960320724 kl_val: -0.1406259811 mse_val: 0.0757589289 acc_val: 0.7368421053 time: 0.5359s
Epoch: 0143 nll_train: 14437.5152994792 kl_train: -0.1546831550 mse_train: 0.0759869226 acc_train: 0.9583333333 nll_val: 14467.6700246711 kl_val: -0.1573495132 mse_val: 0.0761456305 acc_val: 0.8684210526 time: 0.5335s
Epoch: 0144 nll_train: 14425.7465413411 kl_train: -0.1793417434 mse_train: 0.0759249823 acc_train: 0.9166666667 nll_val: 14429.0087376645 kl_val: -0.1606526089 mse_val: 0.0759421548 acc_val: 0.8421052632 time: 0.5354s
Epoch: 0145 nll_train: 14420.3054606120 kl_train: -0.1656796498 mse_train: 0.0758963448 acc_train: 0.9583333333 nll_val: 14464.3639494243 kl_val: -0.1696506931 mse_val: 0.0761282322 acc_val: 0.8947368421 time: 0.5374s
Epoch: 0146 nll_train: 14425.7007242839 kl_train: -0.1629241128 mse_train: 0.0759247402 acc_train: 0.9375000000 nll_val: 14460.4160156250 kl_val: -0.1517160649 mse_val: 0.0761074526 acc_val: 0.7631578947 time: 0.6023s
Epoch: 0147 nll_train: 14427.3816731771 kl_train: -0.1574248979 mse_train: 0.0759335879 acc_train: 0.9375000000 nll_val: 14430.0049342105 kl_val: -0.1704184585 mse_val: 0.0759473957 acc_val: 0.8947368421 time: 0.5366s
Epoch: 0148 nll_train: 14410.7888997396 kl_train: -0.1846822339 mse_train: 0.0758462575 acc_train: 0.9166666667 nll_val: 14438.6546566612 kl_val: -0.1917703105 mse_val: 0.0759929211 acc_val: 0.8421052632 time: 0.5393s
Epoch: 0149 nll_train: 14424.7038981120 kl_train: -0.1726914411 mse_train: 0.0759194950 acc_train: 1.0000000000 nll_val: 14435.7538034539 kl_val: -0.1434623062 mse_val: 0.0759776521 acc_val: 0.8684210526 time: 0.5390s
Epoch: 0150 nll_train: 14398.1286214193 kl_train: -0.1590516921 mse_train: 0.0757796238 acc_train: 0.9791666667 nll_val: 14517.6110197368 kl_val: -0.1641418303 mse_val: 0.0764084788 acc_val: 0.8684210526 time: 0.5364s
Epoch: 0151 nll_train: 14413.7124023438 kl_train: -0.1796772530 mse_train: 0.0758616452 acc_train: 0.9375000000 nll_val: 14500.5929790296 kl_val: -0.1598936798 mse_val: 0.0763189110 acc_val: 0.7631578947 time: 0.5402s
Epoch: 0152 nll_train: 14431.3774414062 kl_train: -0.1579859369 mse_train: 0.0759546182 acc_train: 0.9791666667 nll_val: 14387.5240028783 kl_val: -0.1483294881 mse_val: 0.0757238124 acc_val: 0.8157894737 time: 0.5397s
Epoch: 0153 nll_train: 14413.9333496094 kl_train: -0.1677543856 mse_train: 0.0758628054 acc_train: 0.9375000000 nll_val: 14451.0001027961 kl_val: -0.1561026483 mse_val: 0.0760578944 acc_val: 0.8684210526 time: 0.5397s
Epoch: 0154 nll_train: 14403.3155517578 kl_train: -0.1586859866 mse_train: 0.0758069221 acc_train: 0.9375000000 nll_val: 14435.2016344572 kl_val: -0.1518665533 mse_val: 0.0759747475 acc_val: 0.8947368421 time: 0.5368s
Epoch: 0155 nll_train: 14413.7392985026 kl_train: -0.1646885167 mse_train: 0.0758617870 acc_train: 0.9375000000 nll_val: 14527.7053865132 kl_val: -0.1783707095 mse_val: 0.0764616077 acc_val: 0.7631578947 time: 0.5362s
Epoch: 0156 nll_train: 14421.3848063151 kl_train: -0.1902981376 mse_train: 0.0759020271 acc_train: 0.7708333333 nll_val: 14420.3684724507 kl_val: -0.1693626587 mse_val: 0.0758966780 acc_val: 0.7631578947 time: 0.5389s
Epoch: 0157 nll_train: 14400.0854492188 kl_train: -0.1688614208 mse_train: 0.0757899247 acc_train: 0.9375000000 nll_val: 14412.1814864309 kl_val: -0.1714309078 mse_val: 0.0758535858 acc_val: 0.8684210526 time: 0.5405s
Epoch: 0158 nll_train: 14416.7256673177 kl_train: -0.1715014335 mse_train: 0.0758775035 acc_train: 0.9375000000 nll_val: 14452.7173108553 kl_val: -0.1571853090 mse_val: 0.0760669347 acc_val: 0.7894736842 time: 0.5375s
Epoch: 0159 nll_train: 14431.8128255208 kl_train: -0.1546530022 mse_train: 0.0759569102 acc_train: 0.7291666667 nll_val: 14422.6226356908 kl_val: -0.1539205466 mse_val: 0.0759085405 acc_val: 0.7368421053 time: 0.5408s
Epoch: 0160 nll_train: 14397.7905273438 kl_train: -0.1591140144 mse_train: 0.0757778462 acc_train: 0.9166666667 nll_val: 14423.9346731086 kl_val: -0.1415257172 mse_val: 0.0759154456 acc_val: 0.7368421053 time: 0.5367s
Epoch: 0161 nll_train: 14393.6679280599 kl_train: -0.1604487191 mse_train: 0.0757561470 acc_train: 0.8958333333 nll_val: 14411.8689350329 kl_val: -0.1828835979 mse_val: 0.0758519420 acc_val: 0.8157894737 time: 0.5430s
Epoch: 0162 nll_train: 14394.5918375651 kl_train: -0.1839760592 mse_train: 0.0757610113 acc_train: 0.8750000000 nll_val: 14403.3455489309 kl_val: -0.1793302086 mse_val: 0.0758070812 acc_val: 0.7894736842 time: 0.5370s
Epoch: 0163 nll_train: 14409.9484049479 kl_train: -0.1570230204 mse_train: 0.0758418335 acc_train: 0.9166666667 nll_val: 14359.3772101151 kl_val: -0.1310969999 mse_val: 0.0755756713 acc_val: 0.7894736842 time: 0.5390s
Epoch: 0164 nll_train: 14405.8910319010 kl_train: -0.1654779182 mse_train: 0.0758204800 acc_train: 0.8333333333 nll_val: 14404.8875411184 kl_val: -0.1568971472 mse_val: 0.0758151981 acc_val: 0.6578947368 time: 0.5378s
Epoch: 0165 nll_train: 14388.7370198568 kl_train: -0.1684736566 mse_train: 0.0757301968 acc_train: 0.8333333333 nll_val: 14435.6540912829 kl_val: -0.1642906399 mse_val: 0.0759771270 acc_val: 0.7368421053 time: 0.5372s
Epoch: 0166 nll_train: 14395.5946858724 kl_train: -0.1631863397 mse_train: 0.0757662887 acc_train: 0.9166666667 nll_val: 14466.3238075658 kl_val: -0.1652447915 mse_val: 0.0761385470 acc_val: 0.8157894737 time: 0.5353s
Epoch: 0167 nll_train: 14412.0316975911 kl_train: -0.1749401006 mse_train: 0.0758527983 acc_train: 0.7291666667 nll_val: 14483.5514494243 kl_val: -0.1702765995 mse_val: 0.0762292178 acc_val: 0.7368421053 time: 0.5338s
Epoch: 0168 nll_train: 14377.5953369141 kl_train: -0.1575451978 mse_train: 0.0756715556 acc_train: 0.9166666667 nll_val: 14362.7937397204 kl_val: -0.1452202436 mse_val: 0.0755936503 acc_val: 0.7105263158 time: 0.5351s
Epoch: 0169 nll_train: 14434.7062581380 kl_train: -0.1543901398 mse_train: 0.0759721394 acc_train: 0.9166666667 nll_val: 14354.0039062500 kl_val: -0.1425709124 mse_val: 0.0755473889 acc_val: 0.7631578947 time: 0.5407s
Epoch: 0170 nll_train: 14393.4816487630 kl_train: -0.1576465449 mse_train: 0.0757551667 acc_train: 0.8750000000 nll_val: 14460.4271689967 kl_val: -0.1608155676 mse_val: 0.0761075122 acc_val: 0.8421052632 time: 0.5400s
Epoch: 0171 nll_train: 14378.7771402995 kl_train: -0.1558511006 mse_train: 0.0756777748 acc_train: 0.9583333333 nll_val: 14403.4619140625 kl_val: -0.1577454005 mse_val: 0.0758076945 acc_val: 0.8684210526 time: 0.5394s
Epoch: 0172 nll_train: 14376.4653727214 kl_train: -0.1831846144 mse_train: 0.0756656070 acc_train: 0.7708333333 nll_val: 14420.9382709704 kl_val: -0.1877136376 mse_val: 0.0758996763 acc_val: 0.7368421053 time: 0.5385s
Epoch: 0173 nll_train: 14380.7196858724 kl_train: -0.1878524053 mse_train: 0.0756880000 acc_train: 0.8958333333 nll_val: 14470.5562808388 kl_val: -0.1995375023 mse_val: 0.0761608238 acc_val: 0.8421052632 time: 0.5387s
Epoch: 0174 nll_train: 14382.3190917969 kl_train: -0.1749132747 mse_train: 0.0756964185 acc_train: 0.7291666667 nll_val: 14382.5210731908 kl_val: -0.1622653015 mse_val: 0.0756974785 acc_val: 0.7105263158 time: 0.5367s
Epoch: 0175 nll_train: 14393.6184082031 kl_train: -0.1574886187 mse_train: 0.0757558880 acc_train: 0.7916666667 nll_val: 14409.4758429276 kl_val: -0.1549181197 mse_val: 0.0758393466 acc_val: 0.7631578947 time: 0.5389s
Epoch: 0176 nll_train: 14402.7089436849 kl_train: -0.1750472834 mse_train: 0.0758037313 acc_train: 0.7708333333 nll_val: 14447.5925678454 kl_val: -0.1826295045 mse_val: 0.0760399609 acc_val: 0.7368421053 time: 0.5399s
Epoch: 0177 nll_train: 14373.7226562500 kl_train: -0.1706319340 mse_train: 0.0756511722 acc_train: 0.8958333333 nll_val: 14404.2266652961 kl_val: -0.1781611443 mse_val: 0.0758117194 acc_val: 0.8157894737 time: 0.5388s
Epoch: 0178 nll_train: 14374.9364827474 kl_train: -0.1600205610 mse_train: 0.0756575605 acc_train: 0.8541666667 nll_val: 14387.8651829770 kl_val: -0.1602120380 mse_val: 0.0757256072 acc_val: 0.7894736842 time: 0.5381s
Epoch: 0179 nll_train: 14368.6699625651 kl_train: -0.1717351414 mse_train: 0.0756245786 acc_train: 0.8125000000 nll_val: 14466.5375719572 kl_val: -0.1774163046 mse_val: 0.0761396697 acc_val: 0.8421052632 time: 0.5390s
Epoch: 0180 nll_train: 14378.9573567708 kl_train: -0.1716512678 mse_train: 0.0756787229 acc_train: 0.7916666667 nll_val: 14426.8840460526 kl_val: -0.1609610974 mse_val: 0.0759309675 acc_val: 0.8157894737 time: 0.5406s
Epoch: 0181 nll_train: 14397.0547688802 kl_train: -0.1601982679 mse_train: 0.0757739729 acc_train: 0.8541666667 nll_val: 14384.9026521382 kl_val: -0.1463685357 mse_val: 0.0757100143 acc_val: 0.8157894737 time: 0.5388s
Epoch: 0182 nll_train: 14398.2806396484 kl_train: -0.1419034296 mse_train: 0.0757804246 acc_train: 0.8958333333 nll_val: 14386.0511924342 kl_val: -0.1439608692 mse_val: 0.0757160579 acc_val: 0.8684210526 time: 0.5413s
Epoch: 0183 nll_train: 14380.3158772786 kl_train: -0.1656716255 mse_train: 0.0756858735 acc_train: 0.9583333333 nll_val: 14463.9936780428 kl_val: -0.1843638604 mse_val: 0.0761262841 acc_val: 0.9210526316 time: 0.5398s
Epoch: 0184 nll_train: 14400.3312988281 kl_train: -0.1823083435 mse_train: 0.0757912174 acc_train: 0.7916666667 nll_val: 14617.7348889803 kl_val: -0.1587359682 mse_val: 0.0769354474 acc_val: 0.7368421053 time: 0.5416s
Epoch: 0185 nll_train: 14464.6534016927 kl_train: -0.2125086586 mse_train: 0.0761297536 acc_train: 0.7708333333 nll_val: 14572.9392475329 kl_val: -0.2016267788 mse_val: 0.0766996808 acc_val: 0.7631578947 time: 0.5423s
Epoch: 0186 nll_train: 14395.0040283203 kl_train: -0.1940655578 mse_train: 0.0757631799 acc_train: 0.8125000000 nll_val: 14669.8153268914 kl_val: -0.1711369020 mse_val: 0.0772095546 acc_val: 0.7631578947 time: 0.5403s
Epoch: 0187 nll_train: 14403.4143473307 kl_train: -0.1762611233 mse_train: 0.0758074438 acc_train: 0.8125000000 nll_val: 14685.9031147204 kl_val: -0.1761775887 mse_val: 0.0772942258 acc_val: 0.8157894737 time: 0.5437s
Epoch: 0188 nll_train: 14392.4416503906 kl_train: -0.1537032994 mse_train: 0.0757496933 acc_train: 0.8958333333 nll_val: 14513.5473375822 kl_val: -0.1560493994 mse_val: 0.0763870921 acc_val: 0.7368421053 time: 0.5410s
Epoch: 0189 nll_train: 14373.1606038411 kl_train: -0.1614837047 mse_train: 0.0756482145 acc_train: 0.8958333333 nll_val: 14568.1838507401 kl_val: -0.1736866812 mse_val: 0.0766746527 acc_val: 0.7894736842 time: 0.5409s
Epoch: 0190 nll_train: 14362.8894449870 kl_train: -0.1874389251 mse_train: 0.0755941545 acc_train: 0.9166666667 nll_val: 14588.8466796875 kl_val: -0.1778033454 mse_val: 0.0767834034 acc_val: 0.8421052632 time: 0.5418s
Epoch: 0191 nll_train: 14362.1901855469 kl_train: -0.1547672354 mse_train: 0.0755904744 acc_train: 0.9583333333 nll_val: 14541.2913240132 kl_val: -0.1249703334 mse_val: 0.0765331125 acc_val: 0.8421052632 time: 0.5429s
Epoch: 0192 nll_train: 14379.0375976562 kl_train: -0.1616216280 mse_train: 0.0756791467 acc_train: 0.8958333333 nll_val: 14470.0736019737 kl_val: -0.1824358204 mse_val: 0.0761582828 acc_val: 0.7894736842 time: 0.5424s
Epoch: 0193 nll_train: 14349.9703776042 kl_train: -0.1656093411 mse_train: 0.0755261585 acc_train: 0.9375000000 nll_val: 14556.4935752467 kl_val: -0.1485199999 mse_val: 0.0766131223 acc_val: 0.8421052632 time: 0.5401s
Epoch: 0194 nll_train: 14353.3979085286 kl_train: -0.1641930050 mse_train: 0.0755441985 acc_train: 0.9375000000 nll_val: 14590.6999897204 kl_val: -0.1803905909 mse_val: 0.0767931577 acc_val: 0.7894736842 time: 0.5427s
Epoch: 0195 nll_train: 14353.2704671224 kl_train: -0.1828026486 mse_train: 0.0755435294 acc_train: 0.8750000000 nll_val: 14609.3097245066 kl_val: -0.2023256696 mse_val: 0.0768911054 acc_val: 0.7894736842 time: 0.5381s
Epoch: 0196 nll_train: 14348.9837239583 kl_train: -0.1745323362 mse_train: 0.0755209678 acc_train: 0.8333333333 nll_val: 14437.5214843750 kl_val: -0.1652872296 mse_val: 0.0759869539 acc_val: 0.7894736842 time: 0.5412s
Epoch: 0197 nll_train: 14356.8079427083 kl_train: -0.1542340418 mse_train: 0.0755621466 acc_train: 0.9583333333 nll_val: 14362.7498458059 kl_val: -0.1750003558 mse_val: 0.0755934209 acc_val: 0.8947368421 time: 0.5384s
Epoch: 0198 nll_train: 14348.6312662760 kl_train: -0.1773710828 mse_train: 0.0755191122 acc_train: 0.9166666667 nll_val: 14452.9978412829 kl_val: -0.1744554023 mse_val: 0.0760684104 acc_val: 0.8157894737 time: 0.5419s
Epoch: 0199 nll_train: 14339.8157958984 kl_train: -0.1615302858 mse_train: 0.0754727148 acc_train: 0.8541666667 nll_val: 14360.6626747533 kl_val: -0.1488059998 mse_val: 0.0755824353 acc_val: 0.8684210526 time: 0.5415s
Optimization finished
Best epoch 38
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 15381.0583881579 kl_test: -0.0994555415 mse_test: 0.0809529392 acc_test: 0.5526315789
MSE: [ 0.081011526287 , 0.080290593207 , 0.080103643239 , 0.080273114145 , 0.080395102501 , 0.080384969711 , 0.080185592175 , 0.080198340118 , 0.080545037985 , 0.080644205213 , 0.080917701125 , 0.080871589482 , 0.081385709345 , 0.080874405801 , 0.080620370805 , 0.080816924572 , 0.080397345126 , 0.080621942878 , 0.080324865878 ]
Accuracy for experiment id 10 is 0.25
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 16905.6083984375 kl_train: -0.1808325847 mse_train: 0.0889768889 acc_train: 0.1041666667 nll_val: 14615.0041118421 kl_val: -0.2328980816 mse_val: 0.0769210736 acc_val: 0.0000000000 time: 0.5486s
Best model so far, saving...
Epoch: 0001 nll_train: 13024.6464029948 kl_train: -0.1770112713 mse_train: 0.0685507713 acc_train: 0.0000000000 nll_val: 13210.5292454770 kl_val: -0.1621087405 mse_val: 0.0695291007 acc_val: 0.0000000000 time: 0.5516s
Best model so far, saving...
Epoch: 0002 nll_train: 12409.8021647135 kl_train: -0.2042253825 mse_train: 0.0653147493 acc_train: 0.0000000000 nll_val: 12618.4649979441 kl_val: -0.1870568152 mse_val: 0.0664129741 acc_val: 0.0000000000 time: 0.5484s
Best model so far, saving...
Epoch: 0003 nll_train: 12065.5881754557 kl_train: -0.2198367616 mse_train: 0.0635030967 acc_train: 0.0000000000 nll_val: 12213.2620785362 kl_val: -0.2092031860 mse_val: 0.0642803268 acc_val: 0.1052631579 time: 0.5502s
Best model so far, saving...
Epoch: 0004 nll_train: 11999.2016194661 kl_train: -0.1788289888 mse_train: 0.0631536927 acc_train: 0.0208333333 nll_val: 11831.6150287829 kl_val: -0.1572769896 mse_val: 0.0622716587 acc_val: 0.1842105263 time: 0.5468s
Best model so far, saving...
Epoch: 0005 nll_train: 11944.6982014974 kl_train: -0.1430961502 mse_train: 0.0628668334 acc_train: 0.1041666667 nll_val: 11774.8303865132 kl_val: -0.1245474262 mse_val: 0.0619727916 acc_val: 0.3421052632 time: 0.5480s
Best model so far, saving...
Epoch: 0006 nll_train: 11809.5222981771 kl_train: -0.1262662342 mse_train: 0.0621553805 acc_train: 0.2916666667 nll_val: 11522.5037520559 kl_val: -0.1173459472 mse_val: 0.0606447580 acc_val: 0.4473684211 time: 0.5483s
Best model so far, saving...
Epoch: 0007 nll_train: 11761.8780924479 kl_train: -0.1207425740 mse_train: 0.0619046222 acc_train: 0.2500000000 nll_val: 11506.1522923520 kl_val: -0.1179756748 mse_val: 0.0605586973 acc_val: 0.4736842105 time: 0.5528s
Best model so far, saving...
Epoch: 0008 nll_train: 11756.0710042318 kl_train: -0.1127989062 mse_train: 0.0618740572 acc_train: 0.4166666667 nll_val: 11468.3396381579 kl_val: -0.1097804586 mse_val: 0.0603596827 acc_val: 0.4473684211 time: 0.5485s
Best model so far, saving...
Epoch: 0009 nll_train: 11738.1701660156 kl_train: -0.1075551097 mse_train: 0.0617798429 acc_train: 0.5416666667 nll_val: 11456.6748560855 kl_val: -0.1191367656 mse_val: 0.0602982883 acc_val: 0.5526315789 time: 0.5505s
Best model so far, saving...
Epoch: 0010 nll_train: 11717.8841145833 kl_train: -0.1124024978 mse_train: 0.0616730743 acc_train: 0.6041666667 nll_val: 11408.2579152961 kl_val: -0.1055612246 mse_val: 0.0600434616 acc_val: 0.5526315789 time: 0.5466s
Best model so far, saving...
Epoch: 0011 nll_train: 11718.2883300781 kl_train: -0.1081528210 mse_train: 0.0616752021 acc_train: 0.6041666667 nll_val: 11419.1872944079 kl_val: -0.1044938933 mse_val: 0.0601009874 acc_val: 0.6052631579 time: 0.5474s
Epoch: 0012 nll_train: 11718.2097981771 kl_train: -0.1122078169 mse_train: 0.0616747880 acc_train: 0.8125000000 nll_val: 11405.2379214638 kl_val: -0.1161283361 mse_val: 0.0600275668 acc_val: 0.5789473684 time: 0.5465s
Best model so far, saving...
Epoch: 0013 nll_train: 11710.2836100260 kl_train: -0.1120994072 mse_train: 0.0616330728 acc_train: 0.6666666667 nll_val: 11379.4410464638 kl_val: -0.1089756708 mse_val: 0.0598917968 acc_val: 0.5526315789 time: 0.5479s
Best model so far, saving...
Epoch: 0014 nll_train: 11710.4081217448 kl_train: -0.1178923864 mse_train: 0.0616337269 acc_train: 0.7291666667 nll_val: 11398.0299136513 kl_val: -0.1143331239 mse_val: 0.0599896306 acc_val: 0.6052631579 time: 0.5473s
Epoch: 0015 nll_train: 11694.1720784505 kl_train: -0.1120212609 mse_train: 0.0615482737 acc_train: 0.6666666667 nll_val: 11389.7188527961 kl_val: -0.1158683186 mse_val: 0.0599458886 acc_val: 0.5789473684 time: 0.5469s
Epoch: 0016 nll_train: 11695.0906168620 kl_train: -0.1196858749 mse_train: 0.0615531096 acc_train: 0.6875000000 nll_val: 11385.5105365954 kl_val: -0.1252831377 mse_val: 0.0599237408 acc_val: 0.5789473684 time: 0.5574s
Epoch: 0017 nll_train: 11685.8323974609 kl_train: -0.1233667638 mse_train: 0.0615043816 acc_train: 0.7291666667 nll_val: 11380.3487356086 kl_val: -0.1251699877 mse_val: 0.0598965724 acc_val: 0.6052631579 time: 0.5474s
Epoch: 0018 nll_train: 11686.3487141927 kl_train: -0.1237256449 mse_train: 0.0615070992 acc_train: 0.7291666667 nll_val: 11411.8663137336 kl_val: -0.1238250199 mse_val: 0.0600624549 acc_val: 0.6052631579 time: 0.5470s
Epoch: 0019 nll_train: 11691.6368001302 kl_train: -0.1158490966 mse_train: 0.0615349310 acc_train: 0.6875000000 nll_val: 11377.0389083059 kl_val: -0.1180980278 mse_val: 0.0598791518 acc_val: 0.5789473684 time: 0.5469s
Best model so far, saving...
Epoch: 0020 nll_train: 11677.4028727214 kl_train: -0.1314731563 mse_train: 0.0614600160 acc_train: 0.6666666667 nll_val: 11379.4310752467 kl_val: -0.1314626509 mse_val: 0.0598917445 acc_val: 0.6052631579 time: 0.5480s
Epoch: 0021 nll_train: 11682.0213216146 kl_train: -0.1411956670 mse_train: 0.0614843220 acc_train: 0.7708333333 nll_val: 11402.8331620066 kl_val: -0.1341398715 mse_val: 0.0600149126 acc_val: 0.6578947368 time: 0.5459s
Epoch: 0022 nll_train: 11679.0447998047 kl_train: -0.1375286678 mse_train: 0.0614686570 acc_train: 0.7500000000 nll_val: 11384.1270045230 kl_val: -0.1490313728 mse_val: 0.0599164590 acc_val: 0.6052631579 time: 0.5452s
Epoch: 0023 nll_train: 11673.9567057292 kl_train: -0.1368565218 mse_train: 0.0614418768 acc_train: 0.7500000000 nll_val: 11378.3350123355 kl_val: -0.1410593038 mse_val: 0.0598859750 acc_val: 0.6052631579 time: 0.5452s
Epoch: 0024 nll_train: 11662.2746988932 kl_train: -0.1443381822 mse_train: 0.0613803941 acc_train: 0.7500000000 nll_val: 11373.9683902138 kl_val: -0.1462473116 mse_val: 0.0598629925 acc_val: 0.6052631579 time: 0.5460s
Best model so far, saving...
Epoch: 0025 nll_train: 11662.0910237630 kl_train: -0.1435617649 mse_train: 0.0613794266 acc_train: 0.7708333333 nll_val: 11371.2984169408 kl_val: -0.1557112505 mse_val: 0.0598489387 acc_val: 0.6315789474 time: 0.5472s
Best model so far, saving...
Epoch: 0026 nll_train: 11670.0225016276 kl_train: -0.1443246886 mse_train: 0.0614211719 acc_train: 0.7500000000 nll_val: 11343.0784333882 kl_val: -0.1424453341 mse_val: 0.0597004136 acc_val: 0.6315789474 time: 0.5482s
Best model so far, saving...
Epoch: 0027 nll_train: 11643.2997233073 kl_train: -0.1440027446 mse_train: 0.0612805262 acc_train: 0.7916666667 nll_val: 11361.2965666118 kl_val: -0.1536335118 mse_val: 0.0597962974 acc_val: 0.6052631579 time: 0.5557s
Epoch: 0028 nll_train: 11643.1579182943 kl_train: -0.1433472217 mse_train: 0.0612797788 acc_train: 0.7500000000 nll_val: 11335.1278268914 kl_val: -0.1420234748 mse_val: 0.0596585687 acc_val: 0.6052631579 time: 0.5454s
Best model so far, saving...
Epoch: 0029 nll_train: 11642.7262369792 kl_train: -0.1536020118 mse_train: 0.0612775070 acc_train: 0.7291666667 nll_val: 11352.2544202303 kl_val: -0.1503371353 mse_val: 0.0597487070 acc_val: 0.6052631579 time: 0.5503s
Epoch: 0030 nll_train: 11637.6760660807 kl_train: -0.1524590974 mse_train: 0.0612509269 acc_train: 0.8125000000 nll_val: 11346.5864000822 kl_val: -0.1512795338 mse_val: 0.0597188749 acc_val: 0.6315789474 time: 0.5461s
Epoch: 0031 nll_train: 11648.1596272786 kl_train: -0.1499440642 mse_train: 0.0613061038 acc_train: 0.7291666667 nll_val: 11342.5728310033 kl_val: -0.1539913875 mse_val: 0.0596977525 acc_val: 0.6052631579 time: 0.5470s
Epoch: 0032 nll_train: 11635.2443033854 kl_train: -0.1518676588 mse_train: 0.0612381278 acc_train: 0.7916666667 nll_val: 11337.2837685033 kl_val: -0.1518258692 mse_val: 0.0596699160 acc_val: 0.6052631579 time: 0.5457s
Epoch: 0033 nll_train: 11629.3571370443 kl_train: -0.1510174330 mse_train: 0.0612071430 acc_train: 0.7500000000 nll_val: 11361.7177220395 kl_val: -0.1615837954 mse_val: 0.0597985154 acc_val: 0.6052631579 time: 0.5466s
Epoch: 0034 nll_train: 11631.8420003255 kl_train: -0.1611118832 mse_train: 0.0612202217 acc_train: 0.7708333333 nll_val: 11353.0342824836 kl_val: -0.1656916816 mse_val: 0.0597528133 acc_val: 0.6315789474 time: 0.5463s
Epoch: 0035 nll_train: 11621.6460774740 kl_train: -0.1644510472 mse_train: 0.0611665586 acc_train: 0.7708333333 nll_val: 11351.8425164474 kl_val: -0.1605990313 mse_val: 0.0597465395 acc_val: 0.6578947368 time: 0.5464s
Epoch: 0036 nll_train: 11631.6951904297 kl_train: -0.1549006185 mse_train: 0.0612194493 acc_train: 0.8125000000 nll_val: 11382.7010176809 kl_val: -0.1614606686 mse_val: 0.0599089522 acc_val: 0.6578947368 time: 0.5466s
Epoch: 0037 nll_train: 11624.8833821615 kl_train: -0.1622536837 mse_train: 0.0611835960 acc_train: 0.6458333333 nll_val: 11390.9341077303 kl_val: -0.1843775009 mse_val: 0.0599522879 acc_val: 0.6315789474 time: 0.5459s
Epoch: 0038 nll_train: 11613.7554117839 kl_train: -0.1600046856 mse_train: 0.0611250293 acc_train: 0.7083333333 nll_val: 11395.8599403783 kl_val: -0.1636063151 mse_val: 0.0599782100 acc_val: 0.6052631579 time: 0.5455s
Epoch: 0039 nll_train: 11612.1821289062 kl_train: -0.1662393666 mse_train: 0.0611167490 acc_train: 0.6875000000 nll_val: 11388.2950246711 kl_val: -0.1656782792 mse_val: 0.0599383957 acc_val: 0.6052631579 time: 0.5467s
Epoch: 0040 nll_train: 11620.5426432292 kl_train: -0.1553462859 mse_train: 0.0611607502 acc_train: 0.7500000000 nll_val: 11375.5448190789 kl_val: -0.1694288822 mse_val: 0.0598712891 acc_val: 0.6578947368 time: 0.5464s
Epoch: 0041 nll_train: 11612.7985432943 kl_train: -0.1629096170 mse_train: 0.0611199931 acc_train: 0.6875000000 nll_val: 11398.4380653783 kl_val: -0.1750248545 mse_val: 0.0599917801 acc_val: 0.6052631579 time: 0.5457s
Epoch: 0042 nll_train: 11617.0802408854 kl_train: -0.1674883263 mse_train: 0.0611425280 acc_train: 0.7500000000 nll_val: 11397.2486122533 kl_val: -0.1799566832 mse_val: 0.0599855187 acc_val: 0.6052631579 time: 0.5436s
Epoch: 0043 nll_train: 11611.4877115885 kl_train: -0.1638053181 mse_train: 0.0611130920 acc_train: 0.7083333333 nll_val: 11372.7491262336 kl_val: -0.1617508535 mse_val: 0.0598565744 acc_val: 0.6052631579 time: 0.5471s
Epoch: 0044 nll_train: 11607.0321451823 kl_train: -0.1607472363 mse_train: 0.0610896431 acc_train: 0.6875000000 nll_val: 11359.1204255757 kl_val: -0.1512370678 mse_val: 0.0597848441 acc_val: 0.6315789474 time: 0.5516s
Epoch: 0045 nll_train: 11608.8421223958 kl_train: -0.1580403453 mse_train: 0.0610991701 acc_train: 0.6666666667 nll_val: 11384.5852179276 kl_val: -0.1634307849 mse_val: 0.0599188701 acc_val: 0.5789473684 time: 0.5449s
Epoch: 0046 nll_train: 11612.2685139974 kl_train: -0.1630649138 mse_train: 0.0611172033 acc_train: 0.6875000000 nll_val: 11376.7668585526 kl_val: -0.1578691272 mse_val: 0.0598777223 acc_val: 0.6052631579 time: 0.5452s
Epoch: 0047 nll_train: 11622.7580159505 kl_train: -0.1578889064 mse_train: 0.0611724108 acc_train: 0.7916666667 nll_val: 11413.3427220395 kl_val: -0.1650775181 mse_val: 0.0600702259 acc_val: 0.6578947368 time: 0.5433s
Epoch: 0048 nll_train: 11605.5740152995 kl_train: -0.1614022556 mse_train: 0.0610819693 acc_train: 0.7291666667 nll_val: 11393.1298828125 kl_val: -0.1798157598 mse_val: 0.0599638414 acc_val: 0.6315789474 time: 0.5455s
Epoch: 0049 nll_train: 11598.4760335286 kl_train: -0.1688861124 mse_train: 0.0610446118 acc_train: 0.6666666667 nll_val: 11436.4756887336 kl_val: -0.1901306502 mse_val: 0.0601919764 acc_val: 0.6052631579 time: 0.5449s
Epoch: 0050 nll_train: 11591.9329020182 kl_train: -0.1702933498 mse_train: 0.0610101740 acc_train: 0.6875000000 nll_val: 11408.9729132401 kl_val: -0.1861192666 mse_val: 0.0600472257 acc_val: 0.6052631579 time: 0.5433s
Epoch: 0051 nll_train: 11592.1868082682 kl_train: -0.1743823321 mse_train: 0.0610115089 acc_train: 0.6875000000 nll_val: 11438.9553865132 kl_val: -0.1994221273 mse_val: 0.0602050275 acc_val: 0.6052631579 time: 0.5445s
Epoch: 0052 nll_train: 11592.7230224609 kl_train: -0.1634150312 mse_train: 0.0610143314 acc_train: 0.7500000000 nll_val: 11444.4042968750 kl_val: -0.1882900011 mse_val: 0.0602337065 acc_val: 0.6578947368 time: 0.5467s
Epoch: 0053 nll_train: 11582.4156494141 kl_train: -0.1772406648 mse_train: 0.0609600825 acc_train: 0.6250000000 nll_val: 11440.9080489309 kl_val: -0.2102436906 mse_val: 0.0602153053 acc_val: 0.6052631579 time: 0.5440s
Epoch: 0054 nll_train: 11583.7386474609 kl_train: -0.1658706395 mse_train: 0.0609670458 acc_train: 0.6250000000 nll_val: 11438.8977179276 kl_val: -0.1869510102 mse_val: 0.0602047263 acc_val: 0.6052631579 time: 0.5472s
Epoch: 0055 nll_train: 11583.9986979167 kl_train: -0.1859139747 mse_train: 0.0609684158 acc_train: 0.6458333333 nll_val: 11476.7672183388 kl_val: -0.1901321443 mse_val: 0.0604040385 acc_val: 0.6052631579 time: 0.5471s
Epoch: 0056 nll_train: 11586.7116699219 kl_train: -0.1608314719 mse_train: 0.0609826936 acc_train: 0.7500000000 nll_val: 11431.8889802632 kl_val: -0.1715260077 mse_val: 0.0601678369 acc_val: 0.6315789474 time: 0.5461s
Epoch: 0057 nll_train: 11589.6994628906 kl_train: -0.1652609656 mse_train: 0.0609984160 acc_train: 0.7083333333 nll_val: 11433.8472450658 kl_val: -0.1913686418 mse_val: 0.0601781444 acc_val: 0.6315789474 time: 0.5443s
Epoch: 0058 nll_train: 11585.3866780599 kl_train: -0.1751977938 mse_train: 0.0609757211 acc_train: 0.6458333333 nll_val: 11411.5418379934 kl_val: -0.1934123541 mse_val: 0.0600607464 acc_val: 0.6052631579 time: 0.5459s
Epoch: 0059 nll_train: 11576.1763916016 kl_train: -0.1700789714 mse_train: 0.0609272453 acc_train: 0.6875000000 nll_val: 11448.1066509046 kl_val: -0.1884600528 mse_val: 0.0602531933 acc_val: 0.6052631579 time: 0.5459s
Epoch: 0060 nll_train: 11585.2613932292 kl_train: -0.1684556461 mse_train: 0.0609750613 acc_train: 0.6666666667 nll_val: 11436.8935546875 kl_val: -0.1831519596 mse_val: 0.0601941769 acc_val: 0.6315789474 time: 0.5454s
Epoch: 0061 nll_train: 11578.0382893880 kl_train: -0.1690603287 mse_train: 0.0609370448 acc_train: 0.6458333333 nll_val: 11400.5433285362 kl_val: -0.1745851044 mse_val: 0.0600028589 acc_val: 0.6052631579 time: 0.5453s
Epoch: 0062 nll_train: 11571.6725260417 kl_train: -0.1566609588 mse_train: 0.0609035391 acc_train: 0.7500000000 nll_val: 11423.3548519737 kl_val: -0.1718384886 mse_val: 0.0601229203 acc_val: 0.6315789474 time: 0.5457s
Epoch: 0063 nll_train: 11569.0570475260 kl_train: -0.1643653198 mse_train: 0.0608897759 acc_train: 0.6458333333 nll_val: 11498.7650596217 kl_val: -0.1817685483 mse_val: 0.0605198163 acc_val: 0.6052631579 time: 0.5423s
Epoch: 0064 nll_train: 11571.3881835938 kl_train: -0.1671655249 mse_train: 0.0609020432 acc_train: 0.6666666667 nll_val: 11467.7384868421 kl_val: -0.1871594120 mse_val: 0.0603565187 acc_val: 0.6052631579 time: 0.5435s
Epoch: 0065 nll_train: 11566.9511718750 kl_train: -0.1640180862 mse_train: 0.0608786903 acc_train: 0.7083333333 nll_val: 11470.9715254934 kl_val: -0.1787101646 mse_val: 0.0603735349 acc_val: 0.6315789474 time: 0.5442s
Epoch: 0066 nll_train: 11558.8916015625 kl_train: -0.1716761887 mse_train: 0.0608362725 acc_train: 0.6875000000 nll_val: 11489.5714946546 kl_val: -0.1935151698 mse_val: 0.0604714285 acc_val: 0.6052631579 time: 0.5443s
Epoch: 0067 nll_train: 11564.5550537109 kl_train: -0.1757796866 mse_train: 0.0608660790 acc_train: 0.7083333333 nll_val: 11510.9505037007 kl_val: -0.1988281404 mse_val: 0.0605839489 acc_val: 0.6315789474 time: 0.5446s
Epoch: 0068 nll_train: 11557.3351236979 kl_train: -0.1638328700 mse_train: 0.0608280790 acc_train: 0.7083333333 nll_val: 11470.7332956414 kl_val: -0.1721871687 mse_val: 0.0603722816 acc_val: 0.6052631579 time: 0.5453s
Epoch: 0069 nll_train: 11563.9512532552 kl_train: -0.1676868256 mse_train: 0.0608629012 acc_train: 0.7083333333 nll_val: 11475.8189761513 kl_val: -0.1941832920 mse_val: 0.0603990474 acc_val: 0.6052631579 time: 0.5455s
Epoch: 0070 nll_train: 11562.4506835938 kl_train: -0.1612797308 mse_train: 0.0608550026 acc_train: 0.6666666667 nll_val: 11441.4002364309 kl_val: -0.1648356585 mse_val: 0.0602178960 acc_val: 0.6052631579 time: 0.5446s
Epoch: 0071 nll_train: 11558.4900309245 kl_train: -0.1584343348 mse_train: 0.0608341598 acc_train: 0.6250000000 nll_val: 11462.4303042763 kl_val: -0.1796296086 mse_val: 0.0603285808 acc_val: 0.6052631579 time: 0.5474s
Epoch: 0072 nll_train: 11566.6608886719 kl_train: -0.1782999253 mse_train: 0.0608771642 acc_train: 0.7083333333 nll_val: 11457.8743832237 kl_val: -0.1839337321 mse_val: 0.0603046033 acc_val: 0.6052631579 time: 0.5454s
Epoch: 0073 nll_train: 11560.8607177734 kl_train: -0.1620979908 mse_train: 0.0608466367 acc_train: 0.7708333333 nll_val: 11439.7364823191 kl_val: -0.1711267351 mse_val: 0.0602091396 acc_val: 0.6578947368 time: 0.5447s
Epoch: 0074 nll_train: 11552.0389811198 kl_train: -0.1662065048 mse_train: 0.0608002044 acc_train: 0.6458333333 nll_val: 11479.4203844572 kl_val: -0.1676872279 mse_val: 0.0604180011 acc_val: 0.6052631579 time: 0.5449s
Epoch: 0075 nll_train: 11554.5747070312 kl_train: -0.1648701662 mse_train: 0.0608135524 acc_train: 0.6875000000 nll_val: 11500.0613178454 kl_val: -0.2055965466 mse_val: 0.0605266386 acc_val: 0.6052631579 time: 0.5472s
Epoch: 0076 nll_train: 11564.8443196615 kl_train: -0.1692796393 mse_train: 0.0608676032 acc_train: 0.6458333333 nll_val: 11544.6507504112 kl_val: -0.1796997123 mse_val: 0.0607613202 acc_val: 0.6578947368 time: 0.5464s
Epoch: 0077 nll_train: 11569.9026692708 kl_train: -0.1710601983 mse_train: 0.0608942253 acc_train: 0.7291666667 nll_val: 11490.7413137336 kl_val: -0.1879692831 mse_val: 0.0604775856 acc_val: 0.6315789474 time: 0.5460s
Epoch: 0078 nll_train: 11553.6427001953 kl_train: -0.1767277153 mse_train: 0.0608086482 acc_train: 0.7291666667 nll_val: 11495.2867495888 kl_val: -0.1813413877 mse_val: 0.0605015078 acc_val: 0.6315789474 time: 0.5442s
Epoch: 0079 nll_train: 11554.3862304688 kl_train: -0.1775939325 mse_train: 0.0608125588 acc_train: 0.7291666667 nll_val: 11504.1871916118 kl_val: -0.1864969338 mse_val: 0.0605483526 acc_val: 0.6052631579 time: 0.5434s
Epoch: 0080 nll_train: 11561.6026204427 kl_train: -0.1752981149 mse_train: 0.0608505411 acc_train: 0.6875000000 nll_val: 11524.4799033717 kl_val: -0.2150984206 mse_val: 0.0606551561 acc_val: 0.6052631579 time: 0.5434s
Epoch: 0081 nll_train: 11549.8487141927 kl_train: -0.1713226146 mse_train: 0.0607886766 acc_train: 0.6458333333 nll_val: 11501.4472142270 kl_val: -0.2108196871 mse_val: 0.0605339324 acc_val: 0.6052631579 time: 0.5452s
Epoch: 0082 nll_train: 11554.4653727214 kl_train: -0.1881292270 mse_train: 0.0608129759 acc_train: 0.7083333333 nll_val: 11522.2633634868 kl_val: -0.2040594075 mse_val: 0.0606434912 acc_val: 0.6315789474 time: 0.5456s
Epoch: 0083 nll_train: 11557.5265706380 kl_train: -0.1814034904 mse_train: 0.0608290886 acc_train: 0.5833333333 nll_val: 11507.9955797697 kl_val: -0.2048611551 mse_val: 0.0605683978 acc_val: 0.6052631579 time: 0.5445s
Epoch: 0084 nll_train: 11555.8204345703 kl_train: -0.1776973422 mse_train: 0.0608201087 acc_train: 0.7291666667 nll_val: 11519.5129523026 kl_val: -0.1929059389 mse_val: 0.0606290153 acc_val: 0.6578947368 time: 0.5445s
Epoch: 0085 nll_train: 11551.4917805990 kl_train: -0.1659146665 mse_train: 0.0607973266 acc_train: 0.7291666667 nll_val: 11479.9977898849 kl_val: -0.1657095712 mse_val: 0.0604210419 acc_val: 0.6315789474 time: 0.5504s
Epoch: 0086 nll_train: 11542.2001546224 kl_train: -0.1667985826 mse_train: 0.0607484221 acc_train: 0.6666666667 nll_val: 11576.0870168586 kl_val: -0.2010953105 mse_val: 0.0609267740 acc_val: 0.5789473684 time: 0.5451s
Epoch: 0087 nll_train: 11558.2821858724 kl_train: -0.1682086736 mse_train: 0.0608330638 acc_train: 0.6458333333 nll_val: 11551.3717619243 kl_val: -0.1822800119 mse_val: 0.0607966932 acc_val: 0.6052631579 time: 0.5468s
Epoch: 0088 nll_train: 11553.4460449219 kl_train: -0.1749854786 mse_train: 0.0608076109 acc_train: 0.7083333333 nll_val: 11533.5107421875 kl_val: -0.2006925435 mse_val: 0.0607026908 acc_val: 0.6315789474 time: 0.5460s
Epoch: 0089 nll_train: 11553.1051839193 kl_train: -0.1728918205 mse_train: 0.0608058181 acc_train: 0.6041666667 nll_val: 11541.9965049342 kl_val: -0.2210096944 mse_val: 0.0607473511 acc_val: 0.6052631579 time: 0.5460s
Epoch: 0090 nll_train: 11539.8522135417 kl_train: -0.1747669708 mse_train: 0.0607360646 acc_train: 0.6875000000 nll_val: 11580.2634662829 kl_val: -0.2059552395 mse_val: 0.0609487566 acc_val: 0.6052631579 time: 0.5439s
Epoch: 0091 nll_train: 11550.8049723307 kl_train: -0.1686353078 mse_train: 0.0607937107 acc_train: 0.6250000000 nll_val: 11530.0002055921 kl_val: -0.1910358456 mse_val: 0.0606842116 acc_val: 0.6315789474 time: 0.5473s
Epoch: 0092 nll_train: 11543.7092692057 kl_train: -0.1742589495 mse_train: 0.0607563648 acc_train: 0.7291666667 nll_val: 11549.4122121711 kl_val: -0.2065900207 mse_val: 0.0607863804 acc_val: 0.6578947368 time: 0.5463s
Epoch: 0093 nll_train: 11545.7558593750 kl_train: -0.1720523952 mse_train: 0.0607671374 acc_train: 0.7500000000 nll_val: 11520.0038034539 kl_val: -0.1939658672 mse_val: 0.0606316012 acc_val: 0.6052631579 time: 0.5446s
Epoch: 0094 nll_train: 11546.8051350911 kl_train: -0.1672324389 mse_train: 0.0607726585 acc_train: 0.7083333333 nll_val: 11558.7489720395 kl_val: -0.1875029493 mse_val: 0.0608355221 acc_val: 0.6052631579 time: 0.5460s
Epoch: 0095 nll_train: 11540.8190917969 kl_train: -0.1738779601 mse_train: 0.0607411542 acc_train: 0.6666666667 nll_val: 11552.9516858553 kl_val: -0.2260044186 mse_val: 0.0608050094 acc_val: 0.6315789474 time: 0.5462s
Epoch: 0096 nll_train: 11539.2720133464 kl_train: -0.1876155734 mse_train: 0.0607330102 acc_train: 0.6458333333 nll_val: 11542.8952508224 kl_val: -0.1956927816 mse_val: 0.0607520805 acc_val: 0.6052631579 time: 0.5446s
Epoch: 0097 nll_train: 11533.7827962240 kl_train: -0.1725902067 mse_train: 0.0607041200 acc_train: 0.6875000000 nll_val: 11573.7485094572 kl_val: -0.1898269238 mse_val: 0.0609144663 acc_val: 0.6052631579 time: 0.5508s
Epoch: 0098 nll_train: 11529.4516601562 kl_train: -0.1687633457 mse_train: 0.0606813268 acc_train: 0.6666666667 nll_val: 11517.7033305921 kl_val: -0.1916498530 mse_val: 0.0606194917 acc_val: 0.5789473684 time: 0.5469s
Epoch: 0099 nll_train: 11541.6349283854 kl_train: -0.1707649926 mse_train: 0.0607454479 acc_train: 0.6250000000 nll_val: 11568.5281147204 kl_val: -0.1912247562 mse_val: 0.0608869917 acc_val: 0.5789473684 time: 0.5458s
Epoch: 0100 nll_train: 11535.2776285807 kl_train: -0.1665177007 mse_train: 0.0607119892 acc_train: 0.6458333333 nll_val: 11585.9902857730 kl_val: -0.2091577163 mse_val: 0.0609788969 acc_val: 0.5526315789 time: 0.5480s
Epoch: 0101 nll_train: 11536.5871582031 kl_train: -0.1679172413 mse_train: 0.0607188807 acc_train: 0.6875000000 nll_val: 11598.7639288651 kl_val: -0.2022861729 mse_val: 0.0610461261 acc_val: 0.6052631579 time: 0.5473s
Epoch: 0102 nll_train: 11540.4965006510 kl_train: -0.1728404568 mse_train: 0.0607394550 acc_train: 0.6041666667 nll_val: 11494.0454872533 kl_val: -0.1756803256 mse_val: 0.0604949774 acc_val: 0.5789473684 time: 0.5446s
Epoch: 0103 nll_train: 11522.6246337891 kl_train: -0.1749634743 mse_train: 0.0606453937 acc_train: 0.6666666667 nll_val: 11660.2453227796 kl_val: -0.2123232531 mse_val: 0.0613697126 acc_val: 0.6052631579 time: 0.5449s
Epoch: 0104 nll_train: 11532.2421468099 kl_train: -0.1713260968 mse_train: 0.0606960131 acc_train: 0.7291666667 nll_val: 11561.7839740954 kl_val: -0.1970711229 mse_val: 0.0608514955 acc_val: 0.6052631579 time: 0.5455s
Epoch: 0105 nll_train: 11527.4075927734 kl_train: -0.1728017088 mse_train: 0.0606705664 acc_train: 0.7083333333 nll_val: 11607.0263157895 kl_val: -0.1952694270 mse_val: 0.0610896131 acc_val: 0.5789473684 time: 0.5433s
Epoch: 0106 nll_train: 11524.2663981120 kl_train: -0.1740011262 mse_train: 0.0606540348 acc_train: 0.6666666667 nll_val: 11598.7743626645 kl_val: -0.1987825244 mse_val: 0.0610461810 acc_val: 0.6052631579 time: 0.5438s
Epoch: 0107 nll_train: 11525.1301269531 kl_train: -0.1823223522 mse_train: 0.0606585789 acc_train: 0.6666666667 nll_val: 11570.7030222039 kl_val: -0.2116061904 mse_val: 0.0608984361 acc_val: 0.5789473684 time: 0.5442s
Epoch: 0108 nll_train: 11523.6439208984 kl_train: -0.1751625575 mse_train: 0.0606507591 acc_train: 0.6666666667 nll_val: 11599.8747430099 kl_val: -0.1973312066 mse_val: 0.0610519722 acc_val: 0.5789473684 time: 0.5478s
Epoch: 0109 nll_train: 11526.8722330729 kl_train: -0.1911240922 mse_train: 0.0606677487 acc_train: 0.6250000000 nll_val: 11588.9591899671 kl_val: -0.1965086378 mse_val: 0.0609945204 acc_val: 0.6052631579 time: 0.5464s
Epoch: 0110 nll_train: 11534.9844970703 kl_train: -0.1762974827 mse_train: 0.0607104435 acc_train: 0.6875000000 nll_val: 11558.9947574013 kl_val: -0.1909845169 mse_val: 0.0608368151 acc_val: 0.6052631579 time: 0.5440s
Epoch: 0111 nll_train: 11531.7848307292 kl_train: -0.1756820021 mse_train: 0.0606936053 acc_train: 0.6250000000 nll_val: 11599.7492290296 kl_val: -0.1993748330 mse_val: 0.0610513132 acc_val: 0.6315789474 time: 0.5462s
Epoch: 0112 nll_train: 11532.6896565755 kl_train: -0.1716622968 mse_train: 0.0606983659 acc_train: 0.7083333333 nll_val: 11524.0826994243 kl_val: -0.1845700702 mse_val: 0.0606530673 acc_val: 0.6315789474 time: 0.5473s
Epoch: 0113 nll_train: 11522.0055338542 kl_train: -0.1818151126 mse_train: 0.0606421356 acc_train: 0.6250000000 nll_val: 11615.2545744243 kl_val: -0.1991261671 mse_val: 0.0611329190 acc_val: 0.6052631579 time: 0.5463s
Epoch: 0114 nll_train: 11525.4561767578 kl_train: -0.1778984002 mse_train: 0.0606602967 acc_train: 0.6875000000 nll_val: 11571.8324424342 kl_val: -0.2020369019 mse_val: 0.0609043817 acc_val: 0.5000000000 time: 0.5447s
Epoch: 0115 nll_train: 11521.4651692708 kl_train: -0.1768321212 mse_train: 0.0606392908 acc_train: 0.6666666667 nll_val: 11616.5980160362 kl_val: -0.2053746016 mse_val: 0.0611399902 acc_val: 0.6842105263 time: 0.5447s
Epoch: 0116 nll_train: 11527.4325358073 kl_train: -0.1848417039 mse_train: 0.0606706978 acc_train: 0.6875000000 nll_val: 11533.1729029605 kl_val: -0.1857588903 mse_val: 0.0607009112 acc_val: 0.6315789474 time: 0.5470s
Epoch: 0117 nll_train: 11521.0203857422 kl_train: -0.1724204582 mse_train: 0.0606369488 acc_train: 0.6458333333 nll_val: 11580.0556640625 kl_val: -0.1874283787 mse_val: 0.0609476609 acc_val: 0.5789473684 time: 0.5469s
Epoch: 0118 nll_train: 11529.5142822266 kl_train: -0.1683034068 mse_train: 0.0606816526 acc_train: 0.7083333333 nll_val: 11545.4059930099 kl_val: -0.1734594762 mse_val: 0.0607652935 acc_val: 0.6052631579 time: 0.5448s
Epoch: 0119 nll_train: 11521.8873291016 kl_train: -0.1712241539 mse_train: 0.0606415132 acc_train: 0.7083333333 nll_val: 11608.9096936678 kl_val: -0.2008673337 mse_val: 0.0610995238 acc_val: 0.6052631579 time: 0.5456s
Epoch: 0120 nll_train: 11527.7973632812 kl_train: -0.1810410793 mse_train: 0.0606726181 acc_train: 0.7083333333 nll_val: 11568.7302117599 kl_val: -0.2029838664 mse_val: 0.0608880538 acc_val: 0.5789473684 time: 0.5479s
Epoch: 0121 nll_train: 11531.1365966797 kl_train: -0.1731102876 mse_train: 0.0606901924 acc_train: 0.6875000000 nll_val: 11625.6240234375 kl_val: -0.1968144312 mse_val: 0.0611874947 acc_val: 0.6052631579 time: 0.5455s
Epoch: 0122 nll_train: 11535.2884114583 kl_train: -0.1626294293 mse_train: 0.0607120446 acc_train: 0.7083333333 nll_val: 11570.9697779605 kl_val: -0.1797650256 mse_val: 0.0608998406 acc_val: 0.5789473684 time: 0.5453s
Epoch: 0123 nll_train: 11530.8532307943 kl_train: -0.1676318565 mse_train: 0.0606887024 acc_train: 0.6666666667 nll_val: 11615.2085731908 kl_val: -0.2267340539 mse_val: 0.0611326767 acc_val: 0.6578947368 time: 0.5452s
Epoch: 0124 nll_train: 11525.2215169271 kl_train: -0.1773193972 mse_train: 0.0606590598 acc_train: 0.6458333333 nll_val: 11593.7161287007 kl_val: -0.1783133349 mse_val: 0.0610195581 acc_val: 0.6052631579 time: 0.5461s
Epoch: 0125 nll_train: 11531.3113606771 kl_train: -0.1671805643 mse_train: 0.0606911108 acc_train: 0.6250000000 nll_val: 11657.6844675164 kl_val: -0.1850799408 mse_val: 0.0613562357 acc_val: 0.6052631579 time: 0.5427s
Epoch: 0126 nll_train: 11536.2060139974 kl_train: -0.1694117089 mse_train: 0.0607168740 acc_train: 0.7291666667 nll_val: 11632.6620579770 kl_val: -0.2046669761 mse_val: 0.0612245365 acc_val: 0.6315789474 time: 0.5434s
Epoch: 0127 nll_train: 11542.6520182292 kl_train: -0.1712892636 mse_train: 0.0607508007 acc_train: 0.7291666667 nll_val: 11598.4379111842 kl_val: -0.1806458831 mse_val: 0.0610444107 acc_val: 0.6052631579 time: 0.5448s
Epoch: 0128 nll_train: 11534.7285156250 kl_train: -0.1759275493 mse_train: 0.0607090971 acc_train: 0.6250000000 nll_val: 11569.8257606908 kl_val: -0.1855913989 mse_val: 0.0608938191 acc_val: 0.6052631579 time: 0.5453s
Epoch: 0129 nll_train: 11529.7661946615 kl_train: -0.1725286758 mse_train: 0.0606829794 acc_train: 0.7083333333 nll_val: 11658.0961143092 kl_val: -0.2110175381 mse_val: 0.0613584003 acc_val: 0.6052631579 time: 0.5447s
Epoch: 0130 nll_train: 11560.6127115885 kl_train: -0.1602240534 mse_train: 0.0608453291 acc_train: 0.7500000000 nll_val: 11578.8800884046 kl_val: -0.1643361402 mse_val: 0.0609414744 acc_val: 0.6578947368 time: 0.5509s
Epoch: 0131 nll_train: 11534.1518961589 kl_train: -0.1661974452 mse_train: 0.0607060630 acc_train: 0.6458333333 nll_val: 11582.8889802632 kl_val: -0.1821308360 mse_val: 0.0609625743 acc_val: 0.5263157895 time: 0.5462s
Epoch: 0132 nll_train: 11522.5704752604 kl_train: -0.1756228792 mse_train: 0.0606451076 acc_train: 0.7291666667 nll_val: 11506.5519120066 kl_val: -0.1561355003 mse_val: 0.0605608021 acc_val: 0.6315789474 time: 0.5466s
Epoch: 0133 nll_train: 11527.9964599609 kl_train: -0.1713077637 mse_train: 0.0606736662 acc_train: 0.7500000000 nll_val: 11695.6615953947 kl_val: -0.2029905974 mse_val: 0.0615561142 acc_val: 0.6052631579 time: 0.5451s
Epoch: 0134 nll_train: 11537.1082763672 kl_train: -0.1786205365 mse_train: 0.0607216225 acc_train: 0.6875000000 nll_val: 11558.9620168586 kl_val: -0.1755247096 mse_val: 0.0608366406 acc_val: 0.6315789474 time: 0.5461s
Epoch: 0135 nll_train: 11537.7510172526 kl_train: -0.1685767140 mse_train: 0.0607250058 acc_train: 0.7291666667 nll_val: 11647.3463712993 kl_val: -0.1960789350 mse_val: 0.0613018225 acc_val: 0.6052631579 time: 0.5446s
Epoch: 0136 nll_train: 11530.7151692708 kl_train: -0.1870006351 mse_train: 0.0606879744 acc_train: 0.7083333333 nll_val: 11633.9400185033 kl_val: -0.1894635135 mse_val: 0.0612312632 acc_val: 0.6052631579 time: 0.5461s
Epoch: 0137 nll_train: 11529.3405354818 kl_train: -0.1822709236 mse_train: 0.0606807397 acc_train: 0.7500000000 nll_val: 11632.0070415296 kl_val: -0.2111212799 mse_val: 0.0612210898 acc_val: 0.6578947368 time: 0.5455s
Epoch: 0138 nll_train: 11526.5577799479 kl_train: -0.1861113145 mse_train: 0.0606660941 acc_train: 0.6250000000 nll_val: 11594.7657277961 kl_val: -0.1964266771 mse_val: 0.0610250838 acc_val: 0.6052631579 time: 0.5455s
Epoch: 0139 nll_train: 11516.5661214193 kl_train: -0.1727870234 mse_train: 0.0606135066 acc_train: 0.6875000000 nll_val: 11608.2249691612 kl_val: -0.1851703144 mse_val: 0.0610959222 acc_val: 0.6315789474 time: 0.5455s
Epoch: 0140 nll_train: 11519.7412516276 kl_train: -0.1771753272 mse_train: 0.0606302173 acc_train: 0.7291666667 nll_val: 11635.8712479441 kl_val: -0.1947393543 mse_val: 0.0612414299 acc_val: 0.6052631579 time: 0.5469s
Epoch: 0141 nll_train: 11514.6267089844 kl_train: -0.1754761155 mse_train: 0.0606033000 acc_train: 0.7083333333 nll_val: 11648.2159231086 kl_val: -0.2202902334 mse_val: 0.0613063993 acc_val: 0.6315789474 time: 0.5452s
Epoch: 0142 nll_train: 11527.1601155599 kl_train: -0.1705585085 mse_train: 0.0606692640 acc_train: 0.7083333333 nll_val: 11607.4175575658 kl_val: -0.1831771441 mse_val: 0.0610916703 acc_val: 0.5789473684 time: 0.5471s
Epoch: 0143 nll_train: 11507.9570719401 kl_train: -0.1787359559 mse_train: 0.0605681948 acc_train: 0.6875000000 nll_val: 11634.9182771382 kl_val: -0.1983223197 mse_val: 0.0612364119 acc_val: 0.6052631579 time: 0.5464s
Epoch: 0144 nll_train: 11519.1928304036 kl_train: -0.1728488539 mse_train: 0.0606273318 acc_train: 0.7291666667 nll_val: 11608.1332750822 kl_val: -0.1883559027 mse_val: 0.0610954381 acc_val: 0.6315789474 time: 0.5446s
Epoch: 0145 nll_train: 11522.3286539714 kl_train: -0.1810041688 mse_train: 0.0606438350 acc_train: 0.6666666667 nll_val: 11555.1568153783 kl_val: -0.1916705299 mse_val: 0.0608166143 acc_val: 0.6052631579 time: 0.5457s
Epoch: 0146 nll_train: 11535.5250651042 kl_train: -0.1871879299 mse_train: 0.0607132899 acc_train: 0.7083333333 nll_val: 11617.2309313322 kl_val: -0.2117327193 mse_val: 0.0611433210 acc_val: 0.6052631579 time: 0.5462s
Epoch: 0147 nll_train: 11549.2497558594 kl_train: -0.1933535341 mse_train: 0.0607855263 acc_train: 0.7291666667 nll_val: 11666.6246916118 kl_val: -0.1956589457 mse_val: 0.0614032877 acc_val: 0.6052631579 time: 0.5478s
Epoch: 0148 nll_train: 11530.1604410807 kl_train: -0.1964419720 mse_train: 0.0606850549 acc_train: 0.7083333333 nll_val: 11729.9552837171 kl_val: -0.2326252570 mse_val: 0.0617366080 acc_val: 0.6578947368 time: 0.5475s
Epoch: 0149 nll_train: 11531.8309326172 kl_train: -0.1935874829 mse_train: 0.0606938456 acc_train: 0.6875000000 nll_val: 11644.9644325658 kl_val: -0.2208462757 mse_val: 0.0612892871 acc_val: 0.6052631579 time: 0.5467s
Epoch: 0150 nll_train: 11547.8425292969 kl_train: -0.1583429504 mse_train: 0.0607781196 acc_train: 0.7291666667 nll_val: 11556.2031250000 kl_val: -0.1679096300 mse_val: 0.0608221226 acc_val: 0.6578947368 time: 0.5467s
Epoch: 0151 nll_train: 11514.1160481771 kl_train: -0.1723876946 mse_train: 0.0606006103 acc_train: 0.6875000000 nll_val: 11539.7311883224 kl_val: -0.1962359446 mse_val: 0.0607354272 acc_val: 0.6052631579 time: 0.5429s
Epoch: 0152 nll_train: 11502.9054768880 kl_train: -0.1777218040 mse_train: 0.0605416082 acc_train: 0.6666666667 nll_val: 11588.8132709704 kl_val: -0.2038995184 mse_val: 0.0609937532 acc_val: 0.6052631579 time: 0.5485s
Epoch: 0153 nll_train: 11512.9453125000 kl_train: -0.1654833636 mse_train: 0.0605944477 acc_train: 0.7083333333 nll_val: 11550.4541015625 kl_val: -0.1825414433 mse_val: 0.0607918640 acc_val: 0.6052631579 time: 0.5454s
Epoch: 0154 nll_train: 11511.9951171875 kl_train: -0.1739118077 mse_train: 0.0605894481 acc_train: 0.7083333333 nll_val: 11529.0071957237 kl_val: -0.1907654718 mse_val: 0.0606789865 acc_val: 0.5789473684 time: 0.5479s
Epoch: 0155 nll_train: 11520.4383544922 kl_train: -0.1711249215 mse_train: 0.0606338865 acc_train: 0.7083333333 nll_val: 11555.2221936678 kl_val: -0.2078418700 mse_val: 0.0608169593 acc_val: 0.6578947368 time: 0.5463s
Epoch: 0156 nll_train: 11518.6731770833 kl_train: -0.1913172522 mse_train: 0.0606245967 acc_train: 0.6666666667 nll_val: 11546.7013774671 kl_val: -0.2128327732 mse_val: 0.0607721129 acc_val: 0.6315789474 time: 0.5452s
Epoch: 0157 nll_train: 11540.6905517578 kl_train: -0.1738253543 mse_train: 0.0607404773 acc_train: 0.7083333333 nll_val: 11626.6644736842 kl_val: -0.2004862483 mse_val: 0.0611929739 acc_val: 0.6315789474 time: 0.5433s
Epoch: 0158 nll_train: 11523.8905436198 kl_train: -0.1669312626 mse_train: 0.0606520556 acc_train: 0.7083333333 nll_val: 11551.4868935033 kl_val: -0.1991966782 mse_val: 0.0607972980 acc_val: 0.6578947368 time: 0.5442s
Epoch: 0159 nll_train: 11519.0035807292 kl_train: -0.1722491086 mse_train: 0.0606263342 acc_train: 0.7291666667 nll_val: 11562.7475328947 kl_val: -0.1929949522 mse_val: 0.0608565660 acc_val: 0.6052631579 time: 0.5445s
Epoch: 0160 nll_train: 11522.4197591146 kl_train: -0.1846867890 mse_train: 0.0606443156 acc_train: 0.6250000000 nll_val: 11579.8258120888 kl_val: -0.2007467606 mse_val: 0.0609464518 acc_val: 0.6315789474 time: 0.5490s
Epoch: 0161 nll_train: 11504.7520345052 kl_train: -0.1755973172 mse_train: 0.0605513261 acc_train: 0.7083333333 nll_val: 11583.2593030428 kl_val: -0.2020206883 mse_val: 0.0609645214 acc_val: 0.6315789474 time: 0.5476s
Epoch: 0162 nll_train: 11504.1351318359 kl_train: -0.1853919653 mse_train: 0.0605480792 acc_train: 0.6666666667 nll_val: 11542.1620579770 kl_val: -0.1924666390 mse_val: 0.0607482211 acc_val: 0.6315789474 time: 0.5456s
Epoch: 0163 nll_train: 11499.2885742188 kl_train: -0.1767261283 mse_train: 0.0605225723 acc_train: 0.7083333333 nll_val: 11582.6575863487 kl_val: -0.2088851913 mse_val: 0.0609613559 acc_val: 0.5263157895 time: 0.5458s
Epoch: 0164 nll_train: 11491.8780110677 kl_train: -0.1771421451 mse_train: 0.0604835687 acc_train: 0.6458333333 nll_val: 11592.7624383224 kl_val: -0.1997809614 mse_val: 0.0610145395 acc_val: 0.6315789474 time: 0.5450s
Epoch: 0165 nll_train: 11496.6311035156 kl_train: -0.1676579705 mse_train: 0.0605085844 acc_train: 0.7083333333 nll_val: 11577.9174547697 kl_val: -0.1929268892 mse_val: 0.0609364084 acc_val: 0.6052631579 time: 0.5440s
Epoch: 0166 nll_train: 11514.8373616536 kl_train: -0.1792610909 mse_train: 0.0606044081 acc_train: 0.7291666667 nll_val: 11601.9651007401 kl_val: -0.2133323111 mse_val: 0.0610629744 acc_val: 0.6052631579 time: 0.5486s
Epoch: 0167 nll_train: 11505.5817871094 kl_train: -0.1806431230 mse_train: 0.0605556949 acc_train: 0.7291666667 nll_val: 11546.4002364309 kl_val: -0.1989044790 mse_val: 0.0607705277 acc_val: 0.6052631579 time: 0.5482s
Epoch: 0168 nll_train: 11510.9683837891 kl_train: -0.1760780226 mse_train: 0.0605840441 acc_train: 0.7500000000 nll_val: 11554.3823499178 kl_val: -0.1942429880 mse_val: 0.0608125404 acc_val: 0.6842105263 time: 0.5475s
Epoch: 0169 nll_train: 11497.8334960938 kl_train: -0.1775438242 mse_train: 0.0605149140 acc_train: 0.7291666667 nll_val: 11573.5374177632 kl_val: -0.2102573836 mse_val: 0.0609133557 acc_val: 0.6052631579 time: 0.5474s
Epoch: 0170 nll_train: 11496.7947998047 kl_train: -0.1750638746 mse_train: 0.0605094461 acc_train: 0.7083333333 nll_val: 11562.0684107730 kl_val: -0.1912059804 mse_val: 0.0608529905 acc_val: 0.5526315789 time: 0.5545s
Epoch: 0171 nll_train: 11499.0093994141 kl_train: -0.1971736594 mse_train: 0.0605211025 acc_train: 0.6458333333 nll_val: 11565.1229440789 kl_val: -0.2143079011 mse_val: 0.0608690677 acc_val: 0.5263157895 time: 0.5536s
Epoch: 0172 nll_train: 11491.0957031250 kl_train: -0.1803309312 mse_train: 0.0604794510 acc_train: 0.6458333333 nll_val: 11535.3475020559 kl_val: -0.2076175621 mse_val: 0.0607123563 acc_val: 0.6842105263 time: 0.5475s
Epoch: 0173 nll_train: 11483.3263346354 kl_train: -0.1766833958 mse_train: 0.0604385591 acc_train: 0.6666666667 nll_val: 11539.1911492599 kl_val: -0.2043422374 mse_val: 0.0607325848 acc_val: 0.6842105263 time: 0.5460s
Epoch: 0174 nll_train: 11483.2043863932 kl_train: -0.1787341026 mse_train: 0.0604379193 acc_train: 0.6458333333 nll_val: 11522.0221011513 kl_val: -0.1838300652 mse_val: 0.0606422228 acc_val: 0.6052631579 time: 0.5438s
Epoch: 0175 nll_train: 11484.2695719401 kl_train: -0.1687603764 mse_train: 0.0604435238 acc_train: 0.6875000000 nll_val: 11592.0927220395 kl_val: -0.2073892849 mse_val: 0.0610110124 acc_val: 0.6315789474 time: 0.5438s
Epoch: 0176 nll_train: 11493.4523111979 kl_train: -0.1777101147 mse_train: 0.0604918556 acc_train: 0.6666666667 nll_val: 11550.2946648849 kl_val: -0.1782767373 mse_val: 0.0607910239 acc_val: 0.6052631579 time: 0.5462s
Epoch: 0177 nll_train: 11496.4723307292 kl_train: -0.1652821194 mse_train: 0.0605077500 acc_train: 0.6666666667 nll_val: 11607.2644942434 kl_val: -0.2039910720 mse_val: 0.0610908648 acc_val: 0.6052631579 time: 0.5435s
Epoch: 0178 nll_train: 11491.1748860677 kl_train: -0.1754638121 mse_train: 0.0604798685 acc_train: 0.6458333333 nll_val: 11528.4191509046 kl_val: -0.1805950572 mse_val: 0.0606758898 acc_val: 0.6052631579 time: 0.5432s
Epoch: 0179 nll_train: 11485.3406982422 kl_train: -0.1716409192 mse_train: 0.0604491613 acc_train: 0.6666666667 nll_val: 11574.6189350329 kl_val: -0.1908336909 mse_val: 0.0609190478 acc_val: 0.6842105263 time: 0.5451s
Epoch: 0180 nll_train: 11476.7387288411 kl_train: -0.1728245659 mse_train: 0.0604038881 acc_train: 0.6666666667 nll_val: 11546.0498046875 kl_val: -0.1846275690 mse_val: 0.0607686845 acc_val: 0.6052631579 time: 0.5495s
Epoch: 0181 nll_train: 11480.4653727214 kl_train: -0.1729921270 mse_train: 0.0604235035 acc_train: 0.7083333333 nll_val: 11619.6113795230 kl_val: -0.2249371276 mse_val: 0.0611558515 acc_val: 0.6315789474 time: 0.5444s
Epoch: 0182 nll_train: 11506.1784261068 kl_train: -0.1724444333 mse_train: 0.0605588333 acc_train: 0.7083333333 nll_val: 11595.3777754934 kl_val: -0.1773731175 mse_val: 0.0610283031 acc_val: 0.6052631579 time: 0.5451s
Epoch: 0183 nll_train: 11492.4782714844 kl_train: -0.1612391323 mse_train: 0.0604867272 acc_train: 0.6666666667 nll_val: 11617.1150287829 kl_val: -0.1958676961 mse_val: 0.0611427103 acc_val: 0.6315789474 time: 0.5432s
Epoch: 0184 nll_train: 11494.7627360026 kl_train: -0.1684942326 mse_train: 0.0604987512 acc_train: 0.6250000000 nll_val: 11602.9325657895 kl_val: -0.1672569964 mse_val: 0.0610680662 acc_val: 0.6052631579 time: 0.5451s
Epoch: 0185 nll_train: 11487.5354003906 kl_train: -0.1770650099 mse_train: 0.0604607134 acc_train: 0.7083333333 nll_val: 11641.2613075658 kl_val: -0.1801427069 mse_val: 0.0612697978 acc_val: 0.6052631579 time: 0.5477s
Epoch: 0186 nll_train: 11492.3689371745 kl_train: -0.1633782601 mse_train: 0.0604861545 acc_train: 0.7291666667 nll_val: 11654.7258429276 kl_val: -0.2142276607 mse_val: 0.0613406634 acc_val: 0.6842105263 time: 0.5581s
Epoch: 0187 nll_train: 11502.8157145182 kl_train: -0.1923773612 mse_train: 0.0605411362 acc_train: 0.6458333333 nll_val: 11603.1836451480 kl_val: -0.1755884211 mse_val: 0.0610693891 acc_val: 0.6578947368 time: 0.5464s
Epoch: 0188 nll_train: 11496.4914957682 kl_train: -0.1712282424 mse_train: 0.0605078495 acc_train: 0.6875000000 nll_val: 11573.5785361842 kl_val: -0.2094558023 mse_val: 0.0609135734 acc_val: 0.6315789474 time: 0.5433s
Epoch: 0189 nll_train: 11481.8029785156 kl_train: -0.1741219660 mse_train: 0.0604305430 acc_train: 0.7083333333 nll_val: 11646.7565789474 kl_val: -0.2271929352 mse_val: 0.0612987192 acc_val: 0.6842105263 time: 0.5459s
Epoch: 0190 nll_train: 11492.1870524089 kl_train: -0.1909642067 mse_train: 0.0604851958 acc_train: 0.6458333333 nll_val: 11600.7843338816 kl_val: -0.1855875897 mse_val: 0.0610567598 acc_val: 0.6315789474 time: 0.5458s
Epoch: 0191 nll_train: 11478.3939615885 kl_train: -0.1743238512 mse_train: 0.0604125999 acc_train: 0.7083333333 nll_val: 11649.8323910362 kl_val: -0.1805779699 mse_val: 0.0613149069 acc_val: 0.6315789474 time: 0.5470s
Epoch: 0192 nll_train: 11486.8317057292 kl_train: -0.1762832018 mse_train: 0.0604570089 acc_train: 0.6875000000 nll_val: 11551.3663651316 kl_val: -0.1678449766 mse_val: 0.0607966667 acc_val: 0.6315789474 time: 0.5478s
Epoch: 0193 nll_train: 11485.5597737630 kl_train: -0.1645722125 mse_train: 0.0604503139 acc_train: 0.7083333333 nll_val: 11578.9041426809 kl_val: -0.2017115511 mse_val: 0.0609416013 acc_val: 0.6315789474 time: 0.5472s
Epoch: 0194 nll_train: 11480.4820556641 kl_train: -0.1730679215 mse_train: 0.0604235899 acc_train: 0.6875000000 nll_val: 11592.7701994243 kl_val: -0.2037062817 mse_val: 0.0610145797 acc_val: 0.6842105263 time: 0.5448s
Epoch: 0195 nll_train: 11485.0332845052 kl_train: -0.1692251253 mse_train: 0.0604475434 acc_train: 0.6666666667 nll_val: 11603.6514699836 kl_val: -0.1901287789 mse_val: 0.0610718506 acc_val: 0.6842105263 time: 0.5603s
Epoch: 0196 nll_train: 11495.2527669271 kl_train: -0.1566037593 mse_train: 0.0605013299 acc_train: 0.7708333333 nll_val: 11586.4569284539 kl_val: -0.1563601396 mse_val: 0.0609813509 acc_val: 0.6842105263 time: 0.5474s
Epoch: 0197 nll_train: 11485.1923828125 kl_train: -0.1736459049 mse_train: 0.0604483811 acc_train: 0.7083333333 nll_val: 11610.8016550164 kl_val: -0.1716688381 mse_val: 0.0611094821 acc_val: 0.6052631579 time: 0.5475s
Epoch: 0198 nll_train: 11496.7080078125 kl_train: -0.1599388927 mse_train: 0.0605089903 acc_train: 0.7083333333 nll_val: 11609.4785156250 kl_val: -0.1799313865 mse_val: 0.0611025181 acc_val: 0.6842105263 time: 0.5454s
Epoch: 0199 nll_train: 11472.8704427083 kl_train: -0.1633139833 mse_train: 0.0603835282 acc_train: 0.6875000000 nll_val: 11503.8397923520 kl_val: -0.1838719947 mse_val: 0.0605465268 acc_val: 0.5789473684 time: 0.5436s
Optimization finished
Best epoch 28
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 13498.2296463816 kl_test: -0.0165749961 mse_test: 0.0710433141 acc_test: 0.0000000000
MSE: [ 0.070948652923 , 0.068248294294 , 0.068340539932 , 0.068216659129 , 0.068480290473 , 0.068338274956 , 0.068462058902 , 0.068247102201 , 0.068391196430 , 0.068291075528 , 0.068279772997 , 0.068167164922 , 0.068424999714 , 0.068290457129 , 0.068393133581 , 0.068343259394 , 0.068348392844 , 0.068155325949 , 0.068080984056 ]
Accuracy for experiment id 11 is 0.0
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 15405.2268473307 kl_train: -0.2500474565 mse_train: 0.0810801423 acc_train: 0.1250000000 nll_val: 14241.0960629112 kl_val: -0.4363599843 mse_val: 0.0749531377 acc_val: 0.0263157895 time: 0.5369s
Best model so far, saving...
Epoch: 0001 nll_train: 12201.2662353516 kl_train: -0.3342663820 mse_train: 0.0642171907 acc_train: 0.0000000000 nll_val: 12601.5662520559 kl_val: -0.4228556940 mse_val: 0.0663240340 acc_val: 0.0263157895 time: 0.5449s
Best model so far, saving...
Epoch: 0002 nll_train: 11599.9145100911 kl_train: -0.3665931523 mse_train: 0.0610521816 acc_train: 0.0000000000 nll_val: 12107.4487561678 kl_val: -0.3034342057 mse_val: 0.0637234151 acc_val: 0.2105263158 time: 0.5571s
Best model so far, saving...
Epoch: 0003 nll_train: 11451.3800455729 kl_train: -0.2454039492 mse_train: 0.0602704204 acc_train: 0.0000000000 nll_val: 11631.1903268914 kl_val: -0.2053443186 mse_val: 0.0612167916 acc_val: 0.3684210526 time: 0.5434s
Best model so far, saving...
Epoch: 0004 nll_train: 11321.1306559245 kl_train: -0.1878647016 mse_train: 0.0595848984 acc_train: 0.0000000000 nll_val: 11314.7225534539 kl_val: -0.1691048494 mse_val: 0.0595511716 acc_val: 0.3421052632 time: 0.5466s
Best model so far, saving...
Epoch: 0005 nll_train: 11197.4375813802 kl_train: -0.1565785408 mse_train: 0.0589338834 acc_train: 0.0833333333 nll_val: 11175.7879831414 kl_val: -0.1434333399 mse_val: 0.0588199371 acc_val: 0.5000000000 time: 0.5542s
Best model so far, saving...
Epoch: 0006 nll_train: 11185.4322509766 kl_train: -0.1350517394 mse_train: 0.0588706957 acc_train: 0.1041666667 nll_val: 11095.8312602796 kl_val: -0.1287710694 mse_val: 0.0583991114 acc_val: 0.4473684211 time: 0.5464s
Best model so far, saving...
Epoch: 0007 nll_train: 11166.8929036458 kl_train: -0.1205066068 mse_train: 0.0587731195 acc_train: 0.1666666667 nll_val: 11056.6128700658 kl_val: -0.1172259834 mse_val: 0.0581926996 acc_val: 0.5000000000 time: 0.5445s
Best model so far, saving...
Epoch: 0008 nll_train: 11161.3452555339 kl_train: -0.1141516091 mse_train: 0.0587439237 acc_train: 0.1458333333 nll_val: 11037.1594366776 kl_val: -0.1122601209 mse_val: 0.0580903129 acc_val: 0.4473684211 time: 0.5471s
Best model so far, saving...
Epoch: 0009 nll_train: 11143.0738932292 kl_train: -0.1090734342 mse_train: 0.0586477586 acc_train: 0.1875000000 nll_val: 11020.2323704770 kl_val: -0.1031266600 mse_val: 0.0580012245 acc_val: 0.5526315789 time: 0.5435s
Best model so far, saving...
Epoch: 0010 nll_train: 11148.4813639323 kl_train: -0.1017576473 mse_train: 0.0586762194 acc_train: 0.3125000000 nll_val: 11012.9058388158 kl_val: -0.1037339200 mse_val: 0.0579626633 acc_val: 0.5789473684 time: 0.5459s
Best model so far, saving...
Epoch: 0011 nll_train: 11140.6713460286 kl_train: -0.1013086954 mse_train: 0.0586351110 acc_train: 0.3958333333 nll_val: 11006.5466694079 kl_val: -0.1026362130 mse_val: 0.0579291953 acc_val: 0.6842105263 time: 0.5427s
Best model so far, saving...
Epoch: 0012 nll_train: 11127.8701171875 kl_train: -0.1077280159 mse_train: 0.0585677357 acc_train: 0.4375000000 nll_val: 10996.6386718750 kl_val: -0.1043564897 mse_val: 0.0578770445 acc_val: 0.6842105263 time: 0.5426s
Best model so far, saving...
Epoch: 0013 nll_train: 11137.6838378906 kl_train: -0.1067139766 mse_train: 0.0586193884 acc_train: 0.5416666667 nll_val: 10985.7144325658 kl_val: -0.1045917586 mse_val: 0.0578195496 acc_val: 0.6578947368 time: 0.5437s
Best model so far, saving...
Epoch: 0014 nll_train: 11124.9420166016 kl_train: -0.1096373266 mse_train: 0.0585523259 acc_train: 0.5000000000 nll_val: 10988.0921052632 kl_val: -0.1052515220 mse_val: 0.0578320634 acc_val: 0.6842105263 time: 0.5435s
Epoch: 0015 nll_train: 11118.3048502604 kl_train: -0.1056693057 mse_train: 0.0585173940 acc_train: 0.5208333333 nll_val: 10964.6056229441 kl_val: -0.1043426277 mse_val: 0.0577084506 acc_val: 0.6578947368 time: 0.5411s
Best model so far, saving...
Epoch: 0016 nll_train: 11117.0510253906 kl_train: -0.1085335513 mse_train: 0.0585107945 acc_train: 0.4375000000 nll_val: 10966.1965460526 kl_val: -0.0968160406 mse_val: 0.0577168239 acc_val: 0.6842105263 time: 0.5447s
Epoch: 0017 nll_train: 11111.7177734375 kl_train: -0.0974140506 mse_train: 0.0584827261 acc_train: 0.5625000000 nll_val: 10948.7962068257 kl_val: -0.1008827278 mse_val: 0.0576252443 acc_val: 0.6842105263 time: 0.5394s
Best model so far, saving...
Epoch: 0018 nll_train: 11111.2608235677 kl_train: -0.1032943670 mse_train: 0.0584803207 acc_train: 0.5833333333 nll_val: 10968.8514083059 kl_val: -0.1098144266 mse_val: 0.0577307973 acc_val: 0.7368421053 time: 0.5454s
Epoch: 0019 nll_train: 11104.9005533854 kl_train: -0.1095356417 mse_train: 0.0584468458 acc_train: 0.5833333333 nll_val: 10972.7154091283 kl_val: -0.1142180515 mse_val: 0.0577511334 acc_val: 0.6842105263 time: 0.5412s
Epoch: 0020 nll_train: 11105.0154215495 kl_train: -0.1199851865 mse_train: 0.0584474495 acc_train: 0.5416666667 nll_val: 10980.1046977796 kl_val: -0.1149598811 mse_val: 0.0577900251 acc_val: 0.6842105263 time: 0.5396s
Epoch: 0021 nll_train: 11095.3981119792 kl_train: -0.1160888048 mse_train: 0.0583968313 acc_train: 0.5833333333 nll_val: 10962.0394222862 kl_val: -0.1221041911 mse_val: 0.0576949455 acc_val: 0.7368421053 time: 0.5422s
Epoch: 0022 nll_train: 11100.7792154948 kl_train: -0.1224751528 mse_train: 0.0584251547 acc_train: 0.6041666667 nll_val: 10987.7404399671 kl_val: -0.1206302204 mse_val: 0.0578302133 acc_val: 0.7631578947 time: 0.5484s
Epoch: 0023 nll_train: 11106.6413574219 kl_train: -0.1219219562 mse_train: 0.0584560069 acc_train: 0.6041666667 nll_val: 10944.1204255757 kl_val: -0.1167742704 mse_val: 0.0576006349 acc_val: 0.6578947368 time: 0.5412s
Best model so far, saving...
Epoch: 0024 nll_train: 11093.4499104818 kl_train: -0.1237545706 mse_train: 0.0583865795 acc_train: 0.5416666667 nll_val: 10947.6499280428 kl_val: -0.1226555066 mse_val: 0.0576192103 acc_val: 0.7105263158 time: 0.5444s
Epoch: 0025 nll_train: 11089.3854980469 kl_train: -0.1238998209 mse_train: 0.0583651865 acc_train: 0.6250000000 nll_val: 10951.9364206414 kl_val: -0.1207004821 mse_val: 0.0576417734 acc_val: 0.7368421053 time: 0.5422s
Epoch: 0026 nll_train: 11090.2990315755 kl_train: -0.1224515059 mse_train: 0.0583699953 acc_train: 0.6041666667 nll_val: 10951.3009354441 kl_val: -0.1227821912 mse_val: 0.0576384271 acc_val: 0.7368421053 time: 0.5405s
Epoch: 0027 nll_train: 11084.6103922526 kl_train: -0.1212399856 mse_train: 0.0583400559 acc_train: 0.6041666667 nll_val: 10936.5021073191 kl_val: -0.1198277077 mse_val: 0.0575605386 acc_val: 0.7368421053 time: 0.5418s
Best model so far, saving...
Epoch: 0028 nll_train: 11091.9870605469 kl_train: -0.1254238999 mse_train: 0.0583788798 acc_train: 0.6041666667 nll_val: 10932.1976768092 kl_val: -0.1128245965 mse_val: 0.0575378837 acc_val: 0.7105263158 time: 0.5457s
Best model so far, saving...
Epoch: 0029 nll_train: 11078.2922770182 kl_train: -0.1127589635 mse_train: 0.0583068008 acc_train: 0.6041666667 nll_val: 10947.8178453947 kl_val: -0.1205802328 mse_val: 0.0576200946 acc_val: 0.6578947368 time: 0.5459s
Epoch: 0030 nll_train: 11088.5686442057 kl_train: -0.1235591192 mse_train: 0.0583608882 acc_train: 0.5625000000 nll_val: 10933.8484272204 kl_val: -0.1243371552 mse_val: 0.0575465717 acc_val: 0.6842105263 time: 0.5429s
Epoch: 0031 nll_train: 11072.7320963542 kl_train: -0.1400723740 mse_train: 0.0582775370 acc_train: 0.5833333333 nll_val: 10959.1010999178 kl_val: -0.1406069735 mse_val: 0.0576794800 acc_val: 0.7105263158 time: 0.5418s
Epoch: 0032 nll_train: 11081.4300537109 kl_train: -0.1294104370 mse_train: 0.0583233177 acc_train: 0.6666666667 nll_val: 10947.7201377467 kl_val: -0.1162428589 mse_val: 0.0576195811 acc_val: 0.7631578947 time: 0.5442s
Epoch: 0033 nll_train: 11077.8882649740 kl_train: -0.1284917326 mse_train: 0.0583046768 acc_train: 0.6458333333 nll_val: 10957.9185855263 kl_val: -0.1425107470 mse_val: 0.0576732551 acc_val: 0.7105263158 time: 0.5425s
Epoch: 0034 nll_train: 11068.6949055990 kl_train: -0.1554059442 mse_train: 0.0582562890 acc_train: 0.6250000000 nll_val: 10981.4350842928 kl_val: -0.1548213978 mse_val: 0.0577970263 acc_val: 0.7368421053 time: 0.5406s
Epoch: 0035 nll_train: 11076.8492431641 kl_train: -0.1529253448 mse_train: 0.0582992076 acc_train: 0.6041666667 nll_val: 10971.3668277138 kl_val: -0.1411878443 mse_val: 0.0577440366 acc_val: 0.6842105263 time: 0.5412s
Epoch: 0036 nll_train: 11076.1854248047 kl_train: -0.1412611554 mse_train: 0.0582957130 acc_train: 0.6250000000 nll_val: 10958.2013260691 kl_val: -0.1415764242 mse_val: 0.0576747446 acc_val: 0.7105263158 time: 0.5429s
Epoch: 0037 nll_train: 11069.6752929688 kl_train: -0.1453961612 mse_train: 0.0582614482 acc_train: 0.6041666667 nll_val: 10973.5119243421 kl_val: -0.1396511303 mse_val: 0.0577553264 acc_val: 0.7105263158 time: 0.5420s
Epoch: 0038 nll_train: 11078.6896972656 kl_train: -0.1439525488 mse_train: 0.0583088929 acc_train: 0.6041666667 nll_val: 10957.7117084704 kl_val: -0.1310800588 mse_val: 0.0576721675 acc_val: 0.6315789474 time: 0.5440s
Epoch: 0039 nll_train: 11072.5693766276 kl_train: -0.1355845993 mse_train: 0.0582766798 acc_train: 0.6250000000 nll_val: 10956.7652652138 kl_val: -0.1324964119 mse_val: 0.0576671854 acc_val: 0.7105263158 time: 0.5420s
Epoch: 0040 nll_train: 11073.3371582031 kl_train: -0.1365566573 mse_train: 0.0582807199 acc_train: 0.4791666667 nll_val: 10944.7301089638 kl_val: -0.1277184561 mse_val: 0.0576038419 acc_val: 0.6315789474 time: 0.5432s
Epoch: 0041 nll_train: 11069.6050211589 kl_train: -0.1217623791 mse_train: 0.0582610796 acc_train: 0.5208333333 nll_val: 10942.7096011513 kl_val: -0.1263758862 mse_val: 0.0575932084 acc_val: 0.6315789474 time: 0.5436s
Epoch: 0042 nll_train: 11064.1182454427 kl_train: -0.1321103306 mse_train: 0.0582322005 acc_train: 0.5625000000 nll_val: 10969.9936780428 kl_val: -0.1370261759 mse_val: 0.0577368091 acc_val: 0.7105263158 time: 0.5412s
Epoch: 0043 nll_train: 11061.7203776042 kl_train: -0.1441999683 mse_train: 0.0582195818 acc_train: 0.5625000000 nll_val: 10972.7660875822 kl_val: -0.1511412490 mse_val: 0.0577513991 acc_val: 0.6578947368 time: 0.5416s
Epoch: 0044 nll_train: 11062.7834065755 kl_train: -0.1422934306 mse_train: 0.0582251774 acc_train: 0.6041666667 nll_val: 10969.4057874178 kl_val: -0.1351564754 mse_val: 0.0577337154 acc_val: 0.7368421053 time: 0.5427s
Epoch: 0045 nll_train: 11062.8098958333 kl_train: -0.1440074872 mse_train: 0.0582253149 acc_train: 0.5625000000 nll_val: 10967.9905941612 kl_val: -0.1343081468 mse_val: 0.0577262669 acc_val: 0.6842105263 time: 0.5439s
Epoch: 0046 nll_train: 11070.8325195312 kl_train: -0.1424160680 mse_train: 0.0582675387 acc_train: 0.6250000000 nll_val: 10987.0668688322 kl_val: -0.1460809292 mse_val: 0.0578266698 acc_val: 0.6578947368 time: 0.5438s
Epoch: 0047 nll_train: 11058.7646077474 kl_train: -0.1488516716 mse_train: 0.0582040247 acc_train: 0.6250000000 nll_val: 10988.1180612664 kl_val: -0.1418871366 mse_val: 0.0578322016 acc_val: 0.7105263158 time: 0.5399s
Epoch: 0048 nll_train: 11060.1306152344 kl_train: -0.1426070808 mse_train: 0.0582112122 acc_train: 0.5625000000 nll_val: 10989.4218750000 kl_val: -0.1455152945 mse_val: 0.0578390628 acc_val: 0.7368421053 time: 0.5398s
Epoch: 0049 nll_train: 11060.5404459635 kl_train: -0.1518056088 mse_train: 0.0582133699 acc_train: 0.6250000000 nll_val: 10984.7156661184 kl_val: -0.1451139368 mse_val: 0.0578142924 acc_val: 0.7368421053 time: 0.5410s
Epoch: 0050 nll_train: 11055.4394124349 kl_train: -0.1494498461 mse_train: 0.0581865228 acc_train: 0.6041666667 nll_val: 10975.3914473684 kl_val: -0.1545880676 mse_val: 0.0577652170 acc_val: 0.6842105263 time: 0.5418s
Epoch: 0051 nll_train: 11057.5211995443 kl_train: -0.1526487752 mse_train: 0.0581974794 acc_train: 0.6458333333 nll_val: 10964.8581414474 kl_val: -0.1364449521 mse_val: 0.0577097808 acc_val: 0.7105263158 time: 0.5435s
Epoch: 0052 nll_train: 11048.3047281901 kl_train: -0.1571176828 mse_train: 0.0581489736 acc_train: 0.5625000000 nll_val: 10996.7935341283 kl_val: -0.1678245715 mse_val: 0.0578778600 acc_val: 0.7105263158 time: 0.5404s
Epoch: 0053 nll_train: 11053.0696207682 kl_train: -0.1472475166 mse_train: 0.0581740498 acc_train: 0.6458333333 nll_val: 10978.4867393092 kl_val: -0.1410747461 mse_val: 0.0577815091 acc_val: 0.6842105263 time: 0.5426s
Epoch: 0054 nll_train: 11051.2184651693 kl_train: -0.1383441010 mse_train: 0.0581643065 acc_train: 0.6666666667 nll_val: 10981.6719263980 kl_val: -0.1324156084 mse_val: 0.0577982735 acc_val: 0.8157894737 time: 0.5430s
Epoch: 0055 nll_train: 11059.1253662109 kl_train: -0.1376228348 mse_train: 0.0582059238 acc_train: 0.7500000000 nll_val: 10986.8755139803 kl_val: -0.1422750887 mse_val: 0.0578256606 acc_val: 0.7368421053 time: 0.5436s
Epoch: 0056 nll_train: 11048.8755696615 kl_train: -0.1433283308 mse_train: 0.0581519771 acc_train: 0.5833333333 nll_val: 10971.0803865132 kl_val: -0.1405357205 mse_val: 0.0577425284 acc_val: 0.7105263158 time: 0.5406s
Epoch: 0057 nll_train: 11050.5095214844 kl_train: -0.1441027634 mse_train: 0.0581605773 acc_train: 0.6875000000 nll_val: 11019.4914679276 kl_val: -0.1454552518 mse_val: 0.0579973255 acc_val: 0.7631578947 time: 0.5388s
Epoch: 0058 nll_train: 11053.1522623698 kl_train: -0.1508795051 mse_train: 0.0581744852 acc_train: 0.6458333333 nll_val: 10976.7164370888 kl_val: -0.1483319288 mse_val: 0.0577721929 acc_val: 0.7368421053 time: 0.5427s
Epoch: 0059 nll_train: 11048.7675374349 kl_train: -0.1525594800 mse_train: 0.0581514078 acc_train: 0.6666666667 nll_val: 10996.5152138158 kl_val: -0.1525441608 mse_val: 0.0578763956 acc_val: 0.7631578947 time: 0.5390s
Epoch: 0060 nll_train: 11052.2874755859 kl_train: -0.1563991262 mse_train: 0.0581699332 acc_train: 0.6250000000 nll_val: 10992.6224300987 kl_val: -0.1478396654 mse_val: 0.0578559086 acc_val: 0.6842105263 time: 0.5446s
Epoch: 0061 nll_train: 11047.3799641927 kl_train: -0.1400441648 mse_train: 0.0581441065 acc_train: 0.6041666667 nll_val: 10962.7062088816 kl_val: -0.1374173011 mse_val: 0.0576984547 acc_val: 0.7105263158 time: 0.5400s
Epoch: 0062 nll_train: 11047.1378580729 kl_train: -0.1471177532 mse_train: 0.0581428305 acc_train: 0.6250000000 nll_val: 10983.8351151316 kl_val: -0.1533347556 mse_val: 0.0578096589 acc_val: 0.7105263158 time: 0.5435s
Epoch: 0063 nll_train: 11047.8072102865 kl_train: -0.1490614290 mse_train: 0.0581463532 acc_train: 0.6666666667 nll_val: 10993.2919407895 kl_val: -0.1465690269 mse_val: 0.0578594312 acc_val: 0.6842105263 time: 0.5430s
Epoch: 0064 nll_train: 11053.3305257161 kl_train: -0.1519311505 mse_train: 0.0581754255 acc_train: 0.5625000000 nll_val: 10982.0158305921 kl_val: -0.1631957438 mse_val: 0.0578000832 acc_val: 0.6578947368 time: 0.5410s
Epoch: 0065 nll_train: 11045.6209309896 kl_train: -0.1557090602 mse_train: 0.0581348465 acc_train: 0.5833333333 nll_val: 10989.6501850329 kl_val: -0.1503092490 mse_val: 0.0578402638 acc_val: 0.7105263158 time: 0.5410s
Epoch: 0066 nll_train: 11048.9024251302 kl_train: -0.1591091631 mse_train: 0.0581521187 acc_train: 0.6458333333 nll_val: 10997.6387232730 kl_val: -0.1603121071 mse_val: 0.0578823105 acc_val: 0.7368421053 time: 0.5435s
Epoch: 0067 nll_train: 11038.9947509766 kl_train: -0.1524986677 mse_train: 0.0580999727 acc_train: 0.6666666667 nll_val: 11043.1111739309 kl_val: -0.1524655250 mse_val: 0.0581216381 acc_val: 0.7631578947 time: 0.5432s
Epoch: 0068 nll_train: 11037.0160725911 kl_train: -0.1597035428 mse_train: 0.0580895582 acc_train: 0.6250000000 nll_val: 11034.9601665296 kl_val: -0.1534453572 mse_val: 0.0580787384 acc_val: 0.7631578947 time: 0.5407s
Epoch: 0069 nll_train: 11042.9477132161 kl_train: -0.1478032852 mse_train: 0.0581207769 acc_train: 0.6875000000 nll_val: 11017.2869037829 kl_val: -0.1436375932 mse_val: 0.0579857201 acc_val: 0.7368421053 time: 0.5416s
Epoch: 0070 nll_train: 11043.6585286458 kl_train: -0.1487590584 mse_train: 0.0581245191 acc_train: 0.6250000000 nll_val: 11005.7388466283 kl_val: -0.1508149105 mse_val: 0.0579249426 acc_val: 0.7105263158 time: 0.5428s
Epoch: 0071 nll_train: 11049.5662434896 kl_train: -0.1495711453 mse_train: 0.0581556127 acc_train: 0.5833333333 nll_val: 10991.8502775493 kl_val: -0.1579554704 mse_val: 0.0578518453 acc_val: 0.6842105263 time: 0.5414s
Epoch: 0072 nll_train: 11038.5418701172 kl_train: -0.1571836627 mse_train: 0.0580975888 acc_train: 0.6041666667 nll_val: 11028.8595291941 kl_val: -0.1684449013 mse_val: 0.0580466299 acc_val: 0.7105263158 time: 0.5425s
Epoch: 0073 nll_train: 11038.1289469401 kl_train: -0.1536217347 mse_train: 0.0580954147 acc_train: 0.6250000000 nll_val: 11024.9626850329 kl_val: -0.1480472798 mse_val: 0.0580261193 acc_val: 0.7894736842 time: 0.5430s
Epoch: 0074 nll_train: 11041.3413492839 kl_train: -0.1538698956 mse_train: 0.0581123244 acc_train: 0.6458333333 nll_val: 11050.5795641447 kl_val: -0.1539994157 mse_val: 0.0581609448 acc_val: 0.7631578947 time: 0.5421s
Epoch: 0075 nll_train: 11036.0123697917 kl_train: -0.1597236131 mse_train: 0.0580842759 acc_train: 0.6250000000 nll_val: 11054.2816097862 kl_val: -0.1582165231 mse_val: 0.0581804306 acc_val: 0.7368421053 time: 0.5411s
Epoch: 0076 nll_train: 11053.6888427734 kl_train: -0.1567313612 mse_train: 0.0581773096 acc_train: 0.6666666667 nll_val: 11026.9266036184 kl_val: -0.1534045562 mse_val: 0.0580364548 acc_val: 0.7368421053 time: 0.5439s
Epoch: 0077 nll_train: 11036.6825358073 kl_train: -0.1625118147 mse_train: 0.0580878036 acc_train: 0.6458333333 nll_val: 11038.5434313322 kl_val: -0.1565207106 mse_val: 0.0580975962 acc_val: 0.7631578947 time: 0.5445s
Epoch: 0078 nll_train: 11036.7222493490 kl_train: -0.1599931670 mse_train: 0.0580880120 acc_train: 0.6250000000 nll_val: 11023.8827097039 kl_val: -0.1480289094 mse_val: 0.0580204355 acc_val: 0.7105263158 time: 0.5439s
Epoch: 0079 nll_train: 11042.8005777995 kl_train: -0.1563893106 mse_train: 0.0581200030 acc_train: 0.6666666667 nll_val: 11076.1690481086 kl_val: -0.1667444330 mse_val: 0.0582956260 acc_val: 0.7105263158 time: 0.5437s
Epoch: 0080 nll_train: 11041.3140462240 kl_train: -0.1596347482 mse_train: 0.0581121791 acc_train: 0.6250000000 nll_val: 10997.5900493421 kl_val: -0.1562162971 mse_val: 0.0578820521 acc_val: 0.7631578947 time: 0.5422s
Epoch: 0081 nll_train: 11040.3138834635 kl_train: -0.1543286610 mse_train: 0.0581069160 acc_train: 0.6666666667 nll_val: 11030.4762541118 kl_val: -0.1482718446 mse_val: 0.0580551399 acc_val: 0.7105263158 time: 0.5424s
Epoch: 0082 nll_train: 11045.3627115885 kl_train: -0.1573646618 mse_train: 0.0581334870 acc_train: 0.6666666667 nll_val: 11016.9129831414 kl_val: -0.1570818632 mse_val: 0.0579837535 acc_val: 0.7894736842 time: 0.5436s
Epoch: 0083 nll_train: 11040.3356933594 kl_train: -0.1539678484 mse_train: 0.0581070314 acc_train: 0.6041666667 nll_val: 10986.7379728618 kl_val: -0.1546487071 mse_val: 0.0578249367 acc_val: 0.6842105263 time: 0.5429s
Epoch: 0084 nll_train: 11032.7693684896 kl_train: -0.1542396424 mse_train: 0.0580672075 acc_train: 0.6250000000 nll_val: 11006.4104646382 kl_val: -0.1584868580 mse_val: 0.0579284777 acc_val: 0.7894736842 time: 0.5404s
Epoch: 0085 nll_train: 11032.5707194010 kl_train: -0.1560988265 mse_train: 0.0580661612 acc_train: 0.6250000000 nll_val: 11001.9614000822 kl_val: -0.1485545137 mse_val: 0.0579050605 acc_val: 0.6842105263 time: 0.5428s
Epoch: 0086 nll_train: 11029.6063232422 kl_train: -0.1587267763 mse_train: 0.0580505609 acc_train: 0.6250000000 nll_val: 11011.3638980263 kl_val: -0.1740895745 mse_val: 0.0579545480 acc_val: 0.7894736842 time: 0.5425s
Epoch: 0087 nll_train: 11029.8057861328 kl_train: -0.1706445105 mse_train: 0.0580516102 acc_train: 0.6458333333 nll_val: 11055.7161800987 kl_val: -0.1648638405 mse_val: 0.0581879798 acc_val: 0.8157894737 time: 0.5432s
Epoch: 0088 nll_train: 11035.8696289062 kl_train: -0.1754447104 mse_train: 0.0580835238 acc_train: 0.6250000000 nll_val: 11042.6795847039 kl_val: -0.1786214799 mse_val: 0.0581193674 acc_val: 0.7105263158 time: 0.5435s
Epoch: 0089 nll_train: 11030.5933430990 kl_train: -0.1647638157 mse_train: 0.0580557552 acc_train: 0.6458333333 nll_val: 11019.2634662829 kl_val: -0.1573444973 mse_val: 0.0579961232 acc_val: 0.7368421053 time: 0.5414s
Epoch: 0090 nll_train: 11035.0035400391 kl_train: -0.1567761389 mse_train: 0.0580789670 acc_train: 0.6458333333 nll_val: 11015.3272512336 kl_val: -0.1604456172 mse_val: 0.0579754065 acc_val: 0.7368421053 time: 0.5430s
Epoch: 0091 nll_train: 11024.4491373698 kl_train: -0.1583771644 mse_train: 0.0580234163 acc_train: 0.7083333333 nll_val: 11016.9260382401 kl_val: -0.1728182474 mse_val: 0.0579838217 acc_val: 0.6842105263 time: 0.5412s
Epoch: 0092 nll_train: 11044.3779703776 kl_train: -0.1556290323 mse_train: 0.0581283046 acc_train: 0.6041666667 nll_val: 11025.5824424342 kl_val: -0.1588708390 mse_val: 0.0580293828 acc_val: 0.6842105263 time: 0.5434s
Epoch: 0093 nll_train: 11034.2049153646 kl_train: -0.1673405267 mse_train: 0.0580747644 acc_train: 0.6458333333 nll_val: 11003.3441097862 kl_val: -0.1650235735 mse_val: 0.0579123368 acc_val: 0.7368421053 time: 0.5421s
Epoch: 0094 nll_train: 11031.0735270182 kl_train: -0.1605647489 mse_train: 0.0580582816 acc_train: 0.6250000000 nll_val: 11020.5850637336 kl_val: -0.1727278578 mse_val: 0.0580030797 acc_val: 0.7105263158 time: 0.5425s
Epoch: 0095 nll_train: 11035.5858968099 kl_train: -0.1681005005 mse_train: 0.0580820305 acc_train: 0.6250000000 nll_val: 11023.5489823191 kl_val: -0.1669580576 mse_val: 0.0580186775 acc_val: 0.7631578947 time: 0.5409s
Epoch: 0096 nll_train: 11043.7932942708 kl_train: -0.1461199131 mse_train: 0.0581252285 acc_train: 0.6875000000 nll_val: 11012.1547594572 kl_val: -0.1592632175 mse_val: 0.0579587082 acc_val: 0.7105263158 time: 0.5427s
Epoch: 0097 nll_train: 11047.6643473307 kl_train: -0.1647329237 mse_train: 0.0581456035 acc_train: 0.6041666667 nll_val: 11010.8337273849 kl_val: -0.1712604063 mse_val: 0.0579517590 acc_val: 0.7631578947 time: 0.5428s
Epoch: 0098 nll_train: 11049.3277587891 kl_train: -0.1679894732 mse_train: 0.0581543582 acc_train: 0.7083333333 nll_val: 11015.3597347862 kl_val: -0.1753182599 mse_val: 0.0579755791 acc_val: 0.7631578947 time: 0.5412s
Epoch: 0099 nll_train: 11041.5445963542 kl_train: -0.1527445614 mse_train: 0.0581133937 acc_train: 0.6458333333 nll_val: 11030.5067331414 kl_val: -0.1491373362 mse_val: 0.0580552975 acc_val: 0.7105263158 time: 0.5427s
Epoch: 0100 nll_train: 11030.9821370443 kl_train: -0.1698500700 mse_train: 0.0580578002 acc_train: 0.6250000000 nll_val: 11057.1349712171 kl_val: -0.2041972915 mse_val: 0.0581954459 acc_val: 0.7105263158 time: 0.5405s
Epoch: 0101 nll_train: 11031.4433593750 kl_train: -0.1710103868 mse_train: 0.0580602265 acc_train: 0.6041666667 nll_val: 11058.4638671875 kl_val: -0.1831005110 mse_val: 0.0582024422 acc_val: 0.6842105263 time: 0.5413s
Epoch: 0102 nll_train: 11033.8729654948 kl_train: -0.1631672721 mse_train: 0.0580730160 acc_train: 0.6458333333 nll_val: 11038.8316714638 kl_val: -0.1607117814 mse_val: 0.0580991146 acc_val: 0.7894736842 time: 0.5433s
Epoch: 0103 nll_train: 11028.4253336589 kl_train: -0.1689613300 mse_train: 0.0580443443 acc_train: 0.7291666667 nll_val: 11025.0379831414 kl_val: -0.1628950203 mse_val: 0.0580265183 acc_val: 0.7631578947 time: 0.5418s
Epoch: 0104 nll_train: 11024.0524088542 kl_train: -0.1444923983 mse_train: 0.0580213284 acc_train: 0.6875000000 nll_val: 11059.2295949836 kl_val: -0.1603823485 mse_val: 0.0582064714 acc_val: 0.6578947368 time: 0.5441s
Epoch: 0105 nll_train: 11034.3678385417 kl_train: -0.1645951156 mse_train: 0.0580756214 acc_train: 0.6041666667 nll_val: 11037.6423211349 kl_val: -0.1870126767 mse_val: 0.0580928535 acc_val: 0.7368421053 time: 0.5434s
Epoch: 0106 nll_train: 11028.5617675781 kl_train: -0.1733568404 mse_train: 0.0580450625 acc_train: 0.7291666667 nll_val: 11057.9696751645 kl_val: -0.1766244306 mse_val: 0.0581998413 acc_val: 0.7631578947 time: 0.5416s
Epoch: 0107 nll_train: 11029.8776855469 kl_train: -0.1681305543 mse_train: 0.0580519883 acc_train: 0.6875000000 nll_val: 11044.0086862664 kl_val: -0.1782563901 mse_val: 0.0581263629 acc_val: 0.7894736842 time: 0.5424s
Epoch: 0108 nll_train: 11027.2542724609 kl_train: -0.1554619167 mse_train: 0.0580381819 acc_train: 0.6875000000 nll_val: 11052.8569592928 kl_val: -0.1682391371 mse_val: 0.0581729310 acc_val: 0.7894736842 time: 0.5430s
Epoch: 0109 nll_train: 11036.9197184245 kl_train: -0.1784845445 mse_train: 0.0580890509 acc_train: 0.6041666667 nll_val: 11060.3469366776 kl_val: -0.1596159974 mse_val: 0.0582123522 acc_val: 0.8421052632 time: 0.5419s
Epoch: 0110 nll_train: 11016.5259195964 kl_train: -0.1648424817 mse_train: 0.0579817158 acc_train: 0.6458333333 nll_val: 11084.1180612664 kl_val: -0.1771870824 mse_val: 0.0583374626 acc_val: 0.6578947368 time: 0.5475s
Epoch: 0111 nll_train: 11025.8542887370 kl_train: -0.1530504460 mse_train: 0.0580308121 acc_train: 0.6875000000 nll_val: 11062.4090768914 kl_val: -0.1621515461 mse_val: 0.0582232048 acc_val: 0.7105263158 time: 0.5534s
Epoch: 0112 nll_train: 11017.3885091146 kl_train: -0.1596614600 mse_train: 0.0579862551 acc_train: 0.6250000000 nll_val: 11043.8457545230 kl_val: -0.1524077293 mse_val: 0.0581255038 acc_val: 0.7631578947 time: 0.5486s
Epoch: 0113 nll_train: 11013.5673014323 kl_train: -0.1559192349 mse_train: 0.0579661437 acc_train: 0.7708333333 nll_val: 11090.1438116776 kl_val: -0.1851772439 mse_val: 0.0583691791 acc_val: 0.7894736842 time: 0.5481s
Epoch: 0114 nll_train: 11014.7162679036 kl_train: -0.1691345566 mse_train: 0.0579721909 acc_train: 0.7291666667 nll_val: 11054.1720805921 kl_val: -0.1712271112 mse_val: 0.0581798548 acc_val: 0.8684210526 time: 0.5455s
Epoch: 0115 nll_train: 11010.7841796875 kl_train: -0.1731628881 mse_train: 0.0579514972 acc_train: 0.6875000000 nll_val: 11104.9142680921 kl_val: -0.1955287825 mse_val: 0.0584469179 acc_val: 0.7105263158 time: 0.5461s
Epoch: 0116 nll_train: 11012.5122477214 kl_train: -0.1783337103 mse_train: 0.0579605907 acc_train: 0.6875000000 nll_val: 11077.9958881579 kl_val: -0.1824127012 mse_val: 0.0583052412 acc_val: 0.7368421053 time: 0.5445s
Epoch: 0117 nll_train: 11006.6805419922 kl_train: -0.1521647781 mse_train: 0.0579298975 acc_train: 0.7500000000 nll_val: 11050.7705078125 kl_val: -0.1709490340 mse_val: 0.0581619498 acc_val: 0.7894736842 time: 0.5414s
Epoch: 0118 nll_train: 11006.5515950521 kl_train: -0.1759027559 mse_train: 0.0579292189 acc_train: 0.5833333333 nll_val: 11067.3736636513 kl_val: -0.1624923882 mse_val: 0.0582493357 acc_val: 0.7368421053 time: 0.5461s
Epoch: 0119 nll_train: 11003.7714436849 kl_train: -0.1618683816 mse_train: 0.0579145861 acc_train: 0.7708333333 nll_val: 11074.9965049342 kl_val: -0.1820689130 mse_val: 0.0582894559 acc_val: 0.8157894737 time: 0.5440s
Epoch: 0120 nll_train: 11004.1234944661 kl_train: -0.1785920632 mse_train: 0.0579164394 acc_train: 0.6666666667 nll_val: 11075.8533614309 kl_val: -0.1917078738 mse_val: 0.0582939669 acc_val: 0.7105263158 time: 0.5479s
Epoch: 0121 nll_train: 11003.2692871094 kl_train: -0.1907519909 mse_train: 0.0579119440 acc_train: 0.6041666667 nll_val: 11098.4695723684 kl_val: -0.1890695197 mse_val: 0.0584129991 acc_val: 0.7105263158 time: 0.5425s
Epoch: 0122 nll_train: 10993.8927001953 kl_train: -0.1888629974 mse_train: 0.0578625930 acc_train: 0.5416666667 nll_val: 11108.9100534539 kl_val: -0.1802777977 mse_val: 0.0584679479 acc_val: 0.5526315789 time: 0.5419s
Epoch: 0123 nll_train: 10997.3860270182 kl_train: -0.1617699666 mse_train: 0.0578809789 acc_train: 0.6875000000 nll_val: 11094.9057360197 kl_val: -0.1900840253 mse_val: 0.0583942407 acc_val: 0.6842105263 time: 0.5419s
Epoch: 0124 nll_train: 10992.4937337240 kl_train: -0.1824665787 mse_train: 0.0578552300 acc_train: 0.6666666667 nll_val: 11085.0300164474 kl_val: -0.1830471448 mse_val: 0.0583422649 acc_val: 0.7105263158 time: 0.5425s
Epoch: 0125 nll_train: 10994.6593017578 kl_train: -0.1666042016 mse_train: 0.0578666280 acc_train: 0.6458333333 nll_val: 11106.8997224507 kl_val: -0.1790666008 mse_val: 0.0584573687 acc_val: 0.7105263158 time: 0.5416s
Epoch: 0126 nll_train: 10993.8666992188 kl_train: -0.1640328237 mse_train: 0.0578624558 acc_train: 0.6250000000 nll_val: 11077.3449321546 kl_val: -0.1995287036 mse_val: 0.0583018147 acc_val: 0.7368421053 time: 0.5474s
Epoch: 0127 nll_train: 10989.7601318359 kl_train: -0.1819917960 mse_train: 0.0578408442 acc_train: 0.5833333333 nll_val: 11056.3693462171 kl_val: -0.1862894752 mse_val: 0.0581914175 acc_val: 0.7105263158 time: 0.5461s
Epoch: 0128 nll_train: 10988.8326009115 kl_train: -0.1744573402 mse_train: 0.0578359626 acc_train: 0.7291666667 nll_val: 11095.3604543586 kl_val: -0.1773378673 mse_val: 0.0583966361 acc_val: 0.7368421053 time: 0.5426s
Epoch: 0129 nll_train: 10993.1422526042 kl_train: -0.1684401836 mse_train: 0.0578586434 acc_train: 0.5625000000 nll_val: 11073.2735402961 kl_val: -0.1873023957 mse_val: 0.0582803874 acc_val: 0.5789473684 time: 0.5398s
Epoch: 0130 nll_train: 10989.2095540365 kl_train: -0.1580006285 mse_train: 0.0578379459 acc_train: 0.7291666667 nll_val: 11083.6981907895 kl_val: -0.1741908749 mse_val: 0.0583352529 acc_val: 0.7631578947 time: 0.5413s
Epoch: 0131 nll_train: 10988.4104410807 kl_train: -0.1753340950 mse_train: 0.0578337391 acc_train: 0.6041666667 nll_val: 11108.2246093750 kl_val: -0.2006113612 mse_val: 0.0584643424 acc_val: 0.5789473684 time: 0.5434s
Epoch: 0132 nll_train: 10993.2452392578 kl_train: -0.1693500175 mse_train: 0.0578591876 acc_train: 0.6666666667 nll_val: 11048.9854543586 kl_val: -0.1577656155 mse_val: 0.0581525556 acc_val: 0.8157894737 time: 0.5409s
Epoch: 0133 nll_train: 10999.0485026042 kl_train: -0.1506804461 mse_train: 0.0578897302 acc_train: 0.6875000000 nll_val: 11035.2152035362 kl_val: -0.1621227202 mse_val: 0.0580800807 acc_val: 0.8421052632 time: 0.5415s
Epoch: 0134 nll_train: 10992.5351562500 kl_train: -0.1719929359 mse_train: 0.0578554488 acc_train: 0.6458333333 nll_val: 11113.5166015625 kl_val: -0.2027204978 mse_val: 0.0584921949 acc_val: 0.6578947368 time: 0.5425s
Epoch: 0135 nll_train: 10996.8192545573 kl_train: -0.1745332380 mse_train: 0.0578779969 acc_train: 0.6666666667 nll_val: 11070.1413959704 kl_val: -0.1873721853 mse_val: 0.0582639018 acc_val: 0.7894736842 time: 0.5435s
Epoch: 0136 nll_train: 10982.5570068359 kl_train: -0.1778493126 mse_train: 0.0578029314 acc_train: 0.6041666667 nll_val: 11129.6410875822 kl_val: -0.1912245821 mse_val: 0.0585770595 acc_val: 0.7368421053 time: 0.5419s
Epoch: 0137 nll_train: 10993.6119384766 kl_train: -0.1817359735 mse_train: 0.0578611162 acc_train: 0.6250000000 nll_val: 11063.3489412007 kl_val: -0.1769186474 mse_val: 0.0582281530 acc_val: 0.8157894737 time: 0.5457s
Epoch: 0138 nll_train: 10978.7415364583 kl_train: -0.1689344278 mse_train: 0.0577828515 acc_train: 0.6458333333 nll_val: 11078.1884765625 kl_val: -0.1704386610 mse_val: 0.0583062537 acc_val: 0.7105263158 time: 0.5406s
Epoch: 0139 nll_train: 10982.5423177083 kl_train: -0.1811169839 mse_train: 0.0578028549 acc_train: 0.6666666667 nll_val: 11096.9103104441 kl_val: -0.2055446623 mse_val: 0.0584047909 acc_val: 0.8157894737 time: 0.5403s
Epoch: 0140 nll_train: 10982.3399251302 kl_train: -0.1826468886 mse_train: 0.0578017894 acc_train: 0.6875000000 nll_val: 11108.1957236842 kl_val: -0.1877506066 mse_val: 0.0584641883 acc_val: 0.7105263158 time: 0.5421s
Epoch: 0141 nll_train: 10993.3627929688 kl_train: -0.1579110852 mse_train: 0.0578598028 acc_train: 0.7083333333 nll_val: 11054.1141550164 kl_val: -0.1672138474 mse_val: 0.0581795491 acc_val: 0.7631578947 time: 0.5442s
Epoch: 0142 nll_train: 10982.2975667318 kl_train: -0.1637987836 mse_train: 0.0578015666 acc_train: 0.6875000000 nll_val: 11089.1997841283 kl_val: -0.1810366660 mse_val: 0.0583642100 acc_val: 0.7631578947 time: 0.5427s
Epoch: 0143 nll_train: 10983.1356608073 kl_train: -0.1678159159 mse_train: 0.0578059782 acc_train: 0.6458333333 nll_val: 11074.1760896382 kl_val: -0.1584489491 mse_val: 0.0582851389 acc_val: 0.8684210526 time: 0.5397s
Epoch: 0144 nll_train: 10998.7483723958 kl_train: -0.1728426889 mse_train: 0.0578881493 acc_train: 0.6666666667 nll_val: 11083.8674444901 kl_val: -0.1856109884 mse_val: 0.0583361452 acc_val: 0.8684210526 time: 0.5393s
Epoch: 0145 nll_train: 10984.7108561198 kl_train: -0.1721179625 mse_train: 0.0578142701 acc_train: 0.7083333333 nll_val: 11074.2148437500 kl_val: -0.1710020327 mse_val: 0.0582853407 acc_val: 0.7894736842 time: 0.5424s
Epoch: 0146 nll_train: 10982.0333251953 kl_train: -0.1616202602 mse_train: 0.0578001748 acc_train: 0.6250000000 nll_val: 11096.6284950658 kl_val: -0.1824370968 mse_val: 0.0584033075 acc_val: 0.7105263158 time: 0.5417s
Epoch: 0147 nll_train: 10976.9434814453 kl_train: -0.1817436609 mse_train: 0.0577733874 acc_train: 0.6666666667 nll_val: 11105.3035053454 kl_val: -0.1914235197 mse_val: 0.0584489658 acc_val: 0.7368421053 time: 0.5430s
Epoch: 0148 nll_train: 10983.1726888021 kl_train: -0.1738664995 mse_train: 0.0578061721 acc_train: 0.6666666667 nll_val: 11124.2050781250 kl_val: -0.2010727705 mse_val: 0.0585484483 acc_val: 0.7105263158 time: 0.5422s
Epoch: 0149 nll_train: 10980.7729492188 kl_train: -0.1620894733 mse_train: 0.0577935427 acc_train: 0.6458333333 nll_val: 11083.8496093750 kl_val: -0.1935381034 mse_val: 0.0583360513 acc_val: 0.7894736842 time: 0.5437s
Epoch: 0150 nll_train: 10986.0889078776 kl_train: -0.1711451700 mse_train: 0.0578215206 acc_train: 0.6041666667 nll_val: 11066.3784950658 kl_val: -0.1742402915 mse_val: 0.0582440976 acc_val: 0.8157894737 time: 0.5413s
Epoch: 0151 nll_train: 10978.5358479818 kl_train: -0.1797545534 mse_train: 0.0577817680 acc_train: 0.6666666667 nll_val: 11093.9234683388 kl_val: -0.1785540365 mse_val: 0.0583890688 acc_val: 0.7631578947 time: 0.5424s
Epoch: 0152 nll_train: 10983.2386881510 kl_train: -0.1629936335 mse_train: 0.0578065204 acc_train: 0.6875000000 nll_val: 11134.2233758224 kl_val: -0.1931882924 mse_val: 0.0586011755 acc_val: 0.6842105263 time: 0.5421s
Epoch: 0153 nll_train: 10979.7423502604 kl_train: -0.1702746823 mse_train: 0.0577881181 acc_train: 0.5625000000 nll_val: 11096.7374074836 kl_val: -0.1788923211 mse_val: 0.0584038820 acc_val: 0.6842105263 time: 0.5439s
Epoch: 0154 nll_train: 10980.1933593750 kl_train: -0.1564469657 mse_train: 0.0577904916 acc_train: 0.6250000000 nll_val: 11104.1253597862 kl_val: -0.1632005349 mse_val: 0.0584427671 acc_val: 0.8157894737 time: 0.5453s
Epoch: 0155 nll_train: 10973.1686197917 kl_train: -0.1595194560 mse_train: 0.0577535202 acc_train: 0.6875000000 nll_val: 11110.4293277138 kl_val: -0.1667221867 mse_val: 0.0584759451 acc_val: 0.8421052632 time: 0.5435s
Epoch: 0156 nll_train: 10981.2922770182 kl_train: -0.1677287603 mse_train: 0.0577962746 acc_train: 0.7500000000 nll_val: 11036.6215563322 kl_val: -0.1765232227 mse_val: 0.0580874805 acc_val: 0.9210526316 time: 0.5445s
Epoch: 0157 nll_train: 10978.7193196615 kl_train: -0.1729660726 mse_train: 0.0577827333 acc_train: 0.6875000000 nll_val: 11133.0930304276 kl_val: -0.1707256915 mse_val: 0.0585952274 acc_val: 0.7894736842 time: 0.5422s
Epoch: 0158 nll_train: 10970.8874511719 kl_train: -0.1687328387 mse_train: 0.0577415138 acc_train: 0.6250000000 nll_val: 11103.0376233553 kl_val: -0.1876409901 mse_val: 0.0584370423 acc_val: 0.8157894737 time: 0.5413s
Epoch: 0159 nll_train: 10973.7727864583 kl_train: -0.1739396276 mse_train: 0.0577566988 acc_train: 0.7291666667 nll_val: 11091.0721114309 kl_val: -0.1905832251 mse_val: 0.0583740636 acc_val: 0.8421052632 time: 0.5416s
Epoch: 0160 nll_train: 10968.8835856120 kl_train: -0.1738516275 mse_train: 0.0577309670 acc_train: 0.7708333333 nll_val: 11169.3245271382 kl_val: -0.2083457301 mse_val: 0.0587859205 acc_val: 0.8421052632 time: 0.5445s
Epoch: 0161 nll_train: 10971.2074788411 kl_train: -0.1833110877 mse_train: 0.0577431979 acc_train: 0.7708333333 nll_val: 11131.8838918586 kl_val: -0.1882234471 mse_val: 0.0585888610 acc_val: 0.8684210526 time: 0.5426s
Epoch: 0162 nll_train: 10971.1206461589 kl_train: -0.1692998564 mse_train: 0.0577427410 acc_train: 0.7500000000 nll_val: 11128.1587685033 kl_val: -0.1910407386 mse_val: 0.0585692574 acc_val: 0.8421052632 time: 0.5422s
Epoch: 0163 nll_train: 10966.2954101562 kl_train: -0.1715993971 mse_train: 0.0577173453 acc_train: 0.7083333333 nll_val: 11109.6412417763 kl_val: -0.1777470857 mse_val: 0.0584717965 acc_val: 0.8421052632 time: 0.5418s
Epoch: 0164 nll_train: 10960.0731608073 kl_train: -0.1644881852 mse_train: 0.0576845949 acc_train: 0.7291666667 nll_val: 11132.2225534539 kl_val: -0.1743529329 mse_val: 0.0585906455 acc_val: 0.8157894737 time: 0.5468s
Epoch: 0165 nll_train: 10973.8093261719 kl_train: -0.1768307555 mse_train: 0.0577568915 acc_train: 0.7083333333 nll_val: 11111.2183388158 kl_val: -0.1806619442 mse_val: 0.0584800977 acc_val: 0.7894736842 time: 0.5432s
Epoch: 0166 nll_train: 10972.6927897135 kl_train: -0.1713624274 mse_train: 0.0577510156 acc_train: 0.7083333333 nll_val: 11100.6644736842 kl_val: -0.1940158990 mse_val: 0.0584245508 acc_val: 0.8421052632 time: 0.5428s
Epoch: 0167 nll_train: 10960.1564534505 kl_train: -0.1816982524 mse_train: 0.0576850334 acc_train: 0.7291666667 nll_val: 11108.4521484375 kl_val: -0.1904102799 mse_val: 0.0584655379 acc_val: 0.8421052632 time: 0.5412s
Epoch: 0168 nll_train: 10971.7103678385 kl_train: -0.1655600443 mse_train: 0.0577458451 acc_train: 0.6875000000 nll_val: 11094.3580386513 kl_val: -0.1892885703 mse_val: 0.0583913589 acc_val: 0.8157894737 time: 0.5434s
Epoch: 0169 nll_train: 10983.8504231771 kl_train: -0.1633891668 mse_train: 0.0578097391 acc_train: 0.7708333333 nll_val: 11103.7910156250 kl_val: -0.1888679807 mse_val: 0.0584410053 acc_val: 0.8421052632 time: 0.5395s
Epoch: 0170 nll_train: 10972.2141927083 kl_train: -0.1843552881 mse_train: 0.0577484945 acc_train: 0.7291666667 nll_val: 11128.9040398849 kl_val: -0.1882466509 mse_val: 0.0585731788 acc_val: 0.8947368421 time: 0.5456s
Epoch: 0171 nll_train: 10970.8364257812 kl_train: -0.1780266557 mse_train: 0.0577412445 acc_train: 0.6250000000 nll_val: 11084.3182565789 kl_val: -0.1512832014 mse_val: 0.0583385190 acc_val: 0.8157894737 time: 0.5437s
Epoch: 0172 nll_train: 10970.3452555339 kl_train: -0.1664905977 mse_train: 0.0577386594 acc_train: 0.6666666667 nll_val: 11145.5418379934 kl_val: -0.2119065202 mse_val: 0.0586607468 acc_val: 0.8157894737 time: 0.5421s
Epoch: 0173 nll_train: 10971.5154215495 kl_train: -0.1877460911 mse_train: 0.0577448170 acc_train: 0.7083333333 nll_val: 11092.4663342928 kl_val: -0.1828440691 mse_val: 0.0583814026 acc_val: 0.8684210526 time: 0.5431s
Epoch: 0174 nll_train: 10959.4676513672 kl_train: -0.1741082234 mse_train: 0.0576814082 acc_train: 0.6666666667 nll_val: 11185.5253906250 kl_val: -0.1925983456 mse_val: 0.0588711873 acc_val: 0.6578947368 time: 0.5433s
Epoch: 0175 nll_train: 10966.1176757812 kl_train: -0.1795122906 mse_train: 0.0577164091 acc_train: 0.6458333333 nll_val: 11149.0120785362 kl_val: -0.2212388743 mse_val: 0.0586790117 acc_val: 0.7368421053 time: 0.5431s
Epoch: 0176 nll_train: 10969.0133463542 kl_train: -0.1865146638 mse_train: 0.0577316497 acc_train: 0.7291666667 nll_val: 11133.1729029605 kl_val: -0.1961602369 mse_val: 0.0585956479 acc_val: 0.7894736842 time: 0.5427s
Epoch: 0177 nll_train: 10965.2424316406 kl_train: -0.1677542984 mse_train: 0.0577118018 acc_train: 0.6875000000 nll_val: 11130.2662417763 kl_val: -0.1657073415 mse_val: 0.0585803486 acc_val: 0.8421052632 time: 0.5428s
Epoch: 0178 nll_train: 10982.1483561198 kl_train: -0.1591315574 mse_train: 0.0578007804 acc_train: 0.6875000000 nll_val: 11105.5494962993 kl_val: -0.1620591989 mse_val: 0.0584502604 acc_val: 0.7105263158 time: 0.5409s
Epoch: 0179 nll_train: 10976.7649332682 kl_train: -0.1496200077 mse_train: 0.0577724470 acc_train: 0.7708333333 nll_val: 11129.3526418586 kl_val: -0.1731949707 mse_val: 0.0585755406 acc_val: 0.9210526316 time: 0.5452s
Epoch: 0180 nll_train: 10965.3588460286 kl_train: -0.1725292594 mse_train: 0.0577124148 acc_train: 0.6666666667 nll_val: 11078.6923314145 kl_val: -0.1677707191 mse_val: 0.0583089069 acc_val: 0.8157894737 time: 0.5421s
Epoch: 0181 nll_train: 10969.2824707031 kl_train: -0.1664471825 mse_train: 0.0577330659 acc_train: 0.7083333333 nll_val: 11103.7170538651 kl_val: -0.1997313931 mse_val: 0.0584406182 acc_val: 0.8157894737 time: 0.5457s
Epoch: 0182 nll_train: 10979.4715576172 kl_train: -0.1739354845 mse_train: 0.0577866922 acc_train: 0.6458333333 nll_val: 11132.1830283717 kl_val: -0.1849851059 mse_val: 0.0585904368 acc_val: 0.8684210526 time: 0.5482s
Epoch: 0183 nll_train: 10967.6284179688 kl_train: -0.1742506636 mse_train: 0.0577243602 acc_train: 0.7291666667 nll_val: 11120.5229749178 kl_val: -0.1652480011 mse_val: 0.0585290684 acc_val: 0.8421052632 time: 0.5418s
Epoch: 0184 nll_train: 10961.9106445312 kl_train: -0.1690104228 mse_train: 0.0576942676 acc_train: 0.7291666667 nll_val: 11139.4016755757 kl_val: -0.2089470554 mse_val: 0.0586284305 acc_val: 0.8157894737 time: 0.5420s
Epoch: 0185 nll_train: 10954.0016276042 kl_train: -0.1803474209 mse_train: 0.0576526400 acc_train: 0.6041666667 nll_val: 11139.7768297697 kl_val: -0.1875068330 mse_val: 0.0586304045 acc_val: 0.6842105263 time: 0.5432s
Epoch: 0186 nll_train: 10963.4854736328 kl_train: -0.1734805309 mse_train: 0.0577025549 acc_train: 0.6666666667 nll_val: 11167.5159333882 kl_val: -0.1990680573 mse_val: 0.0587763994 acc_val: 0.6842105263 time: 0.5438s
Epoch: 0187 nll_train: 10962.6904296875 kl_train: -0.1621762685 mse_train: 0.0576983707 acc_train: 0.7500000000 nll_val: 11113.5113075658 kl_val: -0.1828660773 mse_val: 0.0584921653 acc_val: 0.8421052632 time: 0.5422s
Epoch: 0188 nll_train: 10971.5815429688 kl_train: -0.1858673291 mse_train: 0.0577451669 acc_train: 0.7083333333 nll_val: 11160.0464124178 kl_val: -0.1796519317 mse_val: 0.0587370858 acc_val: 0.8421052632 time: 0.5418s
Epoch: 0189 nll_train: 10965.8328450521 kl_train: -0.1738387433 mse_train: 0.0577149106 acc_train: 0.7708333333 nll_val: 11194.3459087171 kl_val: -0.2016694169 mse_val: 0.0589176097 acc_val: 0.8421052632 time: 0.5410s
Epoch: 0190 nll_train: 10978.8740641276 kl_train: -0.1587678548 mse_train: 0.0577835484 acc_train: 0.7500000000 nll_val: 11099.1602590461 kl_val: -0.1664187269 mse_val: 0.0584166313 acc_val: 0.8157894737 time: 0.5418s
Epoch: 0191 nll_train: 10987.8283691406 kl_train: -0.1694708718 mse_train: 0.0578306769 acc_train: 0.6458333333 nll_val: 11108.6891961349 kl_val: -0.1810490673 mse_val: 0.0584667866 acc_val: 0.8157894737 time: 0.5419s
Epoch: 0192 nll_train: 10978.2363688151 kl_train: -0.1618395854 mse_train: 0.0577801930 acc_train: 0.7291666667 nll_val: 11060.9703433388 kl_val: -0.1765430491 mse_val: 0.0582156340 acc_val: 0.8157894737 time: 0.5424s
Epoch: 0193 nll_train: 10975.5062662760 kl_train: -0.1792121797 mse_train: 0.0577658221 acc_train: 0.6875000000 nll_val: 11267.9497841283 kl_val: -0.2129954264 mse_val: 0.0593049983 acc_val: 0.7631578947 time: 0.5427s
Epoch: 0194 nll_train: 11014.2240397135 kl_train: -0.1810442886 mse_train: 0.0579696007 acc_train: 0.7083333333 nll_val: 11071.4695723684 kl_val: -0.1804057258 mse_val: 0.0582708920 acc_val: 0.7105263158 time: 0.5479s
Epoch: 0195 nll_train: 11014.8863932292 kl_train: -0.1515989300 mse_train: 0.0579730862 acc_train: 0.6666666667 nll_val: 11098.0330489309 kl_val: -0.1730188022 mse_val: 0.0584107000 acc_val: 0.7894736842 time: 0.5426s
Epoch: 0196 nll_train: 11008.3791503906 kl_train: -0.1780617367 mse_train: 0.0579388381 acc_train: 0.6875000000 nll_val: 11073.1307051809 kl_val: -0.1901774873 mse_val: 0.0582796347 acc_val: 0.7368421053 time: 0.5412s
Epoch: 0197 nll_train: 11006.9415690104 kl_train: -0.1897987512 mse_train: 0.0579312711 acc_train: 0.7291666667 nll_val: 11137.4733244243 kl_val: -0.2036170712 mse_val: 0.0586182781 acc_val: 0.8684210526 time: 0.5435s
Epoch: 0198 nll_train: 11024.7850748698 kl_train: -0.1766361324 mse_train: 0.0580251858 acc_train: 0.7291666667 nll_val: 11080.7863384046 kl_val: -0.1801625949 mse_val: 0.0583199274 acc_val: 0.7894736842 time: 0.5414s
Epoch: 0199 nll_train: 11002.7751057943 kl_train: -0.1562282819 mse_train: 0.0579093421 acc_train: 0.6875000000 nll_val: 11169.2296977796 kl_val: -0.1527625260 mse_val: 0.0587854207 acc_val: 0.7894736842 time: 0.5442s
Optimization finished
Best epoch 28
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 12003.4775904605 kl_test: -0.0461590383 mse_test: 0.0631761986 acc_test: 1.0000000000
MSE: [ 0.062612153590 , 0.061629798263 , 0.061390649527 , 0.061527635902 , 0.061776135117 , 0.061377178878 , 0.061839781702 , 0.061844158918 , 0.061669833958 , 0.061665955931 , 0.061733651906 , 0.061749257147 , 0.061728734523 , 0.061772841960 , 0.061495549977 , 0.061767905951 , 0.061676401645 , 0.062149066478 , 0.062036018819 ]
Accuracy for experiment id 12 is 0.5
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 20208.6439208984 kl_train: -0.2035774980 mse_train: 0.1063612845 acc_train: 0.1458333333 nll_val: 17929.6394942434 kl_val: -0.3319543977 mse_val: 0.0943665250 acc_val: 0.0000000000 time: 0.5491s
Best model so far, saving...
Epoch: 0001 nll_train: 15756.8673502604 kl_train: -0.1800556363 mse_train: 0.0829308815 acc_train: 0.0000000000 nll_val: 16228.1382092928 kl_val: -0.2290688306 mse_val: 0.0854112549 acc_val: 0.0000000000 time: 0.5524s
Best model so far, saving...
Epoch: 0002 nll_train: 15195.4508463542 kl_train: -0.2665048819 mse_train: 0.0799760583 acc_train: 0.0000000000 nll_val: 15616.1781969572 kl_val: -0.2507083251 mse_val: 0.0821904128 acc_val: 0.0000000000 time: 0.5455s
Best model so far, saving...
Epoch: 0003 nll_train: 14942.5587158203 kl_train: -0.2375349626 mse_train: 0.0786450473 acc_train: 0.0000000000 nll_val: 15153.8364514803 kl_val: -0.1779592657 mse_val: 0.0797570356 acc_val: 0.2894736842 time: 0.5474s
Best model so far, saving...
Epoch: 0004 nll_train: 14775.7403157552 kl_train: -0.1475859564 mse_train: 0.0777670552 acc_train: 0.0833333333 nll_val: 14685.3729954770 kl_val: -0.1129797419 mse_val: 0.0772914381 acc_val: 0.3947368421 time: 0.5514s
Best model so far, saving...
Epoch: 0005 nll_train: 14692.9851481120 kl_train: -0.1128242767 mse_train: 0.0773315014 acc_train: 0.1875000000 nll_val: 14729.5656352796 kl_val: -0.0997037252 mse_val: 0.0775240303 acc_val: 0.4210526316 time: 0.5457s
Epoch: 0006 nll_train: 14702.3018798828 kl_train: -0.0980683162 mse_train: 0.0773805364 acc_train: 0.2916666667 nll_val: 14667.6773231908 kl_val: -0.0834219252 mse_val: 0.0771983019 acc_val: 0.4736842105 time: 0.5446s
Best model so far, saving...
Epoch: 0007 nll_train: 14714.6511230469 kl_train: -0.0885853476 mse_train: 0.0774455344 acc_train: 0.3958333333 nll_val: 14834.8041221217 kl_val: -0.0729789936 mse_val: 0.0780779166 acc_val: 0.5000000000 time: 0.5442s
Epoch: 0008 nll_train: 14710.9865722656 kl_train: -0.0785409198 mse_train: 0.0774262457 acc_train: 0.5416666667 nll_val: 14407.6005859375 kl_val: -0.0780564100 mse_val: 0.0758294753 acc_val: 0.5789473684 time: 0.5447s
Best model so far, saving...
Epoch: 0009 nll_train: 14661.4619140625 kl_train: -0.0815405329 mse_train: 0.0771655898 acc_train: 0.6041666667 nll_val: 14411.0856291118 kl_val: -0.0842474564 mse_val: 0.0758478194 acc_val: 0.5263157895 time: 0.5486s
Epoch: 0010 nll_train: 14656.3111165365 kl_train: -0.0892582353 mse_train: 0.0771384806 acc_train: 0.5625000000 nll_val: 14442.3501233553 kl_val: -0.0845443763 mse_val: 0.0760123698 acc_val: 0.6052631579 time: 0.5439s
Epoch: 0011 nll_train: 14665.6163330078 kl_train: -0.0875214459 mse_train: 0.0771874548 acc_train: 0.6458333333 nll_val: 14344.8989000822 kl_val: -0.0826710984 mse_val: 0.0754994676 acc_val: 0.6052631579 time: 0.5460s
Best model so far, saving...
Epoch: 0012 nll_train: 14693.3265787760 kl_train: -0.0848481858 mse_train: 0.0773332968 acc_train: 0.7083333333 nll_val: 14223.6366159539 kl_val: -0.0786985812 mse_val: 0.0748612453 acc_val: 0.6052631579 time: 0.5482s
Best model so far, saving...
Epoch: 0013 nll_train: 14738.6029866536 kl_train: -0.0797725030 mse_train: 0.0775715946 acc_train: 0.8333333333 nll_val: 14207.0368009868 kl_val: -0.0753586731 mse_val: 0.0747738794 acc_val: 0.6578947368 time: 0.5466s
Best model so far, saving...
Epoch: 0014 nll_train: 14766.1403401693 kl_train: -0.0789740357 mse_train: 0.0777165281 acc_train: 0.8541666667 nll_val: 14282.1771175987 kl_val: -0.0747528343 mse_val: 0.0751693551 acc_val: 0.6842105263 time: 0.5485s
Epoch: 0015 nll_train: 14723.5263264974 kl_train: -0.0828059837 mse_train: 0.0774922452 acc_train: 0.8125000000 nll_val: 14171.5199424342 kl_val: -0.0773466427 mse_val: 0.0745869479 acc_val: 0.6578947368 time: 0.5450s
Best model so far, saving...
Epoch: 0016 nll_train: 14668.1660156250 kl_train: -0.0815200497 mse_train: 0.0772008734 acc_train: 0.7916666667 nll_val: 14157.3461143092 kl_val: -0.0804420345 mse_val: 0.0745123488 acc_val: 0.6578947368 time: 0.5453s
Best model so far, saving...
Epoch: 0017 nll_train: 14662.0546875000 kl_train: -0.0856547141 mse_train: 0.0771687088 acc_train: 0.8125000000 nll_val: 14147.2347347862 kl_val: -0.0841135385 mse_val: 0.0744591300 acc_val: 0.6052631579 time: 0.5455s
Best model so far, saving...
Epoch: 0018 nll_train: 14661.8478597005 kl_train: -0.0928413455 mse_train: 0.0771676207 acc_train: 0.7500000000 nll_val: 14151.0150596217 kl_val: -0.0903040492 mse_val: 0.0744790278 acc_val: 0.6052631579 time: 0.5490s
Epoch: 0019 nll_train: 14655.5552978516 kl_train: -0.0948223726 mse_train: 0.0771345034 acc_train: 0.7708333333 nll_val: 14139.1917146382 kl_val: -0.0892705764 mse_val: 0.0744167990 acc_val: 0.6315789474 time: 0.5455s
Best model so far, saving...
Epoch: 0020 nll_train: 14649.6930338542 kl_train: -0.0896873142 mse_train: 0.0771036469 acc_train: 0.7708333333 nll_val: 14135.4393503289 kl_val: -0.0868118386 mse_val: 0.0743970514 acc_val: 0.6315789474 time: 0.5479s
Best model so far, saving...
Epoch: 0021 nll_train: 14649.1165364583 kl_train: -0.0894568898 mse_train: 0.0771006142 acc_train: 0.7916666667 nll_val: 14126.8956106086 kl_val: -0.0857119782 mse_val: 0.0743520828 acc_val: 0.5789473684 time: 0.5488s
Best model so far, saving...
Epoch: 0022 nll_train: 14638.7115885417 kl_train: -0.0947679582 mse_train: 0.0770458494 acc_train: 0.6875000000 nll_val: 14123.2661903783 kl_val: -0.0937972022 mse_val: 0.0743329792 acc_val: 0.5526315789 time: 0.5473s
Best model so far, saving...
Epoch: 0023 nll_train: 14635.9265136719 kl_train: -0.0957522091 mse_train: 0.0770311933 acc_train: 0.6875000000 nll_val: 14113.9245990954 kl_val: -0.0908281785 mse_val: 0.0742838130 acc_val: 0.5789473684 time: 0.5463s
Best model so far, saving...
Epoch: 0024 nll_train: 14636.1408284505 kl_train: -0.0905163068 mse_train: 0.0770323207 acc_train: 0.7083333333 nll_val: 14125.6697676809 kl_val: -0.0886623446 mse_val: 0.0743456303 acc_val: 0.6315789474 time: 0.5455s
Epoch: 0025 nll_train: 14625.0004882812 kl_train: -0.0942339314 mse_train: 0.0769736883 acc_train: 0.7500000000 nll_val: 14108.0469777961 kl_val: -0.0968391429 mse_val: 0.0742528797 acc_val: 0.6315789474 time: 0.5469s
Best model so far, saving...
Epoch: 0026 nll_train: 14623.3964843750 kl_train: -0.1035982662 mse_train: 0.0769652440 acc_train: 0.7500000000 nll_val: 14113.0490337171 kl_val: -0.1062725292 mse_val: 0.0742792050 acc_val: 0.6052631579 time: 0.5475s
Epoch: 0027 nll_train: 14616.0472819010 kl_train: -0.1123412407 mse_train: 0.0769265643 acc_train: 0.6666666667 nll_val: 14120.1690995066 kl_val: -0.1185441872 mse_val: 0.0743166814 acc_val: 0.5789473684 time: 0.5456s
Epoch: 0028 nll_train: 14623.5360107422 kl_train: -0.1096381250 mse_train: 0.0769659802 acc_train: 0.6875000000 nll_val: 14117.3826069079 kl_val: -0.0972584592 mse_val: 0.0743020136 acc_val: 0.6578947368 time: 0.5445s
Epoch: 0029 nll_train: 14611.3784179688 kl_train: -0.1052853977 mse_train: 0.0769019922 acc_train: 0.7500000000 nll_val: 14110.3103926809 kl_val: -0.1019128232 mse_val: 0.0742647907 acc_val: 0.6578947368 time: 0.5436s
Epoch: 0030 nll_train: 14621.6536458333 kl_train: -0.1132076144 mse_train: 0.0769560725 acc_train: 0.6458333333 nll_val: 14120.6904810855 kl_val: -0.1085811151 mse_val: 0.0743194224 acc_val: 0.6052631579 time: 0.5448s
Epoch: 0031 nll_train: 14612.9198811849 kl_train: -0.1173614291 mse_train: 0.0769101055 acc_train: 0.6666666667 nll_val: 14105.0561780428 kl_val: -0.1112433833 mse_val: 0.0742371374 acc_val: 0.6315789474 time: 0.5450s
Best model so far, saving...
Epoch: 0032 nll_train: 14596.4948323568 kl_train: -0.1164717724 mse_train: 0.0768236568 acc_train: 0.7500000000 nll_val: 14122.5847553454 kl_val: -0.1231192045 mse_val: 0.0743293925 acc_val: 0.6052631579 time: 0.5463s
Epoch: 0033 nll_train: 14592.5288085938 kl_train: -0.1221600802 mse_train: 0.0768027829 acc_train: 0.7083333333 nll_val: 14108.4090768914 kl_val: -0.1227838299 mse_val: 0.0742547861 acc_val: 0.6315789474 time: 0.5458s
Epoch: 0034 nll_train: 14598.1303304036 kl_train: -0.1259878865 mse_train: 0.0768322656 acc_train: 0.8125000000 nll_val: 14112.3503289474 kl_val: -0.1280071669 mse_val: 0.0742755281 acc_val: 0.6578947368 time: 0.5451s
Epoch: 0035 nll_train: 14593.2981363932 kl_train: -0.1310567738 mse_train: 0.0768068330 acc_train: 0.7708333333 nll_val: 14108.2661389803 kl_val: -0.1254920893 mse_val: 0.0742540324 acc_val: 0.6315789474 time: 0.5429s
Epoch: 0036 nll_train: 14593.1171875000 kl_train: -0.1251599444 mse_train: 0.0768058804 acc_train: 0.7291666667 nll_val: 14107.2164370888 kl_val: -0.1215981312 mse_val: 0.0742485100 acc_val: 0.6052631579 time: 0.5440s
Epoch: 0037 nll_train: 14584.5796305339 kl_train: -0.1324719281 mse_train: 0.0767609463 acc_train: 0.7291666667 nll_val: 14115.1699732730 kl_val: -0.1303971751 mse_val: 0.0742903669 acc_val: 0.6578947368 time: 0.5437s
Epoch: 0038 nll_train: 14590.0948079427 kl_train: -0.1317119692 mse_train: 0.0767899719 acc_train: 0.8125000000 nll_val: 14116.9682360197 kl_val: -0.1293066199 mse_val: 0.0742998345 acc_val: 0.6052631579 time: 0.5450s
Epoch: 0039 nll_train: 14580.4792073568 kl_train: -0.1274934014 mse_train: 0.0767393655 acc_train: 0.6666666667 nll_val: 14113.9515830592 kl_val: -0.1314524389 mse_val: 0.0742839553 acc_val: 0.6578947368 time: 0.5433s
Epoch: 0040 nll_train: 14591.8947347005 kl_train: -0.1350015520 mse_train: 0.0767994470 acc_train: 0.7083333333 nll_val: 14107.9875102796 kl_val: -0.1385761307 mse_val: 0.0742525656 acc_val: 0.6578947368 time: 0.5445s
Epoch: 0041 nll_train: 14588.6250000000 kl_train: -0.1325524890 mse_train: 0.0767822371 acc_train: 0.7083333333 nll_val: 14114.8073601974 kl_val: -0.1206820293 mse_val: 0.0742884590 acc_val: 0.5789473684 time: 0.5451s
Epoch: 0042 nll_train: 14583.4300944010 kl_train: -0.1305708345 mse_train: 0.0767548969 acc_train: 0.8125000000 nll_val: 14103.3154810855 kl_val: -0.1261200293 mse_val: 0.0742279783 acc_val: 0.6052631579 time: 0.5495s
Best model so far, saving...
Epoch: 0043 nll_train: 14568.3070882161 kl_train: -0.1353619254 mse_train: 0.0766752992 acc_train: 0.7916666667 nll_val: 14115.2959498355 kl_val: -0.1341878431 mse_val: 0.0742910324 acc_val: 0.6315789474 time: 0.5554s
Epoch: 0044 nll_train: 14573.6573893229 kl_train: -0.1350374259 mse_train: 0.0767034592 acc_train: 0.7083333333 nll_val: 14123.2507709704 kl_val: -0.1309699816 mse_val: 0.0743328974 acc_val: 0.5526315789 time: 0.5438s
Epoch: 0045 nll_train: 14573.7766520182 kl_train: -0.1322025070 mse_train: 0.0767040888 acc_train: 0.7291666667 nll_val: 14120.9047080592 kl_val: -0.1310308576 mse_val: 0.0743205522 acc_val: 0.6315789474 time: 0.5456s
Epoch: 0046 nll_train: 14566.7156575521 kl_train: -0.1394017218 mse_train: 0.0766669259 acc_train: 0.7500000000 nll_val: 14105.9343133224 kl_val: -0.1408832516 mse_val: 0.0742417601 acc_val: 0.5789473684 time: 0.5551s
Epoch: 0047 nll_train: 14565.2659505208 kl_train: -0.1439640842 mse_train: 0.0766592931 acc_train: 0.6666666667 nll_val: 14121.7327816612 kl_val: -0.1308635058 mse_val: 0.0743249092 acc_val: 0.5526315789 time: 0.5454s
Epoch: 0048 nll_train: 14564.8238932292 kl_train: -0.1385042149 mse_train: 0.0766569697 acc_train: 0.6458333333 nll_val: 14120.1132298520 kl_val: -0.1460674103 mse_val: 0.0743163861 acc_val: 0.5789473684 time: 0.5433s
Epoch: 0049 nll_train: 14566.7384033203 kl_train: -0.1515690057 mse_train: 0.0766670443 acc_train: 0.7500000000 nll_val: 14122.2630550987 kl_val: -0.1564769902 mse_val: 0.0743277004 acc_val: 0.5789473684 time: 0.5436s
Epoch: 0050 nll_train: 14560.8590901693 kl_train: -0.1544455544 mse_train: 0.0766361007 acc_train: 0.6458333333 nll_val: 14110.6356907895 kl_val: -0.1576194120 mse_val: 0.0742665035 acc_val: 0.6315789474 time: 0.5441s
Epoch: 0051 nll_train: 14557.2899576823 kl_train: -0.1437430382 mse_train: 0.0766173177 acc_train: 0.7291666667 nll_val: 14118.1832853618 kl_val: -0.1409320835 mse_val: 0.0743062282 acc_val: 0.6315789474 time: 0.5448s
Epoch: 0052 nll_train: 14560.2300211589 kl_train: -0.1389203143 mse_train: 0.0766327904 acc_train: 0.6875000000 nll_val: 14106.8284847862 kl_val: -0.1401097849 mse_val: 0.0742464673 acc_val: 0.5263157895 time: 0.5442s
Epoch: 0053 nll_train: 14559.1036783854 kl_train: -0.1437829823 mse_train: 0.0766268624 acc_train: 0.6666666667 nll_val: 14122.9231085526 kl_val: -0.1314688105 mse_val: 0.0743311740 acc_val: 0.6315789474 time: 0.5464s
Epoch: 0054 nll_train: 14555.2011718750 kl_train: -0.1410418246 mse_train: 0.0766063239 acc_train: 0.7916666667 nll_val: 14116.6529605263 kl_val: -0.1567037498 mse_val: 0.0742981738 acc_val: 0.6315789474 time: 0.5426s
Epoch: 0055 nll_train: 14560.1953125000 kl_train: -0.1606381418 mse_train: 0.0766326057 acc_train: 0.7083333333 nll_val: 14131.7287726151 kl_val: -0.1721163282 mse_val: 0.0743775193 acc_val: 0.6315789474 time: 0.5457s
Epoch: 0056 nll_train: 14558.5403238932 kl_train: -0.1695451715 mse_train: 0.0766238986 acc_train: 0.6666666667 nll_val: 14125.2042557566 kl_val: -0.1517357316 mse_val: 0.0743431821 acc_val: 0.6052631579 time: 0.5445s
Epoch: 0057 nll_train: 14567.1344401042 kl_train: -0.1487631236 mse_train: 0.0766691295 acc_train: 0.6458333333 nll_val: 14111.3241673520 kl_val: -0.1511157955 mse_val: 0.0742701265 acc_val: 0.5789473684 time: 0.5468s
Epoch: 0058 nll_train: 14560.3704427083 kl_train: -0.1439659307 mse_train: 0.0766335285 acc_train: 0.7291666667 nll_val: 14117.7661389803 kl_val: -0.1553619712 mse_val: 0.0743040323 acc_val: 0.6315789474 time: 0.5448s
Epoch: 0059 nll_train: 14561.4207356771 kl_train: -0.1409994826 mse_train: 0.0766390573 acc_train: 0.7083333333 nll_val: 14116.8428762336 kl_val: -0.1291848727 mse_val: 0.0742991731 acc_val: 0.6315789474 time: 0.5473s
Epoch: 0060 nll_train: 14550.9673258464 kl_train: -0.1395339615 mse_train: 0.0765840394 acc_train: 0.6666666667 nll_val: 14117.5741673520 kl_val: -0.1469697705 mse_val: 0.0743030237 acc_val: 0.5526315789 time: 0.5469s
Epoch: 0061 nll_train: 14546.4666748047 kl_train: -0.1497753110 mse_train: 0.0765603520 acc_train: 0.6875000000 nll_val: 14123.1143606086 kl_val: -0.1524943423 mse_val: 0.0743321818 acc_val: 0.6315789474 time: 0.5459s
Epoch: 0062 nll_train: 14550.4626057943 kl_train: -0.1474444053 mse_train: 0.0765813831 acc_train: 0.7291666667 nll_val: 14125.1165193257 kl_val: -0.1532806107 mse_val: 0.0743427204 acc_val: 0.5789473684 time: 0.5435s
Epoch: 0063 nll_train: 14545.6503092448 kl_train: -0.1559101917 mse_train: 0.0765560544 acc_train: 0.6666666667 nll_val: 14115.2513363487 kl_val: -0.1602717215 mse_val: 0.0742907957 acc_val: 0.6052631579 time: 0.5468s
Epoch: 0064 nll_train: 14545.4128011068 kl_train: -0.1464456801 mse_train: 0.0765548057 acc_train: 0.7500000000 nll_val: 14127.5655324836 kl_val: -0.1633908890 mse_val: 0.0743556087 acc_val: 0.5789473684 time: 0.5465s
Epoch: 0065 nll_train: 14539.4612630208 kl_train: -0.1442658026 mse_train: 0.0765234823 acc_train: 0.6875000000 nll_val: 14110.0787417763 kl_val: -0.1465088470 mse_val: 0.0742635721 acc_val: 0.6842105263 time: 0.5445s
Epoch: 0066 nll_train: 14544.8921712240 kl_train: -0.1512275649 mse_train: 0.0765520642 acc_train: 0.7083333333 nll_val: 14132.9020353618 kl_val: -0.1697931780 mse_val: 0.0743836968 acc_val: 0.5526315789 time: 0.5456s
Epoch: 0067 nll_train: 14538.5205485026 kl_train: -0.1575332573 mse_train: 0.0765185303 acc_train: 0.6875000000 nll_val: 14129.1343030428 kl_val: -0.1528951679 mse_val: 0.0743638655 acc_val: 0.5526315789 time: 0.5464s
Epoch: 0068 nll_train: 14537.0588785807 kl_train: -0.1506167110 mse_train: 0.0765108356 acc_train: 0.6875000000 nll_val: 14112.7731805099 kl_val: -0.1674124677 mse_val: 0.0742777537 acc_val: 0.6052631579 time: 0.5450s
Epoch: 0069 nll_train: 14529.9542236328 kl_train: -0.1586901434 mse_train: 0.0764734444 acc_train: 0.6875000000 nll_val: 14120.4148334704 kl_val: -0.1720702809 mse_val: 0.0743179733 acc_val: 0.6052631579 time: 0.5448s
Epoch: 0070 nll_train: 14534.3143717448 kl_train: -0.1554861777 mse_train: 0.0764963933 acc_train: 0.7083333333 nll_val: 14123.4129317434 kl_val: -0.1834687677 mse_val: 0.0743337530 acc_val: 0.5789473684 time: 0.5451s
Epoch: 0071 nll_train: 14533.4336344401 kl_train: -0.1620488161 mse_train: 0.0764917564 acc_train: 0.6875000000 nll_val: 14122.6180612664 kl_val: -0.1715350551 mse_val: 0.0743295682 acc_val: 0.6842105263 time: 0.5427s
Epoch: 0072 nll_train: 14534.5146891276 kl_train: -0.1591336575 mse_train: 0.0764974458 acc_train: 0.7291666667 nll_val: 14141.4860711349 kl_val: -0.1856711546 mse_val: 0.0744288754 acc_val: 0.6315789474 time: 0.5459s
Epoch: 0073 nll_train: 14529.4523111979 kl_train: -0.1645208271 mse_train: 0.0764708019 acc_train: 0.6875000000 nll_val: 14129.7715357730 kl_val: -0.1726551276 mse_val: 0.0743672195 acc_val: 0.6052631579 time: 0.5446s
Epoch: 0074 nll_train: 14527.7584228516 kl_train: -0.1572303927 mse_train: 0.0764618857 acc_train: 0.6666666667 nll_val: 14129.7726151316 kl_val: -0.1705401532 mse_val: 0.0743672252 acc_val: 0.5526315789 time: 0.5450s
Epoch: 0075 nll_train: 14519.9100341797 kl_train: -0.1482530618 mse_train: 0.0764205797 acc_train: 0.7083333333 nll_val: 14135.2171052632 kl_val: -0.1769145637 mse_val: 0.0743958785 acc_val: 0.5526315789 time: 0.5440s
Epoch: 0076 nll_train: 14529.8928629557 kl_train: -0.1566089435 mse_train: 0.0764731184 acc_train: 0.7083333333 nll_val: 14148.7984169408 kl_val: -0.1581480742 mse_val: 0.0744673614 acc_val: 0.5789473684 time: 0.5431s
Epoch: 0077 nll_train: 14527.1392822266 kl_train: -0.1580302060 mse_train: 0.0764586288 acc_train: 0.8125000000 nll_val: 14141.2866981908 kl_val: -0.1788529377 mse_val: 0.0744278251 acc_val: 0.6842105263 time: 0.5466s
Epoch: 0078 nll_train: 14518.2925211589 kl_train: -0.1754144703 mse_train: 0.0764120671 acc_train: 0.7291666667 nll_val: 14129.7072368421 kl_val: -0.1704487801 mse_val: 0.0743668819 acc_val: 0.6578947368 time: 0.5447s
Epoch: 0079 nll_train: 14526.7249348958 kl_train: -0.1529340086 mse_train: 0.0764564457 acc_train: 0.7291666667 nll_val: 14134.0003597862 kl_val: -0.1723053193 mse_val: 0.0743894763 acc_val: 0.6315789474 time: 0.5451s
Epoch: 0080 nll_train: 14513.2444661458 kl_train: -0.1607305945 mse_train: 0.0763854970 acc_train: 0.6875000000 nll_val: 14131.5346422697 kl_val: -0.1744089464 mse_val: 0.0743764994 acc_val: 0.5526315789 time: 0.5453s
Epoch: 0081 nll_train: 14522.3622639974 kl_train: -0.1595931230 mse_train: 0.0764334865 acc_train: 0.6666666667 nll_val: 14155.1776829770 kl_val: -0.1769529742 mse_val: 0.0745009364 acc_val: 0.6052631579 time: 0.5426s
Epoch: 0082 nll_train: 14514.3723144531 kl_train: -0.1675483352 mse_train: 0.0763914338 acc_train: 0.7083333333 nll_val: 14139.5750925164 kl_val: -0.1804448347 mse_val: 0.0744188179 acc_val: 0.6052631579 time: 0.5459s
Epoch: 0083 nll_train: 14512.1520182292 kl_train: -0.1618478050 mse_train: 0.0763797467 acc_train: 0.7291666667 nll_val: 14154.7553453947 kl_val: -0.1765552657 mse_val: 0.0744987130 acc_val: 0.6842105263 time: 0.5458s
Epoch: 0084 nll_train: 14513.9070638021 kl_train: -0.1643806646 mse_train: 0.0763889848 acc_train: 0.7708333333 nll_val: 14141.1617495888 kl_val: -0.1844414394 mse_val: 0.0744271680 acc_val: 0.6578947368 time: 0.5442s
Epoch: 0085 nll_train: 14532.7343343099 kl_train: -0.1606227647 mse_train: 0.0764880755 acc_train: 0.7083333333 nll_val: 14150.1998355263 kl_val: -0.1505077846 mse_val: 0.0744747361 acc_val: 0.6052631579 time: 0.5466s
Epoch: 0086 nll_train: 14529.5650227865 kl_train: -0.1391022770 mse_train: 0.0764713961 acc_train: 0.6875000000 nll_val: 14131.0594161184 kl_val: -0.1717670403 mse_val: 0.0743739972 acc_val: 0.6315789474 time: 0.5453s
Epoch: 0087 nll_train: 14520.0346272786 kl_train: -0.1653276651 mse_train: 0.0764212354 acc_train: 0.6875000000 nll_val: 14143.7212171053 kl_val: -0.1803021407 mse_val: 0.0744406379 acc_val: 0.5000000000 time: 0.5430s
Epoch: 0088 nll_train: 14516.1288655599 kl_train: -0.1485209204 mse_train: 0.0764006774 acc_train: 0.7083333333 nll_val: 14161.1344572368 kl_val: -0.1571734210 mse_val: 0.0745322851 acc_val: 0.6842105263 time: 0.5446s
Epoch: 0089 nll_train: 14522.3689371745 kl_train: -0.1581297306 mse_train: 0.0764335222 acc_train: 0.7500000000 nll_val: 14127.7059518914 kl_val: -0.1745376438 mse_val: 0.0743563458 acc_val: 0.5789473684 time: 0.5442s
Epoch: 0090 nll_train: 14501.2198486328 kl_train: -0.1610549145 mse_train: 0.0763222088 acc_train: 0.6875000000 nll_val: 14138.2940481086 kl_val: -0.1845207897 mse_val: 0.0744120751 acc_val: 0.6052631579 time: 0.5437s
Epoch: 0091 nll_train: 14507.5397542318 kl_train: -0.1633833830 mse_train: 0.0763554705 acc_train: 0.7083333333 nll_val: 14135.9122635691 kl_val: -0.1736447741 mse_val: 0.0743995379 acc_val: 0.6315789474 time: 0.5442s
Epoch: 0092 nll_train: 14502.0246988932 kl_train: -0.1742874878 mse_train: 0.0763264447 acc_train: 0.6875000000 nll_val: 14125.8945312500 kl_val: -0.1968197336 mse_val: 0.0743468149 acc_val: 0.4736842105 time: 0.5435s
Epoch: 0093 nll_train: 14505.2991129557 kl_train: -0.1615205665 mse_train: 0.0763436804 acc_train: 0.7083333333 nll_val: 14146.4243935033 kl_val: -0.1690340081 mse_val: 0.0744548656 acc_val: 0.6052631579 time: 0.5450s
Epoch: 0094 nll_train: 14510.7219238281 kl_train: -0.1697118130 mse_train: 0.0763722217 acc_train: 0.6875000000 nll_val: 14150.0526315789 kl_val: -0.1784063711 mse_val: 0.0744739610 acc_val: 0.5789473684 time: 0.5437s
Epoch: 0095 nll_train: 14499.4938964844 kl_train: -0.1614042191 mse_train: 0.0763131278 acc_train: 0.6875000000 nll_val: 14137.0016961349 kl_val: -0.2003857344 mse_val: 0.0744052735 acc_val: 0.6052631579 time: 0.5460s
Epoch: 0096 nll_train: 14507.9425862630 kl_train: -0.1657367280 mse_train: 0.0763575927 acc_train: 0.7083333333 nll_val: 14108.3840974507 kl_val: -0.1573834231 mse_val: 0.0742546549 acc_val: 0.5526315789 time: 0.5495s
Epoch: 0097 nll_train: 14516.8890787760 kl_train: -0.1384494839 mse_train: 0.0764046805 acc_train: 0.6875000000 nll_val: 14128.0961143092 kl_val: -0.1559963920 mse_val: 0.0743584025 acc_val: 0.6052631579 time: 0.5426s
Epoch: 0098 nll_train: 14503.8913981120 kl_train: -0.1571819112 mse_train: 0.0763362716 acc_train: 0.7500000000 nll_val: 14150.8986430921 kl_val: -0.1936042575 mse_val: 0.0744784123 acc_val: 0.6578947368 time: 0.5444s
Epoch: 0099 nll_train: 14510.3278401693 kl_train: -0.1564176952 mse_train: 0.0763701477 acc_train: 0.7083333333 nll_val: 14145.6088096217 kl_val: -0.1764160348 mse_val: 0.0744505729 acc_val: 0.5526315789 time: 0.5440s
Epoch: 0100 nll_train: 14502.2086588542 kl_train: -0.1542905554 mse_train: 0.0763274155 acc_train: 0.6875000000 nll_val: 14126.5542249178 kl_val: -0.1612866152 mse_val: 0.0743502863 acc_val: 0.5789473684 time: 0.5445s
Epoch: 0101 nll_train: 14528.3241373698 kl_train: -0.1622896291 mse_train: 0.0764648636 acc_train: 0.7291666667 nll_val: 14176.7991879112 kl_val: -0.1956878250 mse_val: 0.0746147317 acc_val: 0.6315789474 time: 0.5437s
Epoch: 0102 nll_train: 14496.7928059896 kl_train: -0.1633225403 mse_train: 0.0762989094 acc_train: 0.7291666667 nll_val: 14142.1197574013 kl_val: -0.1823510559 mse_val: 0.0744322111 acc_val: 0.6315789474 time: 0.5441s
Epoch: 0103 nll_train: 14500.5800374349 kl_train: -0.1762497124 mse_train: 0.0763188416 acc_train: 0.6666666667 nll_val: 14179.3775185033 kl_val: -0.1849495165 mse_val: 0.0746283035 acc_val: 0.5526315789 time: 0.5423s
Epoch: 0104 nll_train: 14504.5600992839 kl_train: -0.1485680705 mse_train: 0.0763397890 acc_train: 0.7500000000 nll_val: 14174.2200349507 kl_val: -0.1863997281 mse_val: 0.0746011583 acc_val: 0.6578947368 time: 0.5436s
Epoch: 0105 nll_train: 14495.1145426432 kl_train: -0.1720751282 mse_train: 0.0762900754 acc_train: 0.6666666667 nll_val: 14157.1849300987 kl_val: -0.1851495638 mse_val: 0.0745114994 acc_val: 0.6052631579 time: 0.5475s
Epoch: 0106 nll_train: 14510.5640869141 kl_train: -0.1618306823 mse_train: 0.0763713893 acc_train: 0.7291666667 nll_val: 14132.0743215461 kl_val: -0.1798050215 mse_val: 0.0743793392 acc_val: 0.5789473684 time: 0.5458s
Epoch: 0107 nll_train: 14487.3614095052 kl_train: -0.1592480817 mse_train: 0.0762492716 acc_train: 0.7083333333 nll_val: 14147.3091591283 kl_val: -0.1810981749 mse_val: 0.0744595212 acc_val: 0.5526315789 time: 0.5439s
Epoch: 0108 nll_train: 14501.3587239583 kl_train: -0.1616537186 mse_train: 0.0763229416 acc_train: 0.7291666667 nll_val: 14195.9605263158 kl_val: -0.1952166644 mse_val: 0.0747155835 acc_val: 0.6315789474 time: 0.5449s
Epoch: 0109 nll_train: 14510.7657063802 kl_train: -0.1659341569 mse_train: 0.0763724510 acc_train: 0.7291666667 nll_val: 14160.8629214638 kl_val: -0.1917145707 mse_val: 0.0745308580 acc_val: 0.5789473684 time: 0.5431s
Epoch: 0110 nll_train: 14487.5695800781 kl_train: -0.1634481121 mse_train: 0.0762503680 acc_train: 0.6458333333 nll_val: 14171.3748972039 kl_val: -0.1724571452 mse_val: 0.0745861840 acc_val: 0.5526315789 time: 0.5457s
Epoch: 0111 nll_train: 14490.4981689453 kl_train: -0.1549555458 mse_train: 0.0762657803 acc_train: 0.7291666667 nll_val: 14167.6514699836 kl_val: -0.1696457914 mse_val: 0.0745665856 acc_val: 0.6842105263 time: 0.5442s
Epoch: 0112 nll_train: 14492.1817626953 kl_train: -0.1721095157 mse_train: 0.0762746426 acc_train: 0.6875000000 nll_val: 14156.8253495066 kl_val: -0.2133508199 mse_val: 0.0745096085 acc_val: 0.5000000000 time: 0.5475s
Epoch: 0113 nll_train: 14502.3269042969 kl_train: -0.1581383108 mse_train: 0.0763280385 acc_train: 0.7083333333 nll_val: 14170.2409025493 kl_val: -0.1671546972 mse_val: 0.0745802153 acc_val: 0.5526315789 time: 0.5483s
Epoch: 0114 nll_train: 14496.3835449219 kl_train: -0.1634152830 mse_train: 0.0762967571 acc_train: 0.6875000000 nll_val: 14159.5522203947 kl_val: -0.1971236256 mse_val: 0.0745239601 acc_val: 0.6315789474 time: 0.5446s
Epoch: 0115 nll_train: 14492.8544514974 kl_train: -0.1760862609 mse_train: 0.0762781821 acc_train: 0.7500000000 nll_val: 14142.1976768092 kl_val: -0.1918432328 mse_val: 0.0744326193 acc_val: 0.6315789474 time: 0.5450s
Epoch: 0116 nll_train: 14483.6222330729 kl_train: -0.1744061320 mse_train: 0.0762295912 acc_train: 0.7291666667 nll_val: 14167.1469469572 kl_val: -0.2049791680 mse_val: 0.0745639340 acc_val: 0.5263157895 time: 0.5465s
Epoch: 0117 nll_train: 14487.1576334635 kl_train: -0.1626632856 mse_train: 0.0762481984 acc_train: 0.7500000000 nll_val: 14178.0396792763 kl_val: -0.2003574811 mse_val: 0.0746212639 acc_val: 0.6052631579 time: 0.5448s
Epoch: 0118 nll_train: 14487.0986735026 kl_train: -0.1688645029 mse_train: 0.0762478892 acc_train: 0.6875000000 nll_val: 14156.8473478618 kl_val: -0.1929224918 mse_val: 0.0745097234 acc_val: 0.5789473684 time: 0.5445s
Epoch: 0119 nll_train: 14496.6104736328 kl_train: -0.1540067385 mse_train: 0.0762979484 acc_train: 0.7500000000 nll_val: 14160.3394325658 kl_val: -0.1734069080 mse_val: 0.0745281022 acc_val: 0.6052631579 time: 0.5452s
Epoch: 0120 nll_train: 14497.3142089844 kl_train: -0.1565714991 mse_train: 0.0763016542 acc_train: 0.7083333333 nll_val: 14166.7914268092 kl_val: -0.1557484453 mse_val: 0.0745620594 acc_val: 0.6052631579 time: 0.5427s
Epoch: 0121 nll_train: 14494.4089355469 kl_train: -0.1516946691 mse_train: 0.0762863643 acc_train: 0.7083333333 nll_val: 14149.7556537829 kl_val: -0.1776907369 mse_val: 0.0744724001 acc_val: 0.5789473684 time: 0.5469s
Epoch: 0122 nll_train: 14483.8837076823 kl_train: -0.1526675439 mse_train: 0.0762309676 acc_train: 0.6875000000 nll_val: 14120.2319078947 kl_val: -0.1630608608 mse_val: 0.0743170114 acc_val: 0.5789473684 time: 0.5442s
Epoch: 0123 nll_train: 14495.1649983724 kl_train: -0.1681310789 mse_train: 0.0762903426 acc_train: 0.7291666667 nll_val: 14181.9917249178 kl_val: -0.1917246353 mse_val: 0.0746420622 acc_val: 0.6315789474 time: 0.5463s
Epoch: 0124 nll_train: 14507.9624837240 kl_train: -0.1666899317 mse_train: 0.0763576970 acc_train: 0.6458333333 nll_val: 14224.6724403783 kl_val: -0.1842317801 mse_val: 0.0748666980 acc_val: 0.5526315789 time: 0.5460s
Epoch: 0125 nll_train: 14499.4981689453 kl_train: -0.1631520198 mse_train: 0.0763131487 acc_train: 0.7291666667 nll_val: 14167.8588096217 kl_val: -0.1771111222 mse_val: 0.0745676799 acc_val: 0.5789473684 time: 0.5464s
Epoch: 0126 nll_train: 14483.6430257161 kl_train: -0.1615719826 mse_train: 0.0762297004 acc_train: 0.6875000000 nll_val: 14155.9216180099 kl_val: -0.1812414594 mse_val: 0.0745048497 acc_val: 0.5789473684 time: 0.5441s
Epoch: 0127 nll_train: 14474.2947998047 kl_train: -0.1509045331 mse_train: 0.0761804980 acc_train: 0.7500000000 nll_val: 14168.7490748355 kl_val: -0.1695717573 mse_val: 0.0745723649 acc_val: 0.6315789474 time: 0.5474s
Epoch: 0128 nll_train: 14483.8538818359 kl_train: -0.1567161347 mse_train: 0.0762308088 acc_train: 0.6875000000 nll_val: 14200.8735608553 kl_val: -0.1926783582 mse_val: 0.0747414414 acc_val: 0.6578947368 time: 0.5462s
Epoch: 0129 nll_train: 14484.5219726562 kl_train: -0.1721063815 mse_train: 0.0762343275 acc_train: 0.6875000000 nll_val: 14187.0988898026 kl_val: -0.1811826202 mse_val: 0.0746689421 acc_val: 0.5526315789 time: 0.5453s
Epoch: 0130 nll_train: 14477.9050699870 kl_train: -0.1639411558 mse_train: 0.0761994990 acc_train: 0.7083333333 nll_val: 14173.1382092928 kl_val: -0.1929368275 mse_val: 0.0745954639 acc_val: 0.6052631579 time: 0.5464s
Epoch: 0131 nll_train: 14496.2982584635 kl_train: -0.1674174384 mse_train: 0.0762963073 acc_train: 0.7500000000 nll_val: 14189.6318873355 kl_val: -0.2004115189 mse_val: 0.0746822740 acc_val: 0.5000000000 time: 0.5442s
Epoch: 0132 nll_train: 14489.2997639974 kl_train: -0.1615584108 mse_train: 0.0762594719 acc_train: 0.7083333333 nll_val: 14193.5837273849 kl_val: -0.1953762072 mse_val: 0.0747030726 acc_val: 0.6315789474 time: 0.5454s
Epoch: 0133 nll_train: 14483.8894042969 kl_train: -0.1711275649 mse_train: 0.0762309971 acc_train: 0.7291666667 nll_val: 14159.9672080592 kl_val: -0.1969916240 mse_val: 0.0745261425 acc_val: 0.6578947368 time: 0.5450s
Epoch: 0134 nll_train: 14489.1526692708 kl_train: -0.1714051251 mse_train: 0.0762586996 acc_train: 0.7291666667 nll_val: 14192.4813939145 kl_val: -0.1817297975 mse_val: 0.0746972708 acc_val: 0.5789473684 time: 0.5456s
Epoch: 0135 nll_train: 14507.7451578776 kl_train: -0.1610487904 mse_train: 0.0763565536 acc_train: 0.7916666667 nll_val: 14162.1726973684 kl_val: -0.1868593207 mse_val: 0.0745377535 acc_val: 0.6052631579 time: 0.5458s
Epoch: 0136 nll_train: 14477.7232259115 kl_train: -0.1647711899 mse_train: 0.0761985440 acc_train: 0.7291666667 nll_val: 14173.2045641447 kl_val: -0.1901042046 mse_val: 0.0745958137 acc_val: 0.6052631579 time: 0.5454s
Epoch: 0137 nll_train: 14505.3328857422 kl_train: -0.1498275610 mse_train: 0.0763438571 acc_train: 0.7291666667 nll_val: 14205.7215768914 kl_val: -0.1766732672 mse_val: 0.0747669551 acc_val: 0.6842105263 time: 0.5442s
Epoch: 0138 nll_train: 14502.7904866536 kl_train: -0.1565228173 mse_train: 0.0763304764 acc_train: 0.6875000000 nll_val: 14186.1993729441 kl_val: -0.1732823308 mse_val: 0.0746642071 acc_val: 0.6052631579 time: 0.5461s
Epoch: 0139 nll_train: 14518.3865966797 kl_train: -0.1500964298 mse_train: 0.0764125618 acc_train: 0.7083333333 nll_val: 14197.3307976974 kl_val: -0.1603827551 mse_val: 0.0747227951 acc_val: 0.6052631579 time: 0.5451s
Epoch: 0140 nll_train: 14484.8229166667 kl_train: -0.1603379588 mse_train: 0.0762359099 acc_train: 0.6666666667 nll_val: 14221.9647923520 kl_val: -0.2226900396 mse_val: 0.0748524460 acc_val: 0.4736842105 time: 0.5450s
Epoch: 0141 nll_train: 14478.1380208333 kl_train: -0.1625374484 mse_train: 0.0762007261 acc_train: 0.6458333333 nll_val: 14201.4253186678 kl_val: -0.1814821276 mse_val: 0.0747443440 acc_val: 0.4736842105 time: 0.5453s
Epoch: 0142 nll_train: 14479.7027180990 kl_train: -0.1624621833 mse_train: 0.0762089623 acc_train: 0.7500000000 nll_val: 14165.6115851151 kl_val: -0.1827398217 mse_val: 0.0745558494 acc_val: 0.6052631579 time: 0.5451s
Epoch: 0143 nll_train: 14485.6781412760 kl_train: -0.1564039839 mse_train: 0.0762404098 acc_train: 0.7500000000 nll_val: 14200.0253392270 kl_val: -0.1706635850 mse_val: 0.0747369759 acc_val: 0.6315789474 time: 0.5463s
Epoch: 0144 nll_train: 14473.9039306641 kl_train: -0.1521713094 mse_train: 0.0761784411 acc_train: 0.7291666667 nll_val: 14193.8483244243 kl_val: -0.1853890074 mse_val: 0.0747044651 acc_val: 0.6578947368 time: 0.5447s
Epoch: 0145 nll_train: 14522.0323079427 kl_train: -0.1688515820 mse_train: 0.0764317486 acc_train: 0.7916666667 nll_val: 14222.7607935855 kl_val: -0.2085087613 mse_val: 0.0748566352 acc_val: 0.6578947368 time: 0.5436s
Epoch: 0146 nll_train: 14479.7065022786 kl_train: -0.1802311239 mse_train: 0.0762089818 acc_train: 0.7916666667 nll_val: 14173.2520559211 kl_val: -0.1850146016 mse_val: 0.0745960633 acc_val: 0.6578947368 time: 0.5447s
Epoch: 0147 nll_train: 14468.6338704427 kl_train: -0.1749011173 mse_train: 0.0761507067 acc_train: 0.7708333333 nll_val: 14186.9948601974 kl_val: -0.1982653557 mse_val: 0.0746683945 acc_val: 0.6052631579 time: 0.5429s
Epoch: 0148 nll_train: 14464.4665934245 kl_train: -0.1639475804 mse_train: 0.0761287725 acc_train: 0.7500000000 nll_val: 14181.4653577303 kl_val: -0.1734639594 mse_val: 0.0746392914 acc_val: 0.6315789474 time: 0.5439s
Epoch: 0149 nll_train: 14471.0408528646 kl_train: -0.1678025449 mse_train: 0.0761633730 acc_train: 0.7500000000 nll_val: 14172.6573293586 kl_val: -0.1894673172 mse_val: 0.0745929358 acc_val: 0.6842105263 time: 0.5460s
Epoch: 0150 nll_train: 14487.1848958333 kl_train: -0.1700043697 mse_train: 0.0762483414 acc_train: 0.7916666667 nll_val: 14206.2988281250 kl_val: -0.2171220097 mse_val: 0.0747699949 acc_val: 0.5526315789 time: 0.5442s
Epoch: 0151 nll_train: 14473.9226074219 kl_train: -0.1781368454 mse_train: 0.0761785411 acc_train: 0.7500000000 nll_val: 14214.6464843750 kl_val: -0.2082545083 mse_val: 0.0748139294 acc_val: 0.5526315789 time: 0.5439s
Epoch: 0152 nll_train: 14481.3800048828 kl_train: -0.1686998342 mse_train: 0.0762177915 acc_train: 0.7708333333 nll_val: 14193.1321443257 kl_val: -0.1818483707 mse_val: 0.0747006979 acc_val: 0.7105263158 time: 0.5445s
Epoch: 0153 nll_train: 14479.5674235026 kl_train: -0.1628817866 mse_train: 0.0762082495 acc_train: 0.7708333333 nll_val: 14188.9537417763 kl_val: -0.1677260764 mse_val: 0.0746787059 acc_val: 0.6052631579 time: 0.5460s
Epoch: 0154 nll_train: 14467.7788492839 kl_train: -0.1763422095 mse_train: 0.0761462054 acc_train: 0.7708333333 nll_val: 14192.6853926809 kl_val: -0.2192094177 mse_val: 0.0746983439 acc_val: 0.6315789474 time: 0.5455s
Epoch: 0155 nll_train: 14476.2195638021 kl_train: -0.1738152662 mse_train: 0.0761906301 acc_train: 0.7500000000 nll_val: 14180.4665398849 kl_val: -0.1951384105 mse_val: 0.0746340342 acc_val: 0.5789473684 time: 0.5460s
Epoch: 0156 nll_train: 14475.4298909505 kl_train: -0.1597889795 mse_train: 0.0761864730 acc_train: 0.7708333333 nll_val: 14201.0873766447 kl_val: -0.1995503220 mse_val: 0.0747425637 acc_val: 0.6578947368 time: 0.5431s
Epoch: 0157 nll_train: 14465.2589518229 kl_train: -0.1699118881 mse_train: 0.0761329406 acc_train: 0.7500000000 nll_val: 14172.3811677632 kl_val: -0.1857051536 mse_val: 0.0745914796 acc_val: 0.6578947368 time: 0.5492s
Epoch: 0158 nll_train: 14463.3455403646 kl_train: -0.1730100705 mse_train: 0.0761228714 acc_train: 0.7708333333 nll_val: 14176.7287212171 kl_val: -0.2007009662 mse_val: 0.0746143613 acc_val: 0.6315789474 time: 0.5411s
Epoch: 0159 nll_train: 14479.9119466146 kl_train: -0.1666282145 mse_train: 0.0762100637 acc_train: 0.7916666667 nll_val: 14195.3899568257 kl_val: -0.1952044603 mse_val: 0.0747125788 acc_val: 0.6052631579 time: 0.5457s
Epoch: 0160 nll_train: 14463.1565348307 kl_train: -0.1729141474 mse_train: 0.0761218766 acc_train: 0.7083333333 nll_val: 14174.7356085526 kl_val: -0.1816407583 mse_val: 0.0746038727 acc_val: 0.6315789474 time: 0.5472s
Epoch: 0161 nll_train: 14479.4415283203 kl_train: -0.1753631392 mse_train: 0.0762075877 acc_train: 0.7500000000 nll_val: 14172.2993935033 kl_val: -0.1866155361 mse_val: 0.0745910496 acc_val: 0.6315789474 time: 0.5492s
Epoch: 0162 nll_train: 14509.4155680339 kl_train: -0.1781721879 mse_train: 0.0763653461 acc_train: 0.7916666667 nll_val: 14154.6175986842 kl_val: -0.1876785692 mse_val: 0.0744979876 acc_val: 0.6052631579 time: 0.5471s
Epoch: 0163 nll_train: 14524.9436442057 kl_train: -0.1669231017 mse_train: 0.0764470724 acc_train: 0.7708333333 nll_val: 14204.0763260691 kl_val: -0.2127241963 mse_val: 0.0747582985 acc_val: 0.6842105263 time: 0.5437s
Epoch: 0164 nll_train: 14483.5911865234 kl_train: -0.1837290432 mse_train: 0.0762294289 acc_train: 0.8125000000 nll_val: 14208.9124691612 kl_val: -0.1956462068 mse_val: 0.0747837503 acc_val: 0.6315789474 time: 0.5466s
Epoch: 0165 nll_train: 14482.3495279948 kl_train: -0.1744544233 mse_train: 0.0762228924 acc_train: 0.7083333333 nll_val: 14167.0597245066 kl_val: -0.1689096250 mse_val: 0.0745634711 acc_val: 0.6578947368 time: 0.5549s
Epoch: 0166 nll_train: 14482.3292236328 kl_train: -0.1647104006 mse_train: 0.0762227854 acc_train: 0.7708333333 nll_val: 14198.7833059211 kl_val: -0.2060802861 mse_val: 0.0747304361 acc_val: 0.6052631579 time: 0.5476s
Epoch: 0167 nll_train: 14485.4523111979 kl_train: -0.1629570754 mse_train: 0.0762392239 acc_train: 0.7916666667 nll_val: 14176.6211965461 kl_val: -0.1674368766 mse_val: 0.0746137955 acc_val: 0.6578947368 time: 0.5458s
Epoch: 0168 nll_train: 14482.7967529297 kl_train: -0.1587503099 mse_train: 0.0762252455 acc_train: 0.7916666667 nll_val: 14169.8764391447 kl_val: -0.1634819712 mse_val: 0.0745782970 acc_val: 0.6315789474 time: 0.5472s
Epoch: 0169 nll_train: 14474.3554280599 kl_train: -0.1598832157 mse_train: 0.0761808183 acc_train: 0.7291666667 nll_val: 14187.5389083059 kl_val: -0.1777234501 mse_val: 0.0746712583 acc_val: 0.6842105263 time: 0.5466s
Epoch: 0170 nll_train: 14472.3459472656 kl_train: -0.1631799166 mse_train: 0.0761702427 acc_train: 0.6875000000 nll_val: 14220.0690275493 kl_val: -0.1665789571 mse_val: 0.0748424677 acc_val: 0.6315789474 time: 0.5460s
Epoch: 0171 nll_train: 14456.1686604818 kl_train: -0.1570351711 mse_train: 0.0760850984 acc_train: 0.7500000000 nll_val: 14199.3491467928 kl_val: -0.1994287913 mse_val: 0.0747334173 acc_val: 0.6578947368 time: 0.5433s
Epoch: 0172 nll_train: 14466.4377441406 kl_train: -0.1729403948 mse_train: 0.0761391454 acc_train: 0.8125000000 nll_val: 14209.7969263980 kl_val: -0.2173156668 mse_val: 0.0747884049 acc_val: 0.6578947368 time: 0.5481s
Epoch: 0173 nll_train: 14457.0937093099 kl_train: -0.1702723627 mse_train: 0.0760899678 acc_train: 0.7916666667 nll_val: 14221.2754420230 kl_val: -0.1784244839 mse_val: 0.0748488191 acc_val: 0.6578947368 time: 0.5453s
Epoch: 0174 nll_train: 14461.0435791016 kl_train: -0.1688435751 mse_train: 0.0761107573 acc_train: 0.8125000000 nll_val: 14170.6658614309 kl_val: -0.1851398372 mse_val: 0.0745824511 acc_val: 0.6842105263 time: 0.5453s
Epoch: 0175 nll_train: 14462.0954182943 kl_train: -0.1613580668 mse_train: 0.0761162903 acc_train: 0.8541666667 nll_val: 14202.0187088816 kl_val: -0.1789072517 mse_val: 0.0747474657 acc_val: 0.7368421053 time: 0.5469s
Epoch: 0176 nll_train: 14486.1999104818 kl_train: -0.1661424730 mse_train: 0.0762431580 acc_train: 0.7708333333 nll_val: 14209.7773951480 kl_val: -0.1877246476 mse_val: 0.0747883020 acc_val: 0.5526315789 time: 0.5489s
Epoch: 0177 nll_train: 14492.4274902344 kl_train: -0.1606251026 mse_train: 0.0762759335 acc_train: 0.7708333333 nll_val: 14235.6902240954 kl_val: -0.1620804389 mse_val: 0.0749246860 acc_val: 0.7631578947 time: 0.5522s
Epoch: 0178 nll_train: 14487.6546630859 kl_train: -0.1616639045 mse_train: 0.0762508136 acc_train: 0.7708333333 nll_val: 14200.4317948191 kl_val: -0.1822669083 mse_val: 0.0747391150 acc_val: 0.5789473684 time: 0.5469s
Epoch: 0179 nll_train: 14461.6767985026 kl_train: -0.1564950452 mse_train: 0.0761140891 acc_train: 0.7916666667 nll_val: 14234.0938527961 kl_val: -0.1954240862 mse_val: 0.0749162845 acc_val: 0.6315789474 time: 0.5457s
Epoch: 0180 nll_train: 14471.9165852865 kl_train: -0.1746394473 mse_train: 0.0761679808 acc_train: 0.7708333333 nll_val: 14239.9835526316 kl_val: -0.1786427372 mse_val: 0.0749472825 acc_val: 0.6578947368 time: 0.5471s
Epoch: 0181 nll_train: 14506.7469482422 kl_train: -0.1609372127 mse_train: 0.0763512991 acc_train: 0.7500000000 nll_val: 14203.1678659539 kl_val: -0.1603048405 mse_val: 0.0747535125 acc_val: 0.7368421053 time: 0.5468s
Epoch: 0182 nll_train: 14498.7255045573 kl_train: -0.1635810606 mse_train: 0.0763090826 acc_train: 0.8125000000 nll_val: 14210.1568153783 kl_val: -0.1729366113 mse_val: 0.0747903001 acc_val: 0.7631578947 time: 0.5467s
Epoch: 0183 nll_train: 14474.1091715495 kl_train: -0.1535810940 mse_train: 0.0761795222 acc_train: 0.8333333333 nll_val: 14200.6745990954 kl_val: -0.1711106943 mse_val: 0.0747403932 acc_val: 0.6842105263 time: 0.5459s
Epoch: 0184 nll_train: 14464.5835367839 kl_train: -0.1458596541 mse_train: 0.0761293878 acc_train: 0.7916666667 nll_val: 14205.8008326480 kl_val: -0.1867302437 mse_val: 0.0747673733 acc_val: 0.6578947368 time: 0.5455s
Epoch: 0185 nll_train: 14450.8225504557 kl_train: -0.1610116127 mse_train: 0.0760569621 acc_train: 0.7708333333 nll_val: 14224.8721731086 kl_val: -0.1784720052 mse_val: 0.0748677499 acc_val: 0.7368421053 time: 0.5455s
Epoch: 0186 nll_train: 14447.8695068359 kl_train: -0.1549830614 mse_train: 0.0760414205 acc_train: 0.8541666667 nll_val: 14212.2699424342 kl_val: -0.1695958062 mse_val: 0.0748014209 acc_val: 0.7894736842 time: 0.5430s
Epoch: 0187 nll_train: 14479.0080566406 kl_train: -0.1714319717 mse_train: 0.0762053056 acc_train: 0.8333333333 nll_val: 14236.1229954770 kl_val: -0.1833335906 mse_val: 0.0749269632 acc_val: 0.7894736842 time: 0.5456s
Epoch: 0188 nll_train: 14478.4372151693 kl_train: -0.1589142295 mse_train: 0.0762023008 acc_train: 0.7291666667 nll_val: 14285.9913651316 kl_val: -0.1898497029 mse_val: 0.0751894281 acc_val: 0.4736842105 time: 0.5460s
Epoch: 0189 nll_train: 14462.3357340495 kl_train: -0.1660061326 mse_train: 0.0761175562 acc_train: 0.6458333333 nll_val: 14260.1377981086 kl_val: -0.1863277307 mse_val: 0.0750533556 acc_val: 0.5000000000 time: 0.5454s
Epoch: 0190 nll_train: 14468.8675130208 kl_train: -0.1540378307 mse_train: 0.0761519351 acc_train: 0.7291666667 nll_val: 14207.0607010691 kl_val: -0.1827889877 mse_val: 0.0747740033 acc_val: 0.6315789474 time: 0.5450s
Epoch: 0191 nll_train: 14451.8850097656 kl_train: -0.1625018278 mse_train: 0.0760625523 acc_train: 0.7500000000 nll_val: 14198.6157483553 kl_val: -0.1863058939 mse_val: 0.0747295565 acc_val: 0.6842105263 time: 0.5464s
Epoch: 0192 nll_train: 14467.8662516276 kl_train: -0.1779231873 mse_train: 0.0761466664 acc_train: 0.7916666667 nll_val: 14233.7534436678 kl_val: -0.1759852791 mse_val: 0.0749144911 acc_val: 0.6578947368 time: 0.5456s
Epoch: 0193 nll_train: 14438.8477783203 kl_train: -0.1718755914 mse_train: 0.0759939354 acc_train: 0.7708333333 nll_val: 14244.6707442434 kl_val: -0.2230975604 mse_val: 0.0749719527 acc_val: 0.6578947368 time: 0.5479s
Epoch: 0194 nll_train: 14462.9247639974 kl_train: -0.1686278190 mse_train: 0.0761206577 acc_train: 0.8333333333 nll_val: 14221.9143708882 kl_val: -0.1678932968 mse_val: 0.0748521821 acc_val: 0.7105263158 time: 0.5452s
Epoch: 0195 nll_train: 14484.2924804688 kl_train: -0.1562844360 mse_train: 0.0762331192 acc_train: 0.8541666667 nll_val: 14258.8425678454 kl_val: -0.1743291420 mse_val: 0.0750465397 acc_val: 0.7631578947 time: 0.5465s
Epoch: 0196 nll_train: 14454.2578531901 kl_train: -0.1589393023 mse_train: 0.0760750412 acc_train: 0.8125000000 nll_val: 14232.7619243421 kl_val: -0.1736539759 mse_val: 0.0749092737 acc_val: 0.6578947368 time: 0.5462s
Epoch: 0197 nll_train: 14455.7758382161 kl_train: -0.1599136253 mse_train: 0.0760830319 acc_train: 0.7291666667 nll_val: 14283.7249177632 kl_val: -0.1919149058 mse_val: 0.0751774999 acc_val: 0.6315789474 time: 0.5444s
Epoch: 0198 nll_train: 14466.4962565104 kl_train: -0.1693063571 mse_train: 0.0761394536 acc_train: 0.8333333333 nll_val: 14253.5925678454 kl_val: -0.1809816376 mse_val: 0.0750189065 acc_val: 0.7368421053 time: 0.5447s
Epoch: 0199 nll_train: 14450.0382893880 kl_train: -0.1619555925 mse_train: 0.0760528333 acc_train: 0.8125000000 nll_val: 14213.8942742599 kl_val: -0.2006953155 mse_val: 0.0748099702 acc_val: 0.6842105263 time: 0.5440s
Optimization finished
Best epoch 42
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 15299.7783717105 kl_test: -0.1132079524 mse_test: 0.0805251489 acc_test: 0.6578947368
MSE: [ 0.080672077835 , 0.079681403935 , 0.079472504556 , 0.079447977245 , 0.080167986453 , 0.079703710973 , 0.079932816327 , 0.079441674054 , 0.079661585391 , 0.079246364534 , 0.079381734133 , 0.080022603273 , 0.079454317689 , 0.079897344112 , 0.079762183130 , 0.079483531415 , 0.079232387245 , 0.079524643719 , 0.079850867391 ]
Accuracy for experiment id 13 is 0.5
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 17712.8458658854 kl_train: -0.1972887308 mse_train: 0.0932255030 acc_train: 0.1666666667 nll_val: 17972.3913445724 kl_val: -0.2670617880 mse_val: 0.0945915352 acc_val: 0.0000000000 time: 0.5520s
Best model so far, saving...
Epoch: 0001 nll_train: 14023.9903971354 kl_train: -0.1483001206 mse_train: 0.0738104756 acc_train: 0.0000000000 nll_val: 15316.6307565789 kl_val: -0.1636082373 mse_val: 0.0806138468 acc_val: 0.0000000000 time: 0.5439s
Best model so far, saving...
Epoch: 0002 nll_train: 13600.5462239583 kl_train: -0.1542175937 mse_train: 0.0715818217 acc_train: 0.0000000000 nll_val: 14722.0649157072 kl_val: -0.1750685424 mse_val: 0.0774845530 acc_val: 0.0000000000 time: 0.5425s
Best model so far, saving...
Epoch: 0003 nll_train: 13244.7268066406 kl_train: -0.2026176440 mse_train: 0.0697090888 acc_train: 0.0000000000 nll_val: 14267.9197162829 kl_val: -0.1480537680 mse_val: 0.0750943156 acc_val: 0.1052631579 time: 0.5414s
Best model so far, saving...
Epoch: 0004 nll_train: 13155.7847900391 kl_train: -0.1503774667 mse_train: 0.0692409726 acc_train: 0.0625000000 nll_val: 14167.3159436678 kl_val: -0.1056886790 mse_val: 0.0745648206 acc_val: 0.2631578947 time: 0.5424s
Best model so far, saving...
Epoch: 0005 nll_train: 13113.5273437500 kl_train: -0.1115826052 mse_train: 0.0690185654 acc_train: 0.0833333333 nll_val: 14100.8295127467 kl_val: -0.1080136774 mse_val: 0.0742148902 acc_val: 0.3684210526 time: 0.5429s
Best model so far, saving...
Epoch: 0006 nll_train: 13014.1591796875 kl_train: -0.1008561654 mse_train: 0.0684955747 acc_train: 0.2083333333 nll_val: 13810.1752672697 kl_val: -0.0803907220 mse_val: 0.0726851335 acc_val: 0.3684210526 time: 0.5430s
Best model so far, saving...
Epoch: 0007 nll_train: 12973.9255371094 kl_train: -0.0857438343 mse_train: 0.0682838207 acc_train: 0.2708333333 nll_val: 13787.8291015625 kl_val: -0.0763193208 mse_val: 0.0725675223 acc_val: 0.4210526316 time: 0.5425s
Best model so far, saving...
Epoch: 0008 nll_train: 12964.2863769531 kl_train: -0.0844348497 mse_train: 0.0682330860 acc_train: 0.2916666667 nll_val: 13712.9619654605 kl_val: -0.0793829943 mse_val: 0.0721734833 acc_val: 0.4473684211 time: 0.5444s
Best model so far, saving...
Epoch: 0009 nll_train: 12956.8246256510 kl_train: -0.0811105051 mse_train: 0.0681938133 acc_train: 0.2708333333 nll_val: 13640.4437705592 kl_val: -0.0760226310 mse_val: 0.0717918098 acc_val: 0.3947368421 time: 0.5445s
Best model so far, saving...
Epoch: 0010 nll_train: 12930.6092936198 kl_train: -0.0788855215 mse_train: 0.0680558390 acc_train: 0.2708333333 nll_val: 13629.2101151316 kl_val: -0.0842982919 mse_val: 0.0717326832 acc_val: 0.4473684211 time: 0.5459s
Best model so far, saving...
Epoch: 0011 nll_train: 12938.2623291016 kl_train: -0.0824328327 mse_train: 0.0680961191 acc_train: 0.2916666667 nll_val: 13609.8698601974 kl_val: -0.0808411431 mse_val: 0.0716308953 acc_val: 0.4473684211 time: 0.5417s
Best model so far, saving...
Epoch: 0012 nll_train: 12922.8938395182 kl_train: -0.0806638788 mse_train: 0.0680152294 acc_train: 0.2083333333 nll_val: 13581.7107319079 kl_val: -0.0824109435 mse_val: 0.0714826876 acc_val: 0.4473684211 time: 0.5438s
Best model so far, saving...
Epoch: 0013 nll_train: 12906.3631998698 kl_train: -0.0863735198 mse_train: 0.0679282288 acc_train: 0.2500000000 nll_val: 13604.6645764803 kl_val: -0.0950418781 mse_val: 0.0716034986 acc_val: 0.3684210526 time: 0.5436s
Epoch: 0014 nll_train: 12910.1905517578 kl_train: -0.0931181669 mse_train: 0.0679483721 acc_train: 0.2083333333 nll_val: 13592.7990851151 kl_val: -0.0962962959 mse_val: 0.0715410496 acc_val: 0.4473684211 time: 0.5442s
Epoch: 0015 nll_train: 12904.9123128255 kl_train: -0.1030480644 mse_train: 0.0679205915 acc_train: 0.2500000000 nll_val: 13578.5743215461 kl_val: -0.0986221719 mse_val: 0.0714661806 acc_val: 0.4210526316 time: 0.5425s
Best model so far, saving...
Epoch: 0016 nll_train: 12888.0689697266 kl_train: -0.1070211648 mse_train: 0.0678319409 acc_train: 0.3125000000 nll_val: 13558.3336245888 kl_val: -0.1064639131 mse_val: 0.0713596495 acc_val: 0.4210526316 time: 0.5431s
Best model so far, saving...
Epoch: 0017 nll_train: 12891.3176676432 kl_train: -0.1173466441 mse_train: 0.0678490416 acc_train: 0.3750000000 nll_val: 13551.9708059211 kl_val: -0.1130106120 mse_val: 0.0713261648 acc_val: 0.4736842105 time: 0.5437s
Best model so far, saving...
Epoch: 0018 nll_train: 12872.5662027995 kl_train: -0.1205746882 mse_train: 0.0677503506 acc_train: 0.3125000000 nll_val: 13529.9665912829 kl_val: -0.1129750355 mse_val: 0.0712103501 acc_val: 0.4473684211 time: 0.5434s
Best model so far, saving...
Epoch: 0019 nll_train: 12881.4311930339 kl_train: -0.1165207538 mse_train: 0.0677970083 acc_train: 0.2916666667 nll_val: 13539.6624177632 kl_val: -0.1193429145 mse_val: 0.0712613808 acc_val: 0.4473684211 time: 0.5431s
Epoch: 0020 nll_train: 12870.4835611979 kl_train: -0.1179680461 mse_train: 0.0677393883 acc_train: 0.3333333333 nll_val: 13551.5194284539 kl_val: -0.1080778484 mse_val: 0.0713237869 acc_val: 0.4210526316 time: 0.5435s
Epoch: 0021 nll_train: 12866.7660725911 kl_train: -0.1089538314 mse_train: 0.0677198217 acc_train: 0.4166666667 nll_val: 13553.2297491776 kl_val: -0.1091708815 mse_val: 0.0713327870 acc_val: 0.4736842105 time: 0.5420s
Epoch: 0022 nll_train: 12875.9226481120 kl_train: -0.1162520456 mse_train: 0.0677680162 acc_train: 0.3750000000 nll_val: 13512.6123046875 kl_val: -0.1206699347 mse_val: 0.0711190114 acc_val: 0.4210526316 time: 0.5399s
Best model so far, saving...
Epoch: 0023 nll_train: 12863.5808512370 kl_train: -0.1165967082 mse_train: 0.0677030589 acc_train: 0.2500000000 nll_val: 13506.4565172697 kl_val: -0.1118047763 mse_val: 0.0710866124 acc_val: 0.4473684211 time: 0.5424s
Best model so far, saving...
Epoch: 0024 nll_train: 12860.1919352214 kl_train: -0.1117531979 mse_train: 0.0676852218 acc_train: 0.2708333333 nll_val: 13488.2469161184 kl_val: -0.1146350875 mse_val: 0.0709907740 acc_val: 0.4736842105 time: 0.5447s
Best model so far, saving...
Epoch: 0025 nll_train: 12846.5015869141 kl_train: -0.1254960901 mse_train: 0.0676131668 acc_train: 0.1875000000 nll_val: 13505.5053967928 kl_val: -0.1160237946 mse_val: 0.0710816066 acc_val: 0.4210526316 time: 0.5451s
Epoch: 0026 nll_train: 12848.9731038411 kl_train: -0.1213432339 mse_train: 0.0676261742 acc_train: 0.3333333333 nll_val: 13492.3547491776 kl_val: -0.1156100156 mse_val: 0.0710123926 acc_val: 0.4473684211 time: 0.5408s
Epoch: 0027 nll_train: 12843.3321533203 kl_train: -0.1247704631 mse_train: 0.0675964854 acc_train: 0.3333333333 nll_val: 13486.9638671875 kl_val: -0.1210182493 mse_val: 0.0709840191 acc_val: 0.4210526316 time: 0.5406s
Best model so far, saving...
Epoch: 0028 nll_train: 12832.1420491536 kl_train: -0.1278327790 mse_train: 0.0675375916 acc_train: 0.3541666667 nll_val: 13481.0309416118 kl_val: -0.1163949574 mse_val: 0.0709527949 acc_val: 0.4473684211 time: 0.5436s
Best model so far, saving...
Epoch: 0029 nll_train: 12844.1520182292 kl_train: -0.1246757908 mse_train: 0.0676007997 acc_train: 0.3125000000 nll_val: 13479.4940892270 kl_val: -0.1206977336 mse_val: 0.0709447065 acc_val: 0.4736842105 time: 0.5449s
Best model so far, saving...
Epoch: 0030 nll_train: 12830.5614420573 kl_train: -0.1248263366 mse_train: 0.0675292718 acc_train: 0.2916666667 nll_val: 13474.7061574836 kl_val: -0.1168264519 mse_val: 0.0709195063 acc_val: 0.4736842105 time: 0.5426s
Best model so far, saving...
Epoch: 0031 nll_train: 12829.0759684245 kl_train: -0.1255876324 mse_train: 0.0675214534 acc_train: 0.2500000000 nll_val: 13479.8965871711 kl_val: -0.1296937046 mse_val: 0.0709468218 acc_val: 0.4473684211 time: 0.5451s
Epoch: 0032 nll_train: 12826.5071207682 kl_train: -0.1404557529 mse_train: 0.0675079315 acc_train: 0.3541666667 nll_val: 13476.5148026316 kl_val: -0.1393658087 mse_val: 0.0709290240 acc_val: 0.4736842105 time: 0.5406s
Epoch: 0033 nll_train: 12816.0169677734 kl_train: -0.1460913680 mse_train: 0.0674527222 acc_train: 0.3333333333 nll_val: 13482.3616879112 kl_val: -0.1342430664 mse_val: 0.0709597968 acc_val: 0.4736842105 time: 0.5406s
Epoch: 0034 nll_train: 12817.4196777344 kl_train: -0.1353364404 mse_train: 0.0674601034 acc_train: 0.3125000000 nll_val: 13473.9325143914 kl_val: -0.1351394347 mse_val: 0.0709154353 acc_val: 0.4736842105 time: 0.5412s
Best model so far, saving...
Epoch: 0035 nll_train: 12821.7810058594 kl_train: -0.1462557456 mse_train: 0.0674830596 acc_train: 0.3333333333 nll_val: 13481.8666221217 kl_val: -0.1382873850 mse_val: 0.0709571927 acc_val: 0.4736842105 time: 0.5439s
Epoch: 0036 nll_train: 12812.4319254557 kl_train: -0.1412021751 mse_train: 0.0674338525 acc_train: 0.4791666667 nll_val: 13465.4723478618 kl_val: -0.1274305515 mse_val: 0.0708709057 acc_val: 0.4736842105 time: 0.5417s
Best model so far, saving...
Epoch: 0037 nll_train: 12816.8900146484 kl_train: -0.1330363216 mse_train: 0.0674573154 acc_train: 0.4166666667 nll_val: 13456.2253803454 kl_val: -0.1243476468 mse_val: 0.0708222385 acc_val: 0.4736842105 time: 0.5436s
Best model so far, saving...
Epoch: 0038 nll_train: 12811.0163167318 kl_train: -0.1333666804 mse_train: 0.0674264015 acc_train: 0.2708333333 nll_val: 13474.3154810855 kl_val: -0.1380206997 mse_val: 0.0709174517 acc_val: 0.3947368421 time: 0.5446s
Epoch: 0039 nll_train: 12801.4198404948 kl_train: -0.1446635695 mse_train: 0.0673758943 acc_train: 0.2500000000 nll_val: 13473.4321032072 kl_val: -0.1418353947 mse_val: 0.0709128019 acc_val: 0.4736842105 time: 0.5437s
Epoch: 0040 nll_train: 12821.4405517578 kl_train: -0.1330154619 mse_train: 0.0674812660 acc_train: 0.5208333333 nll_val: 13459.5317125822 kl_val: -0.1180024971 mse_val: 0.0708396421 acc_val: 0.5263157895 time: 0.5418s
Epoch: 0041 nll_train: 12808.3470458984 kl_train: -0.1301303714 mse_train: 0.0674123520 acc_train: 0.3750000000 nll_val: 13474.1642166941 kl_val: -0.1294362110 mse_val: 0.0709166541 acc_val: 0.4736842105 time: 0.5428s
Epoch: 0042 nll_train: 12799.8416341146 kl_train: -0.1360159113 mse_train: 0.0673675900 acc_train: 0.3750000000 nll_val: 13481.3657483553 kl_val: -0.1463243640 mse_val: 0.0709545573 acc_val: 0.4210526316 time: 0.5416s
Epoch: 0043 nll_train: 12802.6659342448 kl_train: -0.1450472716 mse_train: 0.0673824529 acc_train: 0.3125000000 nll_val: 13470.8853310033 kl_val: -0.1293482459 mse_val: 0.0708993962 acc_val: 0.4736842105 time: 0.5467s
Epoch: 0044 nll_train: 12805.8702392578 kl_train: -0.1340424648 mse_train: 0.0673993166 acc_train: 0.3958333333 nll_val: 13482.9524054276 kl_val: -0.1244599294 mse_val: 0.0709629075 acc_val: 0.4736842105 time: 0.5407s
Epoch: 0045 nll_train: 12794.2082519531 kl_train: -0.1398725817 mse_train: 0.0673379391 acc_train: 0.5000000000 nll_val: 13497.2989823191 kl_val: -0.1408049668 mse_val: 0.0710384146 acc_val: 0.5000000000 time: 0.5423s
Epoch: 0046 nll_train: 12790.4056803385 kl_train: -0.1486008242 mse_train: 0.0673179249 acc_train: 0.4791666667 nll_val: 13469.4260896382 kl_val: -0.1261127924 mse_val: 0.0708917172 acc_val: 0.4736842105 time: 0.5405s
Epoch: 0047 nll_train: 12785.7823893229 kl_train: -0.1281569436 mse_train: 0.0672935930 acc_train: 0.3958333333 nll_val: 13476.2172080592 kl_val: -0.1310371349 mse_val: 0.0709274601 acc_val: 0.5000000000 time: 0.5413s
Epoch: 0048 nll_train: 12799.2201334635 kl_train: -0.1497494985 mse_train: 0.0673643167 acc_train: 0.4375000000 nll_val: 13459.7366879112 kl_val: -0.1312531092 mse_val: 0.0708407185 acc_val: 0.4736842105 time: 0.5419s
Epoch: 0049 nll_train: 12801.9192301432 kl_train: -0.1361280524 mse_train: 0.0673785230 acc_train: 0.5625000000 nll_val: 13464.7405427632 kl_val: -0.1282545963 mse_val: 0.0708670563 acc_val: 0.4736842105 time: 0.5464s
Epoch: 0050 nll_train: 12788.7810058594 kl_train: -0.1476856799 mse_train: 0.0673093741 acc_train: 0.5000000000 nll_val: 13453.1212993421 kl_val: -0.1429177734 mse_val: 0.0708059022 acc_val: 0.4736842105 time: 0.5410s
Best model so far, saving...
Epoch: 0051 nll_train: 12790.7264811198 kl_train: -0.1462236888 mse_train: 0.0673196117 acc_train: 0.3333333333 nll_val: 13464.6922286184 kl_val: -0.1334723411 mse_val: 0.0708667999 acc_val: 0.4736842105 time: 0.5427s
Epoch: 0052 nll_train: 12788.9851888021 kl_train: -0.1454807675 mse_train: 0.0673104478 acc_train: 0.3958333333 nll_val: 13456.7151007401 kl_val: -0.1303394574 mse_val: 0.0708248182 acc_val: 0.4473684211 time: 0.5416s
Epoch: 0053 nll_train: 12776.3850504557 kl_train: -0.1311661738 mse_train: 0.0672441328 acc_train: 0.4166666667 nll_val: 13460.2938425164 kl_val: -0.1312696204 mse_val: 0.0708436527 acc_val: 0.4736842105 time: 0.5478s
Epoch: 0054 nll_train: 12791.9493001302 kl_train: -0.1422653962 mse_train: 0.0673260485 acc_train: 0.3125000000 nll_val: 13463.6411389803 kl_val: -0.1293676256 mse_val: 0.0708612706 acc_val: 0.4736842105 time: 0.5423s
Epoch: 0055 nll_train: 12776.0073649089 kl_train: -0.1284127841 mse_train: 0.0672421437 acc_train: 0.4583333333 nll_val: 13447.3436986020 kl_val: -0.1274045674 mse_val: 0.0707754950 acc_val: 0.4736842105 time: 0.5405s
Best model so far, saving...
Epoch: 0056 nll_train: 12790.5052490234 kl_train: -0.1407288012 mse_train: 0.0673184499 acc_train: 0.5625000000 nll_val: 13464.5464638158 kl_val: -0.1333115983 mse_val: 0.0708660348 acc_val: 0.4736842105 time: 0.5444s
Epoch: 0057 nll_train: 12776.6044921875 kl_train: -0.1406014087 mse_train: 0.0672452874 acc_train: 0.3541666667 nll_val: 13468.1719263980 kl_val: -0.1323300690 mse_val: 0.0708851155 acc_val: 0.5000000000 time: 0.5405s
Epoch: 0058 nll_train: 12776.9882405599 kl_train: -0.1456131103 mse_train: 0.0672473069 acc_train: 0.6250000000 nll_val: 13463.8321854441 kl_val: -0.1353091813 mse_val: 0.0708622748 acc_val: 0.4736842105 time: 0.5413s
Epoch: 0059 nll_train: 12767.5672200521 kl_train: -0.1363863507 mse_train: 0.0671977221 acc_train: 0.4375000000 nll_val: 13474.6518297697 kl_val: -0.1377329289 mse_val: 0.0709192216 acc_val: 0.4736842105 time: 0.5419s
Epoch: 0060 nll_train: 12768.7187093099 kl_train: -0.1442148946 mse_train: 0.0672037833 acc_train: 0.5000000000 nll_val: 13468.7800678454 kl_val: -0.1359350642 mse_val: 0.0708883189 acc_val: 0.5263157895 time: 0.5419s
Epoch: 0061 nll_train: 12766.2804768880 kl_train: -0.1453459254 mse_train: 0.0671909506 acc_train: 0.6041666667 nll_val: 13477.5667660362 kl_val: -0.1369627757 mse_val: 0.0709345637 acc_val: 0.5000000000 time: 0.5415s
Epoch: 0062 nll_train: 12769.8470458984 kl_train: -0.1533847563 mse_train: 0.0672097215 acc_train: 0.4166666667 nll_val: 13490.5996093750 kl_val: -0.1593594026 mse_val: 0.0710031543 acc_val: 0.5000000000 time: 0.5411s
Epoch: 0063 nll_train: 12770.9315592448 kl_train: -0.1545620859 mse_train: 0.0672154298 acc_train: 0.5625000000 nll_val: 13475.0378289474 kl_val: -0.1365847215 mse_val: 0.0709212526 acc_val: 0.5263157895 time: 0.5397s
Epoch: 0064 nll_train: 12779.7948811849 kl_train: -0.1369765593 mse_train: 0.0672620788 acc_train: 0.4791666667 nll_val: 13470.9964021382 kl_val: -0.1290914895 mse_val: 0.0708999806 acc_val: 0.4736842105 time: 0.5387s
Epoch: 0065 nll_train: 12768.9152425130 kl_train: -0.1461783011 mse_train: 0.0672048169 acc_train: 0.3125000000 nll_val: 13483.9271689967 kl_val: -0.1346191642 mse_val: 0.0709680385 acc_val: 0.4736842105 time: 0.5427s
Epoch: 0066 nll_train: 12776.6402587891 kl_train: -0.1365239983 mse_train: 0.0672454750 acc_train: 0.4791666667 nll_val: 13512.5687705592 kl_val: -0.1485625921 mse_val: 0.0711187848 acc_val: 0.5263157895 time: 0.5403s
Epoch: 0067 nll_train: 12774.6560465495 kl_train: -0.1510732627 mse_train: 0.0672350319 acc_train: 0.4791666667 nll_val: 13496.9194592928 kl_val: -0.1446492405 mse_val: 0.0710364182 acc_val: 0.5000000000 time: 0.5404s
Epoch: 0068 nll_train: 12758.1218668620 kl_train: -0.1476514262 mse_train: 0.0671480099 acc_train: 0.6250000000 nll_val: 13476.4724506579 kl_val: -0.1393103376 mse_val: 0.0709288022 acc_val: 0.5263157895 time: 0.5392s
Epoch: 0069 nll_train: 12771.4272460938 kl_train: -0.1517963878 mse_train: 0.0672180372 acc_train: 0.4791666667 nll_val: 13500.5205078125 kl_val: -0.1472318361 mse_val: 0.0710553711 acc_val: 0.4736842105 time: 0.5431s
Epoch: 0070 nll_train: 12761.4849853516 kl_train: -0.1547879834 mse_train: 0.0671657125 acc_train: 0.4375000000 nll_val: 13525.1762952303 kl_val: -0.1531236348 mse_val: 0.0711851379 acc_val: 0.5000000000 time: 0.5421s
Epoch: 0071 nll_train: 12765.4294433594 kl_train: -0.1634338132 mse_train: 0.0671864708 acc_train: 0.5000000000 nll_val: 13543.7167968750 kl_val: -0.1467934219 mse_val: 0.0712827215 acc_val: 0.4736842105 time: 0.5426s
Epoch: 0072 nll_train: 12772.9365234375 kl_train: -0.1450282279 mse_train: 0.0672259821 acc_train: 0.4583333333 nll_val: 13470.9468544408 kl_val: -0.1347208698 mse_val: 0.0708997197 acc_val: 0.4736842105 time: 0.5407s
Epoch: 0073 nll_train: 12761.6597493490 kl_train: -0.1477118271 mse_train: 0.0671666300 acc_train: 0.5416666667 nll_val: 13512.1587685033 kl_val: -0.1670334371 mse_val: 0.0711166259 acc_val: 0.5263157895 time: 0.5388s
Epoch: 0074 nll_train: 12751.9371337891 kl_train: -0.1650457171 mse_train: 0.0671154588 acc_train: 0.5000000000 nll_val: 13474.3325452303 kl_val: -0.1228881306 mse_val: 0.0709175403 acc_val: 0.5000000000 time: 0.5430s
Epoch: 0075 nll_train: 12760.2414550781 kl_train: -0.1324680926 mse_train: 0.0671591639 acc_train: 0.5208333333 nll_val: 13518.8422080592 kl_val: -0.1504640873 mse_val: 0.0711518018 acc_val: 0.5000000000 time: 0.5426s
Epoch: 0076 nll_train: 12771.6470133464 kl_train: -0.1547839831 mse_train: 0.0672191929 acc_train: 0.3541666667 nll_val: 13545.8819387336 kl_val: -0.1453439723 mse_val: 0.0712941152 acc_val: 0.5000000000 time: 0.5413s
Epoch: 0077 nll_train: 12757.1467285156 kl_train: -0.1457377691 mse_train: 0.0671428774 acc_train: 0.5833333333 nll_val: 13512.2156147204 kl_val: -0.1476104350 mse_val: 0.0711169241 acc_val: 0.5000000000 time: 0.5437s
Epoch: 0078 nll_train: 12750.8578694661 kl_train: -0.1529959384 mse_train: 0.0671097802 acc_train: 0.5000000000 nll_val: 13513.4418688322 kl_val: -0.1444426912 mse_val: 0.0711233773 acc_val: 0.4736842105 time: 0.5404s
Epoch: 0079 nll_train: 12748.4163818359 kl_train: -0.1528645717 mse_train: 0.0670969298 acc_train: 0.4375000000 nll_val: 13554.6267475329 kl_val: -0.1426982186 mse_val: 0.0713401423 acc_val: 0.5000000000 time: 0.5396s
Epoch: 0080 nll_train: 12771.8985595703 kl_train: -0.1532966681 mse_train: 0.0672205199 acc_train: 0.6041666667 nll_val: 13502.0363898026 kl_val: -0.1353225677 mse_val: 0.0710633497 acc_val: 0.6052631579 time: 0.5423s
Epoch: 0081 nll_train: 12738.1632893880 kl_train: -0.1513799997 mse_train: 0.0670429654 acc_train: 0.6875000000 nll_val: 13499.6539370888 kl_val: -0.1289625242 mse_val: 0.0710508094 acc_val: 0.6315789474 time: 0.5428s
Epoch: 0082 nll_train: 12781.2121175130 kl_train: -0.1607554639 mse_train: 0.0672695379 acc_train: 0.6875000000 nll_val: 13534.2392064145 kl_val: -0.1375262561 mse_val: 0.0712328377 acc_val: 0.6315789474 time: 0.5420s
Epoch: 0083 nll_train: 12754.9988606771 kl_train: -0.1512162049 mse_train: 0.0671315727 acc_train: 0.6250000000 nll_val: 13525.6834395559 kl_val: -0.1305217778 mse_val: 0.0711878093 acc_val: 0.5263157895 time: 0.5490s
Epoch: 0084 nll_train: 12751.8469238281 kl_train: -0.1571544524 mse_train: 0.0671149848 acc_train: 0.6875000000 nll_val: 13518.0709292763 kl_val: -0.1575538991 mse_val: 0.0711477430 acc_val: 0.6052631579 time: 0.5411s
Epoch: 0085 nll_train: 12741.2165120443 kl_train: -0.1572340438 mse_train: 0.0670590349 acc_train: 0.6250000000 nll_val: 13552.0509868421 kl_val: -0.1560280754 mse_val: 0.0713265834 acc_val: 0.5000000000 time: 0.5403s
Epoch: 0086 nll_train: 12745.9122314453 kl_train: -0.1471608191 mse_train: 0.0670837479 acc_train: 0.4791666667 nll_val: 13536.6850842928 kl_val: -0.1275016387 mse_val: 0.0712457103 acc_val: 0.5000000000 time: 0.5438s
Epoch: 0087 nll_train: 12754.7050374349 kl_train: -0.1564633663 mse_train: 0.0671300270 acc_train: 0.4166666667 nll_val: 13595.1182668586 kl_val: -0.1619055675 mse_val: 0.0715532544 acc_val: 0.5000000000 time: 0.5409s
Epoch: 0088 nll_train: 12757.7030436198 kl_train: -0.1796210359 mse_train: 0.0671458052 acc_train: 0.4375000000 nll_val: 13544.2388466283 kl_val: -0.1380643739 mse_val: 0.0712854684 acc_val: 0.4473684211 time: 0.5409s
Epoch: 0089 nll_train: 12742.6695556641 kl_train: -0.1432283577 mse_train: 0.0670666822 acc_train: 0.5416666667 nll_val: 13544.4586245888 kl_val: -0.1370077627 mse_val: 0.0712866256 acc_val: 0.4210526316 time: 0.5394s
Epoch: 0090 nll_train: 12765.2542317708 kl_train: -0.1508883312 mse_train: 0.0671855503 acc_train: 0.3125000000 nll_val: 13561.4972245066 kl_val: -0.1280963158 mse_val: 0.0713763008 acc_val: 0.4473684211 time: 0.5418s
Epoch: 0091 nll_train: 12750.0523274740 kl_train: -0.1392890618 mse_train: 0.0671055382 acc_train: 0.6666666667 nll_val: 13522.3438527961 kl_val: -0.1365861273 mse_val: 0.0711702304 acc_val: 0.6052631579 time: 0.5407s
Epoch: 0092 nll_train: 12758.6009928385 kl_train: -0.1411163689 mse_train: 0.0671505321 acc_train: 0.5833333333 nll_val: 13549.1127158717 kl_val: -0.1174701640 mse_val: 0.0713111182 acc_val: 0.6052631579 time: 0.5416s
Epoch: 0093 nll_train: 12758.3031005859 kl_train: -0.1575213512 mse_train: 0.0671489653 acc_train: 0.5208333333 nll_val: 13586.0939555921 kl_val: -0.1568915550 mse_val: 0.0715057571 acc_val: 0.5789473684 time: 0.5439s
Epoch: 0094 nll_train: 12754.5758870443 kl_train: -0.1575798926 mse_train: 0.0671293461 acc_train: 0.4791666667 nll_val: 13551.1256681743 kl_val: -0.1517325922 mse_val: 0.0713217139 acc_val: 0.4736842105 time: 0.5407s
Epoch: 0095 nll_train: 12733.1123453776 kl_train: -0.1516769296 mse_train: 0.0670163821 acc_train: 0.6458333333 nll_val: 13535.2953844572 kl_val: -0.1534252261 mse_val: 0.0712383960 acc_val: 0.6052631579 time: 0.5424s
Epoch: 0096 nll_train: 12746.0800374349 kl_train: -0.1579604264 mse_train: 0.0670846316 acc_train: 0.6041666667 nll_val: 13594.5982730263 kl_val: -0.1701006082 mse_val: 0.0715505177 acc_val: 0.5789473684 time: 0.5419s
Epoch: 0097 nll_train: 12736.5165201823 kl_train: -0.1665807351 mse_train: 0.0670342979 acc_train: 0.4791666667 nll_val: 13561.0455386513 kl_val: -0.1379824716 mse_val: 0.0713739232 acc_val: 0.5263157895 time: 0.5433s
Epoch: 0098 nll_train: 12751.4005940755 kl_train: -0.1575394540 mse_train: 0.0671126341 acc_train: 0.4583333333 nll_val: 13540.3806537829 kl_val: -0.1516360927 mse_val: 0.0712651634 acc_val: 0.4736842105 time: 0.5401s
Epoch: 0099 nll_train: 12735.0569254557 kl_train: -0.1451416807 mse_train: 0.0670266156 acc_train: 0.6041666667 nll_val: 13553.4518914474 kl_val: -0.1448090559 mse_val: 0.0713339570 acc_val: 0.5263157895 time: 0.5424s
Epoch: 0100 nll_train: 12757.8639729818 kl_train: -0.1519197083 mse_train: 0.0671466522 acc_train: 0.5625000000 nll_val: 13559.8159950658 kl_val: -0.1528757387 mse_val: 0.0713674518 acc_val: 0.5789473684 time: 0.5417s
Epoch: 0101 nll_train: 12745.0022379557 kl_train: -0.1669411675 mse_train: 0.0670789595 acc_train: 0.5833333333 nll_val: 13632.3931949013 kl_val: -0.1550587745 mse_val: 0.0717494386 acc_val: 0.6315789474 time: 0.5435s
Epoch: 0102 nll_train: 12749.4218343099 kl_train: -0.1614738194 mse_train: 0.0671022218 acc_train: 0.6250000000 nll_val: 13616.7598170230 kl_val: -0.1255058250 mse_val: 0.0716671552 acc_val: 0.5789473684 time: 0.5418s
Epoch: 0103 nll_train: 12765.2918701172 kl_train: -0.1445855337 mse_train: 0.0671857476 acc_train: 0.5416666667 nll_val: 13572.5668174342 kl_val: -0.1316131687 mse_val: 0.0714345616 acc_val: 0.6052631579 time: 0.5420s
Epoch: 0104 nll_train: 12731.8959554036 kl_train: -0.1553063073 mse_train: 0.0670099778 acc_train: 0.6250000000 nll_val: 13577.1910464638 kl_val: -0.1658028258 mse_val: 0.0714589006 acc_val: 0.5789473684 time: 0.5409s
Epoch: 0105 nll_train: 12729.6893310547 kl_train: -0.1609738618 mse_train: 0.0669983652 acc_train: 0.6666666667 nll_val: 13585.9420744243 kl_val: -0.1672927486 mse_val: 0.0715049597 acc_val: 0.6578947368 time: 0.5425s
Epoch: 0106 nll_train: 12722.3209635417 kl_train: -0.1666781663 mse_train: 0.0669595827 acc_train: 0.6666666667 nll_val: 13551.4469572368 kl_val: -0.1595628858 mse_val: 0.0713234054 acc_val: 0.6578947368 time: 0.5412s
Epoch: 0107 nll_train: 12740.0615641276 kl_train: -0.1671663153 mse_train: 0.0670529570 acc_train: 0.6458333333 nll_val: 13589.0494962993 kl_val: -0.1473007257 mse_val: 0.0715213138 acc_val: 0.6315789474 time: 0.5434s
Epoch: 0108 nll_train: 12725.9479980469 kl_train: -0.1554239737 mse_train: 0.0669786739 acc_train: 0.7083333333 nll_val: 13621.2284128289 kl_val: -0.1762843116 mse_val: 0.0716906762 acc_val: 0.6578947368 time: 0.5413s
Epoch: 0109 nll_train: 12718.3733723958 kl_train: -0.1661795614 mse_train: 0.0669388087 acc_train: 0.7916666667 nll_val: 13549.6407277961 kl_val: -0.1507530012 mse_val: 0.0713138982 acc_val: 0.6578947368 time: 0.5378s
Epoch: 0110 nll_train: 12736.4043375651 kl_train: -0.1626775125 mse_train: 0.0670337070 acc_train: 0.6458333333 nll_val: 13585.1508532072 kl_val: -0.1453133178 mse_val: 0.0715007919 acc_val: 0.6052631579 time: 0.5399s
Epoch: 0111 nll_train: 12722.4023844401 kl_train: -0.1594970711 mse_train: 0.0669600115 acc_train: 0.6666666667 nll_val: 13605.9393503289 kl_val: -0.1455419765 mse_val: 0.0716102094 acc_val: 0.5526315789 time: 0.5417s
Epoch: 0112 nll_train: 12726.9403889974 kl_train: -0.1561593215 mse_train: 0.0669838965 acc_train: 0.6458333333 nll_val: 13622.2774979441 kl_val: -0.1439496157 mse_val: 0.0716961979 acc_val: 0.6315789474 time: 0.5429s
Epoch: 0113 nll_train: 12730.8472493490 kl_train: -0.1691983094 mse_train: 0.0670044612 acc_train: 0.5833333333 nll_val: 13631.8218544408 kl_val: -0.1473881371 mse_val: 0.0717464284 acc_val: 0.5789473684 time: 0.5416s
Epoch: 0114 nll_train: 12723.4354654948 kl_train: -0.1727428734 mse_train: 0.0669654502 acc_train: 0.5833333333 nll_val: 13647.0232319079 kl_val: -0.1602269717 mse_val: 0.0718264395 acc_val: 0.6052631579 time: 0.5425s
Epoch: 0115 nll_train: 12730.7480875651 kl_train: -0.1642675915 mse_train: 0.0670039388 acc_train: 0.4375000000 nll_val: 13719.0650185033 kl_val: -0.1511440273 mse_val: 0.0722056057 acc_val: 0.4736842105 time: 0.5427s
Epoch: 0116 nll_train: 12733.7510579427 kl_train: -0.1781869171 mse_train: 0.0670197421 acc_train: 0.5208333333 nll_val: 13735.2585834704 kl_val: -0.1537565381 mse_val: 0.0722908350 acc_val: 0.5263157895 time: 0.5511s
Epoch: 0117 nll_train: 12728.1424967448 kl_train: -0.1512140942 mse_train: 0.0669902232 acc_train: 0.6250000000 nll_val: 13510.6167763158 kl_val: -0.1140138160 mse_val: 0.0711085112 acc_val: 0.5263157895 time: 0.5510s
Epoch: 0118 nll_train: 12730.4478759766 kl_train: -0.1298506853 mse_train: 0.0670023582 acc_train: 0.5625000000 nll_val: 13632.8789062500 kl_val: -0.1487952267 mse_val: 0.0717519939 acc_val: 0.6315789474 time: 0.5434s
Epoch: 0119 nll_train: 12713.9170735677 kl_train: -0.1740474015 mse_train: 0.0669153534 acc_train: 0.6666666667 nll_val: 13596.4241365132 kl_val: -0.1758599517 mse_val: 0.0715601278 acc_val: 0.6315789474 time: 0.5430s
Epoch: 0120 nll_train: 12698.8278401693 kl_train: -0.1676912985 mse_train: 0.0668359371 acc_train: 0.7291666667 nll_val: 13696.3844572368 kl_val: -0.1960319688 mse_val: 0.0720862340 acc_val: 0.5789473684 time: 0.5418s
Epoch: 0121 nll_train: 12714.3996582031 kl_train: -0.1895055647 mse_train: 0.0669178916 acc_train: 0.6875000000 nll_val: 13637.5744243421 kl_val: -0.1698455293 mse_val: 0.0717767085 acc_val: 0.4473684211 time: 0.5400s
Epoch: 0122 nll_train: 12727.9034830729 kl_train: -0.1633281003 mse_train: 0.0669889674 acc_train: 0.7291666667 nll_val: 13606.3167660362 kl_val: -0.1355050022 mse_val: 0.0716121934 acc_val: 0.6052631579 time: 0.5408s
Epoch: 0123 nll_train: 12746.7749837240 kl_train: -0.1699563935 mse_train: 0.0670882904 acc_train: 0.5208333333 nll_val: 13573.7226048520 kl_val: -0.1786642314 mse_val: 0.0714406450 acc_val: 0.5789473684 time: 0.5413s
Epoch: 0124 nll_train: 12733.5613606771 kl_train: -0.1529220445 mse_train: 0.0670187430 acc_train: 0.8541666667 nll_val: 13646.5881990132 kl_val: -0.1462966847 mse_val: 0.0718241493 acc_val: 0.6315789474 time: 0.5456s
Epoch: 0125 nll_train: 12718.7942301432 kl_train: -0.1547560375 mse_train: 0.0669410223 acc_train: 0.7083333333 nll_val: 13621.7212171053 kl_val: -0.1303138537 mse_val: 0.0716932716 acc_val: 0.6578947368 time: 0.5419s
Epoch: 0126 nll_train: 12733.3087158203 kl_train: -0.1543924178 mse_train: 0.0670174140 acc_train: 0.7708333333 nll_val: 13543.4784128289 kl_val: -0.1420946690 mse_val: 0.0712814662 acc_val: 0.6315789474 time: 0.5447s
Epoch: 0127 nll_train: 12713.3618977865 kl_train: -0.1593029980 mse_train: 0.0669124334 acc_train: 0.7500000000 nll_val: 13604.2863898026 kl_val: -0.1811191447 mse_val: 0.0716015073 acc_val: 0.6578947368 time: 0.5437s
Epoch: 0128 nll_train: 12717.3538004557 kl_train: -0.1838400910 mse_train: 0.0669334414 acc_train: 0.6041666667 nll_val: 13668.7455797697 kl_val: -0.1654613387 mse_val: 0.0719407662 acc_val: 0.5789473684 time: 0.5404s
Epoch: 0129 nll_train: 12715.6558837891 kl_train: -0.1615885350 mse_train: 0.0669245046 acc_train: 0.7500000000 nll_val: 13629.9388363487 kl_val: -0.1602877116 mse_val: 0.0717365201 acc_val: 0.6842105263 time: 0.5438s
Epoch: 0130 nll_train: 12727.6931152344 kl_train: -0.1700355594 mse_train: 0.0669878605 acc_train: 0.6458333333 nll_val: 13675.6543482730 kl_val: -0.1608048528 mse_val: 0.0719771303 acc_val: 0.7105263158 time: 0.5416s
Epoch: 0131 nll_train: 12733.8815917969 kl_train: -0.1735843675 mse_train: 0.0670204290 acc_train: 0.5000000000 nll_val: 13708.7331414474 kl_val: -0.1402698712 mse_val: 0.0721512272 acc_val: 0.5789473684 time: 0.5368s
Epoch: 0132 nll_train: 12711.5095214844 kl_train: -0.1608882969 mse_train: 0.0669026814 acc_train: 0.8125000000 nll_val: 13746.0395250822 kl_val: -0.1556630229 mse_val: 0.0723475773 acc_val: 0.6842105263 time: 0.5419s
Epoch: 0133 nll_train: 12694.9123942057 kl_train: -0.1635682005 mse_train: 0.0668153296 acc_train: 0.7708333333 nll_val: 13665.4494243421 kl_val: -0.1397532632 mse_val: 0.0719234167 acc_val: 0.6315789474 time: 0.5400s
Epoch: 0134 nll_train: 12700.2333984375 kl_train: -0.1653304975 mse_train: 0.0668433333 acc_train: 0.6875000000 nll_val: 13704.3299239309 kl_val: -0.1903525975 mse_val: 0.0721280518 acc_val: 0.6578947368 time: 0.5409s
Epoch: 0135 nll_train: 12683.1003824870 kl_train: -0.1718848913 mse_train: 0.0667531613 acc_train: 0.7708333333 nll_val: 13663.5419921875 kl_val: -0.1613486993 mse_val: 0.0719133796 acc_val: 0.6315789474 time: 0.5416s
Epoch: 0136 nll_train: 12707.3597005208 kl_train: -0.1759235955 mse_train: 0.0668808403 acc_train: 0.5833333333 nll_val: 13741.1687911184 kl_val: -0.1521354384 mse_val: 0.0723219408 acc_val: 0.7368421053 time: 0.5389s
Epoch: 0137 nll_train: 12702.4272054036 kl_train: -0.1602510487 mse_train: 0.0668548802 acc_train: 0.7291666667 nll_val: 13714.1183182566 kl_val: -0.1519120790 mse_val: 0.0721795698 acc_val: 0.6052631579 time: 0.5391s
Epoch: 0138 nll_train: 12724.7205403646 kl_train: -0.1669365720 mse_train: 0.0669722138 acc_train: 0.5833333333 nll_val: 13717.4095908717 kl_val: -0.1524147521 mse_val: 0.0721968930 acc_val: 0.6052631579 time: 0.5395s
Epoch: 0139 nll_train: 12695.1678059896 kl_train: -0.1699003975 mse_train: 0.0668166741 acc_train: 0.8750000000 nll_val: 13700.0009765625 kl_val: -0.1756865453 mse_val: 0.0721052683 acc_val: 0.6842105263 time: 0.5428s
Epoch: 0140 nll_train: 12691.1189778646 kl_train: -0.1728463365 mse_train: 0.0667953626 acc_train: 0.9166666667 nll_val: 13757.0825966283 kl_val: -0.1721485422 mse_val: 0.0724056963 acc_val: 0.7631578947 time: 0.5404s
Epoch: 0141 nll_train: 12716.6426188151 kl_train: -0.1717489054 mse_train: 0.0669296985 acc_train: 0.8750000000 nll_val: 13793.9487047697 kl_val: -0.1517637948 mse_val: 0.0725997322 acc_val: 0.6842105263 time: 0.5389s
Epoch: 0142 nll_train: 12708.6619466146 kl_train: -0.1623049236 mse_train: 0.0668876936 acc_train: 0.9166666667 nll_val: 13717.2305201480 kl_val: -0.1700574293 mse_val: 0.0721959503 acc_val: 0.7631578947 time: 0.5421s
Epoch: 0143 nll_train: 12697.6432698568 kl_train: -0.1674559092 mse_train: 0.0668297024 acc_train: 0.8541666667 nll_val: 13790.2598170230 kl_val: -0.1381405098 mse_val: 0.0725803158 acc_val: 0.7368421053 time: 0.5455s
Epoch: 0144 nll_train: 12704.2539469401 kl_train: -0.1676341938 mse_train: 0.0668644947 acc_train: 0.7291666667 nll_val: 13694.2217310855 kl_val: -0.1401863663 mse_val: 0.0720748499 acc_val: 0.7368421053 time: 0.5556s
Epoch: 0145 nll_train: 12712.9978027344 kl_train: -0.1659017267 mse_train: 0.0669105157 acc_train: 0.7500000000 nll_val: 13737.7034333882 kl_val: -0.1635012305 mse_val: 0.0723037020 acc_val: 0.7368421053 time: 0.5462s
Epoch: 0146 nll_train: 12692.5419921875 kl_train: -0.1590860393 mse_train: 0.0668028528 acc_train: 0.6875000000 nll_val: 13759.8664165296 kl_val: -0.1698442705 mse_val: 0.0724203494 acc_val: 0.5526315789 time: 0.5428s
Epoch: 0147 nll_train: 12699.6448567708 kl_train: -0.1646579293 mse_train: 0.0668402373 acc_train: 0.6666666667 nll_val: 13670.0327405428 kl_val: -0.1538195539 mse_val: 0.0719475407 acc_val: 0.6052631579 time: 0.5403s
Epoch: 0148 nll_train: 12732.0476481120 kl_train: -0.1429211640 mse_train: 0.0670107771 acc_train: 0.8333333333 nll_val: 13694.2053865132 kl_val: -0.1526845784 mse_val: 0.0720747656 acc_val: 0.6315789474 time: 0.5422s
Epoch: 0149 nll_train: 12724.7062174479 kl_train: -0.1407794443 mse_train: 0.0669721390 acc_train: 0.8125000000 nll_val: 13712.8344469572 kl_val: -0.1402833058 mse_val: 0.0721728135 acc_val: 0.6578947368 time: 0.5405s
Epoch: 0150 nll_train: 12743.4610595703 kl_train: -0.1592903379 mse_train: 0.0670708482 acc_train: 0.8333333333 nll_val: 13620.6236636513 kl_val: -0.1469157158 mse_val: 0.0716874945 acc_val: 0.7631578947 time: 0.5422s
Epoch: 0151 nll_train: 12729.4859212240 kl_train: -0.1499764525 mse_train: 0.0669972945 acc_train: 0.9166666667 nll_val: 13751.2131990132 kl_val: -0.1567146582 mse_val: 0.0723748064 acc_val: 0.6578947368 time: 0.5414s
Epoch: 0152 nll_train: 12727.2011311849 kl_train: -0.1729844250 mse_train: 0.0669852709 acc_train: 0.7500000000 nll_val: 13769.0131578947 kl_val: -0.1503185195 mse_val: 0.0724684923 acc_val: 0.6315789474 time: 0.5398s
Epoch: 0153 nll_train: 12698.8640950521 kl_train: -0.1566158173 mse_train: 0.0668361282 acc_train: 0.7291666667 nll_val: 13782.2061574836 kl_val: -0.1640323840 mse_val: 0.0725379276 acc_val: 0.6842105263 time: 0.5432s
Epoch: 0154 nll_train: 12723.4819335938 kl_train: -0.1913841416 mse_train: 0.0669656952 acc_train: 0.8750000000 nll_val: 13958.6544510691 kl_val: -0.1873660472 mse_val: 0.0734666035 acc_val: 0.7368421053 time: 0.5391s
Epoch: 0155 nll_train: 12696.7706705729 kl_train: -0.1717474591 mse_train: 0.0668251086 acc_train: 0.8125000000 nll_val: 13828.0536595395 kl_val: -0.1826992153 mse_val: 0.0727792292 acc_val: 0.4736842105 time: 0.5441s
Epoch: 0156 nll_train: 12710.7628580729 kl_train: -0.1658348121 mse_train: 0.0668987529 acc_train: 0.6666666667 nll_val: 13690.5507298520 kl_val: -0.1580937811 mse_val: 0.0720555306 acc_val: 0.6315789474 time: 0.5407s
Epoch: 0157 nll_train: 12710.5336914062 kl_train: -0.1514752312 mse_train: 0.0668975475 acc_train: 0.8125000000 nll_val: 13735.2361225329 kl_val: -0.1553633715 mse_val: 0.0722907174 acc_val: 0.7368421053 time: 0.5414s
Epoch: 0158 nll_train: 12711.3600260417 kl_train: -0.1700477997 mse_train: 0.0669018948 acc_train: 0.9375000000 nll_val: 13827.6302425987 kl_val: -0.1689334437 mse_val: 0.0727770021 acc_val: 0.7894736842 time: 0.5428s
Epoch: 0159 nll_train: 12698.5526529948 kl_train: -0.1615517180 mse_train: 0.0668344870 acc_train: 0.8125000000 nll_val: 13854.7751850329 kl_val: -0.1691781204 mse_val: 0.0729198681 acc_val: 0.7368421053 time: 0.5409s
Epoch: 0160 nll_train: 12691.9118245443 kl_train: -0.1695650018 mse_train: 0.0667995363 acc_train: 0.9375000000 nll_val: 13774.5073499178 kl_val: -0.1742005305 mse_val: 0.0724974061 acc_val: 0.7368421053 time: 0.5448s
Epoch: 0161 nll_train: 12702.1163736979 kl_train: -0.1697765086 mse_train: 0.0668532442 acc_train: 0.7708333333 nll_val: 13887.6112767270 kl_val: -0.1578587529 mse_val: 0.0730926898 acc_val: 0.8421052632 time: 0.5439s
Epoch: 0162 nll_train: 12705.9771728516 kl_train: -0.1695780816 mse_train: 0.0668735631 acc_train: 0.8125000000 nll_val: 13896.3530530428 kl_val: -0.1659009708 mse_val: 0.0731387003 acc_val: 0.6315789474 time: 0.5431s
Epoch: 0163 nll_train: 12702.9281412760 kl_train: -0.1535283178 mse_train: 0.0668575154 acc_train: 0.8958333333 nll_val: 13877.5883018092 kl_val: -0.1580324514 mse_val: 0.0730399399 acc_val: 0.6052631579 time: 0.5396s
Epoch: 0164 nll_train: 12718.0202229818 kl_train: -0.1524711816 mse_train: 0.0669369483 acc_train: 0.7708333333 nll_val: 13776.8433388158 kl_val: -0.1455426436 mse_val: 0.0725097029 acc_val: 0.6842105263 time: 0.5472s
Epoch: 0165 nll_train: 12701.1089680990 kl_train: -0.1630948552 mse_train: 0.0668479408 acc_train: 0.7708333333 nll_val: 13963.1157483553 kl_val: -0.1755486688 mse_val: 0.0734900828 acc_val: 0.6315789474 time: 0.5414s
Epoch: 0166 nll_train: 12685.7146402995 kl_train: -0.1712934248 mse_train: 0.0667669202 acc_train: 0.7916666667 nll_val: 13861.1055715461 kl_val: -0.1775817220 mse_val: 0.0729531875 acc_val: 0.6315789474 time: 0.5426s
Epoch: 0167 nll_train: 12715.2639160156 kl_train: -0.1725906711 mse_train: 0.0669224419 acc_train: 0.8333333333 nll_val: 13853.0093030428 kl_val: -0.1412011020 mse_val: 0.0729105763 acc_val: 0.7105263158 time: 0.5436s
Epoch: 0168 nll_train: 12704.5601806641 kl_train: -0.1670527138 mse_train: 0.0668661073 acc_train: 0.6875000000 nll_val: 13934.1462787829 kl_val: -0.1722238189 mse_val: 0.0733376118 acc_val: 0.6578947368 time: 0.5396s
Epoch: 0169 nll_train: 12721.6995442708 kl_train: -0.1642938489 mse_train: 0.0669563129 acc_train: 0.9166666667 nll_val: 13969.9141652961 kl_val: -0.1606241174 mse_val: 0.0735258652 acc_val: 0.7105263158 time: 0.5400s
Epoch: 0170 nll_train: 12690.2046305339 kl_train: -0.1640442188 mse_train: 0.0667905508 acc_train: 0.9166666667 nll_val: 13911.9352384868 kl_val: -0.1634696079 mse_val: 0.0732207126 acc_val: 0.7105263158 time: 0.5415s
Epoch: 0171 nll_train: 12698.0925292969 kl_train: -0.1520359823 mse_train: 0.0668320670 acc_train: 0.8750000000 nll_val: 13894.7329358553 kl_val: -0.1553612826 mse_val: 0.0731301745 acc_val: 0.7105263158 time: 0.5395s
Epoch: 0172 nll_train: 12718.7616780599 kl_train: -0.1677674362 mse_train: 0.0669408507 acc_train: 0.7708333333 nll_val: 13973.5045744243 kl_val: -0.1493808984 mse_val: 0.0735447603 acc_val: 0.6578947368 time: 0.5397s
Epoch: 0173 nll_train: 12726.8437093099 kl_train: -0.1457937990 mse_train: 0.0669833863 acc_train: 0.8958333333 nll_val: 14006.2562705592 kl_val: -0.1452997821 mse_val: 0.0737171385 acc_val: 0.6842105263 time: 0.5432s
Epoch: 0174 nll_train: 12720.9458821615 kl_train: -0.1706246457 mse_train: 0.0669523479 acc_train: 0.7500000000 nll_val: 14039.0344880757 kl_val: -0.1580523809 mse_val: 0.0738896549 acc_val: 0.6315789474 time: 0.5413s
Epoch: 0175 nll_train: 12702.0850830078 kl_train: -0.1539689644 mse_train: 0.0668530790 acc_train: 0.7916666667 nll_val: 14058.4653063322 kl_val: -0.1576409461 mse_val: 0.0739919224 acc_val: 0.4210526316 time: 0.5428s
Epoch: 0176 nll_train: 12795.6490885417 kl_train: -0.1440797122 mse_train: 0.0673455220 acc_train: 0.4375000000 nll_val: 14119.9630448191 kl_val: -0.1394597457 mse_val: 0.0743155944 acc_val: 0.5526315789 time: 0.5442s
Epoch: 0177 nll_train: 12749.8222656250 kl_train: -0.1585967171 mse_train: 0.0671043287 acc_train: 0.7708333333 nll_val: 14008.2917865954 kl_val: -0.1607835767 mse_val: 0.0737278510 acc_val: 0.7368421053 time: 0.5397s
Epoch: 0178 nll_train: 12772.3999837240 kl_train: -0.1672189760 mse_train: 0.0672231594 acc_train: 0.9166666667 nll_val: 14319.9338507401 kl_val: -0.1567247302 mse_val: 0.0753680730 acc_val: 0.6315789474 time: 0.5420s
Epoch: 0179 nll_train: 12758.1100667318 kl_train: -0.1657819692 mse_train: 0.0671479486 acc_train: 0.8958333333 nll_val: 14094.4828330592 kl_val: -0.1699299279 mse_val: 0.0741814893 acc_val: 0.6315789474 time: 0.5413s
Epoch: 0180 nll_train: 12743.4906819661 kl_train: -0.1650244383 mse_train: 0.0670710040 acc_train: 0.8958333333 nll_val: 13981.3557771382 kl_val: -0.1717214624 mse_val: 0.0735860832 acc_val: 0.6315789474 time: 0.5408s
Epoch: 0181 nll_train: 12722.5801595052 kl_train: -0.1560433637 mse_train: 0.0669609479 acc_train: 0.8750000000 nll_val: 13968.8243729441 kl_val: -0.1764106201 mse_val: 0.0735201294 acc_val: 0.7894736842 time: 0.5400s
Epoch: 0182 nll_train: 12768.6232096354 kl_train: -0.1784840003 mse_train: 0.0672032796 acc_train: 0.8125000000 nll_val: 13990.3853824013 kl_val: -0.1739149752 mse_val: 0.0736336087 acc_val: 0.7105263158 time: 0.5427s
Epoch: 0183 nll_train: 12756.7915852865 kl_train: -0.1662685086 mse_train: 0.0671410076 acc_train: 0.8333333333 nll_val: 13852.9361122533 kl_val: -0.1525291381 mse_val: 0.0729101908 acc_val: 0.6578947368 time: 0.5426s
Epoch: 0184 nll_train: 12773.3458251953 kl_train: -0.1646265096 mse_train: 0.0672281366 acc_train: 0.8750000000 nll_val: 13922.9497841283 kl_val: -0.1698022281 mse_val: 0.0732786842 acc_val: 0.7631578947 time: 0.5428s
Epoch: 0185 nll_train: 12786.0674641927 kl_train: -0.1692275753 mse_train: 0.0672950923 acc_train: 0.7708333333 nll_val: 14035.1176500822 kl_val: -0.1683238906 mse_val: 0.0738690392 acc_val: 0.6842105263 time: 0.5424s
Epoch: 0186 nll_train: 12787.1651611328 kl_train: -0.1578362668 mse_train: 0.0673008702 acc_train: 0.8541666667 nll_val: 13898.3042763158 kl_val: -0.1259103621 mse_val: 0.0731489688 acc_val: 0.7631578947 time: 0.5425s
Epoch: 0187 nll_train: 12757.6915283203 kl_train: -0.1564608601 mse_train: 0.0671457443 acc_train: 0.7916666667 nll_val: 13974.8976665296 kl_val: -0.1727649671 mse_val: 0.0735520952 acc_val: 0.6052631579 time: 0.5430s
Epoch: 0188 nll_train: 12799.1607259115 kl_train: -0.1711633895 mse_train: 0.0673640040 acc_train: 0.5833333333 nll_val: 14031.0853207237 kl_val: -0.1506168952 mse_val: 0.0738478181 acc_val: 0.6052631579 time: 0.5415s
Epoch: 0189 nll_train: 12789.0380452474 kl_train: -0.1557302006 mse_train: 0.0673107258 acc_train: 0.9166666667 nll_val: 13759.6998869243 kl_val: -0.1684918568 mse_val: 0.0724194722 acc_val: 0.8157894737 time: 0.5451s
Epoch: 0190 nll_train: 12780.0848388672 kl_train: -0.1743253845 mse_train: 0.0672636049 acc_train: 0.8125000000 nll_val: 13810.5316097862 kl_val: -0.1483924130 mse_val: 0.0726870093 acc_val: 0.7368421053 time: 0.5414s
Epoch: 0191 nll_train: 12803.7672526042 kl_train: -0.1599249402 mse_train: 0.0673882496 acc_train: 0.6458333333 nll_val: 13807.2456311678 kl_val: -0.1665456279 mse_val: 0.0726697141 acc_val: 0.8421052632 time: 0.5432s
Epoch: 0192 nll_train: 12800.2906494141 kl_train: -0.1844985041 mse_train: 0.0673699508 acc_train: 0.7916666667 nll_val: 13759.6522409539 kl_val: -0.1830782851 mse_val: 0.0724192242 acc_val: 0.7631578947 time: 0.5421s
Epoch: 0193 nll_train: 12721.8772379557 kl_train: -0.1594484585 mse_train: 0.0669572496 acc_train: 0.9375000000 nll_val: 13880.1835423520 kl_val: -0.1465653848 mse_val: 0.0730535982 acc_val: 0.7631578947 time: 0.5430s
Epoch: 0194 nll_train: 12729.3559570312 kl_train: -0.1710422384 mse_train: 0.0669966110 acc_train: 0.8125000000 nll_val: 13781.8649259868 kl_val: -0.1479643842 mse_val: 0.0725361330 acc_val: 0.7631578947 time: 0.5426s
Epoch: 0195 nll_train: 12706.5075276693 kl_train: -0.1697731425 mse_train: 0.0668763557 acc_train: 0.8750000000 nll_val: 13864.4943976151 kl_val: -0.1762960683 mse_val: 0.0729710236 acc_val: 0.7894736842 time: 0.5419s
Epoch: 0196 nll_train: 12678.8775634766 kl_train: -0.1930386753 mse_train: 0.0667309364 acc_train: 0.8958333333 nll_val: 13995.0859375000 kl_val: -0.1969892892 mse_val: 0.0736583477 acc_val: 0.7368421053 time: 0.5428s
Epoch: 0197 nll_train: 12697.5608317057 kl_train: -0.1803717337 mse_train: 0.0668292690 acc_train: 0.8958333333 nll_val: 13934.6309107730 kl_val: -0.1769000394 mse_val: 0.0733401626 acc_val: 0.6315789474 time: 0.5420s
Epoch: 0198 nll_train: 12668.9101155599 kl_train: -0.1646548829 mse_train: 0.0666784747 acc_train: 0.8125000000 nll_val: 13923.4127775493 kl_val: -0.1590723744 mse_val: 0.0732811203 acc_val: 0.7105263158 time: 0.5418s
Epoch: 0199 nll_train: 12659.0883789062 kl_train: -0.1569918754 mse_train: 0.0666267805 acc_train: 0.7500000000 nll_val: 13786.2042557566 kl_val: -0.1600395814 mse_val: 0.0725589700 acc_val: 0.6578947368 time: 0.5432s
Optimization finished
Best epoch 55
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 14635.5297594572 kl_test: -0.1176207466 mse_test: 0.0770291043 acc_test: 0.3421052632
MSE: [ 0.076598875225 , 0.076044686139 , 0.076278313994 , 0.075800508261 , 0.076463200152 , 0.076557219028 , 0.076362393796 , 0.076158449054 , 0.076009064913 , 0.076163746417 , 0.076280854642 , 0.076529398561 , 0.076579362154 , 0.076273418963 , 0.076300762594 , 0.076204396784 , 0.076087646186 , 0.075780406594 , 0.077132396400 ]
Accuracy for experiment id 14 is 0.0


###############################################################################
Science Cluster
Job 2537218 for user 'bjerkovic'
Finished at: Wed Nov 23 00:49:06 CET 2022

Job details:
============

Name                : exp5_c
User                : bjerkovic
Partition           : csedu
Nodes               : cn48
Cores               : 2
State               : COMPLETED
Submit              : 2022-11-23T00:27:00
Start               : 2022-11-23T00:27:00
End                 : 2022-11-23T00:49:06
Reserved walltime   : 1-06:00:00
Used walltime       :   00:22:06
Used CPU time       :   00:27:32 (efficiency: 62.29%)
% User (Computation): 99.41%
% System (I/O)      :  0.59%
Mem reserved        : 2G/core
Max Mem used        : 2.13G (cn48)
Max Disk Write      : 0.00  (cn48)
Max Disk Read       : 839.68K (cn48)


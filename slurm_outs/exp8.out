/home/bjerkovic/thesis/models/nri/sourcecode/modules.py:27: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.
  nn.init.xavier_normal(m.weight.data)
/home/bjerkovic/thesis/models/nri/sourcecode/modules.py:110: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.
  nn.init.xavier_normal(m.weight.data)
/home/bjerkovic/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/bjerkovic/thesis/models/nri/sourcecode/utils.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  soft_max_1d = F.softmax(trans_input) # dim=1
/home/bjerkovic/thesis/models/nri/nri_train_test.py:117: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data, relations = Variable(data, volatile=True), Variable(
/home/bjerkovic/thesis/models/nri/nri_train_test.py:191: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  data, relations = Variable(data, volatile=True), Variable(
R[write to console]: Loading required package: tVAR

* installing *source* package ‘tVAR’ ...
** using staged installation
** R
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
*** copying figures
** building package indices
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (tVAR)
/home/bjerkovic/.local/lib/python3.8/site-packages/rpy2/robjects/vectors.py:927: UserWarning: R object inheriting from "POSIXct" but without attribute "tzone".
  warnings.warn('R object inheriting from "POSIXct" but without '
----------Iter = 100----------
Loss = 0.108455
Variable usage = 100.00%
----------Iter = 200----------
Loss = 0.105933
Variable usage = 100.00%
----------Iter = 300----------
Loss = 0.103504
Variable usage = 100.00%
----------Iter = 400----------
Loss = 0.101153
Variable usage = 100.00%
----------Iter = 500----------
Loss = 0.098867
Variable usage = 100.00%
----------Iter = 600----------
Loss = 0.096636
Variable usage = 100.00%
----------Iter = 700----------
Loss = 0.094451
Variable usage = 100.00%
----------Iter = 800----------
Loss = 0.092305
Variable usage = 100.00%
----------Iter = 900----------
Loss = 0.090192
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 0.088109
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 0.086051
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 0.084016
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 0.082001
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 0.080005
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 0.078025
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 0.076061
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 0.074112
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 0.072178
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 0.070258
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 0.068351
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 0.066457
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 0.064578
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 0.062712
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 0.060859
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 0.059021
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 0.057197
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 0.055388
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 0.053595
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 0.051817
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 0.050055
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 0.048310
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 0.046583
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 0.044874
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 0.043184
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 0.041514
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 0.039865
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 0.038237
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 0.036631
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 0.035050
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 0.033492
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 0.031960
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 0.030455
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 0.028978
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 0.027530
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 0.026112
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 0.024726
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 0.023380
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 0.022085
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 0.020865
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 0.019756
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 0.018774
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 0.017940
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 0.017233
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 0.016594
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 0.015982
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 0.015380
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 0.014780
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 0.014180
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 0.013580
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 0.012980
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 0.012380
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 0.011780
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 0.011180
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 0.010580
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 0.009980
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 0.009380
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 0.008780
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 0.008180
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 0.007580
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 0.006980
Variable usage = 99.89%
----------Iter = 7100----------
Loss = 0.006381
Variable usage = 99.78%
----------Iter = 7200----------
Loss = 0.005782
Variable usage = 99.56%
----------Iter = 7300----------
Loss = 0.005184
Variable usage = 99.33%
----------Iter = 7400----------
Loss = 0.004587
Variable usage = 98.67%
----------Iter = 7500----------
Loss = 0.003993
Variable usage = 96.44%
----------Iter = 7600----------
Loss = 0.003407
Variable usage = 93.89%
----------Iter = 7700----------
Loss = 0.002836
Variable usage = 88.67%
----------Iter = 7800----------
Loss = 0.002290
Variable usage = 81.11%
----------Iter = 7900----------
Loss = 0.001776
Variable usage = 70.00%
----------Iter = 8000----------
Loss = 0.001320
Variable usage = 60.44%
----------Iter = 8100----------
Loss = 0.000927
Variable usage = 46.33%
----------Iter = 8200----------
Loss = 0.000606
Variable usage = 31.89%
----------Iter = 8300----------
Loss = 0.000373
Variable usage = 20.78%
----------Iter = 8400----------
Loss = 0.000217
Variable usage = 13.67%
----------Iter = 8500----------
Loss = 0.000116
Variable usage = 7.89%
----------Iter = 8600----------
Loss = 0.000050
Variable usage = 3.11%
----------Iter = 8700----------
Loss = 0.000019
Variable usage = 0.78%
----------Iter = 8800----------
Loss = 0.000008
Variable usage = 0.67%
----------Iter = 8900----------
Loss = 0.000004
Variable usage = 0.22%
----------Iter = 9000----------
Loss = 0.000001
Variable usage = 0.00%
----------Iter = 9100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14300----------
Loss = 0.000000
Variable usage = 0.00%
Stopping early
True variable usage = 100.00%
Estimated variable usage = 0.00%
Accuracy = 0.00%
Accuracy for experiment id 0 is 0.0
----------Iter = 100----------
Loss = 0.108803
Variable usage = 100.00%
----------Iter = 200----------
Loss = 0.106286
Variable usage = 100.00%
----------Iter = 300----------
Loss = 0.103862
Variable usage = 100.00%
----------Iter = 400----------
Loss = 0.101515
Variable usage = 100.00%
----------Iter = 500----------
Loss = 0.099232
Variable usage = 100.00%
----------Iter = 600----------
Loss = 0.097003
Variable usage = 100.00%
----------Iter = 700----------
Loss = 0.094820
Variable usage = 100.00%
----------Iter = 800----------
Loss = 0.092675
Variable usage = 100.00%
----------Iter = 900----------
Loss = 0.090563
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 0.088480
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 0.086423
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 0.084388
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 0.082372
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 0.080375
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 0.078395
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 0.076430
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 0.074480
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 0.072545
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 0.070623
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 0.068715
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 0.066820
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 0.064939
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 0.063071
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 0.061217
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 0.059377
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 0.057551
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 0.055740
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 0.053944
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 0.052164
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 0.050400
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 0.048652
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 0.046922
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 0.045210
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 0.043517
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 0.041844
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 0.040191
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 0.038560
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 0.036951
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 0.035365
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 0.033803
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 0.032267
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 0.030757
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 0.029275
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 0.027822
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 0.026399
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 0.025007
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 0.023652
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 0.022341
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 0.021099
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 0.019962
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 0.018962
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 0.018113
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 0.017389
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 0.016741
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 0.016124
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 0.015519
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 0.014919
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 0.014319
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 0.013719
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 0.013119
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 0.012519
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 0.011919
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 0.011319
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 0.010719
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 0.010119
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 0.009519
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 0.008919
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 0.008319
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 0.007719
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 0.007119
Variable usage = 99.89%
----------Iter = 7100----------
Loss = 0.006519
Variable usage = 99.89%
----------Iter = 7200----------
Loss = 0.005920
Variable usage = 99.89%
----------Iter = 7300----------
Loss = 0.005320
Variable usage = 99.67%
----------Iter = 7400----------
Loss = 0.004721
Variable usage = 99.00%
----------Iter = 7500----------
Loss = 0.004125
Variable usage = 97.78%
----------Iter = 7600----------
Loss = 0.003534
Variable usage = 95.56%
----------Iter = 7700----------
Loss = 0.002954
Variable usage = 90.78%
----------Iter = 7800----------
Loss = 0.002394
Variable usage = 83.44%
----------Iter = 7900----------
Loss = 0.001870
Variable usage = 72.22%
----------Iter = 8000----------
Loss = 0.001399
Variable usage = 60.22%
----------Iter = 8100----------
Loss = 0.001001
Variable usage = 46.33%
----------Iter = 8200----------
Loss = 0.000677
Variable usage = 33.33%
----------Iter = 8300----------
Loss = 0.000440
Variable usage = 23.33%
----------Iter = 8400----------
Loss = 0.000270
Variable usage = 15.11%
----------Iter = 8500----------
Loss = 0.000154
Variable usage = 9.44%
----------Iter = 8600----------
Loss = 0.000081
Variable usage = 4.78%
----------Iter = 8700----------
Loss = 0.000040
Variable usage = 2.00%
----------Iter = 8800----------
Loss = 0.000021
Variable usage = 1.11%
----------Iter = 8900----------
Loss = 0.000011
Variable usage = 0.78%
----------Iter = 9000----------
Loss = 0.000005
Variable usage = 0.33%
----------Iter = 9100----------
Loss = 0.000002
Variable usage = 0.11%
----------Iter = 9200----------
Loss = 0.000001
Variable usage = 0.00%
----------Iter = 9300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14000----------
Loss = 0.000000
Variable usage = 0.00%
Stopping early
True variable usage = 100.00%
Estimated variable usage = 0.00%
Accuracy = 0.00%
Accuracy for experiment id 1 is 0.0
----------Iter = 100----------
Loss = 0.108753
Variable usage = 100.00%
----------Iter = 200----------
Loss = 0.106211
Variable usage = 100.00%
----------Iter = 300----------
Loss = 0.103767
Variable usage = 100.00%
----------Iter = 400----------
Loss = 0.101403
Variable usage = 100.00%
----------Iter = 500----------
Loss = 0.099107
Variable usage = 100.00%
----------Iter = 600----------
Loss = 0.096867
Variable usage = 100.00%
----------Iter = 700----------
Loss = 0.094675
Variable usage = 100.00%
----------Iter = 800----------
Loss = 0.092523
Variable usage = 100.00%
----------Iter = 900----------
Loss = 0.090406
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 0.088320
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 0.086259
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 0.084221
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 0.082204
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 0.080206
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 0.078225
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 0.076261
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 0.074311
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 0.072376
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 0.070455
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 0.068548
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 0.066654
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 0.064774
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 0.062907
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 0.061055
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 0.059216
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 0.057392
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 0.055583
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 0.053789
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 0.052010
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 0.050248
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 0.048503
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 0.046775
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 0.045066
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 0.043375
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 0.041704
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 0.040054
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 0.038425
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 0.036819
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 0.035236
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 0.033677
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 0.032144
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 0.030637
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 0.029158
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 0.027708
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 0.026288
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 0.024901
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 0.023552
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 0.022253
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 0.021021
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 0.019891
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 0.018905
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 0.018064
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 0.017346
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 0.016703
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 0.016089
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 0.015487
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 0.014887
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 0.014287
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 0.013687
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 0.013087
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 0.012487
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 0.011887
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 0.011287
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 0.010687
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 0.010087
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 0.009487
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 0.008887
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 0.008287
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 0.007687
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 0.007087
Variable usage = 100.00%
----------Iter = 7100----------
Loss = 0.006487
Variable usage = 99.89%
----------Iter = 7200----------
Loss = 0.005887
Variable usage = 99.67%
----------Iter = 7300----------
Loss = 0.005288
Variable usage = 99.44%
----------Iter = 7400----------
Loss = 0.004691
Variable usage = 98.56%
----------Iter = 7500----------
Loss = 0.004096
Variable usage = 97.89%
----------Iter = 7600----------
Loss = 0.003507
Variable usage = 95.78%
----------Iter = 7700----------
Loss = 0.002925
Variable usage = 90.44%
----------Iter = 7800----------
Loss = 0.002364
Variable usage = 83.78%
----------Iter = 7900----------
Loss = 0.001842
Variable usage = 72.33%
----------Iter = 8000----------
Loss = 0.001372
Variable usage = 60.33%
----------Iter = 8100----------
Loss = 0.000974
Variable usage = 46.67%
----------Iter = 8200----------
Loss = 0.000650
Variable usage = 33.78%
----------Iter = 8300----------
Loss = 0.000410
Variable usage = 22.22%
----------Iter = 8400----------
Loss = 0.000243
Variable usage = 13.89%
----------Iter = 8500----------
Loss = 0.000136
Variable usage = 7.78%
----------Iter = 8600----------
Loss = 0.000073
Variable usage = 4.89%
----------Iter = 8700----------
Loss = 0.000035
Variable usage = 2.44%
----------Iter = 8800----------
Loss = 0.000012
Variable usage = 0.56%
----------Iter = 8900----------
Loss = 0.000002
Variable usage = 0.00%
----------Iter = 9000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14100----------
Loss = 0.000000
Variable usage = 0.00%
Stopping early
True variable usage = 100.00%
Estimated variable usage = 0.00%
Accuracy = 0.00%
Accuracy for experiment id 2 is 0.0
----------Iter = 100----------
Loss = 0.108525
Variable usage = 100.00%
----------Iter = 200----------
Loss = 0.105992
Variable usage = 100.00%
----------Iter = 300----------
Loss = 0.103555
Variable usage = 100.00%
----------Iter = 400----------
Loss = 0.101197
Variable usage = 100.00%
----------Iter = 500----------
Loss = 0.098906
Variable usage = 100.00%
----------Iter = 600----------
Loss = 0.096670
Variable usage = 100.00%
----------Iter = 700----------
Loss = 0.094481
Variable usage = 100.00%
----------Iter = 800----------
Loss = 0.092333
Variable usage = 100.00%
----------Iter = 900----------
Loss = 0.090218
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 0.088133
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 0.086075
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 0.084039
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 0.082023
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 0.080027
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 0.078047
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 0.076083
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 0.074135
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 0.072201
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 0.070281
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 0.068375
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 0.066483
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 0.064604
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 0.062738
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 0.060887
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 0.059050
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 0.057227
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 0.055419
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 0.053627
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 0.051850
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 0.050089
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 0.048345
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 0.046619
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 0.044912
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 0.043223
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 0.041554
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 0.039906
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 0.038279
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 0.036675
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 0.035094
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 0.033538
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 0.032007
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 0.030503
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 0.029027
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 0.027580
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 0.026164
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 0.024783
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 0.023441
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 0.022147
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 0.020921
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 0.019801
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 0.018829
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 0.018000
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 0.017284
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 0.016647
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 0.016035
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 0.015433
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 0.014833
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 0.014233
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 0.013633
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 0.013033
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 0.012433
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 0.011833
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 0.011233
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 0.010633
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 0.010033
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 0.009433
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 0.008833
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 0.008233
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 0.007633
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 0.007033
Variable usage = 100.00%
----------Iter = 7100----------
Loss = 0.006433
Variable usage = 99.67%
----------Iter = 7200----------
Loss = 0.005833
Variable usage = 99.44%
----------Iter = 7300----------
Loss = 0.005236
Variable usage = 99.00%
----------Iter = 7400----------
Loss = 0.004640
Variable usage = 98.33%
----------Iter = 7500----------
Loss = 0.004047
Variable usage = 97.22%
----------Iter = 7600----------
Loss = 0.003460
Variable usage = 94.22%
----------Iter = 7700----------
Loss = 0.002887
Variable usage = 90.11%
----------Iter = 7800----------
Loss = 0.002333
Variable usage = 82.22%
----------Iter = 7900----------
Loss = 0.001815
Variable usage = 72.33%
----------Iter = 8000----------
Loss = 0.001348
Variable usage = 59.22%
----------Iter = 8100----------
Loss = 0.000951
Variable usage = 45.67%
----------Iter = 8200----------
Loss = 0.000637
Variable usage = 33.22%
----------Iter = 8300----------
Loss = 0.000399
Variable usage = 23.22%
----------Iter = 8400----------
Loss = 0.000233
Variable usage = 14.33%
----------Iter = 8500----------
Loss = 0.000121
Variable usage = 7.56%
----------Iter = 8600----------
Loss = 0.000056
Variable usage = 3.89%
----------Iter = 8700----------
Loss = 0.000023
Variable usage = 1.56%
----------Iter = 8800----------
Loss = 0.000009
Variable usage = 0.89%
----------Iter = 8900----------
Loss = 0.000003
Variable usage = 0.11%
----------Iter = 9000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 15000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 15100----------
Loss = 0.000000
Variable usage = 0.00%
Stopping early
True variable usage = 100.00%
Estimated variable usage = 0.00%
Accuracy = 0.00%
Accuracy for experiment id 3 is 0.0
----------Iter = 100----------
Loss = 0.108559
Variable usage = 100.00%
----------Iter = 200----------
Loss = 0.106019
Variable usage = 100.00%
----------Iter = 300----------
Loss = 0.103576
Variable usage = 100.00%
----------Iter = 400----------
Loss = 0.101214
Variable usage = 100.00%
----------Iter = 500----------
Loss = 0.098919
Variable usage = 100.00%
----------Iter = 600----------
Loss = 0.096680
Variable usage = 100.00%
----------Iter = 700----------
Loss = 0.094489
Variable usage = 100.00%
----------Iter = 800----------
Loss = 0.092338
Variable usage = 100.00%
----------Iter = 900----------
Loss = 0.090222
Variable usage = 100.00%
----------Iter = 1000----------
Loss = 0.088136
Variable usage = 100.00%
----------Iter = 1100----------
Loss = 0.086076
Variable usage = 100.00%
----------Iter = 1200----------
Loss = 0.084039
Variable usage = 100.00%
----------Iter = 1300----------
Loss = 0.082023
Variable usage = 100.00%
----------Iter = 1400----------
Loss = 0.080025
Variable usage = 100.00%
----------Iter = 1500----------
Loss = 0.078045
Variable usage = 100.00%
----------Iter = 1600----------
Loss = 0.076081
Variable usage = 100.00%
----------Iter = 1700----------
Loss = 0.074132
Variable usage = 100.00%
----------Iter = 1800----------
Loss = 0.072197
Variable usage = 100.00%
----------Iter = 1900----------
Loss = 0.070277
Variable usage = 100.00%
----------Iter = 2000----------
Loss = 0.068371
Variable usage = 100.00%
----------Iter = 2100----------
Loss = 0.066478
Variable usage = 100.00%
----------Iter = 2200----------
Loss = 0.064598
Variable usage = 100.00%
----------Iter = 2300----------
Loss = 0.062733
Variable usage = 100.00%
----------Iter = 2400----------
Loss = 0.060881
Variable usage = 100.00%
----------Iter = 2500----------
Loss = 0.059044
Variable usage = 100.00%
----------Iter = 2600----------
Loss = 0.057221
Variable usage = 100.00%
----------Iter = 2700----------
Loss = 0.055412
Variable usage = 100.00%
----------Iter = 2800----------
Loss = 0.053619
Variable usage = 100.00%
----------Iter = 2900----------
Loss = 0.051842
Variable usage = 100.00%
----------Iter = 3000----------
Loss = 0.050081
Variable usage = 100.00%
----------Iter = 3100----------
Loss = 0.048337
Variable usage = 100.00%
----------Iter = 3200----------
Loss = 0.046611
Variable usage = 100.00%
----------Iter = 3300----------
Loss = 0.044903
Variable usage = 100.00%
----------Iter = 3400----------
Loss = 0.043214
Variable usage = 100.00%
----------Iter = 3500----------
Loss = 0.041545
Variable usage = 100.00%
----------Iter = 3600----------
Loss = 0.039896
Variable usage = 100.00%
----------Iter = 3700----------
Loss = 0.038269
Variable usage = 100.00%
----------Iter = 3800----------
Loss = 0.036665
Variable usage = 100.00%
----------Iter = 3900----------
Loss = 0.035084
Variable usage = 100.00%
----------Iter = 4000----------
Loss = 0.033527
Variable usage = 100.00%
----------Iter = 4100----------
Loss = 0.031996
Variable usage = 100.00%
----------Iter = 4200----------
Loss = 0.030492
Variable usage = 100.00%
----------Iter = 4300----------
Loss = 0.029016
Variable usage = 100.00%
----------Iter = 4400----------
Loss = 0.027568
Variable usage = 100.00%
----------Iter = 4500----------
Loss = 0.026151
Variable usage = 100.00%
----------Iter = 4600----------
Loss = 0.024767
Variable usage = 100.00%
----------Iter = 4700----------
Loss = 0.023417
Variable usage = 100.00%
----------Iter = 4800----------
Loss = 0.022120
Variable usage = 100.00%
----------Iter = 4900----------
Loss = 0.020898
Variable usage = 100.00%
----------Iter = 5000----------
Loss = 0.019783
Variable usage = 100.00%
----------Iter = 5100----------
Loss = 0.018810
Variable usage = 100.00%
----------Iter = 5200----------
Loss = 0.017987
Variable usage = 100.00%
----------Iter = 5300----------
Loss = 0.017282
Variable usage = 100.00%
----------Iter = 5400----------
Loss = 0.016642
Variable usage = 100.00%
----------Iter = 5500----------
Loss = 0.016029
Variable usage = 100.00%
----------Iter = 5600----------
Loss = 0.015426
Variable usage = 100.00%
----------Iter = 5700----------
Loss = 0.014825
Variable usage = 100.00%
----------Iter = 5800----------
Loss = 0.014225
Variable usage = 100.00%
----------Iter = 5900----------
Loss = 0.013625
Variable usage = 100.00%
----------Iter = 6000----------
Loss = 0.013025
Variable usage = 100.00%
----------Iter = 6100----------
Loss = 0.012425
Variable usage = 100.00%
----------Iter = 6200----------
Loss = 0.011825
Variable usage = 100.00%
----------Iter = 6300----------
Loss = 0.011225
Variable usage = 100.00%
----------Iter = 6400----------
Loss = 0.010625
Variable usage = 100.00%
----------Iter = 6500----------
Loss = 0.010025
Variable usage = 100.00%
----------Iter = 6600----------
Loss = 0.009425
Variable usage = 100.00%
----------Iter = 6700----------
Loss = 0.008825
Variable usage = 100.00%
----------Iter = 6800----------
Loss = 0.008225
Variable usage = 100.00%
----------Iter = 6900----------
Loss = 0.007625
Variable usage = 100.00%
----------Iter = 7000----------
Loss = 0.007025
Variable usage = 99.89%
----------Iter = 7100----------
Loss = 0.006425
Variable usage = 99.67%
----------Iter = 7200----------
Loss = 0.005826
Variable usage = 99.44%
----------Iter = 7300----------
Loss = 0.005229
Variable usage = 99.00%
----------Iter = 7400----------
Loss = 0.004633
Variable usage = 98.33%
----------Iter = 7500----------
Loss = 0.004041
Variable usage = 97.44%
----------Iter = 7600----------
Loss = 0.003454
Variable usage = 94.11%
----------Iter = 7700----------
Loss = 0.002878
Variable usage = 89.11%
----------Iter = 7800----------
Loss = 0.002325
Variable usage = 80.56%
----------Iter = 7900----------
Loss = 0.001815
Variable usage = 71.56%
----------Iter = 8000----------
Loss = 0.001357
Variable usage = 59.33%
----------Iter = 8100----------
Loss = 0.000965
Variable usage = 47.22%
----------Iter = 8200----------
Loss = 0.000644
Variable usage = 33.22%
----------Iter = 8300----------
Loss = 0.000404
Variable usage = 23.22%
----------Iter = 8400----------
Loss = 0.000237
Variable usage = 13.89%
----------Iter = 8500----------
Loss = 0.000123
Variable usage = 7.89%
----------Iter = 8600----------
Loss = 0.000058
Variable usage = 3.44%
----------Iter = 8700----------
Loss = 0.000025
Variable usage = 1.67%
----------Iter = 8800----------
Loss = 0.000010
Variable usage = 0.33%
----------Iter = 8900----------
Loss = 0.000005
Variable usage = 0.33%
----------Iter = 9000----------
Loss = 0.000003
Variable usage = 0.22%
----------Iter = 9100----------
Loss = 0.000001
Variable usage = 0.00%
----------Iter = 9200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 9900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 10900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 11900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 12900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 13900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14000----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14100----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14200----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14300----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 14500----------
Loss = 0.000000
Variable usage = 0.00%
Stopping early
True variable usage = 100.00%
Estimated variable usage = 0.00%
Accuracy = 0.00%
Accuracy for experiment id 4 is 0.0
----------Iter = 100----------
Loss = 0.007833
Variable usage = 96.67%
----------Iter = 200----------
Loss = 0.006113
Variable usage = 92.44%
----------Iter = 300----------
Loss = 0.004606
Variable usage = 85.56%
----------Iter = 400----------
Loss = 0.003329
Variable usage = 77.56%
----------Iter = 500----------
Loss = 0.002301
Variable usage = 66.89%
----------Iter = 600----------
Loss = 0.001536
Variable usage = 54.67%
----------Iter = 700----------
Loss = 0.001028
Variable usage = 45.44%
----------Iter = 800----------
Loss = 0.000692
Variable usage = 35.56%
----------Iter = 900----------
Loss = 0.000446
Variable usage = 26.33%
----------Iter = 1000----------
Loss = 0.000261
Variable usage = 18.22%
----------Iter = 1100----------
Loss = 0.000127
Variable usage = 9.56%
----------Iter = 1200----------
Loss = 0.000044
Variable usage = 2.89%
----------Iter = 1300----------
Loss = 0.000005
Variable usage = 0.00%
----------Iter = 1400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1900----------
Loss = 0.000000
Variable usage = 0.00%
Stopping early
True variable usage = 100.00%
Estimated variable usage = 0.00%
Accuracy = 0.00%
Accuracy for experiment id 5 is 0.0
----------Iter = 100----------
Loss = 0.007745
Variable usage = 95.33%
----------Iter = 200----------
Loss = 0.006043
Variable usage = 91.44%
----------Iter = 300----------
Loss = 0.004555
Variable usage = 85.22%
----------Iter = 400----------
Loss = 0.003280
Variable usage = 76.00%
----------Iter = 500----------
Loss = 0.002271
Variable usage = 66.11%
----------Iter = 600----------
Loss = 0.001525
Variable usage = 53.11%
----------Iter = 700----------
Loss = 0.001038
Variable usage = 43.00%
----------Iter = 800----------
Loss = 0.000705
Variable usage = 36.00%
----------Iter = 900----------
Loss = 0.000463
Variable usage = 26.89%
----------Iter = 1000----------
Loss = 0.000273
Variable usage = 18.44%
----------Iter = 1100----------
Loss = 0.000139
Variable usage = 11.67%
----------Iter = 1200----------
Loss = 0.000047
Variable usage = 2.33%
----------Iter = 1300----------
Loss = 0.000005
Variable usage = 0.00%
----------Iter = 1400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1900----------
Loss = 0.000000
Variable usage = 0.00%
Stopping early
True variable usage = 100.00%
Estimated variable usage = 0.00%
Accuracy = 0.00%
Accuracy for experiment id 6 is 0.0
----------Iter = 100----------
Loss = 0.007637
Variable usage = 96.00%
----------Iter = 200----------
Loss = 0.005937
Variable usage = 92.89%
----------Iter = 300----------
Loss = 0.004443
Variable usage = 84.89%
----------Iter = 400----------
Loss = 0.003207
Variable usage = 75.67%
----------Iter = 500----------
Loss = 0.002229
Variable usage = 65.00%
----------Iter = 600----------
Loss = 0.001509
Variable usage = 52.33%
----------Iter = 700----------
Loss = 0.001030
Variable usage = 43.56%
----------Iter = 800----------
Loss = 0.000702
Variable usage = 35.22%
----------Iter = 900----------
Loss = 0.000461
Variable usage = 26.67%
----------Iter = 1000----------
Loss = 0.000276
Variable usage = 20.00%
----------Iter = 1100----------
Loss = 0.000135
Variable usage = 11.00%
----------Iter = 1200----------
Loss = 0.000044
Variable usage = 2.56%
----------Iter = 1300----------
Loss = 0.000005
Variable usage = 0.11%
----------Iter = 1400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 2000----------
Loss = 0.000000
Variable usage = 0.00%
Stopping early
True variable usage = 100.00%
Estimated variable usage = 0.00%
Accuracy = 0.00%
Accuracy for experiment id 7 is 0.0
----------Iter = 100----------
Loss = 0.007673
Variable usage = 96.22%
----------Iter = 200----------
Loss = 0.005954
Variable usage = 91.33%
----------Iter = 300----------
Loss = 0.004452
Variable usage = 84.56%
----------Iter = 400----------
Loss = 0.003209
Variable usage = 75.67%
----------Iter = 500----------
Loss = 0.002229
Variable usage = 65.11%
----------Iter = 600----------
Loss = 0.001505
Variable usage = 54.44%
----------Iter = 700----------
Loss = 0.001010
Variable usage = 43.56%
----------Iter = 800----------
Loss = 0.000673
Variable usage = 34.89%
----------Iter = 900----------
Loss = 0.000434
Variable usage = 24.56%
----------Iter = 1000----------
Loss = 0.000255
Variable usage = 17.56%
----------Iter = 1100----------
Loss = 0.000127
Variable usage = 10.56%
----------Iter = 1200----------
Loss = 0.000042
Variable usage = 2.56%
----------Iter = 1300----------
Loss = 0.000006
Variable usage = 0.00%
----------Iter = 1400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 2000----------
Loss = 0.000000
Variable usage = 0.00%
Stopping early
True variable usage = 100.00%
Estimated variable usage = 0.00%
Accuracy = 0.00%
Accuracy for experiment id 8 is 0.0
----------Iter = 100----------
Loss = 0.007987
Variable usage = 96.44%
----------Iter = 200----------
Loss = 0.006259
Variable usage = 91.89%
----------Iter = 300----------
Loss = 0.004730
Variable usage = 87.44%
----------Iter = 400----------
Loss = 0.003413
Variable usage = 78.78%
----------Iter = 500----------
Loss = 0.002354
Variable usage = 66.00%
----------Iter = 600----------
Loss = 0.001597
Variable usage = 54.22%
----------Iter = 700----------
Loss = 0.001092
Variable usage = 45.11%
----------Iter = 800----------
Loss = 0.000745
Variable usage = 36.00%
----------Iter = 900----------
Loss = 0.000495
Variable usage = 28.89%
----------Iter = 1000----------
Loss = 0.000300
Variable usage = 20.00%
----------Iter = 1100----------
Loss = 0.000153
Variable usage = 12.44%
----------Iter = 1200----------
Loss = 0.000056
Variable usage = 3.67%
----------Iter = 1300----------
Loss = 0.000009
Variable usage = 0.11%
----------Iter = 1400----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1500----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1600----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1700----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1800----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 1900----------
Loss = 0.000000
Variable usage = 0.00%
----------Iter = 2000----------
Loss = 0.000000
Variable usage = 0.00%
Stopping early
True variable usage = 100.00%
Estimated variable usage = 0.00%
Accuracy = 0.00%
Accuracy for experiment id 9 is 0.0
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 1709374.0078125000 kl_train: -11.4974472523 mse_train: 8.9967040196 acc_train: 0.4844827586 nll_val: 12685.6923828125 kl_val: -14.7488660812 mse_val: 0.0667667985 acc_val: 1.0000000000 time: 3.3204s
Best model so far, saving...
Epoch: 0001 nll_train: 32223.6757812500 kl_train: -10.3636550903 mse_train: 0.1695982814 acc_train: 0.4040229885 nll_val: 260263.2968750000 kl_val: -1.0576074123 mse_val: 1.3698067665 acc_val: 1.0000000000 time: 2.9496s
Epoch: 0002 nll_train: 124791.7812500000 kl_train: -6.8582274914 mse_train: 0.6567988396 acc_train: 0.5229885057 nll_val: 535636.8125000000 kl_val: -0.3308765590 mse_val: 2.8191411495 acc_val: 1.0000000000 time: 3.1168s
Epoch: 0003 nll_train: 133471.7031250000 kl_train: -5.7202062607 mse_train: 0.7024826109 acc_train: 0.5091954023 nll_val: 513807.2812500000 kl_val: -0.3303006887 mse_val: 2.7042489052 acc_val: 1.0000000000 time: 2.9561s
Epoch: 0004 nll_train: 93035.8281250000 kl_train: -5.3396749496 mse_train: 0.4896622151 acc_train: 0.4890804598 nll_val: 258609.6093750000 kl_val: -0.7247402072 mse_val: 1.3611030579 acc_val: 1.0000000000 time: 2.9925s
Epoch: 0005 nll_train: 40895.2373046875 kl_train: -5.2369966507 mse_train: 0.2152380645 acc_train: 0.4735632184 nll_val: 19344.3828125000 kl_val: -2.1977543831 mse_val: 0.1018125191 acc_val: 1.0000000000 time: 3.0781s
Epoch: 0006 nll_train: 4577.3117675781 kl_train: -5.2673356533 mse_train: 0.0240911124 acc_train: 0.4448275862 nll_val: 71101.0546875000 kl_val: -6.6364822388 mse_val: 0.3742160797 acc_val: 0.9758620690 time: 2.9695s
Epoch: 0007 nll_train: 29153.9228515625 kl_train: -5.3279583454 mse_train: 0.1534416750 acc_train: 0.4241379310 nll_val: 46543.2304687500 kl_val: -14.7206726074 mse_val: 0.2449643612 acc_val: 0.8689655172 time: 2.9867s
Epoch: 0008 nll_train: 18322.7802734375 kl_train: -5.3800735474 mse_train: 0.0964356847 acc_train: 0.4068965517 nll_val: 8246.1552734375 kl_val: -15.0471000671 mse_val: 0.0434008166 acc_val: 0.0172413793 time: 2.9898s
Best model so far, saving...
Epoch: 0009 nll_train: 2619.4308471680 kl_train: -5.4300506115 mse_train: 0.0137864782 acc_train: 0.3936781609 nll_val: 8100.3359375000 kl_val: -6.9133200645 mse_val: 0.0426333472 acc_val: 0.0000000000 time: 2.9669s
Best model so far, saving...
Epoch: 0010 nll_train: 7562.5891113281 kl_train: -5.4905037880 mse_train: 0.0398030970 acc_train: 0.3850574713 nll_val: 8430.4970703125 kl_val: -2.2237513065 mse_val: 0.0443710350 acc_val: 0.0000000000 time: 2.9577s
Epoch: 0011 nll_train: 10748.9179687500 kl_train: -5.6010289192 mse_train: 0.0565732475 acc_train: 0.3718390805 nll_val: 8493.4375000000 kl_val: -0.6944484711 mse_val: 0.0447022952 acc_val: 0.0000000000 time: 2.9946s
Epoch: 0012 nll_train: 5759.3282470703 kl_train: -5.7834014893 mse_train: 0.0303122513 acc_train: 0.3574712644 nll_val: 8508.1464843750 kl_val: -0.3260380626 mse_val: 0.0447797216 acc_val: 0.0000000000 time: 2.9620s
Epoch: 0013 nll_train: 1738.3250732422 kl_train: -5.9825503826 mse_train: 0.0091490799 acc_train: 0.3448275862 nll_val: 8450.5859375000 kl_val: -0.2421678901 mse_val: 0.0444767661 acc_val: 0.0000000000 time: 3.0122s
Epoch: 0014 nll_train: 2622.1187133789 kl_train: -6.1591215134 mse_train: 0.0138006248 acc_train: 0.3333333333 nll_val: 8397.1650390625 kl_val: -0.2265660167 mse_val: 0.0441955999 acc_val: 0.0000000000 time: 2.9658s
Epoch: 0015 nll_train: 3368.7838745117 kl_train: -6.3008697033 mse_train: 0.0177304419 acc_train: 0.3183908046 nll_val: 8318.1953125000 kl_val: -0.2391072065 mse_val: 0.0437799767 acc_val: 0.0000000000 time: 2.9828s
Epoch: 0016 nll_train: 1901.7598876953 kl_train: -6.4102828503 mse_train: 0.0100092634 acc_train: 0.3097701149 nll_val: 8199.4277343750 kl_val: -0.2953992486 mse_val: 0.0431548767 acc_val: 0.0000000000 time: 2.9984s
Epoch: 0017 nll_train: 2094.9804077148 kl_train: -6.4980444908 mse_train: 0.0110262115 acc_train: 0.3028735632 nll_val: 7940.1318359375 kl_val: -0.3808583021 mse_val: 0.0417901613 acc_val: 0.0000000000 time: 2.9988s
Best model so far, saving...
Epoch: 0018 nll_train: 2066.0178222656 kl_train: -6.5842638016 mse_train: 0.0108737780 acc_train: 0.2896551724 nll_val: 7820.4082031250 kl_val: -0.4369002879 mse_val: 0.0411600396 acc_val: 0.0000000000 time: 2.9821s
Best model so far, saving...
Epoch: 0019 nll_train: 1374.4364929199 kl_train: -6.6725115776 mse_train: 0.0072338755 acc_train: 0.2850574713 nll_val: 7727.3544921875 kl_val: -0.4859227240 mse_val: 0.0406702869 acc_val: 0.0000000000 time: 2.9710s
Best model so far, saving...
Epoch: 0020 nll_train: 1649.8330078125 kl_train: -6.7490196228 mse_train: 0.0086833306 acc_train: 0.2827586207 nll_val: 7541.9003906250 kl_val: -0.5499600172 mse_val: 0.0396942087 acc_val: 0.0000000000 time: 2.9716s
Best model so far, saving...
Epoch: 0021 nll_train: 1632.7943420410 kl_train: -6.8068609238 mse_train: 0.0085936539 acc_train: 0.2798850575 nll_val: 7170.0253906250 kl_val: -0.6148504615 mse_val: 0.0377369747 acc_val: 0.0011494253 time: 2.9691s
Best model so far, saving...
Epoch: 0022 nll_train: 1454.3080139160 kl_train: -6.8496677876 mse_train: 0.0076542529 acc_train: 0.2758620690 nll_val: 6827.7714843750 kl_val: -0.7157254815 mse_val: 0.0359356403 acc_val: 0.0137931034 time: 2.9658s
Best model so far, saving...
Epoch: 0023 nll_train: 1553.3292236328 kl_train: -6.8867151737 mse_train: 0.0081754173 acc_train: 0.2729885057 nll_val: 6518.3999023438 kl_val: -0.8864897490 mse_val: 0.0343073644 acc_val: 0.0379310345 time: 3.0069s
Best model so far, saving...
Epoch: 0024 nll_train: 1412.5700073242 kl_train: -6.9253034592 mse_train: 0.0074345791 acc_train: 0.2689655172 nll_val: 6383.2988281250 kl_val: -1.0981723070 mse_val: 0.0335963108 acc_val: 0.0609195402 time: 3.0395s
Best model so far, saving...
Epoch: 0025 nll_train: 1394.4968872070 kl_train: -6.9622845650 mse_train: 0.0073394574 acc_train: 0.2678160920 nll_val: 6209.5097656250 kl_val: -1.2809773684 mse_val: 0.0326816328 acc_val: 0.0666666667 time: 2.9994s
Best model so far, saving...
Epoch: 0026 nll_train: 1455.2089233398 kl_train: -6.9908170700 mse_train: 0.0076589934 acc_train: 0.2626436782 nll_val: 5910.5756835938 kl_val: -1.4360727072 mse_val: 0.0311082881 acc_val: 0.0678160920 time: 2.9854s
Best model so far, saving...
Epoch: 0027 nll_train: 1408.2960510254 kl_train: -7.0110499859 mse_train: 0.0074120843 acc_train: 0.2603448276 nll_val: 5677.5366210938 kl_val: -1.5942997932 mse_val: 0.0298817698 acc_val: 0.0701149425 time: 2.9590s
Best model so far, saving...
Epoch: 0028 nll_train: 1413.7340087891 kl_train: -7.0276467800 mse_train: 0.0074407052 acc_train: 0.2603448276 nll_val: 5585.5888671875 kl_val: -1.7526838779 mse_val: 0.0293978322 acc_val: 0.0793103448 time: 2.9792s
Best model so far, saving...
Epoch: 0029 nll_train: 1379.9971313477 kl_train: -7.0445716381 mse_train: 0.0072631423 acc_train: 0.2591954023 nll_val: 5531.2871093750 kl_val: -1.9260622263 mse_val: 0.0291120354 acc_val: 0.0885057471 time: 2.9376s
Best model so far, saving...
Epoch: 0030 nll_train: 1365.0982360840 kl_train: -7.0612108707 mse_train: 0.0071847274 acc_train: 0.2574712644 nll_val: 5518.6840820312 kl_val: -2.1423211098 mse_val: 0.0290457048 acc_val: 0.1091954023 time: 2.9579s
Best model so far, saving...
Epoch: 0031 nll_train: 1393.3359680176 kl_train: -7.0745496750 mse_train: 0.0073333476 acc_train: 0.2580459770 nll_val: 5404.9965820312 kl_val: -2.4040875435 mse_val: 0.0284473468 acc_val: 0.1264367816 time: 2.9624s
Best model so far, saving...
Epoch: 0032 nll_train: 1389.9944152832 kl_train: -7.0836923122 mse_train: 0.0073157599 acc_train: 0.2574712644 nll_val: 5257.4604492188 kl_val: -2.6786682606 mse_val: 0.0276708417 acc_val: 0.1494252874 time: 2.9577s
Best model so far, saving...
Epoch: 0033 nll_train: 1386.6708374023 kl_train: -7.0908386707 mse_train: 0.0072982672 acc_train: 0.2563218391 nll_val: 5168.2705078125 kl_val: -2.9383599758 mse_val: 0.0272014178 acc_val: 0.1827586207 time: 2.9673s
Best model so far, saving...
Epoch: 0034 nll_train: 1375.8536987305 kl_train: -7.0984969139 mse_train: 0.0072413351 acc_train: 0.2551724138 nll_val: 5148.3798828125 kl_val: -3.1828634739 mse_val: 0.0270967335 acc_val: 0.2045977011 time: 2.9877s
Best model so far, saving...
Epoch: 0035 nll_train: 1361.0244140625 kl_train: -7.1069622040 mse_train: 0.0071632863 acc_train: 0.2540229885 nll_val: 5206.8090820312 kl_val: -3.4323737621 mse_val: 0.0274042543 acc_val: 0.2195402299 time: 3.0776s
Epoch: 0036 nll_train: 1371.1859741211 kl_train: -7.1145143509 mse_train: 0.0072167688 acc_train: 0.2551724138 nll_val: 5182.4980468750 kl_val: -3.6979539394 mse_val: 0.0272763036 acc_val: 0.2333333333 time: 2.9758s
Epoch: 0037 nll_train: 1377.6767883301 kl_train: -7.1200573444 mse_train: 0.0072509302 acc_train: 0.2545977011 nll_val: 5091.3149414062 kl_val: -3.9719026089 mse_val: 0.0267963931 acc_val: 0.2448275862 time: 2.9687s
Best model so far, saving...
Epoch: 0038 nll_train: 1375.5450439453 kl_train: -7.1241431236 mse_train: 0.0072397112 acc_train: 0.2540229885 nll_val: 5041.1308593750 kl_val: -4.2204213142 mse_val: 0.0265322663 acc_val: 0.2540229885 time: 2.9926s
Best model so far, saving...
Epoch: 0039 nll_train: 1371.6658020020 kl_train: -7.1287152767 mse_train: 0.0072192934 acc_train: 0.2545977011 nll_val: 5024.4492187500 kl_val: -4.4255108833 mse_val: 0.0264444705 acc_val: 0.2620689655 time: 2.9747s
Best model so far, saving...
Epoch: 0040 nll_train: 1361.8316040039 kl_train: -7.1344618797 mse_train: 0.0071675344 acc_train: 0.2540229885 nll_val: 5068.5097656250 kl_val: -4.5935859680 mse_val: 0.0266763642 acc_val: 0.2724137931 time: 2.9757s
Epoch: 0041 nll_train: 1362.0223083496 kl_train: -7.1404633522 mse_train: 0.0071685381 acc_train: 0.2522988506 nll_val: 5077.8369140625 kl_val: -4.7456107140 mse_val: 0.0267254561 acc_val: 0.2816091954 time: 2.9603s
Epoch: 0042 nll_train: 1367.6504211426 kl_train: -7.1455607414 mse_train: 0.0071981591 acc_train: 0.2505747126 nll_val: 5042.9360351562 kl_val: -4.8832941055 mse_val: 0.0265417714 acc_val: 0.2896551724 time: 2.9757s
Epoch: 0043 nll_train: 1366.9700012207 kl_train: -7.1497504711 mse_train: 0.0071945784 acc_train: 0.2505747126 nll_val: 5006.0932617188 kl_val: -5.0013403893 mse_val: 0.0263478551 acc_val: 0.2988505747 time: 2.9858s
Best model so far, saving...
Epoch: 0044 nll_train: 1363.9129028320 kl_train: -7.1538820267 mse_train: 0.0071784887 acc_train: 0.2511494253 nll_val: 4983.0200195312 kl_val: -5.0866460800 mse_val: 0.0262264162 acc_val: 0.3057471264 time: 2.9817s
Best model so far, saving...
Epoch: 0045 nll_train: 1358.4931030273 kl_train: -7.1589105129 mse_train: 0.0071499634 acc_train: 0.2528735632 nll_val: 5009.8085937500 kl_val: -5.1484661102 mse_val: 0.0263674129 acc_val: 0.3114942529 time: 3.0141s
Epoch: 0046 nll_train: 1357.0740661621 kl_train: -7.1644215584 mse_train: 0.0071424951 acc_train: 0.2528735632 nll_val: 5027.2788085938 kl_val: -5.2013216019 mse_val: 0.0264593586 acc_val: 0.3183908046 time: 3.1043s
Epoch: 0047 nll_train: 1359.2344360352 kl_train: -7.1696932316 mse_train: 0.0071538649 acc_train: 0.2511494253 nll_val: 5010.9614257812 kl_val: -5.2518191338 mse_val: 0.0263734814 acc_val: 0.3206896552 time: 3.0097s
Epoch: 0048 nll_train: 1360.1127014160 kl_train: -7.1744656563 mse_train: 0.0071584871 acc_train: 0.2505747126 nll_val: 4989.5966796875 kl_val: -5.2976074219 mse_val: 0.0262610354 acc_val: 0.3229885057 time: 3.0089s
Epoch: 0049 nll_train: 1358.7154541016 kl_train: -7.1790597439 mse_train: 0.0071511336 acc_train: 0.2494252874 nll_val: 4975.4716796875 kl_val: -5.3311977386 mse_val: 0.0261866897 acc_val: 0.3252873563 time: 2.9371s
Best model so far, saving...
Epoch: 0050 nll_train: 1354.8530578613 kl_train: -7.1840658188 mse_train: 0.0071308056 acc_train: 0.2494252874 nll_val: 4973.8525390625 kl_val: -5.3553857803 mse_val: 0.0261781719 acc_val: 0.3287356322 time: 2.9039s
Best model so far, saving...
Epoch: 0051 nll_train: 1352.8438720703 kl_train: -7.1893398762 mse_train: 0.0071202305 acc_train: 0.2488505747 nll_val: 4979.9531250000 kl_val: -5.3760843277 mse_val: 0.0262102820 acc_val: 0.3298850575 time: 2.9374s
Epoch: 0052 nll_train: 1358.7755126953 kl_train: -7.1944599152 mse_train: 0.0071514499 acc_train: 0.2482758621 nll_val: 4976.2778320312 kl_val: -5.3984179497 mse_val: 0.0261909347 acc_val: 0.3321839080 time: 3.0143s
Epoch: 0053 nll_train: 1354.7026367188 kl_train: -7.1994462013 mse_train: 0.0071300135 acc_train: 0.2482758621 nll_val: 4974.9682617188 kl_val: -5.4153723717 mse_val: 0.0261840411 acc_val: 0.3344827586 time: 3.0095s
Epoch: 0054 nll_train: 1354.3451843262 kl_train: -7.2045838833 mse_train: 0.0071281322 acc_train: 0.2482758621 nll_val: 4967.1875000000 kl_val: -5.4255023003 mse_val: 0.0261430927 acc_val: 0.3344827586 time: 3.0519s
Best model so far, saving...
Epoch: 0055 nll_train: 1351.4635314941 kl_train: -7.2100656033 mse_train: 0.0071129650 acc_train: 0.2477011494 nll_val: 4972.3281250000 kl_val: -5.4326362610 mse_val: 0.0261701476 acc_val: 0.3367816092 time: 3.0875s
Epoch: 0056 nll_train: 1350.1807861328 kl_train: -7.2156481743 mse_train: 0.0071062139 acc_train: 0.2494252874 nll_val: 4967.3095703125 kl_val: -5.4391956329 mse_val: 0.0261437353 acc_val: 0.3390804598 time: 3.0889s
Epoch: 0057 nll_train: 1351.3649902344 kl_train: -7.2212555408 mse_train: 0.0071124469 acc_train: 0.2494252874 nll_val: 4956.3178710938 kl_val: -5.4456100464 mse_val: 0.0260858815 acc_val: 0.3402298851 time: 3.0867s
Best model so far, saving...
Epoch: 0058 nll_train: 1351.2507324219 kl_train: -7.2267217636 mse_train: 0.0071118462 acc_train: 0.2488505747 nll_val: 4943.8471679688 kl_val: -5.4537353516 mse_val: 0.0260202438 acc_val: 0.3402298851 time: 3.1878s
Best model so far, saving...
Epoch: 0059 nll_train: 1350.4660949707 kl_train: -7.2321610451 mse_train: 0.0071077162 acc_train: 0.2488505747 nll_val: 4944.5268554688 kl_val: -5.4558577538 mse_val: 0.0260238238 acc_val: 0.3413793103 time: 3.1624s
Epoch: 0060 nll_train: 1348.1596679688 kl_train: -7.2379927635 mse_train: 0.0070955772 acc_train: 0.2494252874 nll_val: 4946.1674804688 kl_val: -5.4563341141 mse_val: 0.0260324590 acc_val: 0.3413793103 time: 3.2601s
Epoch: 0061 nll_train: 1348.6719665527 kl_train: -7.2439961433 mse_train: 0.0070982728 acc_train: 0.2488505747 nll_val: 4947.7949218750 kl_val: -5.4580378532 mse_val: 0.0260410253 acc_val: 0.3413793103 time: 3.3386s
Epoch: 0062 nll_train: 1347.8684692383 kl_train: -7.2499866486 mse_train: 0.0070940442 acc_train: 0.2494252874 nll_val: 4941.8427734375 kl_val: -5.4593310356 mse_val: 0.0260096975 acc_val: 0.3425287356 time: 3.1970s
Best model so far, saving...
Epoch: 0063 nll_train: 1347.3616027832 kl_train: -7.2560069561 mse_train: 0.0070913765 acc_train: 0.2482758621 nll_val: 4939.5063476562 kl_val: -5.4628572464 mse_val: 0.0259974021 acc_val: 0.3436781609 time: 3.2844s
Best model so far, saving...
Epoch: 0064 nll_train: 1347.0540466309 kl_train: -7.2619528770 mse_train: 0.0070897577 acc_train: 0.2459770115 nll_val: 4937.0000000000 kl_val: -5.4642443657 mse_val: 0.0259842090 acc_val: 0.3448275862 time: 3.2378s
Best model so far, saving...
Epoch: 0065 nll_train: 1345.8309936523 kl_train: -7.2681078911 mse_train: 0.0070833205 acc_train: 0.2465517241 nll_val: 4934.3999023438 kl_val: -5.4639701843 mse_val: 0.0259705223 acc_val: 0.3436781609 time: 3.3152s
Best model so far, saving...
Epoch: 0066 nll_train: 1345.3481445312 kl_train: -7.2744028568 mse_train: 0.0070807799 acc_train: 0.2454022989 nll_val: 4929.3627929688 kl_val: -5.4656972885 mse_val: 0.0259440150 acc_val: 0.3448275862 time: 3.5397s
Best model so far, saving...
Epoch: 0067 nll_train: 1345.9628906250 kl_train: -7.2807271481 mse_train: 0.0070840151 acc_train: 0.2459770115 nll_val: 4922.6499023438 kl_val: -5.4673018456 mse_val: 0.0259086844 acc_val: 0.3459770115 time: 3.5219s
Best model so far, saving...
Epoch: 0068 nll_train: 1343.7663269043 kl_train: -7.2871744633 mse_train: 0.0070724540 acc_train: 0.2471264368 nll_val: 4925.4960937500 kl_val: -5.4659385681 mse_val: 0.0259236619 acc_val: 0.3448275862 time: 3.4938s
Epoch: 0069 nll_train: 1343.3071594238 kl_train: -7.2938289642 mse_train: 0.0070700372 acc_train: 0.2471264368 nll_val: 4913.4863281250 kl_val: -5.4680142403 mse_val: 0.0258604530 acc_val: 0.3459770115 time: 3.4714s
Best model so far, saving...
Epoch: 0070 nll_train: 1343.0492858887 kl_train: -7.3004462719 mse_train: 0.0070686803 acc_train: 0.2471264368 nll_val: 4911.3066406250 kl_val: -5.4684190750 mse_val: 0.0258489791 acc_val: 0.3471264368 time: 3.4400s
Best model so far, saving...
Epoch: 0071 nll_train: 1343.4754028320 kl_train: -7.3072319031 mse_train: 0.0070709224 acc_train: 0.2442528736 nll_val: 4913.5922851562 kl_val: -5.4681200981 mse_val: 0.0258610118 acc_val: 0.3471264368 time: 3.4873s
Epoch: 0072 nll_train: 1341.2200012207 kl_train: -7.3140702248 mse_train: 0.0070590522 acc_train: 0.2431034483 nll_val: 4903.3789062500 kl_val: -5.4698081017 mse_val: 0.0258072615 acc_val: 0.3471264368 time: 3.5596s
Best model so far, saving...
Epoch: 0073 nll_train: 1341.7539367676 kl_train: -7.3208665848 mse_train: 0.0070618623 acc_train: 0.2413793103 nll_val: 4900.8896484375 kl_val: -5.4706125259 mse_val: 0.0257941559 acc_val: 0.3482758621 time: 3.7804s
Best model so far, saving...
Epoch: 0074 nll_train: 1342.3106994629 kl_train: -7.3278992176 mse_train: 0.0070647934 acc_train: 0.2396551724 nll_val: 4903.5546875000 kl_val: -5.4660711288 mse_val: 0.0258081798 acc_val: 0.3482758621 time: 3.7485s
Epoch: 0075 nll_train: 1339.6528015137 kl_train: -7.3354408741 mse_train: 0.0070508043 acc_train: 0.2396551724 nll_val: 4910.3964843750 kl_val: -5.4602417946 mse_val: 0.0258441940 acc_val: 0.3459770115 time: 3.7241s
Epoch: 0076 nll_train: 1339.8015136719 kl_train: -7.3431177139 mse_train: 0.0070515866 acc_train: 0.2402298851 nll_val: 4902.4169921875 kl_val: -5.4583601952 mse_val: 0.0258021913 acc_val: 0.3471264368 time: 3.7421s
Epoch: 0077 nll_train: 1339.3532409668 kl_train: -7.3507130146 mse_train: 0.0070492272 acc_train: 0.2408045977 nll_val: 4896.4472656250 kl_val: -5.4577159882 mse_val: 0.0257707704 acc_val: 0.3482758621 time: 3.7695s
Best model so far, saving...
Epoch: 0078 nll_train: 1338.4468688965 kl_train: -7.3583545685 mse_train: 0.0070444569 acc_train: 0.2413793103 nll_val: 4894.5771484375 kl_val: -5.4539918900 mse_val: 0.0257609319 acc_val: 0.3471264368 time: 3.7472s
Best model so far, saving...
Epoch: 0079 nll_train: 1337.2811889648 kl_train: -7.3662767410 mse_train: 0.0070383216 acc_train: 0.2419540230 nll_val: 4895.3588867188 kl_val: -5.4526352882 mse_val: 0.0257650428 acc_val: 0.3471264368 time: 3.7499s
Epoch: 0080 nll_train: 1337.0409545898 kl_train: -7.3741972446 mse_train: 0.0070370572 acc_train: 0.2419540230 nll_val: 4885.1840820312 kl_val: -5.4497675896 mse_val: 0.0257114954 acc_val: 0.3471264368 time: 3.7947s
Best model so far, saving...
Epoch: 0081 nll_train: 1341.0379638672 kl_train: -7.3823320866 mse_train: 0.0070580951 acc_train: 0.2436781609 nll_val: 4885.8930664062 kl_val: -5.4478926659 mse_val: 0.0257152263 acc_val: 0.3482758621 time: 3.7060s
Epoch: 0082 nll_train: 1335.0670776367 kl_train: -7.3905627728 mse_train: 0.0070266686 acc_train: 0.2419540230 nll_val: 4894.5288085938 kl_val: -5.4469127655 mse_val: 0.0257606786 acc_val: 0.3482758621 time: 3.7362s
Epoch: 0083 nll_train: 1336.3221740723 kl_train: -7.3989794254 mse_train: 0.0070332745 acc_train: 0.2413793103 nll_val: 4896.7553710938 kl_val: -5.4415869713 mse_val: 0.0257723927 acc_val: 0.3482758621 time: 3.7204s
Epoch: 0084 nll_train: 1336.4303894043 kl_train: -7.4077775478 mse_train: 0.0070338440 acc_train: 0.2408045977 nll_val: 4886.7255859375 kl_val: -5.4309186935 mse_val: 0.0257196054 acc_val: 0.3471264368 time: 3.7179s
Epoch: 0085 nll_train: 1334.6175537109 kl_train: -7.4170279503 mse_train: 0.0070243024 acc_train: 0.2402298851 nll_val: 4888.0371093750 kl_val: -5.4201850891 mse_val: 0.0257265083 acc_val: 0.3471264368 time: 3.7008s
Epoch: 0086 nll_train: 1334.3548278809 kl_train: -7.4264750481 mse_train: 0.0070229198 acc_train: 0.2396551724 nll_val: 4888.8383789062 kl_val: -5.4058828354 mse_val: 0.0257307272 acc_val: 0.3482758621 time: 3.7219s
Epoch: 0087 nll_train: 1337.1396484375 kl_train: -7.4363629818 mse_train: 0.0070375768 acc_train: 0.2402298851 nll_val: 4892.6435546875 kl_val: -5.3898439407 mse_val: 0.0257507544 acc_val: 0.3482758621 time: 3.7313s
Epoch: 0088 nll_train: 1332.5628051758 kl_train: -7.4466185570 mse_train: 0.0070134874 acc_train: 0.2396551724 nll_val: 4890.4604492188 kl_val: -5.3800292015 mse_val: 0.0257392656 acc_val: 0.3482758621 time: 3.7232s
Epoch: 0089 nll_train: 1335.2533874512 kl_train: -7.4567155838 mse_train: 0.0070276491 acc_train: 0.2390804598 nll_val: 4873.2797851562 kl_val: -5.3710451126 mse_val: 0.0256488435 acc_val: 0.3482758621 time: 3.7432s
Best model so far, saving...
Epoch: 0090 nll_train: 1335.8658142090 kl_train: -7.4669773579 mse_train: 0.0070308727 acc_train: 0.2396551724 nll_val: 4890.5361328125 kl_val: -5.3630023003 mse_val: 0.0257396605 acc_val: 0.3482758621 time: 3.7440s
Epoch: 0091 nll_train: 1328.9543457031 kl_train: -7.4774637222 mse_train: 0.0069944962 acc_train: 0.2385057471 nll_val: 4899.9062500000 kl_val: -5.3591361046 mse_val: 0.0257889777 acc_val: 0.3494252874 time: 3.7514s
Epoch: 0092 nll_train: 1334.5431518555 kl_train: -7.4875872135 mse_train: 0.0070239111 acc_train: 0.2373563218 nll_val: 4878.6801757812 kl_val: -5.3663172722 mse_val: 0.0256772637 acc_val: 0.3505747126 time: 3.7267s
Epoch: 0093 nll_train: 1334.1319274902 kl_train: -7.4972422123 mse_train: 0.0070217462 acc_train: 0.2385057471 nll_val: 4857.2607421875 kl_val: -5.3640866280 mse_val: 0.0255645290 acc_val: 0.3505747126 time: 3.7812s
Best model so far, saving...
Epoch: 0094 nll_train: 1328.3445434570 kl_train: -7.5079293251 mse_train: 0.0069912865 acc_train: 0.2385057471 nll_val: 4870.4404296875 kl_val: -5.3407793045 mse_val: 0.0256338939 acc_val: 0.3505747126 time: 3.7231s
Epoch: 0095 nll_train: 1326.7469787598 kl_train: -7.5200235844 mse_train: 0.0069828781 acc_train: 0.2373563218 nll_val: 4886.0766601562 kl_val: -5.3186731339 mse_val: 0.0257161930 acc_val: 0.3505747126 time: 3.6591s
Epoch: 0096 nll_train: 1328.5802612305 kl_train: -7.5323307514 mse_train: 0.0069925281 acc_train: 0.2373563218 nll_val: 4870.7431640625 kl_val: -5.3025484085 mse_val: 0.0256354865 acc_val: 0.3505747126 time: 3.4726s
Epoch: 0097 nll_train: 1327.6789245605 kl_train: -7.5447278023 mse_train: 0.0069877841 acc_train: 0.2367816092 nll_val: 4866.9086914062 kl_val: -5.2779889107 mse_val: 0.0256153103 acc_val: 0.3494252874 time: 3.4035s
Epoch: 0098 nll_train: 1323.3533630371 kl_train: -7.5580112934 mse_train: 0.0069650175 acc_train: 0.2362068966 nll_val: 4875.4267578125 kl_val: -5.2472853661 mse_val: 0.0256601423 acc_val: 0.3494252874 time: 3.3652s
Epoch: 0099 nll_train: 1325.0872497559 kl_train: -7.5720019341 mse_train: 0.0069741434 acc_train: 0.2350574713 nll_val: 4878.6166992188 kl_val: -5.2202987671 mse_val: 0.0256769322 acc_val: 0.3494252874 time: 3.3702s
Epoch: 0100 nll_train: 1326.7796630859 kl_train: -7.5861649513 mse_train: 0.0069830506 acc_train: 0.2356321839 nll_val: 4877.8652343750 kl_val: -5.1949706078 mse_val: 0.0256729759 acc_val: 0.3471264368 time: 3.2004s
Epoch: 0101 nll_train: 1324.0000000000 kl_train: -7.6005532742 mse_train: 0.0069684208 acc_train: 0.2344827586 nll_val: 4876.4355468750 kl_val: -5.1703672409 mse_val: 0.0256654490 acc_val: 0.3471264368 time: 3.4509s
Epoch: 0102 nll_train: 1322.3102111816 kl_train: -7.6152429581 mse_train: 0.0069595273 acc_train: 0.2333333333 nll_val: 4880.5283203125 kl_val: -5.1461005211 mse_val: 0.0256869886 acc_val: 0.3448275862 time: 6.3532s
Epoch: 0103 nll_train: 1323.6092834473 kl_train: -7.6301651001 mse_train: 0.0069663642 acc_train: 0.2339080460 nll_val: 4880.3833007812 kl_val: -5.1269774437 mse_val: 0.0256862268 acc_val: 0.3425287356 time: 5.6271s
Epoch: 0104 nll_train: 1322.7837524414 kl_train: -7.6451034546 mse_train: 0.0069620195 acc_train: 0.2344827586 nll_val: 4872.2895507812 kl_val: -5.1083903313 mse_val: 0.0256436300 acc_val: 0.3413793103 time: 3.5934s
Epoch: 0105 nll_train: 1320.6123657227 kl_train: -7.6601872444 mse_train: 0.0069505918 acc_train: 0.2344827586 nll_val: 4879.2172851562 kl_val: -5.0922927856 mse_val: 0.0256800894 acc_val: 0.3413793103 time: 3.5339s
Epoch: 0106 nll_train: 1321.6081848145 kl_train: -7.6752841473 mse_train: 0.0069558314 acc_train: 0.2321839080 nll_val: 4870.0419921875 kl_val: -5.0784935951 mse_val: 0.0256317984 acc_val: 0.3390804598 time: 3.4859s
Epoch: 0107 nll_train: 1321.0039062500 kl_train: -7.6904015541 mse_train: 0.0069526513 acc_train: 0.2298850575 nll_val: 4874.5937500000 kl_val: -5.0632629395 mse_val: 0.0256557576 acc_val: 0.3390804598 time: 3.4023s
Epoch: 0108 nll_train: 1318.8509216309 kl_train: -7.7056746483 mse_train: 0.0069413205 acc_train: 0.2293103448 nll_val: 4878.8774414062 kl_val: -5.0502581596 mse_val: 0.0256782975 acc_val: 0.3379310345 time: 3.5726s
Epoch: 0109 nll_train: 1318.8737487793 kl_train: -7.7209610939 mse_train: 0.0069414400 acc_train: 0.2293103448 nll_val: 4878.7280273438 kl_val: -5.0385856628 mse_val: 0.0256775171 acc_val: 0.3379310345 time: 3.4153s
Epoch: 0110 nll_train: 1319.6448974609 kl_train: -7.7361476421 mse_train: 0.0069454988 acc_train: 0.2270114943 nll_val: 4865.9101562500 kl_val: -5.0290932655 mse_val: 0.0256100520 acc_val: 0.3379310345 time: 3.4575s
Epoch: 0111 nll_train: 1318.5090942383 kl_train: -7.7513298988 mse_train: 0.0069395214 acc_train: 0.2247126437 nll_val: 4868.2832031250 kl_val: -5.0143504143 mse_val: 0.0256225467 acc_val: 0.3390804598 time: 3.4910s
Epoch: 0112 nll_train: 1316.0335693359 kl_train: -7.7667667866 mse_train: 0.0069264920 acc_train: 0.2247126437 nll_val: 4877.7421875000 kl_val: -5.0000224113 mse_val: 0.0256723221 acc_val: 0.3379310345 time: 3.5613s
Epoch: 0113 nll_train: 1317.5021362305 kl_train: -7.7821779251 mse_train: 0.0069342216 acc_train: 0.2212643678 nll_val: 4867.5703125000 kl_val: -4.9866976738 mse_val: 0.0256187879 acc_val: 0.3367816092 time: 3.5797s
Epoch: 0114 nll_train: 1317.5567321777 kl_train: -7.7975182533 mse_train: 0.0069345079 acc_train: 0.2201149425 nll_val: 4863.6318359375 kl_val: -4.9745020866 mse_val: 0.0255980622 acc_val: 0.3367816092 time: 3.8853s
Epoch: 0115 nll_train: 1316.1037902832 kl_train: -7.8127419949 mse_train: 0.0069268618 acc_train: 0.2178160920 nll_val: 4866.2114257812 kl_val: -4.9624853134 mse_val: 0.0256116390 acc_val: 0.3379310345 time: 3.8125s
Epoch: 0116 nll_train: 1314.7038574219 kl_train: -7.8278982639 mse_train: 0.0069194939 acc_train: 0.2143678161 nll_val: 4864.2124023438 kl_val: -4.9497795105 mse_val: 0.0256011188 acc_val: 0.3379310345 time: 3.9478s
Epoch: 0117 nll_train: 1314.7062683105 kl_train: -7.8429305553 mse_train: 0.0069195065 acc_train: 0.2137931034 nll_val: 4864.4248046875 kl_val: -4.9395394325 mse_val: 0.0256022327 acc_val: 0.3379310345 time: 3.5508s
Epoch: 0118 nll_train: 1315.1074523926 kl_train: -7.8577754498 mse_train: 0.0069216177 acc_train: 0.2143678161 nll_val: 4857.6562500000 kl_val: -4.9281053543 mse_val: 0.0255666114 acc_val: 0.3367816092 time: 3.3512s
Epoch: 0119 nll_train: 1314.4662780762 kl_train: -7.8724803925 mse_train: 0.0069182435 acc_train: 0.2126436782 nll_val: 4860.9487304688 kl_val: -4.9140472412 mse_val: 0.0255839396 acc_val: 0.3356321839 time: 3.2189s
Epoch: 0120 nll_train: 1312.3600769043 kl_train: -7.8870460987 mse_train: 0.0069071577 acc_train: 0.2103448276 nll_val: 4863.4238281250 kl_val: -4.9026598930 mse_val: 0.0255969651 acc_val: 0.3356321839 time: 3.3430s
Epoch: 0121 nll_train: 1312.9725646973 kl_train: -7.9012241364 mse_train: 0.0069103816 acc_train: 0.2086206897 nll_val: 4858.2583007812 kl_val: -4.8927783966 mse_val: 0.0255697798 acc_val: 0.3356321839 time: 3.3512s
Epoch: 0122 nll_train: 1313.1482238770 kl_train: -7.9150569439 mse_train: 0.0069113058 acc_train: 0.2040229885 nll_val: 4861.1586914062 kl_val: -4.8822479248 mse_val: 0.0255850460 acc_val: 0.3379310345 time: 3.3456s
Epoch: 0123 nll_train: 1311.8985290527 kl_train: -7.9286150932 mse_train: 0.0069047288 acc_train: 0.2000000000 nll_val: 4855.8178710938 kl_val: -4.8709282875 mse_val: 0.0255569350 acc_val: 0.3390804598 time: 3.3213s
Best model so far, saving...
Epoch: 0124 nll_train: 1310.0034179688 kl_train: -7.9419496059 mse_train: 0.0068947546 acc_train: 0.2000000000 nll_val: 4857.9702148438 kl_val: -4.8579111099 mse_val: 0.0255682655 acc_val: 0.3390804598 time: 3.4687s
Epoch: 0125 nll_train: 1311.2167968750 kl_train: -7.9549241066 mse_train: 0.0069011407 acc_train: 0.1988505747 nll_val: 4855.7475585938 kl_val: -4.8468041420 mse_val: 0.0255565625 acc_val: 0.3390804598 time: 3.4731s
Best model so far, saving...
Epoch: 0126 nll_train: 1310.9261474609 kl_train: -7.9674401283 mse_train: 0.0068996111 acc_train: 0.1942528736 nll_val: 4855.2402343750 kl_val: -4.8327240944 mse_val: 0.0255538952 acc_val: 0.3390804598 time: 3.3845s
Best model so far, saving...
Epoch: 0127 nll_train: 1308.8132629395 kl_train: -7.9795663357 mse_train: 0.0068884903 acc_train: 0.1936781609 nll_val: 4851.8515625000 kl_val: -4.8138723373 mse_val: 0.0255360603 acc_val: 0.3379310345 time: 3.3397s
Best model so far, saving...
Epoch: 0128 nll_train: 1308.7975769043 kl_train: -7.9912331104 mse_train: 0.0068884082 acc_train: 0.1902298851 nll_val: 4855.5507812500 kl_val: -4.7991390228 mse_val: 0.0255555268 acc_val: 0.3379310345 time: 3.5148s
Epoch: 0129 nll_train: 1308.7628479004 kl_train: -8.0022230148 mse_train: 0.0068882252 acc_train: 0.1879310345 nll_val: 4849.0878906250 kl_val: -4.7871756554 mse_val: 0.0255215149 acc_val: 0.3379310345 time: 3.8400s
Best model so far, saving...
Epoch: 0130 nll_train: 1307.6119689941 kl_train: -8.0125188828 mse_train: 0.0068821682 acc_train: 0.1862068966 nll_val: 4846.8911132812 kl_val: -4.7757205963 mse_val: 0.0255099498 acc_val: 0.3379310345 time: 4.0066s
Best model so far, saving...
Epoch: 0131 nll_train: 1307.4332885742 kl_train: -8.0221333504 mse_train: 0.0068812271 acc_train: 0.1856321839 nll_val: 4857.1997070312 kl_val: -4.7603392601 mse_val: 0.0255642049 acc_val: 0.3379310345 time: 3.9173s
Epoch: 0132 nll_train: 1305.6594543457 kl_train: -8.0310845375 mse_train: 0.0068718910 acc_train: 0.1816091954 nll_val: 4848.1713867188 kl_val: -4.7470011711 mse_val: 0.0255166870 acc_val: 0.3379310345 time: 3.8540s
Epoch: 0133 nll_train: 1306.8398437500 kl_train: -8.0392713547 mse_train: 0.0068781046 acc_train: 0.1775862069 nll_val: 4855.5009765625 kl_val: -4.7342467308 mse_val: 0.0255552661 acc_val: 0.3390804598 time: 3.7984s
Epoch: 0134 nll_train: 1304.5233154297 kl_train: -8.0466766357 mse_train: 0.0068659120 acc_train: 0.1752873563 nll_val: 4850.3696289062 kl_val: -4.7263388634 mse_val: 0.0255282633 acc_val: 0.3402298851 time: 3.7983s
Epoch: 0135 nll_train: 1305.1120300293 kl_train: -8.0531892776 mse_train: 0.0068690105 acc_train: 0.1718390805 nll_val: 4842.7587890625 kl_val: -4.7224612236 mse_val: 0.0254882015 acc_val: 0.3413793103 time: 3.7841s
Best model so far, saving...
Epoch: 0136 nll_train: 1308.3022460938 kl_train: -8.0588040352 mse_train: 0.0068858007 acc_train: 0.1689655172 nll_val: 4850.5615234375 kl_val: -4.7141060829 mse_val: 0.0255292691 acc_val: 0.3436781609 time: 3.7850s
Epoch: 0137 nll_train: 1301.2230834961 kl_train: -8.0635690689 mse_train: 0.0068485418 acc_train: 0.1660919540 nll_val: 4872.3730468750 kl_val: -4.7082018852 mse_val: 0.0256440658 acc_val: 0.3448275862 time: 3.8174s
Epoch: 0138 nll_train: 1306.1564331055 kl_train: -8.0671887398 mse_train: 0.0068745071 acc_train: 0.1637931034 nll_val: 4856.7011718750 kl_val: -4.7144131660 mse_val: 0.0255615823 acc_val: 0.3448275862 time: 3.8480s
Epoch: 0139 nll_train: 1306.8142395020 kl_train: -8.0696334839 mse_train: 0.0068779695 acc_train: 0.1603448276 nll_val: 4853.8940429688 kl_val: -4.7197976112 mse_val: 0.0255468041 acc_val: 0.3448275862 time: 3.8175s
Epoch: 0140 nll_train: 1302.0019226074 kl_train: -8.0709786415 mse_train: 0.0068526412 acc_train: 0.1597701149 nll_val: 4854.9038085938 kl_val: -4.7202029228 mse_val: 0.0255521219 acc_val: 0.3448275862 time: 5.6202s
Epoch: 0141 nll_train: 1302.6753540039 kl_train: -8.0711369514 mse_train: 0.0068561857 acc_train: 0.1568965517 nll_val: 4844.1079101562 kl_val: -4.7216944695 mse_val: 0.0254953057 acc_val: 0.3448275862 time: 4.1531s
Epoch: 0142 nll_train: 1302.2474670410 kl_train: -8.0702829361 mse_train: 0.0068539337 acc_train: 0.1528735632 nll_val: 4845.0458984375 kl_val: -4.7233915329 mse_val: 0.0255002398 acc_val: 0.3448275862 time: 4.0438s
Epoch: 0143 nll_train: 1301.4346008301 kl_train: -8.0684480667 mse_train: 0.0068496555 acc_train: 0.1471264368 nll_val: 4857.2231445312 kl_val: -4.7278499603 mse_val: 0.0255643316 acc_val: 0.3471264368 time: 4.3103s
Epoch: 0144 nll_train: 1300.4696655273 kl_train: -8.0656490326 mse_train: 0.0068445767 acc_train: 0.1459770115 nll_val: 4853.2441406250 kl_val: -4.7394299507 mse_val: 0.0255433861 acc_val: 0.3494252874 time: 3.9356s
Epoch: 0145 nll_train: 1301.8246459961 kl_train: -8.0620350838 mse_train: 0.0068517086 acc_train: 0.1367816092 nll_val: 4847.5581054688 kl_val: -4.7516064644 mse_val: 0.0255134609 acc_val: 0.3505747126 time: 4.0179s
Epoch: 0146 nll_train: 1299.0455017090 kl_train: -8.0575447083 mse_train: 0.0068370810 acc_train: 0.1350574713 nll_val: 4850.1040039062 kl_val: -4.7637224197 mse_val: 0.0255268607 acc_val: 0.3528735632 time: 4.1878s
Epoch: 0147 nll_train: 1298.5769042969 kl_train: -8.0521354675 mse_train: 0.0068346150 acc_train: 0.1350574713 nll_val: 4852.3032226562 kl_val: -4.7790040970 mse_val: 0.0255384352 acc_val: 0.3551724138 time: 3.8783s
Epoch: 0148 nll_train: 1300.6519775391 kl_train: -8.0458755493 mse_train: 0.0068455362 acc_train: 0.1362068966 nll_val: 4851.0131835938 kl_val: -4.7993354797 mse_val: 0.0255316440 acc_val: 0.3597701149 time: 3.9036s
Epoch: 0149 nll_train: 1297.8904418945 kl_train: -8.0388970375 mse_train: 0.0068310024 acc_train: 0.1344827586 nll_val: 4849.7553710938 kl_val: -4.8179879189 mse_val: 0.0255250260 acc_val: 0.3655172414 time: 3.6744s
Epoch: 0150 nll_train: 1297.0228881836 kl_train: -8.0310776234 mse_train: 0.0068264357 acc_train: 0.1316091954 nll_val: 4849.9980468750 kl_val: -4.8359026909 mse_val: 0.0255263057 acc_val: 0.3666666667 time: 3.8260s
Epoch: 0151 nll_train: 1297.7957458496 kl_train: -8.0224366188 mse_train: 0.0068305043 acc_train: 0.1275862069 nll_val: 4853.0468750000 kl_val: -4.8566703796 mse_val: 0.0255423505 acc_val: 0.3689655172 time: 5.9370s
Epoch: 0152 nll_train: 1296.6554565430 kl_train: -8.0131852627 mse_train: 0.0068245017 acc_train: 0.1258620690 nll_val: 4840.4550781250 kl_val: -4.8791341782 mse_val: 0.0254760757 acc_val: 0.3712643678 time: 3.8510s
Best model so far, saving...
Epoch: 0153 nll_train: 1295.8296813965 kl_train: -8.0033783913 mse_train: 0.0068201558 acc_train: 0.1224137931 nll_val: 4849.2597656250 kl_val: -4.9058866501 mse_val: 0.0255224165 acc_val: 0.3724137931 time: 3.7928s
Epoch: 0154 nll_train: 1296.4572143555 kl_train: -7.9931612015 mse_train: 0.0068234588 acc_train: 0.1235632184 nll_val: 4842.2675781250 kl_val: -4.9325404167 mse_val: 0.0254856143 acc_val: 0.3735632184 time: 4.0856s
Epoch: 0155 nll_train: 1294.3419799805 kl_train: -7.9823825359 mse_train: 0.0068123254 acc_train: 0.1241379310 nll_val: 4846.0771484375 kl_val: -4.9567470551 mse_val: 0.0255056676 acc_val: 0.3770114943 time: 3.8138s
Epoch: 0156 nll_train: 1293.6826477051 kl_train: -7.9710392952 mse_train: 0.0068088555 acc_train: 0.1235632184 nll_val: 4850.3271484375 kl_val: -4.9837841988 mse_val: 0.0255280379 acc_val: 0.3816091954 time: 3.7753s
Epoch: 0157 nll_train: 1294.0198364258 kl_train: -7.9594151974 mse_train: 0.0068106306 acc_train: 0.1212643678 nll_val: 4840.8520507812 kl_val: -5.0126504898 mse_val: 0.0254781693 acc_val: 0.3816091954 time: 3.9508s
Epoch: 0158 nll_train: 1292.8208312988 kl_train: -7.9474771023 mse_train: 0.0068043199 acc_train: 0.1172413793 nll_val: 4842.6152343750 kl_val: -5.0391507149 mse_val: 0.0254874472 acc_val: 0.3827586207 time: 3.7397s
Epoch: 0159 nll_train: 1291.5625305176 kl_train: -7.9350349903 mse_train: 0.0067976970 acc_train: 0.1172413793 nll_val: 4841.3208007812 kl_val: -5.0670170784 mse_val: 0.0254806336 acc_val: 0.3827586207 time: 4.0353s
Epoch: 0160 nll_train: 1292.0093383789 kl_train: -7.9224188328 mse_train: 0.0068000485 acc_train: 0.1172413793 nll_val: 4841.7919921875 kl_val: -5.0970249176 mse_val: 0.0254831146 acc_val: 0.3827586207 time: 4.2105s
Epoch: 0161 nll_train: 1292.1835327148 kl_train: -7.9096393585 mse_train: 0.0068009655 acc_train: 0.1160919540 nll_val: 4844.1372070312 kl_val: -5.1271228790 mse_val: 0.0254954584 acc_val: 0.3827586207 time: 3.8832s
Epoch: 0162 nll_train: 1290.4516601562 kl_train: -7.8966341019 mse_train: 0.0067918506 acc_train: 0.1160919540 nll_val: 4842.3100585938 kl_val: -5.1543335915 mse_val: 0.0254858434 acc_val: 0.3908045977 time: 4.0395s
Epoch: 0163 nll_train: 1289.9000244141 kl_train: -7.8831789494 mse_train: 0.0067889473 acc_train: 0.1143678161 nll_val: 4836.5219726562 kl_val: -5.1839804649 mse_val: 0.0254553761 acc_val: 0.3908045977 time: 4.3742s
Best model so far, saving...
Epoch: 0164 nll_train: 1290.6262207031 kl_train: -7.8696274757 mse_train: 0.0067927697 acc_train: 0.1137931034 nll_val: 4833.2675781250 kl_val: -5.2160558701 mse_val: 0.0254382454 acc_val: 0.3954022989 time: 3.8624s
Best model so far, saving...
Epoch: 0165 nll_train: 1289.3262939453 kl_train: -7.8561663628 mse_train: 0.0067859284 acc_train: 0.1137931034 nll_val: 4836.3178710938 kl_val: -5.2499589920 mse_val: 0.0254543032 acc_val: 0.3988505747 time: 4.2488s
Epoch: 0166 nll_train: 1289.1979064941 kl_train: -7.8428025246 mse_train: 0.0067852524 acc_train: 0.1137931034 nll_val: 4835.0375976562 kl_val: -5.2856979370 mse_val: 0.0254475679 acc_val: 0.4034482759 time: 3.9323s
Epoch: 0167 nll_train: 1288.9407653809 kl_train: -7.8295428753 mse_train: 0.0067838985 acc_train: 0.1132183908 nll_val: 4829.8354492188 kl_val: -5.3204202652 mse_val: 0.0254201889 acc_val: 0.4034482759 time: 3.8446s
Best model so far, saving...
Epoch: 0168 nll_train: 1288.2020568848 kl_train: -7.8163342476 mse_train: 0.0067800103 acc_train: 0.1126436782 nll_val: 4825.8471679688 kl_val: -5.3543996811 mse_val: 0.0253991932 acc_val: 0.4057471264 time: 3.9612s
Best model so far, saving...
Epoch: 0169 nll_train: 1287.1495666504 kl_train: -7.8031299114 mse_train: 0.0067744715 acc_train: 0.1126436782 nll_val: 4828.7778320312 kl_val: -5.3893613815 mse_val: 0.0254146177 acc_val: 0.4080459770 time: 4.6936s
Epoch: 0170 nll_train: 1288.0208435059 kl_train: -7.7901771069 mse_train: 0.0067790571 acc_train: 0.1126436782 nll_val: 4825.2441406250 kl_val: -5.4254064560 mse_val: 0.0253960174 acc_val: 0.4126436782 time: 4.1034s
Best model so far, saving...
Epoch: 0171 nll_train: 1287.7114562988 kl_train: -7.7774260044 mse_train: 0.0067774283 acc_train: 0.1126436782 nll_val: 4830.9189453125 kl_val: -5.4589796066 mse_val: 0.0254258867 acc_val: 0.4183908046 time: 4.0362s
Epoch: 0172 nll_train: 1285.4924316406 kl_train: -7.7646522522 mse_train: 0.0067657491 acc_train: 0.1126436782 nll_val: 4827.3911132812 kl_val: -5.4918107986 mse_val: 0.0254073236 acc_val: 0.4264367816 time: 5.3275s
Epoch: 0173 nll_train: 1286.4637145996 kl_train: -7.7519972324 mse_train: 0.0067708616 acc_train: 0.1109195402 nll_val: 4824.5844726562 kl_val: -5.5274534225 mse_val: 0.0253925491 acc_val: 0.4310344828 time: 4.2004s
Best model so far, saving...
Epoch: 0174 nll_train: 1287.4260253906 kl_train: -7.7397449017 mse_train: 0.0067759255 acc_train: 0.1126436782 nll_val: 4823.3779296875 kl_val: -5.5640645027 mse_val: 0.0253861975 acc_val: 0.4356321839 time: 3.8361s
Best model so far, saving...
Epoch: 0175 nll_train: 1284.8022460938 kl_train: -7.7278692722 mse_train: 0.0067621167 acc_train: 0.1114942529 nll_val: 4825.8979492188 kl_val: -5.5993018150 mse_val: 0.0253994670 acc_val: 0.4367816092 time: 3.9120s
Epoch: 0176 nll_train: 1285.9265441895 kl_train: -7.7160966396 mse_train: 0.0067680338 acc_train: 0.1114942529 nll_val: 4821.6791992188 kl_val: -5.6337966919 mse_val: 0.0253772568 acc_val: 0.4402298851 time: 3.6462s
Best model so far, saving...
Epoch: 0177 nll_train: 1284.5960998535 kl_train: -7.7045531273 mse_train: 0.0067610318 acc_train: 0.1109195402 nll_val: 4815.8657226562 kl_val: -5.6718025208 mse_val: 0.0253466591 acc_val: 0.4436781609 time: 4.0347s
Best model so far, saving...
Epoch: 0178 nll_train: 1283.7129516602 kl_train: -7.6936213970 mse_train: 0.0067563840 acc_train: 0.1114942529 nll_val: 4815.4677734375 kl_val: -5.7130723000 mse_val: 0.0253445636 acc_val: 0.4494252874 time: 3.9948s
Best model so far, saving...
Epoch: 0179 nll_train: 1285.0017700195 kl_train: -7.6833217144 mse_train: 0.0067631665 acc_train: 0.1109195402 nll_val: 4815.8432617188 kl_val: -5.7515501976 mse_val: 0.0253465418 acc_val: 0.4528735632 time: 3.8855s
Epoch: 0180 nll_train: 1283.3692016602 kl_train: -7.6731688976 mse_train: 0.0067545745 acc_train: 0.1114942529 nll_val: 4816.0942382812 kl_val: -5.7871189117 mse_val: 0.0253478661 acc_val: 0.4540229885 time: 3.7521s
Epoch: 0181 nll_train: 1283.8388671875 kl_train: -7.6632409096 mse_train: 0.0067570462 acc_train: 0.1114942529 nll_val: 4814.1713867188 kl_val: -5.8212275505 mse_val: 0.0253377408 acc_val: 0.4597701149 time: 3.8504s
Best model so far, saving...
Epoch: 0182 nll_train: 1283.4111022949 kl_train: -7.6535162926 mse_train: 0.0067547950 acc_train: 0.1114942529 nll_val: 4817.1630859375 kl_val: -5.8554730415 mse_val: 0.0253534894 acc_val: 0.4666666667 time: 3.7891s
Epoch: 0183 nll_train: 1283.4112548828 kl_train: -7.6441712379 mse_train: 0.0067547956 acc_train: 0.1114942529 nll_val: 4812.5903320312 kl_val: -5.8895144463 mse_val: 0.0253294222 acc_val: 0.4689655172 time: 3.7289s
Best model so far, saving...
Epoch: 0184 nll_train: 1283.7449951172 kl_train: -7.6352140903 mse_train: 0.0067565525 acc_train: 0.1126436782 nll_val: 4814.4809570312 kl_val: -5.9218640327 mse_val: 0.0253393725 acc_val: 0.4712643678 time: 3.8580s
Epoch: 0185 nll_train: 1282.6315612793 kl_train: -7.6264712811 mse_train: 0.0067506924 acc_train: 0.1132183908 nll_val: 4815.9042968750 kl_val: -5.9536581039 mse_val: 0.0253468622 acc_val: 0.4758620690 time: 3.9486s
Epoch: 0186 nll_train: 1282.6424255371 kl_train: -7.6180031300 mse_train: 0.0067507493 acc_train: 0.1132183908 nll_val: 4811.4511718750 kl_val: -5.9846115112 mse_val: 0.0253234226 acc_val: 0.4781609195 time: 3.9843s
Best model so far, saving...
Epoch: 0187 nll_train: 1282.7892456055 kl_train: -7.6099169254 mse_train: 0.0067515218 acc_train: 0.1143678161 nll_val: 4815.1572265625 kl_val: -6.0170640945 mse_val: 0.0253429320 acc_val: 0.4816091954 time: 3.8638s
Epoch: 0188 nll_train: 1281.0422058105 kl_train: -7.6023957729 mse_train: 0.0067423276 acc_train: 0.1137931034 nll_val: 4810.2490234375 kl_val: -6.0499119759 mse_val: 0.0253171008 acc_val: 0.4850574713 time: 4.1517s
Best model so far, saving...
Epoch: 0189 nll_train: 1282.4279785156 kl_train: -7.5951676369 mse_train: 0.0067496210 acc_train: 0.1132183908 nll_val: 4813.3876953125 kl_val: -6.0814337730 mse_val: 0.0253336187 acc_val: 0.4885057471 time: 3.6737s
Epoch: 0190 nll_train: 1280.9648742676 kl_train: -7.5882542133 mse_train: 0.0067419203 acc_train: 0.1143678161 nll_val: 4814.2475585938 kl_val: -6.1128997803 mse_val: 0.0253381412 acc_val: 0.4919540230 time: 3.6033s
Epoch: 0191 nll_train: 1281.1369018555 kl_train: -7.5816817284 mse_train: 0.0067428260 acc_train: 0.1155172414 nll_val: 4810.9218750000 kl_val: -6.1451687813 mse_val: 0.0253206417 acc_val: 0.4977011494 time: 3.5101s
Epoch: 0192 nll_train: 1282.5441589355 kl_train: -7.5755326748 mse_train: 0.0067502322 acc_train: 0.1160919540 nll_val: 4810.8178710938 kl_val: -6.1768951416 mse_val: 0.0253200922 acc_val: 0.5011494253 time: 3.6867s
Epoch: 0193 nll_train: 1279.8338012695 kl_train: -7.5697700977 mse_train: 0.0067359672 acc_train: 0.1160919540 nll_val: 4813.8808593750 kl_val: -6.2098832130 mse_val: 0.0253362115 acc_val: 0.5022988506 time: 3.7045s
Epoch: 0194 nll_train: 1280.5165405273 kl_train: -7.5645492077 mse_train: 0.0067395600 acc_train: 0.1160919540 nll_val: 4810.3305664062 kl_val: -6.2417988777 mse_val: 0.0253175292 acc_val: 0.5057471264 time: 3.6213s
Epoch: 0195 nll_train: 1279.8803100586 kl_train: -7.5596549511 mse_train: 0.0067362118 acc_train: 0.1155172414 nll_val: 4809.7377929688 kl_val: -6.2710986137 mse_val: 0.0253144056 acc_val: 0.5103448276 time: 3.6575s
Best model so far, saving...
Epoch: 0196 nll_train: 1279.1903686523 kl_train: -7.5549206734 mse_train: 0.0067325804 acc_train: 0.1155172414 nll_val: 4812.4335937500 kl_val: -6.2991375923 mse_val: 0.0253285971 acc_val: 0.5137931034 time: 3.7488s
Epoch: 0197 nll_train: 1280.5463256836 kl_train: -7.5503027439 mse_train: 0.0067397172 acc_train: 0.1149425287 nll_val: 4811.3862304688 kl_val: -6.3260278702 mse_val: 0.0253230799 acc_val: 0.5183908046 time: 3.5958s
Epoch: 0198 nll_train: 1280.0401000977 kl_train: -7.5458624363 mse_train: 0.0067370533 acc_train: 0.1143678161 nll_val: 4814.5922851562 kl_val: -6.3528218269 mse_val: 0.0253399592 acc_val: 0.5229885057 time: 3.5537s
Epoch: 0199 nll_train: 1264.2579956055 kl_train: -7.5419495106 mse_train: 0.0066539889 acc_train: 0.1137931034 nll_val: 4804.3583984375 kl_val: -6.4190006256 mse_val: 0.0252860971 acc_val: 0.5321839080 time: 3.7885s
Best model so far, saving...
Optimization finished
Best epoch 199
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 325.3930358887 kl_test: -5.9224877357 mse_test: 0.0017125948 acc_test: 0.0459770115
MSE: [ 0.000840603490 , 0.000884742767 , 0.000860980770 , 0.000819831272 , 0.000833329745 , 0.000781352806 , 0.001253308961 , 0.001383080031 , 0.001376029570 , 0.001627415651 , 0.001659512403 , 0.001619688468 , 0.001821199199 , 0.001877963194 , 0.002121333033 , 0.002055254765 , 0.002106901957 , 0.002273747697 , 0.002201162744 ]
Accuracy for experiment id 10 is 0.044444444444444446
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 128758.4003906250 kl_train: -12.2242345810 mse_train: 0.6776757911 acc_train: 0.5419540230 nll_val: 10027.9648437500 kl_val: -5.1614117622 mse_val: 0.0527787618 acc_val: 0.0000000000 time: 3.5196s
Best model so far, saving...
Epoch: 0001 nll_train: 64193.5410156250 kl_train: -5.5059049129 mse_train: 0.3378607407 acc_train: 0.3212643678 nll_val: 8499.2773437500 kl_val: -4.2689762115 mse_val: 0.0447330400 acc_val: 0.0000000000 time: 3.5350s
Best model so far, saving...
Epoch: 0002 nll_train: 59247.5765380859 kl_train: -3.9199092388 mse_train: 0.3118293248 acc_train: 0.2419540230 nll_val: 7183.1064453125 kl_val: -12.3106155396 mse_val: 0.0378058180 acc_val: 0.0000000000 time: 3.5902s
Best model so far, saving...
Epoch: 0003 nll_train: 6769.0728759766 kl_train: -4.8028565645 mse_train: 0.0356266983 acc_train: 0.1431034483 nll_val: 65649.5078125000 kl_val: -16.9153728485 mse_val: 0.3455237150 acc_val: 1.0000000000 time: 3.7150s
Epoch: 0004 nll_train: 23380.8007812500 kl_train: -6.4388861656 mse_train: 0.1230568290 acc_train: 0.1074712644 nll_val: 103665.0937500000 kl_val: -16.0414848328 mse_val: 0.5456057191 acc_val: 1.0000000000 time: 3.7906s
Epoch: 0005 nll_train: 23421.1220703125 kl_train: -6.2176530361 mse_train: 0.1232690588 acc_train: 0.0637931034 nll_val: 18296.5644531250 kl_val: -18.6912212372 mse_val: 0.0962976962 acc_val: 0.0000000000 time: 4.6737s
Epoch: 0006 nll_train: 8351.6835937500 kl_train: -4.6162476540 mse_train: 0.0439562304 acc_train: 0.0729885057 nll_val: 8181.5810546875 kl_val: -5.6535520554 mse_val: 0.0430609509 acc_val: 0.0000000000 time: 3.8056s
Epoch: 0007 nll_train: 2254.5170898438 kl_train: -3.4076192379 mse_train: 0.0118658780 acc_train: 0.0948275862 nll_val: 8644.0761718750 kl_val: -1.3430657387 mse_val: 0.0454951376 acc_val: 0.0000000000 time: 3.3841s
Epoch: 0008 nll_train: 9486.2824707031 kl_train: -3.0106574297 mse_train: 0.0499277990 acc_train: 0.1114942529 nll_val: 8815.6279296875 kl_val: -0.6365145445 mse_val: 0.0463980362 acc_val: 0.0000000000 time: 3.2222s
Epoch: 0009 nll_train: 4758.4678344727 kl_train: -2.8480572701 mse_train: 0.0250445660 acc_train: 0.1114942529 nll_val: 8719.3076171875 kl_val: -0.5317211151 mse_val: 0.0458910912 acc_val: 0.0000000000 time: 3.2584s
Epoch: 0010 nll_train: 2220.6835937500 kl_train: -2.7528619766 mse_train: 0.0116878077 acc_train: 0.1137931034 nll_val: 8558.5800781250 kl_val: -0.5265764594 mse_val: 0.0450451523 acc_val: 0.0000000000 time: 3.3500s
Epoch: 0011 nll_train: 3578.2279052734 kl_train: -2.6876543760 mse_train: 0.0188327786 acc_train: 0.1132183908 nll_val: 8288.3076171875 kl_val: -0.5359656215 mse_val: 0.0436226688 acc_val: 0.0000000000 time: 3.3837s
Epoch: 0012 nll_train: 3892.3708496094 kl_train: -2.6415377855 mse_train: 0.0204861611 acc_train: 0.1086206897 nll_val: 8147.0888671875 kl_val: -0.6358146667 mse_val: 0.0428794175 acc_val: 0.0000000000 time: 3.2958s
Epoch: 0013 nll_train: 2601.3504638672 kl_train: -2.6088531017 mse_train: 0.0136913182 acc_train: 0.1040229885 nll_val: 7560.3159179688 kl_val: -0.8009591103 mse_val: 0.0397911370 acc_val: 0.0333333333 time: 3.2431s
Epoch: 0014 nll_train: 1589.3697204590 kl_train: -2.5854173899 mse_train: 0.0083651039 acc_train: 0.0982758621 nll_val: 7199.2685546875 kl_val: -0.9176774025 mse_val: 0.0378908813 acc_val: 0.0344827586 time: 3.2352s
Epoch: 0015 nll_train: 1662.3553771973 kl_train: -2.5724519491 mse_train: 0.0087492375 acc_train: 0.0925287356 nll_val: 7200.3593750000 kl_val: -1.0069186687 mse_val: 0.0378966257 acc_val: 0.0367816092 time: 3.2549s
Epoch: 0016 nll_train: 2119.1898193359 kl_train: -2.5716705322 mse_train: 0.0111536302 acc_train: 0.0862068966 nll_val: 7087.6025390625 kl_val: -1.1248310804 mse_val: 0.0373031721 acc_val: 0.0494252874 time: 3.2388s
Best model so far, saving...
Epoch: 0017 nll_train: 1940.7848510742 kl_train: -2.5837060213 mse_train: 0.0102146579 acc_train: 0.0821839080 nll_val: 6707.8408203125 kl_val: -1.2151988745 mse_val: 0.0353044234 acc_val: 0.0666666667 time: 3.1956s
Best model so far, saving...
Epoch: 0018 nll_train: 1551.6693420410 kl_train: -2.6062386036 mse_train: 0.0081666800 acc_train: 0.0787356322 nll_val: 6404.8955078125 kl_val: -1.2812434435 mse_val: 0.0337099731 acc_val: 0.0666666667 time: 3.2768s
Best model so far, saving...
Epoch: 0019 nll_train: 1548.6095275879 kl_train: -2.6337354183 mse_train: 0.0081505757 acc_train: 0.0764367816 nll_val: 6136.7817382812 kl_val: -1.4286079407 mse_val: 0.0322988518 acc_val: 0.0689655172 time: 3.3992s
Best model so far, saving...
Epoch: 0020 nll_train: 1636.6752319336 kl_train: -2.6606576443 mse_train: 0.0086140793 acc_train: 0.0758620690 nll_val: 6047.9570312500 kl_val: -1.6094108820 mse_val: 0.0318313502 acc_val: 0.0988505747 time: 3.4339s
Best model so far, saving...
Epoch: 0021 nll_train: 1465.2376403809 kl_train: -2.6831649542 mse_train: 0.0077117765 acc_train: 0.0758620690 nll_val: 6026.8769531250 kl_val: -1.7547497749 mse_val: 0.0317204036 acc_val: 0.1011494253 time: 3.2771s
Best model so far, saving...
Epoch: 0022 nll_train: 1342.8428649902 kl_train: -2.7011277676 mse_train: 0.0070675936 acc_train: 0.0758620690 nll_val: 6051.9941406250 kl_val: -1.9119828939 mse_val: 0.0318525992 acc_val: 0.1034482759 time: 3.0889s
Epoch: 0023 nll_train: 1412.5604553223 kl_train: -2.7171117067 mse_train: 0.0074345282 acc_train: 0.0747126437 nll_val: 6066.1611328125 kl_val: -2.0677628517 mse_val: 0.0319271609 acc_val: 0.1172413793 time: 3.3746s
Epoch: 0024 nll_train: 1462.8011474609 kl_train: -2.7335931063 mse_train: 0.0076989529 acc_train: 0.0729885057 nll_val: 5885.1933593750 kl_val: -2.1702191830 mse_val: 0.0309747010 acc_val: 0.1379310345 time: 3.9241s
Best model so far, saving...
Epoch: 0025 nll_train: 1415.0405273438 kl_train: -2.7512211800 mse_train: 0.0074475812 acc_train: 0.0712643678 nll_val: 5645.6367187500 kl_val: -2.2726054192 mse_val: 0.0297138765 acc_val: 0.1735632184 time: 3.8729s
Best model so far, saving...
Epoch: 0026 nll_train: 1385.8451538086 kl_train: -2.7684876919 mse_train: 0.0072939226 acc_train: 0.0695402299 nll_val: 5487.3730468750 kl_val: -2.3641805649 mse_val: 0.0288809072 acc_val: 0.2011494253 time: 3.8732s
Best model so far, saving...
Epoch: 0027 nll_train: 1392.5639648438 kl_train: -2.7831063271 mse_train: 0.0073292841 acc_train: 0.0695402299 nll_val: 5394.3095703125 kl_val: -2.4346995354 mse_val: 0.0283911042 acc_val: 0.2137931034 time: 3.8332s
Best model so far, saving...
Epoch: 0028 nll_train: 1366.9145202637 kl_train: -2.7943874598 mse_train: 0.0071942866 acc_train: 0.0683908046 nll_val: 5343.0502929688 kl_val: -2.5240285397 mse_val: 0.0281213168 acc_val: 0.2310344828 time: 3.8316s
Best model so far, saving...
Epoch: 0029 nll_train: 1328.8364562988 kl_train: -2.8034107685 mse_train: 0.0069938764 acc_train: 0.0678160920 nll_val: 5343.3120117188 kl_val: -2.6362514496 mse_val: 0.0281226933 acc_val: 0.2494252874 time: 3.8061s
Epoch: 0030 nll_train: 1329.0771179199 kl_train: -2.8122437000 mse_train: 0.0069951423 acc_train: 0.0666666667 nll_val: 5362.1767578125 kl_val: -2.7605478764 mse_val: 0.0282219853 acc_val: 0.2586206897 time: 3.8015s
Epoch: 0031 nll_train: 1350.5564880371 kl_train: -2.8221526146 mse_train: 0.0071081927 acc_train: 0.0666666667 nll_val: 5300.1713867188 kl_val: -2.9085125923 mse_val: 0.0278956350 acc_val: 0.2609195402 time: 3.8086s
Best model so far, saving...
Epoch: 0032 nll_train: 1350.7403564453 kl_train: -2.8329269886 mse_train: 0.0071091595 acc_train: 0.0660919540 nll_val: 5201.0502929688 kl_val: -3.0771579742 mse_val: 0.0273739453 acc_val: 0.2712643678 time: 3.7953s
Best model so far, saving...
Epoch: 0033 nll_train: 1337.0524597168 kl_train: -2.8433637619 mse_train: 0.0070371180 acc_train: 0.0655172414 nll_val: 5132.4814453125 kl_val: -3.2473497391 mse_val: 0.0270130597 acc_val: 0.2885057471 time: 3.8452s
Best model so far, saving...
Epoch: 0034 nll_train: 1329.0238037109 kl_train: -2.8526076078 mse_train: 0.0069948615 acc_train: 0.0643678161 nll_val: 5087.2392578125 kl_val: -3.4038610458 mse_val: 0.0267749391 acc_val: 0.3172413793 time: 3.7870s
Best model so far, saving...
Epoch: 0035 nll_train: 1321.6693420410 kl_train: -2.8609116077 mse_train: 0.0069561538 acc_train: 0.0643678161 nll_val: 5057.4550781250 kl_val: -3.5415151119 mse_val: 0.0266181808 acc_val: 0.3379310345 time: 3.7925s
Best model so far, saving...
Epoch: 0036 nll_train: 1313.8051757812 kl_train: -2.8687795401 mse_train: 0.0069147641 acc_train: 0.0637931034 nll_val: 5019.9907226562 kl_val: -3.6584510803 mse_val: 0.0264210030 acc_val: 0.3528735632 time: 3.7945s
Best model so far, saving...
Epoch: 0037 nll_train: 1312.9222412109 kl_train: -2.8768253326 mse_train: 0.0069101171 acc_train: 0.0632183908 nll_val: 4978.4160156250 kl_val: -3.7575764656 mse_val: 0.0262021888 acc_val: 0.3701149425 time: 3.7699s
Best model so far, saving...
Epoch: 0038 nll_train: 1315.5686035156 kl_train: -2.8851191998 mse_train: 0.0069240449 acc_train: 0.0626436782 nll_val: 4939.5380859375 kl_val: -3.8382256031 mse_val: 0.0259975661 acc_val: 0.3816091954 time: 3.7963s
Best model so far, saving...
Epoch: 0039 nll_train: 1317.7190551758 kl_train: -2.8934949636 mse_train: 0.0069353627 acc_train: 0.0626436782 nll_val: 4916.7480468750 kl_val: -3.9019463062 mse_val: 0.0258776173 acc_val: 0.4022988506 time: 3.8080s
Best model so far, saving...
Epoch: 0040 nll_train: 1308.8223876953 kl_train: -2.9016529322 mse_train: 0.0068885382 acc_train: 0.0620689655 nll_val: 4894.9990234375 kl_val: -3.9509723186 mse_val: 0.0257631522 acc_val: 0.4252873563 time: 3.8231s
Best model so far, saving...
Epoch: 0041 nll_train: 1312.7293701172 kl_train: -2.9103615284 mse_train: 0.0069091020 acc_train: 0.0620689655 nll_val: 4868.5800781250 kl_val: -3.9927933216 mse_val: 0.0256241020 acc_val: 0.4425287356 time: 3.8128s
Best model so far, saving...
Epoch: 0042 nll_train: 1312.5937194824 kl_train: -2.9201076031 mse_train: 0.0069083883 acc_train: 0.0620689655 nll_val: 4845.4086914062 kl_val: -4.0235795975 mse_val: 0.0255021509 acc_val: 0.4574712644 time: 3.8114s
Best model so far, saving...
Epoch: 0043 nll_train: 1309.1613464355 kl_train: -2.9298349619 mse_train: 0.0068903227 acc_train: 0.0614942529 nll_val: 4818.2993164062 kl_val: -4.0485668182 mse_val: 0.0253594704 acc_val: 0.4689655172 time: 3.7856s
Best model so far, saving...
Epoch: 0044 nll_train: 1302.1007690430 kl_train: -2.9396247864 mse_train: 0.0068531616 acc_train: 0.0609195402 nll_val: 4798.0493164062 kl_val: -4.0688481331 mse_val: 0.0252528898 acc_val: 0.4781609195 time: 3.7665s
Best model so far, saving...
Epoch: 0045 nll_train: 1299.6629028320 kl_train: -2.9494349957 mse_train: 0.0068403307 acc_train: 0.0609195402 nll_val: 4782.8491210938 kl_val: -4.0846347809 mse_val: 0.0251728874 acc_val: 0.4839080460 time: 3.8008s
Best model so far, saving...
Epoch: 0046 nll_train: 1301.0585327148 kl_train: -2.9591956139 mse_train: 0.0068476766 acc_train: 0.0609195402 nll_val: 4762.5830078125 kl_val: -4.0984907150 mse_val: 0.0250662249 acc_val: 0.4908045977 time: 3.8014s
Best model so far, saving...
Epoch: 0047 nll_train: 1302.0934143066 kl_train: -2.9690070152 mse_train: 0.0068531233 acc_train: 0.0609195402 nll_val: 4755.4248046875 kl_val: -4.1072478294 mse_val: 0.0250285491 acc_val: 0.4942528736 time: 3.7901s
Best model so far, saving...
Epoch: 0048 nll_train: 1300.2177124023 kl_train: -2.9780285358 mse_train: 0.0068432513 acc_train: 0.0609195402 nll_val: 4748.0000000000 kl_val: -4.1116085052 mse_val: 0.0249894690 acc_val: 0.4988505747 time: 3.8045s
Best model so far, saving...
Epoch: 0049 nll_train: 1298.3465881348 kl_train: -2.9861259460 mse_train: 0.0068334030 acc_train: 0.0603448276 nll_val: 4744.3134765625 kl_val: -4.1113100052 mse_val: 0.0249700677 acc_val: 0.5045977011 time: 3.8077s
Best model so far, saving...
Epoch: 0050 nll_train: 1297.9484558105 kl_train: -2.9936803579 mse_train: 0.0068313074 acc_train: 0.0603448276 nll_val: 4736.1176757812 kl_val: -4.1099739075 mse_val: 0.0249269381 acc_val: 0.5057471264 time: 3.8279s
Best model so far, saving...
Epoch: 0051 nll_train: 1297.1272277832 kl_train: -3.0014038086 mse_train: 0.0068269849 acc_train: 0.0609195402 nll_val: 4727.8706054688 kl_val: -4.1096425056 mse_val: 0.0248835254 acc_val: 0.5091954023 time: 3.8114s
Best model so far, saving...
Epoch: 0052 nll_train: 1295.6160888672 kl_train: -3.0094269514 mse_train: 0.0068190313 acc_train: 0.0614942529 nll_val: 4722.3344726562 kl_val: -4.1100707054 mse_val: 0.0248543881 acc_val: 0.5091954023 time: 3.8118s
Best model so far, saving...
Epoch: 0053 nll_train: 1292.7183227539 kl_train: -3.0177644491 mse_train: 0.0068037810 acc_train: 0.0614942529 nll_val: 4723.7124023438 kl_val: -4.1107273102 mse_val: 0.0248616412 acc_val: 0.5091954023 time: 3.8110s
Epoch: 0054 nll_train: 1293.2995910645 kl_train: -3.0265053511 mse_train: 0.0068068398 acc_train: 0.0620689655 nll_val: 4723.6499023438 kl_val: -4.1124148369 mse_val: 0.0248613209 acc_val: 0.5091954023 time: 3.7919s
Epoch: 0055 nll_train: 1292.7960815430 kl_train: -3.0357638597 mse_train: 0.0068041894 acc_train: 0.0620689655 nll_val: 4733.3320312500 kl_val: -4.1153235435 mse_val: 0.0249122735 acc_val: 0.5103448276 time: 3.8093s
Epoch: 0056 nll_train: 1292.2404174805 kl_train: -3.0454487801 mse_train: 0.0068012648 acc_train: 0.0620689655 nll_val: 4738.0878906250 kl_val: -4.1187648773 mse_val: 0.0249373019 acc_val: 0.5091954023 time: 3.7890s
Epoch: 0057 nll_train: 1289.8562011719 kl_train: -3.0549352169 mse_train: 0.0067887161 acc_train: 0.0620689655 nll_val: 4737.5141601562 kl_val: -4.1211600304 mse_val: 0.0249342807 acc_val: 0.5080459770 time: 3.7795s
Epoch: 0058 nll_train: 1289.1985473633 kl_train: -3.0640704632 mse_train: 0.0067852553 acc_train: 0.0614942529 nll_val: 4737.7905273438 kl_val: -4.1216239929 mse_val: 0.0249357373 acc_val: 0.5080459770 time: 3.8164s
Epoch: 0059 nll_train: 1288.7722778320 kl_train: -3.0732827187 mse_train: 0.0067830118 acc_train: 0.0609195402 nll_val: 4741.1000976562 kl_val: -4.1218605042 mse_val: 0.0249531586 acc_val: 0.5080459770 time: 3.7647s
Epoch: 0060 nll_train: 1287.0586853027 kl_train: -3.0826370716 mse_train: 0.0067739928 acc_train: 0.0609195402 nll_val: 4736.1240234375 kl_val: -4.1214079857 mse_val: 0.0249269679 acc_val: 0.5080459770 time: 3.8135s
Epoch: 0061 nll_train: 1286.6758117676 kl_train: -3.0921325684 mse_train: 0.0067719778 acc_train: 0.0609195402 nll_val: 4742.0781250000 kl_val: -4.1225066185 mse_val: 0.0249583051 acc_val: 0.5080459770 time: 5.0987s
Epoch: 0062 nll_train: 1287.3154907227 kl_train: -3.1014307737 mse_train: 0.0067753447 acc_train: 0.0609195402 nll_val: 4745.0307617188 kl_val: -4.1228728294 mse_val: 0.0249738432 acc_val: 0.5091954023 time: 5.2291s
Epoch: 0063 nll_train: 1288.1003112793 kl_train: -3.1107311249 mse_train: 0.0067794749 acc_train: 0.0620689655 nll_val: 4746.7851562500 kl_val: -4.1235380173 mse_val: 0.0249830801 acc_val: 0.5091954023 time: 3.5076s
Epoch: 0064 nll_train: 1286.1137084961 kl_train: -3.1196641922 mse_train: 0.0067690193 acc_train: 0.0626436782 nll_val: 4746.3271484375 kl_val: -4.1237936020 mse_val: 0.0249806680 acc_val: 0.5091954023 time: 3.4357s
Epoch: 0065 nll_train: 1284.6566467285 kl_train: -3.1284554005 mse_train: 0.0067613503 acc_train: 0.0632183908 nll_val: 4745.3193359375 kl_val: -4.1256189346 mse_val: 0.0249753650 acc_val: 0.5080459770 time: 3.5818s
Epoch: 0066 nll_train: 1284.1473999023 kl_train: -3.1372895241 mse_train: 0.0067586702 acc_train: 0.0637931034 nll_val: 4742.9736328125 kl_val: -4.1259446144 mse_val: 0.0249630176 acc_val: 0.5068965517 time: 3.6659s
Epoch: 0067 nll_train: 1283.9609985352 kl_train: -3.1458277702 mse_train: 0.0067576888 acc_train: 0.0632183908 nll_val: 4742.8515625000 kl_val: -4.1259841919 mse_val: 0.0249623749 acc_val: 0.5057471264 time: 3.6122s
Epoch: 0068 nll_train: 1283.3374633789 kl_train: -3.1541906595 mse_train: 0.0067544072 acc_train: 0.0637931034 nll_val: 4743.4360351562 kl_val: -4.1274862289 mse_val: 0.0249654483 acc_val: 0.5045977011 time: 3.7478s
Epoch: 0069 nll_train: 1283.4217224121 kl_train: -3.1624282598 mse_train: 0.0067548505 acc_train: 0.0643678161 nll_val: 4742.7832031250 kl_val: -4.1281514168 mse_val: 0.0249620136 acc_val: 0.5034482759 time: 3.5686s
Epoch: 0070 nll_train: 1283.0290527344 kl_train: -3.1708343029 mse_train: 0.0067527844 acc_train: 0.0637931034 nll_val: 4740.4985351562 kl_val: -4.1277122498 mse_val: 0.0249499902 acc_val: 0.5034482759 time: 3.4034s
Epoch: 0071 nll_train: 1282.4607543945 kl_train: -3.1795395613 mse_train: 0.0067497927 acc_train: 0.0632183908 nll_val: 4738.7324218750 kl_val: -4.1299066544 mse_val: 0.0249406938 acc_val: 0.5011494253 time: 3.6138s
Epoch: 0072 nll_train: 1281.4911193848 kl_train: -3.1887274981 mse_train: 0.0067446898 acc_train: 0.0626436782 nll_val: 4738.4169921875 kl_val: -4.1338930130 mse_val: 0.0249390379 acc_val: 0.5022988506 time: 3.5864s
Epoch: 0073 nll_train: 1280.9932250977 kl_train: -3.1977680922 mse_train: 0.0067420693 acc_train: 0.0620689655 nll_val: 4735.0463867188 kl_val: -4.1392254829 mse_val: 0.0249212962 acc_val: 0.5022988506 time: 3.1075s
Epoch: 0074 nll_train: 1280.9467773438 kl_train: -3.2069573402 mse_train: 0.0067418249 acc_train: 0.0626436782 nll_val: 4735.0166015625 kl_val: -4.1462755203 mse_val: 0.0249211434 acc_val: 0.5034482759 time: 3.1204s
Epoch: 0075 nll_train: 1281.3149108887 kl_train: -3.2166228294 mse_train: 0.0067437626 acc_train: 0.0632183908 nll_val: 4736.8291015625 kl_val: -4.1523199081 mse_val: 0.0249306802 acc_val: 0.5022988506 time: 3.3058s
Epoch: 0076 nll_train: 1279.7883605957 kl_train: -3.2267363071 mse_train: 0.0067357278 acc_train: 0.0626436782 nll_val: 4735.4692382812 kl_val: -4.1580324173 mse_val: 0.0249235202 acc_val: 0.5022988506 time: 3.2570s
Epoch: 0077 nll_train: 1279.7647399902 kl_train: -3.2371150255 mse_train: 0.0067356040 acc_train: 0.0614942529 nll_val: 4736.5737304688 kl_val: -4.1647558212 mse_val: 0.0249293316 acc_val: 0.5011494253 time: 3.2559s
Epoch: 0078 nll_train: 1279.7368774414 kl_train: -3.2479751110 mse_train: 0.0067354571 acc_train: 0.0614942529 nll_val: 4737.1074218750 kl_val: -4.1714801788 mse_val: 0.0249321442 acc_val: 0.5011494253 time: 3.2564s
Epoch: 0079 nll_train: 1278.7672424316 kl_train: -3.2592149973 mse_train: 0.0067303535 acc_train: 0.0609195402 nll_val: 4741.7343750000 kl_val: -4.1781582832 mse_val: 0.0249564946 acc_val: 0.5011494253 time: 3.2681s
Epoch: 0080 nll_train: 1278.5536193848 kl_train: -3.2704389095 mse_train: 0.0067292291 acc_train: 0.0609195402 nll_val: 4744.7885742188 kl_val: -4.1837024689 mse_val: 0.0249725729 acc_val: 0.5011494253 time: 3.2908s
Epoch: 0081 nll_train: 1277.7754516602 kl_train: -3.2815558910 mse_train: 0.0067251336 acc_train: 0.0609195402 nll_val: 4746.3764648438 kl_val: -4.1888957024 mse_val: 0.0249809269 acc_val: 0.5011494253 time: 3.3133s
Epoch: 0082 nll_train: 1278.8621826172 kl_train: -3.2925277948 mse_train: 0.0067308530 acc_train: 0.0609195402 nll_val: 4744.9511718750 kl_val: -4.1951713562 mse_val: 0.0249734260 acc_val: 0.5022988506 time: 3.2593s
Epoch: 0083 nll_train: 1277.9212036133 kl_train: -3.3037811518 mse_train: 0.0067259013 acc_train: 0.0609195402 nll_val: 4746.3115234375 kl_val: -4.2018976212 mse_val: 0.0249805842 acc_val: 0.5022988506 time: 3.2961s
Epoch: 0084 nll_train: 1277.2541503906 kl_train: -3.3152261972 mse_train: 0.0067223902 acc_train: 0.0609195402 nll_val: 4748.5375976562 kl_val: -4.2085909843 mse_val: 0.0249923002 acc_val: 0.5000000000 time: 3.2421s
Epoch: 0085 nll_train: 1277.2299499512 kl_train: -3.3269817829 mse_train: 0.0067222625 acc_train: 0.0603448276 nll_val: 4746.4086914062 kl_val: -4.2178387642 mse_val: 0.0249811001 acc_val: 0.5011494253 time: 3.2952s
Epoch: 0086 nll_train: 1277.1171264648 kl_train: -3.3395079374 mse_train: 0.0067216686 acc_train: 0.0597701149 nll_val: 4747.7187500000 kl_val: -4.2268009186 mse_val: 0.0249879956 acc_val: 0.5011494253 time: 3.1751s
Epoch: 0087 nll_train: 1275.6062011719 kl_train: -3.3520386219 mse_train: 0.0067137169 acc_train: 0.0591954023 nll_val: 4751.4047851562 kl_val: -4.2378163338 mse_val: 0.0250073969 acc_val: 0.5011494253 time: 3.0549s
Epoch: 0088 nll_train: 1276.2233276367 kl_train: -3.3654650450 mse_train: 0.0067169649 acc_train: 0.0591954023 nll_val: 4748.7617187500 kl_val: -4.2499570847 mse_val: 0.0249934848 acc_val: 0.5011494253 time: 3.8983s
Epoch: 0089 nll_train: 1275.1593627930 kl_train: -3.3794022799 mse_train: 0.0067113654 acc_train: 0.0591954023 nll_val: 4750.8999023438 kl_val: -4.2619347572 mse_val: 0.0250047352 acc_val: 0.5000000000 time: 3.8663s
Epoch: 0090 nll_train: 1275.0927124023 kl_train: -3.3936961889 mse_train: 0.0067110137 acc_train: 0.0586206897 nll_val: 4752.0268554688 kl_val: -4.2759122849 mse_val: 0.0250106677 acc_val: 0.5000000000 time: 3.8972s
Epoch: 0091 nll_train: 1274.6641845703 kl_train: -3.4083882570 mse_train: 0.0067087585 acc_train: 0.0586206897 nll_val: 4753.9296875000 kl_val: -4.2885451317 mse_val: 0.0250206832 acc_val: 0.5000000000 time: 3.8945s
Epoch: 0092 nll_train: 1274.0892028809 kl_train: -3.4232972860 mse_train: 0.0067057321 acc_train: 0.0586206897 nll_val: 4752.4868164062 kl_val: -4.3001284599 mse_val: 0.0250130873 acc_val: 0.5011494253 time: 3.8952s
Epoch: 0093 nll_train: 1274.5473327637 kl_train: -3.4383679628 mse_train: 0.0067081433 acc_train: 0.0586206897 nll_val: 4751.9101562500 kl_val: -4.3108224869 mse_val: 0.0250100531 acc_val: 0.5011494253 time: 3.9308s
Epoch: 0094 nll_train: 1274.3691711426 kl_train: -3.4530422688 mse_train: 0.0067072060 acc_train: 0.0591954023 nll_val: 4751.2333984375 kl_val: -4.3215155602 mse_val: 0.0250064917 acc_val: 0.5000000000 time: 3.9490s
Epoch: 0095 nll_train: 1273.5074462891 kl_train: -3.4674621820 mse_train: 0.0067026704 acc_train: 0.0591954023 nll_val: 4756.0581054688 kl_val: -4.3305716515 mse_val: 0.0250318814 acc_val: 0.5000000000 time: 3.9494s
Epoch: 0096 nll_train: 1272.6250000000 kl_train: -3.4815466404 mse_train: 0.0066980261 acc_train: 0.0591954023 nll_val: 4755.9702148438 kl_val: -4.3389277458 mse_val: 0.0250314232 acc_val: 0.5011494253 time: 6.7292s
Epoch: 0097 nll_train: 1272.0428466797 kl_train: -3.4958479404 mse_train: 0.0066949623 acc_train: 0.0591954023 nll_val: 4755.0332031250 kl_val: -4.3472723961 mse_val: 0.0250264909 acc_val: 0.5022988506 time: 5.3668s
Epoch: 0098 nll_train: 1271.6658935547 kl_train: -3.5101997852 mse_train: 0.0066929784 acc_train: 0.0591954023 nll_val: 4756.5937500000 kl_val: -4.3539705276 mse_val: 0.0250347033 acc_val: 0.5011494253 time: 4.7469s
Epoch: 0099 nll_train: 1269.9464721680 kl_train: -3.5244560242 mse_train: 0.0066839283 acc_train: 0.0580459770 nll_val: 4754.7070312500 kl_val: -4.3610901833 mse_val: 0.0250247717 acc_val: 0.5011494253 time: 4.1149s
Epoch: 0100 nll_train: 1270.3471984863 kl_train: -3.5389528275 mse_train: 0.0066860373 acc_train: 0.0580459770 nll_val: 4756.3818359375 kl_val: -4.3661880493 mse_val: 0.0250335857 acc_val: 0.5000000000 time: 3.8217s
Epoch: 0101 nll_train: 1270.0214233398 kl_train: -3.5530610085 mse_train: 0.0066843232 acc_train: 0.0580459770 nll_val: 4756.9428710938 kl_val: -4.3698935509 mse_val: 0.0250365417 acc_val: 0.4988505747 time: 3.5966s
Epoch: 0102 nll_train: 1268.0902099609 kl_train: -3.5669326782 mse_train: 0.0066741587 acc_train: 0.0574712644 nll_val: 4757.7617187500 kl_val: -4.3728480339 mse_val: 0.0250408463 acc_val: 0.4988505747 time: 3.6488s
Epoch: 0103 nll_train: 1269.6720886230 kl_train: -3.5809566975 mse_train: 0.0066824839 acc_train: 0.0563218391 nll_val: 4762.7236328125 kl_val: -4.3789834976 mse_val: 0.0250669625 acc_val: 0.4988505747 time: 3.7380s
Epoch: 0104 nll_train: 1282.7941284180 kl_train: -3.5953581333 mse_train: 0.0067515478 acc_train: 0.0568965517 nll_val: 4756.8940429688 kl_val: -4.3886198997 mse_val: 0.0250362847 acc_val: 0.5011494253 time: 3.6952s
Epoch: 0105 nll_train: 1267.7066345215 kl_train: -3.6100862026 mse_train: 0.0066721404 acc_train: 0.0568965517 nll_val: 4753.4360351562 kl_val: -4.3967714310 mse_val: 0.0250180848 acc_val: 0.5011494253 time: 3.5920s
Epoch: 0106 nll_train: 1264.7644348145 kl_train: -3.6244058609 mse_train: 0.0066566549 acc_train: 0.0563218391 nll_val: 4759.0043945312 kl_val: -4.4025640488 mse_val: 0.0250473898 acc_val: 0.4965517241 time: 3.6004s
Epoch: 0107 nll_train: 1269.1789245605 kl_train: -3.6394970417 mse_train: 0.0066798892 acc_train: 0.0563218391 nll_val: 4758.0693359375 kl_val: -4.4097385406 mse_val: 0.0250424705 acc_val: 0.4965517241 time: 3.8048s
Epoch: 0108 nll_train: 1268.2263793945 kl_train: -3.6554483175 mse_train: 0.0066748755 acc_train: 0.0563218391 nll_val: 4761.1972656250 kl_val: -4.4187483788 mse_val: 0.0250589326 acc_val: 0.4965517241 time: 3.8769s
Epoch: 0109 nll_train: 1264.7156066895 kl_train: -3.6711792946 mse_train: 0.0066563973 acc_train: 0.0563218391 nll_val: 4758.9262695312 kl_val: -4.4297628403 mse_val: 0.0250469781 acc_val: 0.4954022989 time: 3.7973s
Epoch: 0110 nll_train: 1267.7766723633 kl_train: -3.6873815060 mse_train: 0.0066725089 acc_train: 0.0557471264 nll_val: 4761.7490234375 kl_val: -4.4402017593 mse_val: 0.0250618346 acc_val: 0.4954022989 time: 4.7732s
Epoch: 0111 nll_train: 1267.9740905762 kl_train: -3.7043486834 mse_train: 0.0066735479 acc_train: 0.0557471264 nll_val: 4762.7177734375 kl_val: -4.4502868652 mse_val: 0.0250669327 acc_val: 0.4942528736 time: 3.8143s
Epoch: 0112 nll_train: 1263.2939453125 kl_train: -3.7206723690 mse_train: 0.0066489154 acc_train: 0.0557471264 nll_val: 4767.2026367188 kl_val: -4.4582858086 mse_val: 0.0250905436 acc_val: 0.4965517241 time: 4.1594s
Epoch: 0113 nll_train: 1267.6450195312 kl_train: -3.7368845940 mse_train: 0.0066718153 acc_train: 0.0563218391 nll_val: 4768.3090820312 kl_val: -4.4655017853 mse_val: 0.0250963643 acc_val: 0.4965517241 time: 3.8814s
Epoch: 0114 nll_train: 1267.6431274414 kl_train: -3.7530858517 mse_train: 0.0066718055 acc_train: 0.0563218391 nll_val: 4767.6440429688 kl_val: -4.4762010574 mse_val: 0.0250928625 acc_val: 0.4965517241 time: 3.5871s
Epoch: 0115 nll_train: 1262.4296264648 kl_train: -3.7695943117 mse_train: 0.0066443658 acc_train: 0.0563218391 nll_val: 4767.9511718750 kl_val: -4.4866590500 mse_val: 0.0250944793 acc_val: 0.4965517241 time: 4.1103s
Epoch: 0116 nll_train: 1267.9643249512 kl_train: -3.7864904404 mse_train: 0.0066734962 acc_train: 0.0557471264 nll_val: 4766.3051757812 kl_val: -4.4990620613 mse_val: 0.0250858143 acc_val: 0.4977011494 time: 4.0703s
Epoch: 0117 nll_train: 1264.1378173828 kl_train: -3.8033220768 mse_train: 0.0066533572 acc_train: 0.0557471264 nll_val: 4766.9833984375 kl_val: -4.5097894669 mse_val: 0.0250893869 acc_val: 0.4965517241 time: 4.1488s
Epoch: 0118 nll_train: 1262.6738891602 kl_train: -3.8193042278 mse_train: 0.0066456522 acc_train: 0.0563218391 nll_val: 4768.5463867188 kl_val: -4.5202641487 mse_val: 0.0250976104 acc_val: 0.4954022989 time: 3.3295s
Epoch: 0119 nll_train: 1268.5562133789 kl_train: -3.8353251219 mse_train: 0.0066766120 acc_train: 0.0563218391 nll_val: 4769.6015625000 kl_val: -4.5303621292 mse_val: 0.0251031611 acc_val: 0.4954022989 time: 2.7498s
Epoch: 0120 nll_train: 1262.1643066406 kl_train: -3.8515676260 mse_train: 0.0066429701 acc_train: 0.0557471264 nll_val: 4770.5205078125 kl_val: -4.5402951241 mse_val: 0.0251080003 acc_val: 0.4942528736 time: 2.7766s
Epoch: 0121 nll_train: 1263.0607299805 kl_train: -3.8670191765 mse_train: 0.0066476875 acc_train: 0.0557471264 nll_val: 4771.9213867188 kl_val: -4.5472865105 mse_val: 0.0251153745 acc_val: 0.4965517241 time: 2.8169s
Epoch: 0122 nll_train: 1265.3792419434 kl_train: -3.8821569681 mse_train: 0.0066598904 acc_train: 0.0551724138 nll_val: 4773.0205078125 kl_val: -4.5529437065 mse_val: 0.0251211599 acc_val: 0.4965517241 time: 2.7309s
Epoch: 0123 nll_train: 1261.2627868652 kl_train: -3.8972103596 mse_train: 0.0066382248 acc_train: 0.0551724138 nll_val: 4768.9511718750 kl_val: -4.5604934692 mse_val: 0.0250997413 acc_val: 0.4988505747 time: 2.6863s
Epoch: 0124 nll_train: 1263.8945312500 kl_train: -3.9128237963 mse_train: 0.0066520764 acc_train: 0.0551724138 nll_val: 4773.2578125000 kl_val: -4.5700926781 mse_val: 0.0251224134 acc_val: 0.4977011494 time: 2.6758s
Epoch: 0125 nll_train: 1262.2171325684 kl_train: -3.9286707640 mse_train: 0.0066432478 acc_train: 0.0551724138 nll_val: 4774.2392578125 kl_val: -4.5796966553 mse_val: 0.0251275748 acc_val: 0.4977011494 time: 2.5855s
Epoch: 0126 nll_train: 1260.9005737305 kl_train: -3.9446376562 mse_train: 0.0066363192 acc_train: 0.0545977011 nll_val: 4777.8203125000 kl_val: -4.5906543732 mse_val: 0.0251464248 acc_val: 0.4954022989 time: 2.6063s
Epoch: 0127 nll_train: 1263.5363769531 kl_train: -3.9609175920 mse_train: 0.0066501914 acc_train: 0.0540229885 nll_val: 4776.4741210938 kl_val: -4.6037297249 mse_val: 0.0251393374 acc_val: 0.4954022989 time: 2.5634s
Epoch: 0128 nll_train: 1259.2993164062 kl_train: -3.9771884680 mse_train: 0.0066278912 acc_train: 0.0534482759 nll_val: 4775.7187500000 kl_val: -4.6170840263 mse_val: 0.0251353588 acc_val: 0.4954022989 time: 3.8599s
Epoch: 0129 nll_train: 1262.4155273438 kl_train: -3.9936295748 mse_train: 0.0066442912 acc_train: 0.0528735632 nll_val: 4778.2148437500 kl_val: -4.6311235428 mse_val: 0.0251484979 acc_val: 0.4965517241 time: 4.2910s
Epoch: 0130 nll_train: 1259.3778076172 kl_train: -4.0101865530 mse_train: 0.0066283033 acc_train: 0.0528735632 nll_val: 4779.5878906250 kl_val: -4.6460466385 mse_val: 0.0251557268 acc_val: 0.4965517241 time: 3.9234s
Epoch: 0131 nll_train: 1261.7660217285 kl_train: -4.0272991657 mse_train: 0.0066408740 acc_train: 0.0528735632 nll_val: 4776.9018554688 kl_val: -4.6606431007 mse_val: 0.0251415856 acc_val: 0.4965517241 time: 4.1164s
Epoch: 0132 nll_train: 1259.0987854004 kl_train: -4.0447784662 mse_train: 0.0066268353 acc_train: 0.0528735632 nll_val: 4777.5078125000 kl_val: -4.6762342453 mse_val: 0.0251447819 acc_val: 0.4977011494 time: 4.3310s
Epoch: 0133 nll_train: 1262.2587890625 kl_train: -4.0624479055 mse_train: 0.0066434667 acc_train: 0.0528735632 nll_val: 4777.6103515625 kl_val: -4.6923975945 mse_val: 0.0251453165 acc_val: 0.4988505747 time: 3.6213s
Epoch: 0134 nll_train: 1257.1759948730 kl_train: -4.0800915956 mse_train: 0.0066167160 acc_train: 0.0522988506 nll_val: 4776.0561523438 kl_val: -4.7070384026 mse_val: 0.0251371358 acc_val: 0.4988505747 time: 4.0676s
Epoch: 0135 nll_train: 1262.9089965820 kl_train: -4.0978789330 mse_train: 0.0066468893 acc_train: 0.0517241379 nll_val: 4784.4028320312 kl_val: -4.7202138901 mse_val: 0.0251810644 acc_val: 0.4988505747 time: 4.5651s
Epoch: 0136 nll_train: 1255.9764099121 kl_train: -4.1140639782 mse_train: 0.0066104016 acc_train: 0.0522988506 nll_val: 4784.2036132812 kl_val: -4.7301750183 mse_val: 0.0251800157 acc_val: 0.4988505747 time: 4.0064s
Epoch: 0137 nll_train: 1261.0188598633 kl_train: -4.1304768324 mse_train: 0.0066369412 acc_train: 0.0528735632 nll_val: 4780.6689453125 kl_val: -4.7409167290 mse_val: 0.0251614135 acc_val: 0.4988505747 time: 3.6605s
Epoch: 0138 nll_train: 1258.9669189453 kl_train: -4.1477860212 mse_train: 0.0066261416 acc_train: 0.0528735632 nll_val: 4789.7578125000 kl_val: -4.7521762848 mse_val: 0.0252092537 acc_val: 0.4988505747 time: 3.7624s
Epoch: 0139 nll_train: 1257.1979980469 kl_train: -4.1647741795 mse_train: 0.0066168315 acc_train: 0.0528735632 nll_val: 4787.1308593750 kl_val: -4.7635273933 mse_val: 0.0251954235 acc_val: 0.4988505747 time: 4.7575s
Epoch: 0140 nll_train: 1260.1986694336 kl_train: -4.1820778847 mse_train: 0.0066326238 acc_train: 0.0528735632 nll_val: 4791.8916015625 kl_val: -4.7726855278 mse_val: 0.0252204835 acc_val: 0.5000000000 time: 3.8099s
Epoch: 0141 nll_train: 1255.3793640137 kl_train: -4.1982505322 mse_train: 0.0066072596 acc_train: 0.0528735632 nll_val: 4792.5117187500 kl_val: -4.7811722755 mse_val: 0.0252237432 acc_val: 0.5000000000 time: 4.0310s
Epoch: 0142 nll_train: 1256.6632995605 kl_train: -4.2140016556 mse_train: 0.0066140171 acc_train: 0.0517241379 nll_val: 4789.8520507812 kl_val: -4.7896327972 mse_val: 0.0252097454 acc_val: 0.5000000000 time: 3.5923s
Epoch: 0143 nll_train: 1262.6072998047 kl_train: -4.2306255102 mse_train: 0.0066453016 acc_train: 0.0517241379 nll_val: 4792.5024414062 kl_val: -4.7951393127 mse_val: 0.0252236966 acc_val: 0.5000000000 time: 3.9208s
Epoch: 0144 nll_train: 1251.3254699707 kl_train: -4.2464203835 mse_train: 0.0065859230 acc_train: 0.0511494253 nll_val: 4792.6020507812 kl_val: -4.8036594391 mse_val: 0.0252242237 acc_val: 0.4988505747 time: 5.0797s
Epoch: 0145 nll_train: 1264.2194519043 kl_train: -4.2635679245 mse_train: 0.0066537859 acc_train: 0.0500000000 nll_val: 4792.8305664062 kl_val: -4.8143444061 mse_val: 0.0252254214 acc_val: 0.4988505747 time: 4.3683s
Epoch: 0146 nll_train: 1253.5846862793 kl_train: -4.2798829079 mse_train: 0.0065978138 acc_train: 0.0500000000 nll_val: 4794.1757812500 kl_val: -4.8277297020 mse_val: 0.0252324995 acc_val: 0.4988505747 time: 3.8302s
Epoch: 0147 nll_train: 1256.6853637695 kl_train: -4.2962646484 mse_train: 0.0066141332 acc_train: 0.0500000000 nll_val: 4789.1562500000 kl_val: -4.8383536339 mse_val: 0.0252060872 acc_val: 0.4988505747 time: 3.8829s
Epoch: 0148 nll_train: 1261.9645080566 kl_train: -4.3136534691 mse_train: 0.0066419181 acc_train: 0.0500000000 nll_val: 4792.1176757812 kl_val: -4.8489346504 mse_val: 0.0252216738 acc_val: 0.4977011494 time: 3.8586s
Epoch: 0149 nll_train: 1248.7450866699 kl_train: -4.3290624619 mse_train: 0.0065723424 acc_train: 0.0500000000 nll_val: 4796.8476562500 kl_val: -4.8587746620 mse_val: 0.0252465624 acc_val: 0.5000000000 time: 3.8849s
Epoch: 0150 nll_train: 1263.9815979004 kl_train: -4.3461563587 mse_train: 0.0066525346 acc_train: 0.0488505747 nll_val: 4795.0932617188 kl_val: -4.8676896095 mse_val: 0.0252373312 acc_val: 0.5000000000 time: 3.6453s
Epoch: 0151 nll_train: 1251.4439392090 kl_train: -4.3621861935 mse_train: 0.0065865467 acc_train: 0.0482758621 nll_val: 4796.6923828125 kl_val: -4.8738861084 mse_val: 0.0252457485 acc_val: 0.5000000000 time: 3.5797s
Epoch: 0152 nll_train: 1255.2227478027 kl_train: -4.3767018318 mse_train: 0.0066064346 acc_train: 0.0488505747 nll_val: 4794.2563476562 kl_val: -4.8792963028 mse_val: 0.0252329279 acc_val: 0.5000000000 time: 3.6557s
Epoch: 0153 nll_train: 1259.4910888672 kl_train: -4.3918173313 mse_train: 0.0066289003 acc_train: 0.0488505747 nll_val: 4795.2602539062 kl_val: -4.8802957535 mse_val: 0.0252382103 acc_val: 0.4988505747 time: 3.7389s
Epoch: 0154 nll_train: 1247.0276489258 kl_train: -4.4050682783 mse_train: 0.0065633028 acc_train: 0.0488505747 nll_val: 4797.5996093750 kl_val: -4.8836994171 mse_val: 0.0252505224 acc_val: 0.4988505747 time: 3.5429s
Epoch: 0155 nll_train: 1259.6066589355 kl_train: -4.4191066027 mse_train: 0.0066295086 acc_train: 0.0488505747 nll_val: 4795.0395507812 kl_val: -4.8896703720 mse_val: 0.0252370499 acc_val: 0.4988505747 time: 3.7968s
Epoch: 0156 nll_train: 1250.2543640137 kl_train: -4.4322295189 mse_train: 0.0065802861 acc_train: 0.0488505747 nll_val: 4791.1323242188 kl_val: -4.8963327408 mse_val: 0.0252164844 acc_val: 0.4977011494 time: 3.5995s
Epoch: 0157 nll_train: 1257.0675964355 kl_train: -4.4458220005 mse_train: 0.0066161449 acc_train: 0.0500000000 nll_val: 4797.8051757812 kl_val: -4.9118094444 mse_val: 0.0252516009 acc_val: 0.4977011494 time: 4.1571s
Epoch: 0158 nll_train: 1251.7415771484 kl_train: -4.4601352215 mse_train: 0.0065881130 acc_train: 0.0500000000 nll_val: 4798.0463867188 kl_val: -4.9230957031 mse_val: 0.0252528694 acc_val: 0.4977011494 time: 4.9650s
Epoch: 0159 nll_train: 1251.0811767578 kl_train: -4.4740493298 mse_train: 0.0065846379 acc_train: 0.0500000000 nll_val: 4794.2509765625 kl_val: -4.9327507019 mse_val: 0.0252328962 acc_val: 0.4977011494 time: 6.9888s
Epoch: 0160 nll_train: 1252.7894592285 kl_train: -4.4879655838 mse_train: 0.0065936284 acc_train: 0.0500000000 nll_val: 4793.1474609375 kl_val: -4.9440031052 mse_val: 0.0252270922 acc_val: 0.4977011494 time: 4.9622s
Epoch: 0161 nll_train: 1247.9733886719 kl_train: -4.5005877018 mse_train: 0.0065682807 acc_train: 0.0500000000 nll_val: 4804.4150390625 kl_val: -4.9572777748 mse_val: 0.0252863932 acc_val: 0.4977011494 time: 4.0326s
Epoch: 0162 nll_train: 1256.7946472168 kl_train: -4.5138258934 mse_train: 0.0066147083 acc_train: 0.0500000000 nll_val: 4809.0429687500 kl_val: -4.9666872025 mse_val: 0.0253107511 acc_val: 0.4988505747 time: 4.0229s
Epoch: 0163 nll_train: 1247.1662597656 kl_train: -4.5247163773 mse_train: 0.0065640328 acc_train: 0.0500000000 nll_val: 4809.5361328125 kl_val: -4.9728107452 mse_val: 0.0253133476 acc_val: 0.4988505747 time: 4.5278s
Epoch: 0164 nll_train: 1265.0682678223 kl_train: -4.5365588665 mse_train: 0.0066582542 acc_train: 0.0500000000 nll_val: 4822.1533203125 kl_val: -4.9884014130 mse_val: 0.0253797527 acc_val: 0.5000000000 time: 4.2373s
Epoch: 0165 nll_train: 1249.7110290527 kl_train: -4.5487277508 mse_train: 0.0065774269 acc_train: 0.0500000000 nll_val: 4832.7783203125 kl_val: -5.0096931458 mse_val: 0.0254356712 acc_val: 0.5011494253 time: 4.2743s
Epoch: 0166 nll_train: 1265.2093200684 kl_train: -4.5612421036 mse_train: 0.0066589960 acc_train: 0.0500000000 nll_val: 4836.3549804688 kl_val: -5.0391287804 mse_val: 0.0254544970 acc_val: 0.5022988506 time: 4.1372s
Epoch: 0167 nll_train: 1265.2264099121 kl_train: -4.5743713379 mse_train: 0.0066590866 acc_train: 0.0494252874 nll_val: 4837.0253906250 kl_val: -5.0827422142 mse_val: 0.0254580285 acc_val: 0.5022988506 time: 3.8315s
Epoch: 0168 nll_train: 1277.7656250000 kl_train: -4.5912210941 mse_train: 0.0067250824 acc_train: 0.0500000000 nll_val: 4914.3227539062 kl_val: -5.1460647583 mse_val: 0.0258648563 acc_val: 0.5045977011 time: 3.6653s
Epoch: 0169 nll_train: 1301.5417480469 kl_train: -4.6097247601 mse_train: 0.0068502194 acc_train: 0.0488505747 nll_val: 4790.2607421875 kl_val: -5.2180476189 mse_val: 0.0252118967 acc_val: 0.5080459770 time: 3.7396s
Epoch: 0170 nll_train: 1314.2826232910 kl_train: -4.6314210892 mse_train: 0.0069172770 acc_train: 0.0482758621 nll_val: 4827.0532226562 kl_val: -5.2493476868 mse_val: 0.0254055411 acc_val: 0.5137931034 time: 3.6245s
Epoch: 0171 nll_train: 1254.2904052734 kl_train: -4.6325943470 mse_train: 0.0066015279 acc_train: 0.0477011494 nll_val: 4796.6474609375 kl_val: -5.2626094818 mse_val: 0.0252455100 acc_val: 0.5103448276 time: 3.5937s
Epoch: 0172 nll_train: 1257.7976684570 kl_train: -4.6174464226 mse_train: 0.0066199878 acc_train: 0.0471264368 nll_val: 4810.0708007812 kl_val: -5.2604308128 mse_val: 0.0253161583 acc_val: 0.5057471264 time: 4.0179s
Epoch: 0173 nll_train: 1294.1547851562 kl_train: -4.6127421856 mse_train: 0.0068113412 acc_train: 0.0471264368 nll_val: 4801.1762695312 kl_val: -5.2412014008 mse_val: 0.0252693482 acc_val: 0.5034482759 time: 4.8736s
Epoch: 0174 nll_train: 1270.3780212402 kl_train: -4.6084423065 mse_train: 0.0066862000 acc_train: 0.0471264368 nll_val: 4791.7187500000 kl_val: -5.2231035233 mse_val: 0.0252195671 acc_val: 0.4977011494 time: 4.3398s
Epoch: 0175 nll_train: 1234.9338989258 kl_train: -4.5917499065 mse_train: 0.0064996518 acc_train: 0.0488505747 nll_val: 4841.2626953125 kl_val: -5.1898508072 mse_val: 0.0254803281 acc_val: 0.4965517241 time: 4.0003s
Epoch: 0176 nll_train: 1277.4096374512 kl_train: -4.5774791241 mse_train: 0.0067232088 acc_train: 0.0488505747 nll_val: 4805.2138671875 kl_val: -5.1497993469 mse_val: 0.0252905954 acc_val: 0.4942528736 time: 3.8753s
Epoch: 0177 nll_train: 1273.1999206543 kl_train: -4.5689992905 mse_train: 0.0067010520 acc_train: 0.0494252874 nll_val: 4812.4809570312 kl_val: -5.1079850197 mse_val: 0.0253288448 acc_val: 0.4873563218 time: 3.7424s
Epoch: 0178 nll_train: 1238.5396118164 kl_train: -4.5555677414 mse_train: 0.0065186289 acc_train: 0.0522988506 nll_val: 4836.3989257812 kl_val: -5.0725579262 mse_val: 0.0254547317 acc_val: 0.4827586207 time: 5.3592s
Epoch: 0179 nll_train: 1259.9369506836 kl_train: -4.5445928574 mse_train: 0.0066312471 acc_train: 0.0522988506 nll_val: 4816.3110351562 kl_val: -5.0348939896 mse_val: 0.0253490042 acc_val: 0.4804597701 time: 5.0405s
Epoch: 0180 nll_train: 1269.4303588867 kl_train: -4.5408632755 mse_train: 0.0066812118 acc_train: 0.0522988506 nll_val: 4808.0639648438 kl_val: -5.0026540756 mse_val: 0.0253055971 acc_val: 0.4781609195 time: 3.9939s
Epoch: 0181 nll_train: 1244.7280578613 kl_train: -4.5365040302 mse_train: 0.0065511994 acc_train: 0.0522988506 nll_val: 4817.4067382812 kl_val: -4.9766964912 mse_val: 0.0253547728 acc_val: 0.4747126437 time: 3.9863s
Epoch: 0182 nll_train: 1251.4542846680 kl_train: -4.5316014290 mse_train: 0.0065866011 acc_train: 0.0522988506 nll_val: 4813.1694335938 kl_val: -4.9489645958 mse_val: 0.0253324695 acc_val: 0.4735632184 time: 5.1935s
Epoch: 0183 nll_train: 1265.4136352539 kl_train: -4.5321819782 mse_train: 0.0066600713 acc_train: 0.0522988506 nll_val: 4797.8564453125 kl_val: -4.9258856773 mse_val: 0.0252518747 acc_val: 0.4712643678 time: 4.5407s
Epoch: 0184 nll_train: 1249.5222473145 kl_train: -4.5338671207 mse_train: 0.0065764330 acc_train: 0.0528735632 nll_val: 4801.3554687500 kl_val: -4.9082312584 mse_val: 0.0252702869 acc_val: 0.4666666667 time: 4.3191s
Epoch: 0185 nll_train: 1247.0664672852 kl_train: -4.5345013142 mse_train: 0.0065635075 acc_train: 0.0528735632 nll_val: 4805.6484375000 kl_val: -4.8946528435 mse_val: 0.0252928827 acc_val: 0.4666666667 time: 3.7223s
Epoch: 0186 nll_train: 1259.0068969727 kl_train: -4.5388722420 mse_train: 0.0066263514 acc_train: 0.0528735632 nll_val: 4795.8476562500 kl_val: -4.8854141235 mse_val: 0.0252413023 acc_val: 0.4666666667 time: 4.0519s
Epoch: 0187 nll_train: 1252.6754760742 kl_train: -4.5454704762 mse_train: 0.0065930284 acc_train: 0.0528735632 nll_val: 4801.1450195312 kl_val: -4.8809804916 mse_val: 0.0252691787 acc_val: 0.4678160920 time: 3.7157s
Epoch: 0188 nll_train: 1244.6952819824 kl_train: -4.5506322384 mse_train: 0.0065510271 acc_train: 0.0534482759 nll_val: 4805.5507812500 kl_val: -4.8726983070 mse_val: 0.0252923705 acc_val: 0.4666666667 time: 3.7211s
Epoch: 0189 nll_train: 1258.4817810059 kl_train: -4.5564303398 mse_train: 0.0066235877 acc_train: 0.0534482759 nll_val: 4797.0922851562 kl_val: -4.8642387390 mse_val: 0.0252478532 acc_val: 0.4666666667 time: 3.7011s
Epoch: 0190 nll_train: 1251.3193359375 kl_train: -4.5624449253 mse_train: 0.0065858912 acc_train: 0.0540229885 nll_val: 4798.8007812500 kl_val: -4.8695726395 mse_val: 0.0252568480 acc_val: 0.4666666667 time: 3.6234s
Epoch: 0191 nll_train: 1244.9577026367 kl_train: -4.5682559013 mse_train: 0.0065524088 acc_train: 0.0540229885 nll_val: 4803.1850585938 kl_val: -4.8848204613 mse_val: 0.0252799243 acc_val: 0.4678160920 time: 3.4193s
Epoch: 0192 nll_train: 1254.1969909668 kl_train: -4.5761570930 mse_train: 0.0066010363 acc_train: 0.0545977011 nll_val: 4794.7578125000 kl_val: -4.8998098373 mse_val: 0.0252355672 acc_val: 0.4689655172 time: 3.1671s
Epoch: 0193 nll_train: 1250.5520324707 kl_train: -4.5858280659 mse_train: 0.0065818526 acc_train: 0.0551724138 nll_val: 4798.5297851562 kl_val: -4.9154767990 mse_val: 0.0252554193 acc_val: 0.4701149425 time: 3.2071s
Epoch: 0194 nll_train: 1246.1361083984 kl_train: -4.5946321487 mse_train: 0.0065586107 acc_train: 0.0557471264 nll_val: 4798.4628906250 kl_val: -4.9298830032 mse_val: 0.0252550654 acc_val: 0.4701149425 time: 3.2078s
Epoch: 0195 nll_train: 1251.9287109375 kl_train: -4.6037476063 mse_train: 0.0065890986 acc_train: 0.0557471264 nll_val: 4795.0815429688 kl_val: -4.9463462830 mse_val: 0.0252372697 acc_val: 0.4712643678 time: 3.1723s
Epoch: 0196 nll_train: 1249.5764465332 kl_train: -4.6144399643 mse_train: 0.0065767177 acc_train: 0.0563218391 nll_val: 4792.5883789062 kl_val: -4.9686255455 mse_val: 0.0252241474 acc_val: 0.4735632184 time: 3.2090s
Epoch: 0197 nll_train: 1246.9907226562 kl_train: -4.6263370514 mse_train: 0.0065631089 acc_train: 0.0563218391 nll_val: 4801.6733398438 kl_val: -4.9943666458 mse_val: 0.0252719633 acc_val: 0.4747126437 time: 3.2533s
Epoch: 0198 nll_train: 1250.8842163086 kl_train: -4.6399435997 mse_train: 0.0065836008 acc_train: 0.0563218391 nll_val: 4803.2192382812 kl_val: -5.0135726929 mse_val: 0.0252801012 acc_val: 0.4758620690 time: 3.2027s
Epoch: 0199 nll_train: 1235.8266296387 kl_train: -4.6510958672 mse_train: 0.0065043506 acc_train: 0.0568965517 nll_val: 4798.1186523438 kl_val: -5.0338768959 mse_val: 0.0252532531 acc_val: 0.4758620690 time: 3.3486s
Optimization finished
Best epoch 52
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 420.6260986328 kl_test: -5.4747176170 mse_test: 0.0022138215 acc_test: 0.0011494253
MSE: [ 0.001349127037 , 0.001453813282 , 0.002431357047 , 0.002279409440 , 0.003018305637 , 0.002441248158 , 0.004027497955 , 0.003583077108 , 0.004408009350 , 0.003860990517 , 0.004823265597 , 0.003968362231 , 0.005200382788 , 0.004378996324 , 0.005487301387 , 0.004395352677 , 0.005189143121 , 0.004122327548 , 0.005065758247 ]
Accuracy for experiment id 11 is 0.0011111111111111111
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 1708271.5214843750 kl_train: -12.2046599388 mse_train: 8.9909021258 acc_train: 0.2545977011 nll_val: 14116.7529296875 kl_val: -9.1120338440 mse_val: 0.0742987022 acc_val: 1.0000000000 time: 3.4336s
Best model so far, saving...
Epoch: 0001 nll_train: 36441.1250000000 kl_train: -9.8098468781 mse_train: 0.1917953677 acc_train: 0.3862068966 nll_val: 267005.2500000000 kl_val: -1.9599770308 mse_val: 1.4052907228 acc_val: 1.0000000000 time: 3.3704s
Epoch: 0002 nll_train: 121342.4257812500 kl_train: -6.4628086090 mse_train: 0.6386443079 acc_train: 0.5264367816 nll_val: 622504.1250000000 kl_val: -1.0470712185 mse_val: 3.2763373852 acc_val: 1.0000000000 time: 3.3650s
Epoch: 0003 nll_train: 137243.4218750000 kl_train: -5.4942870140 mse_train: 0.7223338187 acc_train: 0.5224137931 nll_val: 674009.8750000000 kl_val: -1.2081164122 mse_val: 3.5474202633 acc_val: 1.0000000000 time: 3.3416s
Epoch: 0004 nll_train: 116181.2617187500 kl_train: -5.2181529999 mse_train: 0.6114802957 acc_train: 0.4977011494 nll_val: 440909.9687500000 kl_val: -2.6763448715 mse_val: 2.3205785751 acc_val: 1.0000000000 time: 3.2600s
Epoch: 0005 nll_train: 68396.4511718750 kl_train: -5.2667748928 mse_train: 0.3599813133 acc_train: 0.4632183908 nll_val: 132689.4843750000 kl_val: -8.0002498627 mse_val: 0.6983656883 acc_val: 1.0000000000 time: 3.1276s
Epoch: 0006 nll_train: 24716.2446289062 kl_train: -5.5050342083 mse_train: 0.1300854981 acc_train: 0.4160919540 nll_val: 7805.4194335938 kl_val: -16.4198570251 mse_val: 0.0410811491 acc_val: 0.6816091954 time: 3.0691s
Best model so far, saving...
Epoch: 0007 nll_train: 5020.8684082031 kl_train: -5.8562462330 mse_train: 0.0264256210 acc_train: 0.3304597701 nll_val: 12358.1650390625 kl_val: -8.6215105057 mse_val: 0.0650429726 acc_val: 0.0000000000 time: 3.0117s
Epoch: 0008 nll_train: 23028.5458984375 kl_train: -6.1486454010 mse_train: 0.1212028787 acc_train: 0.2298850575 nll_val: 9348.6250000000 kl_val: -2.5054426193 mse_val: 0.0492032841 acc_val: 0.0000000000 time: 3.5561s
Epoch: 0009 nll_train: 5150.0782775879 kl_train: -6.3270361423 mse_train: 0.0271056746 acc_train: 0.1218390805 nll_val: 9095.6386718750 kl_val: -0.9697824717 mse_val: 0.0478717834 acc_val: 0.0000000000 time: 4.1469s
Epoch: 0010 nll_train: 4600.2597656250 kl_train: -6.3448135853 mse_train: 0.0242118919 acc_train: 0.0488505747 nll_val: 9361.9707031250 kl_val: -0.5708036423 mse_val: 0.0492735282 acc_val: 0.0000000000 time: 4.1720s
Epoch: 0011 nll_train: 9312.7548828125 kl_train: -6.2348887920 mse_train: 0.0490144938 acc_train: 0.0195402299 nll_val: 9389.0595703125 kl_val: -0.4276745319 mse_val: 0.0494161025 acc_val: 0.0000000000 time: 4.0439s
Epoch: 0012 nll_train: 7491.1469726562 kl_train: -6.0294175148 mse_train: 0.0394270904 acc_train: 0.0247126437 nll_val: 9141.6777343750 kl_val: -0.3750137687 mse_val: 0.0481140912 acc_val: 0.0000000000 time: 4.0879s
Epoch: 0013 nll_train: 3051.7959594727 kl_train: -5.7715492249 mse_train: 0.0160620823 acc_train: 0.0390804598 nll_val: 8989.1708984375 kl_val: -0.3757248521 mse_val: 0.0473114289 acc_val: 0.0000000000 time: 4.0480s
Epoch: 0014 nll_train: 1755.9290466309 kl_train: -5.5285997391 mse_train: 0.0092417323 acc_train: 0.0511494253 nll_val: 9101.7617187500 kl_val: -0.4159992635 mse_val: 0.0479040034 acc_val: 0.0000000000 time: 3.9723s
Epoch: 0015 nll_train: 3645.4846191406 kl_train: -5.3323683739 mse_train: 0.0191867594 acc_train: 0.0735632184 nll_val: 9113.1591796875 kl_val: -0.4809400141 mse_val: 0.0479639955 acc_val: 0.0000000000 time: 3.9892s
Epoch: 0016 nll_train: 3373.6316528320 kl_train: -5.1796994209 mse_train: 0.0177559538 acc_train: 0.0925287356 nll_val: 8841.7929687500 kl_val: -0.5662066936 mse_val: 0.0465357490 acc_val: 0.0000000000 time: 4.0102s
Epoch: 0017 nll_train: 1913.7585754395 kl_train: -5.0619313717 mse_train: 0.0100724124 acc_train: 0.1068965517 nll_val: 8589.7128906250 kl_val: -0.6595416665 mse_val: 0.0452090092 acc_val: 0.0000000000 time: 4.0145s
Epoch: 0018 nll_train: 1955.6844482422 kl_train: -4.9707272053 mse_train: 0.0102930758 acc_train: 0.1166666667 nll_val: 8521.3945312500 kl_val: -0.7857378125 mse_val: 0.0448494442 acc_val: 0.0000000000 time: 4.0029s
Epoch: 0019 nll_train: 2404.4028320312 kl_train: -4.8981344700 mse_train: 0.0126547501 acc_train: 0.1258620690 nll_val: 8379.0595703125 kl_val: -0.9591025114 mse_val: 0.0441003107 acc_val: 0.0000000000 time: 4.0392s
Epoch: 0020 nll_train: 2019.5329589844 kl_train: -4.8390865326 mse_train: 0.0106291203 acc_train: 0.1321839080 nll_val: 8190.2187500000 kl_val: -1.1251882315 mse_val: 0.0431064144 acc_val: 0.0000000000 time: 3.9614s
Epoch: 0021 nll_train: 1495.5730895996 kl_train: -4.7907774448 mse_train: 0.0078714371 acc_train: 0.1339080460 nll_val: 8079.9125976562 kl_val: -1.3505949974 mse_val: 0.0425258502 acc_val: 0.0114942529 time: 4.0552s
Epoch: 0022 nll_train: 1542.9879455566 kl_train: -4.7519376278 mse_train: 0.0081209893 acc_train: 0.1373563218 nll_val: 8024.7792968750 kl_val: -1.6545608044 mse_val: 0.0422356799 acc_val: 0.0344827586 time: 3.9727s
Epoch: 0023 nll_train: 1804.9609069824 kl_train: -4.7216291428 mse_train: 0.0094997941 acc_train: 0.1396551724 nll_val: 7896.3354492188 kl_val: -1.9751911163 mse_val: 0.0415596589 acc_val: 0.0344827586 time: 3.9994s
Epoch: 0024 nll_train: 1705.5114440918 kl_train: -4.6986086369 mse_train: 0.0089763759 acc_train: 0.1408045977 nll_val: 7532.0932617188 kl_val: -2.2650499344 mse_val: 0.0396425910 acc_val: 0.0517241379 time: 3.9532s
Best model so far, saving...
Epoch: 0025 nll_train: 1508.8059692383 kl_train: -4.6811811924 mse_train: 0.0079410841 acc_train: 0.1419540230 nll_val: 7313.2924804688 kl_val: -2.4949116707 mse_val: 0.0384910107 acc_val: 0.0712643678 time: 3.9982s
Best model so far, saving...
Epoch: 0026 nll_train: 1559.0935974121 kl_train: -4.6675527096 mse_train: 0.0082057558 acc_train: 0.1436781609 nll_val: 7244.3471679688 kl_val: -2.7079863548 mse_val: 0.0381281413 acc_val: 0.0873563218 time: 4.0303s
Best model so far, saving...
Epoch: 0027 nll_train: 1577.3919982910 kl_train: -4.6562585831 mse_train: 0.0083020625 acc_train: 0.1471264368 nll_val: 7029.3872070312 kl_val: -2.9215011597 mse_val: 0.0369967781 acc_val: 0.1103448276 time: 4.0172s
Best model so far, saving...
Epoch: 0028 nll_train: 1412.8324584961 kl_train: -4.6466579437 mse_train: 0.0074359601 acc_train: 0.1477011494 nll_val: 6899.5815429688 kl_val: -3.1334888935 mse_val: 0.0363135822 acc_val: 0.1344827586 time: 4.0259s
Best model so far, saving...
Epoch: 0029 nll_train: 1377.8959655762 kl_train: -4.6386725903 mse_train: 0.0072520834 acc_train: 0.1482758621 nll_val: 6865.7226562500 kl_val: -3.3494069576 mse_val: 0.0361353830 acc_val: 0.1678160920 time: 3.9933s
Best model so far, saving...
Epoch: 0030 nll_train: 1451.6094360352 kl_train: -4.6325161457 mse_train: 0.0076400503 acc_train: 0.1471264368 nll_val: 6767.9599609375 kl_val: -3.5829243660 mse_val: 0.0356208384 acc_val: 0.2057471264 time: 4.0194s
Best model so far, saving...
Epoch: 0031 nll_train: 1456.9679260254 kl_train: -4.6281995773 mse_train: 0.0076682523 acc_train: 0.1477011494 nll_val: 6562.2636718750 kl_val: -3.8266465664 mse_val: 0.0345382281 acc_val: 0.2402298851 time: 3.9959s
Best model so far, saving...
Epoch: 0032 nll_train: 1421.6789245605 kl_train: -4.6253035069 mse_train: 0.0074825202 acc_train: 0.1482758621 nll_val: 6452.4238281250 kl_val: -4.0478062630 mse_val: 0.0339601226 acc_val: 0.2873563218 time: 4.0129s
Best model so far, saving...
Epoch: 0033 nll_train: 1418.5115966797 kl_train: -4.6232857704 mse_train: 0.0074658497 acc_train: 0.1482758621 nll_val: 6375.5229492188 kl_val: -4.2190337181 mse_val: 0.0335553847 acc_val: 0.3333333333 time: 4.0336s
Best model so far, saving...
Epoch: 0034 nll_train: 1399.7996520996 kl_train: -4.6216230392 mse_train: 0.0073673662 acc_train: 0.1488505747 nll_val: 6252.2841796875 kl_val: -4.3399052620 mse_val: 0.0329067595 acc_val: 0.3793103448 time: 3.9844s
Best model so far, saving...
Epoch: 0035 nll_train: 1360.8980712891 kl_train: -4.6201434135 mse_train: 0.0071626215 acc_train: 0.1488505747 nll_val: 6171.9194335938 kl_val: -4.4221963882 mse_val: 0.0324837826 acc_val: 0.4229885057 time: 3.9948s
Best model so far, saving...
Epoch: 0036 nll_train: 1362.2953491211 kl_train: -4.6189332008 mse_train: 0.0071699749 acc_train: 0.1488505747 nll_val: 6143.0434570312 kl_val: -4.4806923866 mse_val: 0.0323318020 acc_val: 0.4551724138 time: 4.0139s
Best model so far, saving...
Epoch: 0037 nll_train: 1383.5096130371 kl_train: -4.6181683540 mse_train: 0.0072816291 acc_train: 0.1471264368 nll_val: 6066.2397460938 kl_val: -4.5237522125 mse_val: 0.0319275744 acc_val: 0.4919540230 time: 4.0415s
Best model so far, saving...
Epoch: 0038 nll_train: 1382.4004211426 kl_train: -4.6178581715 mse_train: 0.0072757913 acc_train: 0.1471264368 nll_val: 5978.1577148438 kl_val: -4.5547628403 mse_val: 0.0314639844 acc_val: 0.5195402299 time: 4.0255s
Best model so far, saving...
Epoch: 0039 nll_train: 1377.0932922363 kl_train: -4.6178402901 mse_train: 0.0072478595 acc_train: 0.1465517241 nll_val: 5935.4370117188 kl_val: -4.5755729675 mse_val: 0.0312391389 acc_val: 0.5436781609 time: 4.0402s
Best model so far, saving...
Epoch: 0040 nll_train: 1369.7987060547 kl_train: -4.6178975105 mse_train: 0.0072094668 acc_train: 0.1471264368 nll_val: 5869.5859375000 kl_val: -4.5884227753 mse_val: 0.0308925565 acc_val: 0.5793103448 time: 3.9812s
Best model so far, saving...
Epoch: 0041 nll_train: 1354.4388122559 kl_train: -4.6179153919 mse_train: 0.0071286251 acc_train: 0.1459770115 nll_val: 5834.9023437500 kl_val: -4.5964784622 mse_val: 0.0307100099 acc_val: 0.6126436782 time: 3.8597s
Best model so far, saving...
Epoch: 0042 nll_train: 1347.3406677246 kl_train: -4.6179490089 mse_train: 0.0070912667 acc_train: 0.1477011494 nll_val: 5852.2949218750 kl_val: -4.6019263268 mse_val: 0.0308015514 acc_val: 0.6310344828 time: 3.7477s
Epoch: 0043 nll_train: 1399.0001220703 kl_train: -4.6180620193 mse_train: 0.0073631591 acc_train: 0.1471264368 nll_val: 5756.6479492188 kl_val: -4.6076431274 mse_val: 0.0302981474 acc_val: 0.6471264368 time: 9.1840s
Best model so far, saving...
Epoch: 0044 nll_train: 1358.5712280273 kl_train: -4.6184089184 mse_train: 0.0071503745 acc_train: 0.1465517241 nll_val: 5725.9477539062 kl_val: -4.6116738319 mse_val: 0.0301365666 acc_val: 0.6655172414 time: 4.5078s
Best model so far, saving...
Epoch: 0045 nll_train: 1367.8517150879 kl_train: -4.6188087463 mse_train: 0.0071992197 acc_train: 0.1482758621 nll_val: 5689.6474609375 kl_val: -4.6139993668 mse_val: 0.0299455095 acc_val: 0.6793103448 time: 4.4653s
Best model so far, saving...
Epoch: 0046 nll_train: 1343.1013793945 kl_train: -4.6191568375 mse_train: 0.0070689548 acc_train: 0.1482758621 nll_val: 5671.2124023438 kl_val: -4.6151061058 mse_val: 0.0298484806 acc_val: 0.6908045977 time: 4.2640s
Best model so far, saving...
Epoch: 0047 nll_train: 1344.0640258789 kl_train: -4.6194636822 mse_train: 0.0070740207 acc_train: 0.1488505747 nll_val: 5652.8652343750 kl_val: -4.6162385941 mse_val: 0.0297519211 acc_val: 0.7011494253 time: 4.4912s
Best model so far, saving...
Epoch: 0048 nll_train: 1351.0622253418 kl_train: -4.6197988987 mse_train: 0.0071108539 acc_train: 0.1500000000 nll_val: 5620.9184570312 kl_val: -4.6177134514 mse_val: 0.0295837838 acc_val: 0.7091954023 time: 4.6327s
Best model so far, saving...
Epoch: 0049 nll_train: 1352.3840942383 kl_train: -4.6201970577 mse_train: 0.0071178109 acc_train: 0.1505747126 nll_val: 5587.0458984375 kl_val: -4.6189961433 mse_val: 0.0294055026 acc_val: 0.7160919540 time: 4.4140s
Best model so far, saving...
Epoch: 0050 nll_train: 1348.7243957520 kl_train: -4.6205666065 mse_train: 0.0070985497 acc_train: 0.1500000000 nll_val: 5569.9238281250 kl_val: -4.6195569038 mse_val: 0.0293153841 acc_val: 0.7183908046 time: 4.3581s
Best model so far, saving...
Epoch: 0051 nll_train: 1340.9951782227 kl_train: -4.6208391190 mse_train: 0.0070578685 acc_train: 0.1500000000 nll_val: 5575.7495117188 kl_val: -4.6198644638 mse_val: 0.0293460488 acc_val: 0.7206896552 time: 4.4359s
Epoch: 0052 nll_train: 1337.3240356445 kl_train: -4.6210289001 mse_train: 0.0070385473 acc_train: 0.1500000000 nll_val: 5548.9062500000 kl_val: -4.6205067635 mse_val: 0.0292047691 acc_val: 0.7218390805 time: 3.9079s
Best model so far, saving...
Epoch: 0053 nll_train: 1341.3727416992 kl_train: -4.6212415695 mse_train: 0.0070598557 acc_train: 0.1488505747 nll_val: 5531.7749023438 kl_val: -4.6216421127 mse_val: 0.0291146021 acc_val: 0.7275862069 time: 3.9666s
Best model so far, saving...
Epoch: 0054 nll_train: 1343.8765869141 kl_train: -4.6214985847 mse_train: 0.0070730352 acc_train: 0.1494252874 nll_val: 5518.4096679688 kl_val: -4.6226906776 mse_val: 0.0290442631 acc_val: 0.7310344828 time: 4.0508s
Best model so far, saving...
Epoch: 0055 nll_train: 1343.4908752441 kl_train: -4.6217210293 mse_train: 0.0070710046 acc_train: 0.1517241379 nll_val: 5522.1713867188 kl_val: -4.6237282753 mse_val: 0.0290640555 acc_val: 0.7321839080 time: 4.3943s
Epoch: 0056 nll_train: 1339.9152221680 kl_train: -4.6218836308 mse_train: 0.0070521854 acc_train: 0.1517241379 nll_val: 5497.4301757812 kl_val: -4.6247038841 mse_val: 0.0289338436 acc_val: 0.7356321839 time: 4.1139s
Best model so far, saving...
Epoch: 0057 nll_train: 1334.3729553223 kl_train: -4.6220059395 mse_train: 0.0070230148 acc_train: 0.1522988506 nll_val: 5486.7504882812 kl_val: -4.6254539490 mse_val: 0.0288776327 acc_val: 0.7367816092 time: 3.3595s
Best model so far, saving...
Epoch: 0058 nll_train: 1332.9200744629 kl_train: -4.6220881939 mse_train: 0.0070153687 acc_train: 0.1528735632 nll_val: 5482.0727539062 kl_val: -4.6259756088 mse_val: 0.0288530141 acc_val: 0.7390804598 time: 3.2265s
Best model so far, saving...
Epoch: 0059 nll_train: 1336.7500915527 kl_train: -4.6221575737 mse_train: 0.0070355273 acc_train: 0.1534482759 nll_val: 5468.2441406250 kl_val: -4.6263508797 mse_val: 0.0287802275 acc_val: 0.7413793103 time: 3.2141s
Best model so far, saving...
Epoch: 0060 nll_train: 1334.3827514648 kl_train: -4.6222395897 mse_train: 0.0070230669 acc_train: 0.1534482759 nll_val: 5465.0449218750 kl_val: -4.6267099380 mse_val: 0.0287633911 acc_val: 0.7413793103 time: 3.1672s
Best model so far, saving...
Epoch: 0061 nll_train: 1331.1763610840 kl_train: -4.6223025322 mse_train: 0.0070061911 acc_train: 0.1551724138 nll_val: 5454.6967773438 kl_val: -4.6268525124 mse_val: 0.0287089273 acc_val: 0.7413793103 time: 3.2562s
Best model so far, saving...
Epoch: 0062 nll_train: 1329.8436889648 kl_train: -4.6223213673 mse_train: 0.0069991766 acc_train: 0.1557471264 nll_val: 5443.9648437500 kl_val: -4.6271886826 mse_val: 0.0286524426 acc_val: 0.7436781609 time: 3.2608s
Best model so far, saving...
Epoch: 0063 nll_train: 1332.0825500488 kl_train: -4.6223371029 mse_train: 0.0070109605 acc_train: 0.1551724138 nll_val: 5446.3002929688 kl_val: -4.6274905205 mse_val: 0.0286647342 acc_val: 0.7436781609 time: 3.1886s
Epoch: 0064 nll_train: 1332.2396850586 kl_train: -4.6223163605 mse_train: 0.0070117876 acc_train: 0.1545977011 nll_val: 5428.8623046875 kl_val: -4.6279954910 mse_val: 0.0285729561 acc_val: 0.7448275862 time: 3.2832s
Best model so far, saving...
Epoch: 0065 nll_train: 1329.5795898438 kl_train: -4.6222898960 mse_train: 0.0069977869 acc_train: 0.1545977011 nll_val: 5432.7104492188 kl_val: -4.6283726692 mse_val: 0.0285932105 acc_val: 0.7436781609 time: 3.3654s
Epoch: 0066 nll_train: 1327.9649353027 kl_train: -4.6222331524 mse_train: 0.0069892890 acc_train: 0.1545977011 nll_val: 5430.1308593750 kl_val: -4.6286439896 mse_val: 0.0285796355 acc_val: 0.7436781609 time: 3.2256s
Epoch: 0067 nll_train: 1327.6361999512 kl_train: -4.6221756935 mse_train: 0.0069875584 acc_train: 0.1557471264 nll_val: 5426.6850585938 kl_val: -4.6292500496 mse_val: 0.0285614990 acc_val: 0.7436781609 time: 3.2289s
Best model so far, saving...
Epoch: 0068 nll_train: 1329.2238464355 kl_train: -4.6221249104 mse_train: 0.0069959152 acc_train: 0.1551724138 nll_val: 5419.9492187500 kl_val: -4.6299643517 mse_val: 0.0285260454 acc_val: 0.7436781609 time: 3.2506s
Best model so far, saving...
Epoch: 0069 nll_train: 1328.6389160156 kl_train: -4.6220526695 mse_train: 0.0069928358 acc_train: 0.1551724138 nll_val: 5403.8510742188 kl_val: -4.6307044029 mse_val: 0.0284413230 acc_val: 0.7436781609 time: 3.3197s
Best model so far, saving...
Epoch: 0070 nll_train: 1326.4103393555 kl_train: -4.6219987869 mse_train: 0.0069811074 acc_train: 0.1557471264 nll_val: 5398.9580078125 kl_val: -4.6314182281 mse_val: 0.0284155663 acc_val: 0.7459770115 time: 3.2875s
Best model so far, saving...
Epoch: 0071 nll_train: 1325.2765808105 kl_train: -4.6219313145 mse_train: 0.0069751400 acc_train: 0.1563218391 nll_val: 5390.0253906250 kl_val: -4.6320538521 mse_val: 0.0283685531 acc_val: 0.7471264368 time: 3.2985s
Best model so far, saving...
Epoch: 0072 nll_train: 1326.4713439941 kl_train: -4.6218423843 mse_train: 0.0069814274 acc_train: 0.1563218391 nll_val: 5395.1474609375 kl_val: -4.6327900887 mse_val: 0.0283955093 acc_val: 0.7471264368 time: 3.3554s
Epoch: 0073 nll_train: 1326.3566894531 kl_train: -4.6217219830 mse_train: 0.0069808247 acc_train: 0.1568965517 nll_val: 5392.1025390625 kl_val: -4.6335334778 mse_val: 0.0283794869 acc_val: 0.7482758621 time: 3.2765s
Epoch: 0074 nll_train: 1324.5132141113 kl_train: -4.6216042042 mse_train: 0.0069711219 acc_train: 0.1563218391 nll_val: 5380.8037109375 kl_val: -4.6342735291 mse_val: 0.0283200219 acc_val: 0.7482758621 time: 3.2773s
Best model so far, saving...
Epoch: 0075 nll_train: 1324.3190917969 kl_train: -4.6214747429 mse_train: 0.0069700999 acc_train: 0.1568965517 nll_val: 5387.2158203125 kl_val: -4.6349849701 mse_val: 0.0283537656 acc_val: 0.7482758621 time: 3.3408s
Epoch: 0076 nll_train: 1324.3901672363 kl_train: -4.6213757992 mse_train: 0.0069704747 acc_train: 0.1586206897 nll_val: 5374.2963867188 kl_val: -4.6356072426 mse_val: 0.0282857697 acc_val: 0.7482758621 time: 3.3782s
Best model so far, saving...
Epoch: 0077 nll_train: 1323.7461547852 kl_train: -4.6212975979 mse_train: 0.0069670848 acc_train: 0.1586206897 nll_val: 5367.3925781250 kl_val: -4.6362671852 mse_val: 0.0282494370 acc_val: 0.7517241379 time: 3.3329s
Best model so far, saving...
Epoch: 0078 nll_train: 1323.2580261230 kl_train: -4.6212332249 mse_train: 0.0069645159 acc_train: 0.1591954023 nll_val: 5377.5239257812 kl_val: -4.6371178627 mse_val: 0.0283027571 acc_val: 0.7517241379 time: 3.1583s
Epoch: 0079 nll_train: 1323.1472167969 kl_train: -4.6212434769 mse_train: 0.0069639325 acc_train: 0.1591954023 nll_val: 5369.5541992188 kl_val: -4.6381874084 mse_val: 0.0282608084 acc_val: 0.7505747126 time: 3.0578s
Epoch: 0080 nll_train: 1321.5770874023 kl_train: -4.6212873459 mse_train: 0.0069556693 acc_train: 0.1597701149 nll_val: 5372.4794921875 kl_val: -4.6393246651 mse_val: 0.0282762032 acc_val: 0.7505747126 time: 3.0422s
Epoch: 0081 nll_train: 1321.6832885742 kl_train: -4.6213665009 mse_train: 0.0069562276 acc_train: 0.1603448276 nll_val: 5373.9345703125 kl_val: -4.6402745247 mse_val: 0.0282838643 acc_val: 0.7517241379 time: 3.5987s
Epoch: 0082 nll_train: 1321.7652282715 kl_train: -4.6214396954 mse_train: 0.0069566593 acc_train: 0.1603448276 nll_val: 5364.6718750000 kl_val: -4.6411108971 mse_val: 0.0282351132 acc_val: 0.7517241379 time: 4.1661s
Best model so far, saving...
Epoch: 0083 nll_train: 1321.5812377930 kl_train: -4.6215083599 mse_train: 0.0069556911 acc_train: 0.1603448276 nll_val: 5364.5834960938 kl_val: -4.6418519020 mse_val: 0.0282346476 acc_val: 0.7517241379 time: 4.1082s
Best model so far, saving...
Epoch: 0084 nll_train: 1320.6135559082 kl_train: -4.6215815544 mse_train: 0.0069505970 acc_train: 0.1609195402 nll_val: 5366.9389648438 kl_val: -4.6428055763 mse_val: 0.0282470472 acc_val: 0.7517241379 time: 4.0760s
Epoch: 0085 nll_train: 1320.9169006348 kl_train: -4.6216800213 mse_train: 0.0069521939 acc_train: 0.1614942529 nll_val: 5348.6582031250 kl_val: -4.6435856819 mse_val: 0.0281508341 acc_val: 0.7528735632 time: 4.1377s
Best model so far, saving...
Epoch: 0086 nll_train: 1320.4501037598 kl_train: -4.6217484474 mse_train: 0.0069497372 acc_train: 0.1614942529 nll_val: 5354.6474609375 kl_val: -4.6443490982 mse_val: 0.0281823520 acc_val: 0.7551724138 time: 4.0851s
Epoch: 0087 nll_train: 1319.8029174805 kl_train: -4.6218276024 mse_train: 0.0069463305 acc_train: 0.1614942529 nll_val: 5345.7714843750 kl_val: -4.6451430321 mse_val: 0.0281356387 acc_val: 0.7563218391 time: 6.2790s
Best model so far, saving...
Epoch: 0088 nll_train: 1319.4893188477 kl_train: -4.6219084263 mse_train: 0.0069446799 acc_train: 0.1620689655 nll_val: 5359.5795898438 kl_val: -4.6460409164 mse_val: 0.0282083116 acc_val: 0.7563218391 time: 4.9626s
Epoch: 0089 nll_train: 1318.4265441895 kl_train: -4.6220090389 mse_train: 0.0069390866 acc_train: 0.1632183908 nll_val: 5336.1054687500 kl_val: -4.6469478607 mse_val: 0.0280847661 acc_val: 0.7574712644 time: 6.4316s
Best model so far, saving...
Epoch: 0090 nll_train: 1318.0724792480 kl_train: -4.6220901012 mse_train: 0.0069372230 acc_train: 0.1655172414 nll_val: 5355.7763671875 kl_val: -4.6477212906 mse_val: 0.0281882938 acc_val: 0.7574712644 time: 4.9616s
Epoch: 0091 nll_train: 1317.6822204590 kl_train: -4.6221797466 mse_train: 0.0069351691 acc_train: 0.1660919540 nll_val: 5345.0151367188 kl_val: -4.6484479904 mse_val: 0.0281316545 acc_val: 0.7574712644 time: 4.0336s
Epoch: 0092 nll_train: 1317.4729614258 kl_train: -4.6222820282 mse_train: 0.0069340677 acc_train: 0.1666666667 nll_val: 5338.5405273438 kl_val: -4.6492362022 mse_val: 0.0280975811 acc_val: 0.7563218391 time: 3.7805s
Epoch: 0093 nll_train: 1316.6020812988 kl_train: -4.6224119663 mse_train: 0.0069294845 acc_train: 0.1660919540 nll_val: 5358.7617187500 kl_val: -4.6499218941 mse_val: 0.0282040033 acc_val: 0.7563218391 time: 4.8775s
Epoch: 0094 nll_train: 1316.3049316406 kl_train: -4.6225805283 mse_train: 0.0069279211 acc_train: 0.1678160920 nll_val: 5337.8911132812 kl_val: -4.6505947113 mse_val: 0.0280941632 acc_val: 0.7574712644 time: 4.1221s
Epoch: 0095 nll_train: 1314.7643737793 kl_train: -4.6227996349 mse_train: 0.0069198116 acc_train: 0.1689655172 nll_val: 5340.9213867188 kl_val: -4.6511216164 mse_val: 0.0281101093 acc_val: 0.7574712644 time: 3.8674s
Epoch: 0096 nll_train: 1317.2309265137 kl_train: -4.6230318546 mse_train: 0.0069327935 acc_train: 0.1683908046 nll_val: 5353.7099609375 kl_val: -4.6513757706 mse_val: 0.0281774178 acc_val: 0.7586206897 time: 4.1792s
Epoch: 0097 nll_train: 1316.1305847168 kl_train: -4.6232275963 mse_train: 0.0069270022 acc_train: 0.1678160920 nll_val: 5343.5087890625 kl_val: -4.6516466141 mse_val: 0.0281237308 acc_val: 0.7586206897 time: 4.1169s
Epoch: 0098 nll_train: 1314.2629089355 kl_train: -4.6234560013 mse_train: 0.0069171730 acc_train: 0.1683908046 nll_val: 5329.0429687500 kl_val: -4.6518135071 mse_val: 0.0280475914 acc_val: 0.7586206897 time: 4.0570s
Best model so far, saving...
Epoch: 0099 nll_train: 1316.6199951172 kl_train: -4.6236627102 mse_train: 0.0069295783 acc_train: 0.1689655172 nll_val: 5349.2260742188 kl_val: -4.6520738602 mse_val: 0.0281538200 acc_val: 0.7586206897 time: 4.0536s
Epoch: 0100 nll_train: 1316.6533203125 kl_train: -4.6238942146 mse_train: 0.0069297539 acc_train: 0.1695402299 nll_val: 5325.6928710938 kl_val: -4.6523742676 mse_val: 0.0280299634 acc_val: 0.7574712644 time: 3.6838s
Best model so far, saving...
Epoch: 0101 nll_train: 1314.8189086914 kl_train: -4.6241922379 mse_train: 0.0069200988 acc_train: 0.1701149425 nll_val: 5333.7285156250 kl_val: -4.6526088715 mse_val: 0.0280722547 acc_val: 0.7574712644 time: 4.0383s
Epoch: 0102 nll_train: 1314.0498962402 kl_train: -4.6245033741 mse_train: 0.0069160520 acc_train: 0.1695402299 nll_val: 5329.6650390625 kl_val: -4.6527566910 mse_val: 0.0280508660 acc_val: 0.7574712644 time: 3.9525s
Epoch: 0103 nll_train: 1315.6779785156 kl_train: -4.6248054504 mse_train: 0.0069246205 acc_train: 0.1724137931 nll_val: 5325.8891601562 kl_val: -4.6528058052 mse_val: 0.0280309916 acc_val: 0.7551724138 time: 3.9791s
Epoch: 0104 nll_train: 1314.2099304199 kl_train: -4.6251008511 mse_train: 0.0069168940 acc_train: 0.1741379310 nll_val: 5329.1240234375 kl_val: -4.6526970863 mse_val: 0.0280480143 acc_val: 0.7551724138 time: 3.6928s
Epoch: 0105 nll_train: 1313.3207702637 kl_train: -4.6253130436 mse_train: 0.0069122136 acc_train: 0.1752873563 nll_val: 5322.7695312500 kl_val: -4.6525750160 mse_val: 0.0280145723 acc_val: 0.7551724138 time: 4.3841s
Best model so far, saving...
Epoch: 0106 nll_train: 1312.8680725098 kl_train: -4.6255249977 mse_train: 0.0069098321 acc_train: 0.1747126437 nll_val: 5327.1201171875 kl_val: -4.6524405479 mse_val: 0.0280374736 acc_val: 0.7551724138 time: 5.5631s
Epoch: 0107 nll_train: 1313.9707336426 kl_train: -4.6257576942 mse_train: 0.0069156351 acc_train: 0.1758620690 nll_val: 5316.9614257812 kl_val: -4.6525187492 mse_val: 0.0279840045 acc_val: 0.7551724138 time: 4.0649s
Best model so far, saving...
Epoch: 0108 nll_train: 1313.2027587891 kl_train: -4.6260526180 mse_train: 0.0069115927 acc_train: 0.1752873563 nll_val: 5324.1289062500 kl_val: -4.6526665688 mse_val: 0.0280217305 acc_val: 0.7563218391 time: 3.8651s
Epoch: 0109 nll_train: 1311.6248168945 kl_train: -4.6264088154 mse_train: 0.0069032887 acc_train: 0.1752873563 nll_val: 5326.4721679688 kl_val: -4.6528792381 mse_val: 0.0280340612 acc_val: 0.7586206897 time: 3.7501s
Epoch: 0110 nll_train: 1312.6383361816 kl_train: -4.6267945766 mse_train: 0.0069086222 acc_train: 0.1752873563 nll_val: 5330.6367187500 kl_val: -4.6531500816 mse_val: 0.0280559808 acc_val: 0.7586206897 time: 4.3671s
Epoch: 0111 nll_train: 1311.4504699707 kl_train: -4.6272361279 mse_train: 0.0069023705 acc_train: 0.1764367816 nll_val: 5323.9624023438 kl_val: -4.6533350945 mse_val: 0.0280208513 acc_val: 0.7597701149 time: 4.0606s
Epoch: 0112 nll_train: 1311.3414916992 kl_train: -4.6276690960 mse_train: 0.0069017973 acc_train: 0.1758620690 nll_val: 5309.9868164062 kl_val: -4.6536169052 mse_val: 0.0279472955 acc_val: 0.7620689655 time: 3.8114s
Best model so far, saving...
Epoch: 0113 nll_train: 1311.0601196289 kl_train: -4.6281399727 mse_train: 0.0069003166 acc_train: 0.1764367816 nll_val: 5339.0249023438 kl_val: -4.6537952423 mse_val: 0.0281001292 acc_val: 0.7643678161 time: 4.2225s
Epoch: 0114 nll_train: 1311.2836608887 kl_train: -4.6285886765 mse_train: 0.0069014931 acc_train: 0.1764367816 nll_val: 5323.5249023438 kl_val: -4.6539177895 mse_val: 0.0280185509 acc_val: 0.7666666667 time: 3.9677s
Epoch: 0115 nll_train: 1311.0291442871 kl_train: -4.6290431023 mse_train: 0.0069001524 acc_train: 0.1775862069 nll_val: 5323.6728515625 kl_val: -4.6539964676 mse_val: 0.0280193295 acc_val: 0.7666666667 time: 4.0140s
Epoch: 0116 nll_train: 1310.8460693359 kl_train: -4.6295118332 mse_train: 0.0068991892 acc_train: 0.1775862069 nll_val: 5324.0747070312 kl_val: -4.6540989876 mse_val: 0.0280214418 acc_val: 0.7678160920 time: 4.0517s
Epoch: 0117 nll_train: 1310.2301635742 kl_train: -4.6300101280 mse_train: 0.0068959480 acc_train: 0.1775862069 nll_val: 5312.2719726562 kl_val: -4.6541085243 mse_val: 0.0279593281 acc_val: 0.7712643678 time: 4.8471s
Epoch: 0118 nll_train: 1309.3454895020 kl_train: -4.6305425167 mse_train: 0.0068912916 acc_train: 0.1775862069 nll_val: 5330.2578125000 kl_val: -4.6540656090 mse_val: 0.0280539878 acc_val: 0.7712643678 time: 3.7185s
Epoch: 0119 nll_train: 1309.5518188477 kl_train: -4.6310777664 mse_train: 0.0068923775 acc_train: 0.1775862069 nll_val: 5326.2231445312 kl_val: -4.6540465355 mse_val: 0.0280327536 acc_val: 0.7712643678 time: 3.7237s
Epoch: 0120 nll_train: 1309.8775634766 kl_train: -4.6316714287 mse_train: 0.0068940922 acc_train: 0.1775862069 nll_val: 5320.4082031250 kl_val: -4.6540536880 mse_val: 0.0280021485 acc_val: 0.7724137931 time: 3.7494s
Epoch: 0121 nll_train: 1307.5443420410 kl_train: -4.6323323250 mse_train: 0.0068818118 acc_train: 0.1770114943 nll_val: 5328.5913085938 kl_val: -4.6539406776 mse_val: 0.0280452128 acc_val: 0.7724137931 time: 3.7462s
Epoch: 0122 nll_train: 1309.5587158203 kl_train: -4.6330294609 mse_train: 0.0068924135 acc_train: 0.1781609195 nll_val: 5326.6674804688 kl_val: -4.6539082527 mse_val: 0.0280350912 acc_val: 0.7724137931 time: 4.6926s
Epoch: 0123 nll_train: 1308.8403015137 kl_train: -4.6337652206 mse_train: 0.0068886335 acc_train: 0.1787356322 nll_val: 5314.5092773438 kl_val: -4.6538357735 mse_val: 0.0279711038 acc_val: 0.7735632184 time: 4.0552s
Epoch: 0124 nll_train: 1306.9064636230 kl_train: -4.6345365047 mse_train: 0.0068784545 acc_train: 0.1787356322 nll_val: 5327.4458007812 kl_val: -4.6538538933 mse_val: 0.0280391891 acc_val: 0.7724137931 time: 3.6826s
Epoch: 0125 nll_train: 1307.9261474609 kl_train: -4.6353321075 mse_train: 0.0068838218 acc_train: 0.1793103448 nll_val: 5330.5576171875 kl_val: -4.6537246704 mse_val: 0.0280555673 acc_val: 0.7724137931 time: 4.9995s
Epoch: 0126 nll_train: 1308.0067443848 kl_train: -4.6361145973 mse_train: 0.0068842458 acc_train: 0.1781609195 nll_val: 5322.7070312500 kl_val: -4.6535768509 mse_val: 0.0280142482 acc_val: 0.7735632184 time: 4.3157s
Epoch: 0127 nll_train: 1307.2567443848 kl_train: -4.6369121075 mse_train: 0.0068802983 acc_train: 0.1764367816 nll_val: 5342.5834960938 kl_val: -4.6533865929 mse_val: 0.0281188581 acc_val: 0.7735632184 time: 3.9743s
Epoch: 0128 nll_train: 1307.3948974609 kl_train: -4.6377375126 mse_train: 0.0068810260 acc_train: 0.1764367816 nll_val: 5322.1494140625 kl_val: -4.6531963348 mse_val: 0.0280113090 acc_val: 0.7735632184 time: 3.5959s
Epoch: 0129 nll_train: 1308.6118774414 kl_train: -4.6385619640 mse_train: 0.0068874303 acc_train: 0.1764367816 nll_val: 5340.4589843750 kl_val: -4.6530203819 mse_val: 0.0281076785 acc_val: 0.7735632184 time: 3.9642s
Epoch: 0130 nll_train: 1307.0851745605 kl_train: -4.6393876076 mse_train: 0.0068793952 acc_train: 0.1764367816 nll_val: 5337.3745117188 kl_val: -4.6528759003 mse_val: 0.0280914437 acc_val: 0.7735632184 time: 3.6345s
Epoch: 0131 nll_train: 1304.5414428711 kl_train: -4.6402297020 mse_train: 0.0068660070 acc_train: 0.1758620690 nll_val: 5332.8051757812 kl_val: -4.6527333260 mse_val: 0.0280673932 acc_val: 0.7747126437 time: 3.8141s
Epoch: 0132 nll_train: 1304.8306274414 kl_train: -4.6410920620 mse_train: 0.0068675290 acc_train: 0.1770114943 nll_val: 5341.4257812500 kl_val: -4.6524767876 mse_val: 0.0281127673 acc_val: 0.7747126437 time: 3.8549s
Epoch: 0133 nll_train: 1305.4022521973 kl_train: -4.6419641972 mse_train: 0.0068705379 acc_train: 0.1764367816 nll_val: 5335.9384765625 kl_val: -4.6522479057 mse_val: 0.0280838888 acc_val: 0.7758620690 time: 4.5059s
Epoch: 0134 nll_train: 1303.6861572266 kl_train: -4.6428372860 mse_train: 0.0068615056 acc_train: 0.1770114943 nll_val: 5330.6059570312 kl_val: -4.6521759033 mse_val: 0.0280558169 acc_val: 0.7770114943 time: 3.9752s
Epoch: 0135 nll_train: 1303.2426757812 kl_train: -4.6437628269 mse_train: 0.0068591718 acc_train: 0.1764367816 nll_val: 5334.9384765625 kl_val: -4.6521563530 mse_val: 0.0280786231 acc_val: 0.7770114943 time: 3.4400s
Epoch: 0136 nll_train: 1303.7824707031 kl_train: -4.6447126865 mse_train: 0.0068620123 acc_train: 0.1764367816 nll_val: 5347.7695312500 kl_val: -4.6521949768 mse_val: 0.0281461496 acc_val: 0.7770114943 time: 3.6328s
Epoch: 0137 nll_train: 1302.4551086426 kl_train: -4.6457092762 mse_train: 0.0068550260 acc_train: 0.1770114943 nll_val: 5334.9062500000 kl_val: -4.6521306038 mse_val: 0.0280784536 acc_val: 0.7793103448 time: 3.7253s
Epoch: 0138 nll_train: 1303.7710876465 kl_train: -4.6467189789 mse_train: 0.0068619528 acc_train: 0.1775862069 nll_val: 5333.8759765625 kl_val: -4.6519989967 mse_val: 0.0280730296 acc_val: 0.7793103448 time: 3.7426s
Epoch: 0139 nll_train: 1303.8573303223 kl_train: -4.6477344036 mse_train: 0.0068624066 acc_train: 0.1775862069 nll_val: 5342.8525390625 kl_val: -4.6519079208 mse_val: 0.0281202756 acc_val: 0.7816091954 time: 3.7726s
Epoch: 0140 nll_train: 1302.3167114258 kl_train: -4.6487984657 mse_train: 0.0068542983 acc_train: 0.1770114943 nll_val: 5351.2138671875 kl_val: -4.6518225670 mse_val: 0.0281642806 acc_val: 0.7804597701 time: 3.9586s
Epoch: 0141 nll_train: 1302.5030212402 kl_train: -4.6498599052 mse_train: 0.0068552784 acc_train: 0.1764367816 nll_val: 5330.9892578125 kl_val: -4.6517882347 mse_val: 0.0280578341 acc_val: 0.7804597701 time: 4.2945s
Epoch: 0142 nll_train: 1302.6725463867 kl_train: -4.6510128975 mse_train: 0.0068561715 acc_train: 0.1764367816 nll_val: 5335.4946289062 kl_val: -4.6516962051 mse_val: 0.0280815475 acc_val: 0.7804597701 time: 3.6768s
Epoch: 0143 nll_train: 1302.6617431641 kl_train: -4.6521887779 mse_train: 0.0068561145 acc_train: 0.1764367816 nll_val: 5345.4335937500 kl_val: -4.6515069008 mse_val: 0.0281338580 acc_val: 0.7804597701 time: 3.8041s
Epoch: 0144 nll_train: 1302.0977478027 kl_train: -4.6533837318 mse_train: 0.0068531460 acc_train: 0.1764367816 nll_val: 5337.9042968750 kl_val: -4.6513080597 mse_val: 0.0280942302 acc_val: 0.7793103448 time: 4.1038s
Epoch: 0145 nll_train: 1302.9241638184 kl_train: -4.6546225548 mse_train: 0.0068574956 acc_train: 0.1770114943 nll_val: 5333.4858398438 kl_val: -4.6513237953 mse_val: 0.0280709751 acc_val: 0.7793103448 time: 4.4024s
Epoch: 0146 nll_train: 1301.9936218262 kl_train: -4.6559293270 mse_train: 0.0068525970 acc_train: 0.1764367816 nll_val: 5346.3046875000 kl_val: -4.6512465477 mse_val: 0.0281384438 acc_val: 0.7793103448 time: 4.0599s
Epoch: 0147 nll_train: 1300.9301452637 kl_train: -4.6572246552 mse_train: 0.0068470002 acc_train: 0.1758620690 nll_val: 5334.9223632812 kl_val: -4.6511840820 mse_val: 0.0280785374 acc_val: 0.7781609195 time: 4.8959s
Epoch: 0148 nll_train: 1302.8148498535 kl_train: -4.6585419178 mse_train: 0.0068569201 acc_train: 0.1741379310 nll_val: 5338.5703125000 kl_val: -4.6510791779 mse_val: 0.0280977357 acc_val: 0.7781609195 time: 3.9861s
Epoch: 0149 nll_train: 1302.2463989258 kl_train: -4.6598646641 mse_train: 0.0068539280 acc_train: 0.1747126437 nll_val: 5350.5888671875 kl_val: -4.6510219574 mse_val: 0.0281609911 acc_val: 0.7781609195 time: 3.8723s
Epoch: 0150 nll_train: 1301.1365356445 kl_train: -4.6612310410 mse_train: 0.0068480868 acc_train: 0.1741379310 nll_val: 5351.6835937500 kl_val: -4.6510472298 mse_val: 0.0281667542 acc_val: 0.7770114943 time: 3.6380s
Epoch: 0151 nll_train: 1302.0435791016 kl_train: -4.6626548767 mse_train: 0.0068528603 acc_train: 0.1747126437 nll_val: 5346.3217773438 kl_val: -4.6510381699 mse_val: 0.0281385332 acc_val: 0.7770114943 time: 3.7193s
Epoch: 0152 nll_train: 1301.6982421875 kl_train: -4.6641080379 mse_train: 0.0068510427 acc_train: 0.1747126437 nll_val: 5338.6459960938 kl_val: -4.6508874893 mse_val: 0.0280981343 acc_val: 0.7781609195 time: 3.8752s
Epoch: 0153 nll_train: 1300.2586364746 kl_train: -4.6655423641 mse_train: 0.0068434664 acc_train: 0.1758620690 nll_val: 5353.0405273438 kl_val: -4.6506395340 mse_val: 0.0281738956 acc_val: 0.7781609195 time: 3.8718s
Epoch: 0154 nll_train: 1301.5746459961 kl_train: -4.6670014858 mse_train: 0.0068503926 acc_train: 0.1764367816 nll_val: 5340.1108398438 kl_val: -4.6505255699 mse_val: 0.0281058457 acc_val: 0.7793103448 time: 3.4415s
Epoch: 0155 nll_train: 1302.1809692383 kl_train: -4.6685116291 mse_train: 0.0068535837 acc_train: 0.1764367816 nll_val: 5354.5424804688 kl_val: -4.6504230499 mse_val: 0.0281818025 acc_val: 0.7781609195 time: 3.5050s
Epoch: 0156 nll_train: 1300.1026306152 kl_train: -4.6700413227 mse_train: 0.0068426455 acc_train: 0.1758620690 nll_val: 5361.8037109375 kl_val: -4.6503515244 mse_val: 0.0282200146 acc_val: 0.7793103448 time: 3.6389s
Epoch: 0157 nll_train: 1301.4963073730 kl_train: -4.6716048717 mse_train: 0.0068499806 acc_train: 0.1752873563 nll_val: 5347.5214843750 kl_val: -4.6504135132 mse_val: 0.0281448476 acc_val: 0.7793103448 time: 3.6120s
Epoch: 0158 nll_train: 1300.4233093262 kl_train: -4.6732785702 mse_train: 0.0068443331 acc_train: 0.1758620690 nll_val: 5346.8676757812 kl_val: -4.6505184174 mse_val: 0.0281414054 acc_val: 0.7804597701 time: 3.4431s
Epoch: 0159 nll_train: 1299.9202575684 kl_train: -4.6749854088 mse_train: 0.0068416852 acc_train: 0.1752873563 nll_val: 5351.3081054688 kl_val: -4.6505599022 mse_val: 0.0281647798 acc_val: 0.7839080460 time: 3.3644s
Epoch: 0160 nll_train: 1301.4504089355 kl_train: -4.6766309738 mse_train: 0.0068497395 acc_train: 0.1752873563 nll_val: 5349.5639648438 kl_val: -4.6505870819 mse_val: 0.0281556007 acc_val: 0.7839080460 time: 3.4352s
Epoch: 0161 nll_train: 1299.6781616211 kl_train: -4.6783351898 mse_train: 0.0068404108 acc_train: 0.1741379310 nll_val: 5342.1352539062 kl_val: -4.6506228447 mse_val: 0.0281164981 acc_val: 0.7839080460 time: 3.3358s
Epoch: 0162 nll_train: 1300.0550231934 kl_train: -4.6801166534 mse_train: 0.0068423948 acc_train: 0.1735632184 nll_val: 5342.7343750000 kl_val: -4.6505937576 mse_val: 0.0281196572 acc_val: 0.7839080460 time: 3.4152s
Epoch: 0163 nll_train: 1300.3782958984 kl_train: -4.6819183826 mse_train: 0.0068440968 acc_train: 0.1741379310 nll_val: 5347.6220703125 kl_val: -4.6504421234 mse_val: 0.0281453747 acc_val: 0.7850574713 time: 3.4815s
Epoch: 0164 nll_train: 1299.8288574219 kl_train: -4.6837067604 mse_train: 0.0068412041 acc_train: 0.1741379310 nll_val: 5356.1791992188 kl_val: -4.6501998901 mse_val: 0.0281904154 acc_val: 0.7850574713 time: 3.7078s
Epoch: 0165 nll_train: 1299.4982604980 kl_train: -4.6854715347 mse_train: 0.0068394643 acc_train: 0.1741379310 nll_val: 5358.8666992188 kl_val: -4.6500821114 mse_val: 0.0282045640 acc_val: 0.7839080460 time: 3.7467s
Epoch: 0166 nll_train: 1300.7144165039 kl_train: -4.6872696877 mse_train: 0.0068458648 acc_train: 0.1747126437 nll_val: 5351.8833007812 kl_val: -4.6499991417 mse_val: 0.0281678047 acc_val: 0.7850574713 time: 3.5827s
Epoch: 0167 nll_train: 1298.8978881836 kl_train: -4.6891007423 mse_train: 0.0068363044 acc_train: 0.1747126437 nll_val: 5351.7148437500 kl_val: -4.6500401497 mse_val: 0.0281669181 acc_val: 0.7850574713 time: 3.4590s
Epoch: 0168 nll_train: 1299.4624633789 kl_train: -4.6909959316 mse_train: 0.0068392759 acc_train: 0.1752873563 nll_val: 5343.8940429688 kl_val: -4.6501169205 mse_val: 0.0281257536 acc_val: 0.7862068966 time: 3.3139s
Epoch: 0169 nll_train: 1300.0961914062 kl_train: -4.6929113865 mse_train: 0.0068426111 acc_train: 0.1758620690 nll_val: 5359.8046875000 kl_val: -4.6503100395 mse_val: 0.0282094963 acc_val: 0.7862068966 time: 3.3316s
Epoch: 0170 nll_train: 1298.5947875977 kl_train: -4.6948547363 mse_train: 0.0068347086 acc_train: 0.1752873563 nll_val: 5385.4736328125 kl_val: -4.6505150795 mse_val: 0.0283445939 acc_val: 0.7885057471 time: 3.4156s
Epoch: 0171 nll_train: 1349.0081481934 kl_train: -4.6968438625 mse_train: 0.0071000431 acc_train: 0.1747126437 nll_val: 5356.1284179688 kl_val: -4.6512060165 mse_val: 0.0281901490 acc_val: 0.7885057471 time: 3.5413s
Epoch: 0172 nll_train: 1280.1911315918 kl_train: -4.6991858482 mse_train: 0.0067378475 acc_train: 0.1729885057 nll_val: 5363.8701171875 kl_val: -4.6518616676 mse_val: 0.0282308981 acc_val: 0.7885057471 time: 3.4680s
Epoch: 0173 nll_train: 1307.6161499023 kl_train: -4.7014584541 mse_train: 0.0068821899 acc_train: 0.1735632184 nll_val: 5350.2104492188 kl_val: -4.6521177292 mse_val: 0.0281590018 acc_val: 0.7896551724 time: 4.4380s
Epoch: 0174 nll_train: 1314.9645996094 kl_train: -4.7036230564 mse_train: 0.0069208664 acc_train: 0.1735632184 nll_val: 5344.0234375000 kl_val: -4.6523528099 mse_val: 0.0281264354 acc_val: 0.7896551724 time: 3.7444s
Epoch: 0175 nll_train: 1282.9019165039 kl_train: -4.7059252262 mse_train: 0.0067521153 acc_train: 0.1729885057 nll_val: 5364.6577148438 kl_val: -4.6526665688 mse_val: 0.0282350369 acc_val: 0.7896551724 time: 3.7711s
Epoch: 0176 nll_train: 1308.2580261230 kl_train: -4.7082257271 mse_train: 0.0068855681 acc_train: 0.1724137931 nll_val: 5355.8510742188 kl_val: -4.6529464722 mse_val: 0.0281886887 acc_val: 0.7896551724 time: 3.6436s
Epoch: 0177 nll_train: 1307.5781860352 kl_train: -4.7105605602 mse_train: 0.0068819900 acc_train: 0.1718390805 nll_val: 5365.4614257812 kl_val: -4.6531777382 mse_val: 0.0282392688 acc_val: 0.7896551724 time: 3.6048s
Epoch: 0178 nll_train: 1287.3604125977 kl_train: -4.7129213810 mse_train: 0.0067755803 acc_train: 0.1706896552 nll_val: 5370.4565429688 kl_val: -4.6534185410 mse_val: 0.0282655600 acc_val: 0.7908045977 time: 3.8079s
Epoch: 0179 nll_train: 1307.6238708496 kl_train: -4.7152440548 mse_train: 0.0068822313 acc_train: 0.1695402299 nll_val: 5358.1958007812 kl_val: -4.6535453796 mse_val: 0.0282010287 acc_val: 0.7942528736 time: 4.6776s
Epoch: 0180 nll_train: 1302.3809509277 kl_train: -4.7175123692 mse_train: 0.0068546366 acc_train: 0.1689655172 nll_val: 5362.7309570312 kl_val: -4.6536164284 mse_val: 0.0282248985 acc_val: 0.7931034483 time: 4.0591s
Epoch: 0181 nll_train: 1291.5266418457 kl_train: -4.7197906971 mse_train: 0.0067975086 acc_train: 0.1689655172 nll_val: 5375.6611328125 kl_val: -4.6535215378 mse_val: 0.0282929558 acc_val: 0.7931034483 time: 3.7989s
Epoch: 0182 nll_train: 1306.9273986816 kl_train: -4.7219741344 mse_train: 0.0068785647 acc_train: 0.1678160920 nll_val: 5361.0825195312 kl_val: -4.6534204483 mse_val: 0.0282162223 acc_val: 0.7931034483 time: 3.7365s
Epoch: 0183 nll_train: 1296.9348144531 kl_train: -4.7241854668 mse_train: 0.0068259723 acc_train: 0.1683908046 nll_val: 5364.0351562500 kl_val: -4.6536426544 mse_val: 0.0282317605 acc_val: 0.7931034483 time: 3.5989s
Epoch: 0184 nll_train: 1294.1294555664 kl_train: -4.7264590263 mse_train: 0.0068112080 acc_train: 0.1683908046 nll_val: 5369.1386718750 kl_val: -4.6538290977 mse_val: 0.0282586236 acc_val: 0.7931034483 time: 4.0917s
Epoch: 0185 nll_train: 1305.1362915039 kl_train: -4.7286849022 mse_train: 0.0068691386 acc_train: 0.1683908046 nll_val: 5365.0507812500 kl_val: -4.6540894508 mse_val: 0.0282371119 acc_val: 0.7931034483 time: 4.0154s
Epoch: 0186 nll_train: 1295.1371154785 kl_train: -4.7309956551 mse_train: 0.0068165107 acc_train: 0.1683908046 nll_val: 5375.3037109375 kl_val: -4.6546096802 mse_val: 0.0282910671 acc_val: 0.7931034483 time: 3.8975s
Epoch: 0187 nll_train: 1297.8365783691 kl_train: -4.7334270477 mse_train: 0.0068307190 acc_train: 0.1666666667 nll_val: 5380.5629882812 kl_val: -4.6550302505 mse_val: 0.0283187497 acc_val: 0.7942528736 time: 3.8736s
Epoch: 0188 nll_train: 1300.8694763184 kl_train: -4.7358517647 mse_train: 0.0068466808 acc_train: 0.1672413793 nll_val: 5370.9580078125 kl_val: -4.6555681229 mse_val: 0.0282681976 acc_val: 0.7942528736 time: 3.7224s
Epoch: 0189 nll_train: 1292.7864379883 kl_train: -4.7383763790 mse_train: 0.0068041394 acc_train: 0.1672413793 nll_val: 5372.1840820312 kl_val: -4.6562013626 mse_val: 0.0282746516 acc_val: 0.7942528736 time: 3.7187s
Epoch: 0190 nll_train: 1297.6290893555 kl_train: -4.7409362793 mse_train: 0.0068296259 acc_train: 0.1660919540 nll_val: 5373.4916992188 kl_val: -4.6567716599 mse_val: 0.0282815378 acc_val: 0.7942528736 time: 4.0383s
Epoch: 0191 nll_train: 1298.6565551758 kl_train: -4.7434930801 mse_train: 0.0068350347 acc_train: 0.1655172414 nll_val: 5375.3911132812 kl_val: -4.6574192047 mse_val: 0.0282915346 acc_val: 0.7954022989 time: 4.5022s
Epoch: 0192 nll_train: 1293.4488220215 kl_train: -4.7461135387 mse_train: 0.0068076249 acc_train: 0.1649425287 nll_val: 5368.0595703125 kl_val: -4.6580071449 mse_val: 0.0282529425 acc_val: 0.7965517241 time: 4.0823s
Epoch: 0193 nll_train: 1298.5577087402 kl_train: -4.7486898899 mse_train: 0.0068345143 acc_train: 0.1637931034 nll_val: 5366.7807617188 kl_val: -4.6586108208 mse_val: 0.0282462128 acc_val: 0.7965517241 time: 4.7270s
Epoch: 0194 nll_train: 1296.3216247559 kl_train: -4.7513036728 mse_train: 0.0068227449 acc_train: 0.1620689655 nll_val: 5371.7104492188 kl_val: -4.6592273712 mse_val: 0.0282721594 acc_val: 0.7954022989 time: 4.0039s
Epoch: 0195 nll_train: 1294.3507080078 kl_train: -4.7539517879 mse_train: 0.0068123713 acc_train: 0.1614942529 nll_val: 5370.8930664062 kl_val: -4.6597552299 mse_val: 0.0282678567 acc_val: 0.7954022989 time: 3.6798s
Epoch: 0196 nll_train: 1297.8187255859 kl_train: -4.7565486431 mse_train: 0.0068306249 acc_train: 0.1614942529 nll_val: 5367.1230468750 kl_val: -4.6603131294 mse_val: 0.0282480139 acc_val: 0.7942528736 time: 3.3430s
Epoch: 0197 nll_train: 1294.9037170410 kl_train: -4.7591586113 mse_train: 0.0068152824 acc_train: 0.1609195402 nll_val: 5368.2563476562 kl_val: -4.6610589027 mse_val: 0.0282539781 acc_val: 0.7942528736 time: 3.1607s
Epoch: 0198 nll_train: 1294.6228637695 kl_train: -4.7618641853 mse_train: 0.0068138042 acc_train: 0.1614942529 nll_val: 5358.1206054688 kl_val: -4.6620912552 mse_val: 0.0282006338 acc_val: 0.7942528736 time: 3.1795s
Epoch: 0199 nll_train: 1278.7863769531 kl_train: -4.7643656731 mse_train: 0.0067304542 acc_train: 0.1614942529 nll_val: 5348.2177734375 kl_val: -4.6607198715 mse_val: 0.0281485114 acc_val: 0.7942528736 time: 3.2688s
Optimization finished
Best epoch 112
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 284.1650390625 kl_test: -4.9651589394 mse_test: 0.0014956053 acc_test: 0.1367816092
MSE: [ 0.000708717213 , 0.000705331855 , 0.000682974292 , 0.000748652034 , 0.000740908145 , 0.000693752489 , 0.001200619736 , 0.001234223484 , 0.001200722763 , 0.001418189611 , 0.001433429657 , 0.001369577250 , 0.001631441293 , 0.001674205880 , 0.001871538465 , 0.001944996067 , 0.002003182657 , 0.002080376027 , 0.002131467918 ]
Accuracy for experiment id 12 is 0.1322222222222222
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 3135982.4594726562 kl_train: -11.9432358742 mse_train: 16.5051697250 acc_train: 0.4919540230 nll_val: 11911.0283203125 kl_val: -17.4521350861 mse_val: 0.0626896247 acc_val: 0.0000000000 time: 3.3192s
Best model so far, saving...
Epoch: 0001 nll_train: 51248.8769531250 kl_train: -12.1216387749 mse_train: 0.2697309107 acc_train: 0.4229885057 nll_val: 9201.6982421875 kl_val: -5.3008284569 mse_val: 0.0484299920 acc_val: 0.0000000000 time: 3.2238s
Best model so far, saving...
Epoch: 0002 nll_train: 88795.3828125000 kl_train: -7.3969297409 mse_train: 0.4673440754 acc_train: 0.4074712644 nll_val: 8792.3720703125 kl_val: -2.7730906010 mse_val: 0.0462756380 acc_val: 0.0000000000 time: 3.2655s
Best model so far, saving...
Epoch: 0003 nll_train: 83844.9531250000 kl_train: -6.2165071964 mse_train: 0.4412892014 acc_train: 0.3954022989 nll_val: 8622.4716796875 kl_val: -2.3903527260 mse_val: 0.0453814305 acc_val: 0.0000000000 time: 3.2867s
Best model so far, saving...
Epoch: 0004 nll_train: 52446.7187500000 kl_train: -5.7610983849 mse_train: 0.2760353535 acc_train: 0.3844827586 nll_val: 7787.5991210938 kl_val: -2.9398498535 mse_val: 0.0409873612 acc_val: 0.0000000000 time: 3.4588s
Best model so far, saving...
Epoch: 0005 nll_train: 8294.4483642578 kl_train: -5.5909833908 mse_train: 0.0436549913 acc_train: 0.3626436782 nll_val: 7791.6826171875 kl_val: -4.0141978264 mse_val: 0.0410088561 acc_val: 0.0000000000 time: 3.4462s
Epoch: 0006 nll_train: 20339.3310546875 kl_train: -5.5168125629 mse_train: 0.1070491038 acc_train: 0.3413793103 nll_val: 7468.9458007812 kl_val: -5.6284556389 mse_val: 0.0393102393 acc_val: 0.0000000000 time: 3.3376s
Best model so far, saving...
Epoch: 0007 nll_train: 5273.4819641113 kl_train: -5.4953951836 mse_train: 0.0277551637 acc_train: 0.3281609195 nll_val: 6401.1152343750 kl_val: -7.7154994011 mse_val: 0.0336900763 acc_val: 0.0287356322 time: 3.2835s
Best model so far, saving...
Epoch: 0008 nll_train: 4199.9644775391 kl_train: -5.4906997681 mse_train: 0.0221050768 acc_train: 0.3034482759 nll_val: 7452.3720703125 kl_val: -9.9052562714 mse_val: 0.0392230041 acc_val: 0.0333333333 time: 3.2076s
Epoch: 0009 nll_train: 8445.4204101562 kl_train: -5.4909493923 mse_train: 0.0444495790 acc_train: 0.2885057471 nll_val: 7885.0791015625 kl_val: -11.9294872284 mse_val: 0.0415004157 acc_val: 0.0344827586 time: 3.0774s
Epoch: 0010 nll_train: 5623.4428710938 kl_train: -5.5005846024 mse_train: 0.0295970654 acc_train: 0.2781609195 nll_val: 6492.4238281250 kl_val: -13.4466590881 mse_val: 0.0341706537 acc_val: 0.0367816092 time: 3.0156s
Epoch: 0011 nll_train: 1722.2430419922 kl_train: -5.5136256218 mse_train: 0.0090644365 acc_train: 0.2649425287 nll_val: 7182.7563476562 kl_val: -14.2877731323 mse_val: 0.0378039815 acc_val: 0.0632183908 time: 3.7527s
Epoch: 0012 nll_train: 2701.2297363281 kl_train: -5.5261352062 mse_train: 0.0142169972 acc_train: 0.2517241379 nll_val: 9205.5273437500 kl_val: -14.6042594910 mse_val: 0.0484501459 acc_val: 0.1218390805 time: 4.2655s
Epoch: 0013 nll_train: 3405.4044189453 kl_train: -5.5400764942 mse_train: 0.0179231812 acc_train: 0.2431034483 nll_val: 7193.8271484375 kl_val: -14.5647277832 mse_val: 0.0378622487 acc_val: 0.1701149425 time: 4.1559s
Epoch: 0014 nll_train: 1911.2914123535 kl_train: -5.5553681850 mse_train: 0.0100594275 acc_train: 0.2362068966 nll_val: 6448.5297851562 kl_val: -14.2848329544 mse_val: 0.0339396335 acc_val: 0.2333333333 time: 4.1849s
Epoch: 0015 nll_train: 2142.9896850586 kl_train: -5.5703196526 mse_train: 0.0112788938 acc_train: 0.2293103448 nll_val: 7563.0351562500 kl_val: -13.8607940674 mse_val: 0.0398054495 acc_val: 0.2724137931 time: 4.1310s
Epoch: 0016 nll_train: 2419.2991943359 kl_train: -5.5835051537 mse_train: 0.0127331540 acc_train: 0.2235632184 nll_val: 6608.0595703125 kl_val: -13.3622827530 mse_val: 0.0347792618 acc_val: 0.3034482759 time: 4.1182s
Epoch: 0017 nll_train: 1623.1448974609 kl_train: -5.5945672989 mse_train: 0.0085428683 acc_train: 0.2183908046 nll_val: 6221.3774414062 kl_val: -12.8290557861 mse_val: 0.0327440910 acc_val: 0.3264367816 time: 4.0623s
Best model so far, saving...
Epoch: 0018 nll_train: 1527.9946899414 kl_train: -5.6042428017 mse_train: 0.0080420767 acc_train: 0.2132183908 nll_val: 7079.2949218750 kl_val: -12.2895250320 mse_val: 0.0372594446 acc_val: 0.3367816092 time: 4.0092s
Epoch: 0019 nll_train: 1886.3713378906 kl_train: -5.6140122414 mse_train: 0.0099282708 acc_train: 0.2086206897 nll_val: 6467.9306640625 kl_val: -11.7617034912 mse_val: 0.0340417400 acc_val: 0.3448275862 time: 4.0546s
Epoch: 0020 nll_train: 1660.1609191895 kl_train: -5.6244266033 mse_train: 0.0087376885 acc_train: 0.2057471264 nll_val: 5833.1884765625 kl_val: -11.2501182556 mse_val: 0.0307009891 acc_val: 0.3540229885 time: 4.0882s
Best model so far, saving...
Epoch: 0021 nll_train: 1597.6961364746 kl_train: -5.6349430084 mse_train: 0.0084089260 acc_train: 0.2022988506 nll_val: 6049.3378906250 kl_val: -10.7531204224 mse_val: 0.0318386145 acc_val: 0.3632183908 time: 4.0665s
Epoch: 0022 nll_train: 1653.1348266602 kl_train: -5.6443908215 mse_train: 0.0087007093 acc_train: 0.2005747126 nll_val: 5783.9736328125 kl_val: -10.2746391296 mse_val: 0.0304419678 acc_val: 0.3689655172 time: 4.0683s
Best model so far, saving...
Epoch: 0023 nll_train: 1447.5107727051 kl_train: -5.6522436142 mse_train: 0.0076184782 acc_train: 0.1994252874 nll_val: 5696.3525390625 kl_val: -9.8221578598 mse_val: 0.0299808029 acc_val: 0.3862068966 time: 4.0550s
Best model so far, saving...
Epoch: 0024 nll_train: 1420.1156921387 kl_train: -5.6588795185 mse_train: 0.0074742927 acc_train: 0.1988505747 nll_val: 5773.1196289062 kl_val: -9.4035463333 mse_val: 0.0303848386 acc_val: 0.3977011494 time: 4.0700s
Epoch: 0025 nll_train: 1502.0148925781 kl_train: -5.6651847363 mse_train: 0.0079053410 acc_train: 0.1982758621 nll_val: 5429.8579101562 kl_val: -9.0199680328 mse_val: 0.0285781976 acc_val: 0.4195402299 time: 4.0548s
Best model so far, saving...
Epoch: 0026 nll_train: 1475.8800659180 kl_train: -5.6713225842 mse_train: 0.0077677888 acc_train: 0.1965517241 nll_val: 5318.7783203125 kl_val: -8.6645488739 mse_val: 0.0279935673 acc_val: 0.4367816092 time: 4.0655s
Best model so far, saving...
Epoch: 0027 nll_train: 1427.3231201172 kl_train: -5.6767330170 mse_train: 0.0075122264 acc_train: 0.1965517241 nll_val: 5267.6162109375 kl_val: -8.3338260651 mse_val: 0.0277242940 acc_val: 0.4563218391 time: 3.9984s
Best model so far, saving...
Epoch: 0028 nll_train: 1369.6633911133 kl_train: -5.6811814308 mse_train: 0.0072087541 acc_train: 0.1948275862 nll_val: 5297.0825195312 kl_val: -8.0324363708 mse_val: 0.0278793816 acc_val: 0.4747126437 time: 4.0929s
Epoch: 0029 nll_train: 1406.0097351074 kl_train: -5.6850314140 mse_train: 0.0074000506 acc_train: 0.1954022989 nll_val: 5225.7192382812 kl_val: -7.7675867081 mse_val: 0.0275037866 acc_val: 0.4931034483 time: 4.0546s
Best model so far, saving...
Epoch: 0030 nll_train: 1410.7176513672 kl_train: -5.6885879040 mse_train: 0.0074248288 acc_train: 0.1954022989 nll_val: 5140.3310546875 kl_val: -7.5382618904 mse_val: 0.0270543750 acc_val: 0.5057471264 time: 4.1044s
Best model so far, saving...
Epoch: 0031 nll_train: 1401.7187194824 kl_train: -5.6917257309 mse_train: 0.0073774664 acc_train: 0.1948275862 nll_val: 5114.0078125000 kl_val: -7.3406319618 mse_val: 0.0269158296 acc_val: 0.5275862069 time: 4.0557s
Best model so far, saving...
Epoch: 0032 nll_train: 1371.5959167480 kl_train: -5.6943914890 mse_train: 0.0072189256 acc_train: 0.1971264368 nll_val: 5098.5815429688 kl_val: -7.1686015129 mse_val: 0.0268346388 acc_val: 0.5402298851 time: 4.0305s
Best model so far, saving...
Epoch: 0033 nll_train: 1361.5711669922 kl_train: -5.6965599060 mse_train: 0.0071661639 acc_train: 0.1965517241 nll_val: 5126.8149414062 kl_val: -7.0203757286 mse_val: 0.0269832350 acc_val: 0.5620689655 time: 4.0885s
Epoch: 0034 nll_train: 1383.2585144043 kl_train: -5.6984176636 mse_train: 0.0072803081 acc_train: 0.1971264368 nll_val: 5080.1166992188 kl_val: -6.8925776482 mse_val: 0.0267374553 acc_val: 0.5781609195 time: 4.0856s
Best model so far, saving...
Epoch: 0035 nll_train: 1382.7845764160 kl_train: -5.6999943256 mse_train: 0.0072778132 acc_train: 0.1965517241 nll_val: 5052.6035156250 kl_val: -6.7826352119 mse_val: 0.0265926495 acc_val: 0.5885057471 time: 4.0814s
Best model so far, saving...
Epoch: 0036 nll_train: 1364.7575683594 kl_train: -5.7013084888 mse_train: 0.0071829343 acc_train: 0.1971264368 nll_val: 5044.9296875000 kl_val: -6.6883802414 mse_val: 0.0265522581 acc_val: 0.6068965517 time: 4.1107s
Best model so far, saving...
Epoch: 0037 nll_train: 1352.5736389160 kl_train: -5.7023985386 mse_train: 0.0071188078 acc_train: 0.1965517241 nll_val: 5047.8515625000 kl_val: -6.6077761650 mse_val: 0.0265676398 acc_val: 0.6172413793 time: 4.0567s
Epoch: 0038 nll_train: 1365.4405822754 kl_train: -5.7032217979 mse_train: 0.0071865292 acc_train: 0.1977011494 nll_val: 5008.6176757812 kl_val: -6.5390343666 mse_val: 0.0263611451 acc_val: 0.6344827586 time: 4.1221s
Best model so far, saving...
Epoch: 0039 nll_train: 1375.0259399414 kl_train: -5.7038631439 mse_train: 0.0072369783 acc_train: 0.1971264368 nll_val: 4992.4692382812 kl_val: -6.4815454483 mse_val: 0.0262761526 acc_val: 0.6436781609 time: 4.0506s
Best model so far, saving...
Epoch: 0040 nll_train: 1355.3136596680 kl_train: -5.7043669224 mse_train: 0.0071332295 acc_train: 0.1971264368 nll_val: 4989.3095703125 kl_val: -6.4334607124 mse_val: 0.0262595266 acc_val: 0.6540229885 time: 4.0811s
Best model so far, saving...
Epoch: 0041 nll_train: 1350.3530273438 kl_train: -5.7047376633 mse_train: 0.0071071209 acc_train: 0.1982758621 nll_val: 4992.0175781250 kl_val: -6.3937139511 mse_val: 0.0262737740 acc_val: 0.6597701149 time: 4.0737s
Epoch: 0042 nll_train: 1355.6861877441 kl_train: -5.7049877644 mse_train: 0.0071351900 acc_train: 0.1988505747 nll_val: 4979.6079101562 kl_val: -6.3610024452 mse_val: 0.0262084641 acc_val: 0.6689655172 time: 4.0980s
Best model so far, saving...
Epoch: 0043 nll_train: 1356.9968872070 kl_train: -5.7051043510 mse_train: 0.0071420879 acc_train: 0.1994252874 nll_val: 4968.8652343750 kl_val: -6.3346810341 mse_val: 0.0261519235 acc_val: 0.6712643678 time: 3.9786s
Best model so far, saving...
Epoch: 0044 nll_train: 1353.5843505859 kl_train: -5.7051537037 mse_train: 0.0071241279 acc_train: 0.2000000000 nll_val: 4970.5756835938 kl_val: -6.3138623238 mse_val: 0.0261609238 acc_val: 0.6747126437 time: 3.6570s
Epoch: 0045 nll_train: 1345.5259399414 kl_train: -5.7051517963 mse_train: 0.0070817155 acc_train: 0.2005747126 nll_val: 4968.5136718750 kl_val: -6.2980661392 mse_val: 0.0261500701 acc_val: 0.6804597701 time: 3.6070s
Best model so far, saving...
Epoch: 0046 nll_train: 1350.2597045898 kl_train: -5.7051513195 mse_train: 0.0071066302 acc_train: 0.2011494253 nll_val: 4956.6274414062 kl_val: -6.2865476608 mse_val: 0.0260875132 acc_val: 0.6908045977 time: 4.0566s
Best model so far, saving...
Epoch: 0047 nll_train: 1351.5259704590 kl_train: -5.7052018642 mse_train: 0.0071132936 acc_train: 0.2011494253 nll_val: 4948.5078125000 kl_val: -6.2785215378 mse_val: 0.0260447767 acc_val: 0.6954022989 time: 4.0369s
Best model so far, saving...
Epoch: 0048 nll_train: 1346.9639282227 kl_train: -5.7052736282 mse_train: 0.0070892834 acc_train: 0.2000000000 nll_val: 4944.2465820312 kl_val: -6.2735204697 mse_val: 0.0260223486 acc_val: 0.6965517241 time: 3.9372s
Best model so far, saving...
Epoch: 0049 nll_train: 1345.0866394043 kl_train: -5.7053852081 mse_train: 0.0070794028 acc_train: 0.2000000000 nll_val: 4943.2280273438 kl_val: -6.2708020210 mse_val: 0.0260169860 acc_val: 0.7000000000 time: 3.8469s
Best model so far, saving...
Epoch: 0050 nll_train: 1345.0316772461 kl_train: -5.7054941654 mse_train: 0.0070791143 acc_train: 0.2000000000 nll_val: 4934.1708984375 kl_val: -6.2702798843 mse_val: 0.0259693172 acc_val: 0.7022988506 time: 3.8001s
Best model so far, saving...
Epoch: 0051 nll_train: 1346.7103881836 kl_train: -5.7056877613 mse_train: 0.0070879488 acc_train: 0.2005747126 nll_val: 4921.8554687500 kl_val: -6.2715153694 mse_val: 0.0259045009 acc_val: 0.7045977011 time: 3.5679s
Best model so far, saving...
Epoch: 0052 nll_train: 1344.8517761230 kl_train: -5.7059843540 mse_train: 0.0070781671 acc_train: 0.2011494253 nll_val: 4910.9790039062 kl_val: -6.2739729881 mse_val: 0.0258472543 acc_val: 0.7057471264 time: 3.5570s
Best model so far, saving...
Epoch: 0053 nll_train: 1339.6249084473 kl_train: -5.7063598633 mse_train: 0.0070506568 acc_train: 0.2022988506 nll_val: 4902.5493164062 kl_val: -6.2775497437 mse_val: 0.0258028898 acc_val: 0.7091954023 time: 3.6891s
Best model so far, saving...
Epoch: 0054 nll_train: 1341.5325622559 kl_train: -5.7068064213 mse_train: 0.0070606971 acc_train: 0.2028735632 nll_val: 4890.0756835938 kl_val: -6.2822175026 mse_val: 0.0257372372 acc_val: 0.7103448276 time: 3.6274s
Best model so far, saving...
Epoch: 0055 nll_train: 1345.4524230957 kl_train: -5.7073390484 mse_train: 0.0070813284 acc_train: 0.2034482759 nll_val: 4892.8881835938 kl_val: -6.2873539925 mse_val: 0.0257520415 acc_val: 0.7114942529 time: 3.6665s
Epoch: 0056 nll_train: 1339.7984008789 kl_train: -5.7078835964 mse_train: 0.0070515702 acc_train: 0.2034482759 nll_val: 4900.5112304688 kl_val: -6.2930803299 mse_val: 0.0257921591 acc_val: 0.7126436782 time: 3.6749s
Epoch: 0057 nll_train: 1341.3863525391 kl_train: -5.7085046768 mse_train: 0.0070599284 acc_train: 0.2034482759 nll_val: 4894.9536132812 kl_val: -6.2992811203 mse_val: 0.0257629137 acc_val: 0.7137931034 time: 3.4769s
Epoch: 0058 nll_train: 1339.5175781250 kl_train: -5.7091813087 mse_train: 0.0070500916 acc_train: 0.2045977011 nll_val: 4890.8349609375 kl_val: -6.3056907654 mse_val: 0.0257412344 acc_val: 0.7149425287 time: 3.5197s
Epoch: 0059 nll_train: 1337.2773742676 kl_train: -5.7099244595 mse_train: 0.0070383024 acc_train: 0.2045977011 nll_val: 4890.0405273438 kl_val: -6.3122067451 mse_val: 0.0257370509 acc_val: 0.7172413793 time: 9.6759s
Best model so far, saving...
Epoch: 0060 nll_train: 1337.5109558105 kl_train: -5.7107021809 mse_train: 0.0070395304 acc_train: 0.2040229885 nll_val: 4880.1225585938 kl_val: -6.3187956810 mse_val: 0.0256848559 acc_val: 0.7183908046 time: 3.8027s
Best model so far, saving...
Epoch: 0061 nll_train: 1339.0180969238 kl_train: -5.7114965916 mse_train: 0.0070474632 acc_train: 0.2034482759 nll_val: 4872.8964843750 kl_val: -6.3256568909 mse_val: 0.0256468225 acc_val: 0.7183908046 time: 3.8557s
Best model so far, saving...
Epoch: 0062 nll_train: 1337.0131225586 kl_train: -5.7123136520 mse_train: 0.0070369109 acc_train: 0.2028735632 nll_val: 4867.0288085938 kl_val: -6.3325905800 mse_val: 0.0256159417 acc_val: 0.7218390805 time: 4.1851s
Best model so far, saving...
Epoch: 0063 nll_train: 1334.7416992188 kl_train: -5.7131226063 mse_train: 0.0070249570 acc_train: 0.2034482759 nll_val: 4865.5727539062 kl_val: -6.3397364616 mse_val: 0.0256082788 acc_val: 0.7218390805 time: 3.0801s
Best model so far, saving...
Epoch: 0064 nll_train: 1336.0743713379 kl_train: -5.7139804363 mse_train: 0.0070319702 acc_train: 0.2034482759 nll_val: 4866.5913085938 kl_val: -6.3468580246 mse_val: 0.0256136395 acc_val: 0.7218390805 time: 3.0307s
Epoch: 0065 nll_train: 1338.5992431641 kl_train: -5.7148740292 mse_train: 0.0070452590 acc_train: 0.2034482759 nll_val: 4857.3408203125 kl_val: -6.3539543152 mse_val: 0.0255649500 acc_val: 0.7218390805 time: 3.0572s
Best model so far, saving...
Epoch: 0066 nll_train: 1355.9418945312 kl_train: -5.7157926559 mse_train: 0.0071365364 acc_train: 0.2040229885 nll_val: 4867.4833984375 kl_val: -6.3611688614 mse_val: 0.0256183334 acc_val: 0.7229885057 time: 3.0500s
Epoch: 0067 nll_train: 1330.8614501953 kl_train: -5.7168011665 mse_train: 0.0070045338 acc_train: 0.2040229885 nll_val: 4864.9340820312 kl_val: -6.3684930801 mse_val: 0.0256049186 acc_val: 0.7229885057 time: 2.9887s
Epoch: 0068 nll_train: 1346.8284606934 kl_train: -5.7178359032 mse_train: 0.0070885709 acc_train: 0.2057471264 nll_val: 4842.6010742188 kl_val: -6.3757481575 mse_val: 0.0254873727 acc_val: 0.7252873563 time: 3.0334s
Best model so far, saving...
Epoch: 0069 nll_train: 1338.8366699219 kl_train: -5.7189471722 mse_train: 0.0070465087 acc_train: 0.2063218391 nll_val: 4838.4208984375 kl_val: -6.3829898834 mse_val: 0.0254653711 acc_val: 0.7264367816 time: 2.9985s
Best model so far, saving...
Epoch: 0070 nll_train: 1322.9944458008 kl_train: -5.7201659679 mse_train: 0.0069631287 acc_train: 0.2074712644 nll_val: 4852.2207031250 kl_val: -6.3901696205 mse_val: 0.0255380012 acc_val: 0.7264367816 time: 2.9986s
Epoch: 0071 nll_train: 1333.7340698242 kl_train: -5.7213950157 mse_train: 0.0070196531 acc_train: 0.2068965517 nll_val: 4838.7265625000 kl_val: -6.3971219063 mse_val: 0.0254669823 acc_val: 0.7264367816 time: 3.0093s
Epoch: 0072 nll_train: 1340.2520141602 kl_train: -5.7225978374 mse_train: 0.0070539579 acc_train: 0.2063218391 nll_val: 4814.5976562500 kl_val: -6.4039154053 mse_val: 0.0253399871 acc_val: 0.7264367816 time: 3.0043s
Best model so far, saving...
Epoch: 0073 nll_train: 1341.8583984375 kl_train: -5.7238492966 mse_train: 0.0070624127 acc_train: 0.2068965517 nll_val: 4822.8266601562 kl_val: -6.4109783173 mse_val: 0.0253832974 acc_val: 0.7275862069 time: 3.0609s
Epoch: 0074 nll_train: 1322.6846008301 kl_train: -5.7252192497 mse_train: 0.0069614972 acc_train: 0.2074712644 nll_val: 4836.9316406250 kl_val: -6.4181776047 mse_val: 0.0254575312 acc_val: 0.7287356322 time: 3.0416s
Epoch: 0075 nll_train: 1344.1573486328 kl_train: -5.7266113758 mse_train: 0.0070745122 acc_train: 0.2074712644 nll_val: 4816.3178710938 kl_val: -6.4252090454 mse_val: 0.0253490377 acc_val: 0.7298850575 time: 3.0470s
Epoch: 0076 nll_train: 1333.2395019531 kl_train: -5.7279906273 mse_train: 0.0070170499 acc_train: 0.2080459770 nll_val: 4814.2026367188 kl_val: -6.4320478439 mse_val: 0.0253379066 acc_val: 0.7310344828 time: 3.1403s
Best model so far, saving...
Epoch: 0077 nll_train: 1324.8649291992 kl_train: -5.7294006348 mse_train: 0.0069729731 acc_train: 0.2086206897 nll_val: 4822.8325195312 kl_val: -6.4387826920 mse_val: 0.0253833290 acc_val: 0.7310344828 time: 3.0618s
Epoch: 0078 nll_train: 1327.8844604492 kl_train: -5.7308430672 mse_train: 0.0069888651 acc_train: 0.2097701149 nll_val: 4822.0922851562 kl_val: -6.4453749657 mse_val: 0.0253794324 acc_val: 0.7310344828 time: 2.9982s
Epoch: 0079 nll_train: 1333.8574829102 kl_train: -5.7323157787 mse_train: 0.0070203027 acc_train: 0.2091954023 nll_val: 4816.9965820312 kl_val: -6.4518780708 mse_val: 0.0253526140 acc_val: 0.7310344828 time: 2.9862s
Epoch: 0080 nll_train: 1328.9946899414 kl_train: -5.7338502407 mse_train: 0.0069947087 acc_train: 0.2109195402 nll_val: 4814.8818359375 kl_val: -6.4585418701 mse_val: 0.0253414810 acc_val: 0.7333333333 time: 2.9895s
Epoch: 0081 nll_train: 1325.2460937500 kl_train: -5.7355046272 mse_train: 0.0069749790 acc_train: 0.2114942529 nll_val: 4817.5102539062 kl_val: -6.4651045799 mse_val: 0.0253553167 acc_val: 0.7367816092 time: 3.1228s
Epoch: 0082 nll_train: 1331.4592285156 kl_train: -5.7371346951 mse_train: 0.0070076794 acc_train: 0.2109195402 nll_val: 4811.1591796875 kl_val: -6.4714932442 mse_val: 0.0253218897 acc_val: 0.7367816092 time: 3.0918s
Best model so far, saving...
Epoch: 0083 nll_train: 1329.5507507324 kl_train: -5.7387404442 mse_train: 0.0069976348 acc_train: 0.2120689655 nll_val: 4801.9829101562 kl_val: -6.4777574539 mse_val: 0.0252735913 acc_val: 0.7367816092 time: 3.1287s
Best model so far, saving...
Epoch: 0084 nll_train: 1326.4346008301 kl_train: -5.7403755188 mse_train: 0.0069812346 acc_train: 0.2126436782 nll_val: 4798.4072265625 kl_val: -6.4840531349 mse_val: 0.0252547748 acc_val: 0.7367816092 time: 3.1332s
Best model so far, saving...
Epoch: 0085 nll_train: 1325.3011169434 kl_train: -5.7420704365 mse_train: 0.0069752687 acc_train: 0.2120689655 nll_val: 4803.8652343750 kl_val: -6.4902901649 mse_val: 0.0252835006 acc_val: 0.7379310345 time: 3.1154s
Epoch: 0086 nll_train: 1326.4547424316 kl_train: -5.7437796593 mse_train: 0.0069813408 acc_train: 0.2126436782 nll_val: 4794.8271484375 kl_val: -6.4963703156 mse_val: 0.0252359305 acc_val: 0.7390804598 time: 3.1493s
Best model so far, saving...
Epoch: 0087 nll_train: 1326.7971496582 kl_train: -5.7454655170 mse_train: 0.0069831426 acc_train: 0.2120689655 nll_val: 4794.1894531250 kl_val: -6.5023250580 mse_val: 0.0252325777 acc_val: 0.7413793103 time: 3.1386s
Best model so far, saving...
Epoch: 0088 nll_train: 1324.9460754395 kl_train: -5.7471582890 mse_train: 0.0069734000 acc_train: 0.2120689655 nll_val: 4788.9438476562 kl_val: -6.5084800720 mse_val: 0.0252049658 acc_val: 0.7413793103 time: 3.0980s
Best model so far, saving...
Epoch: 0089 nll_train: 1324.4678344727 kl_train: -5.7489018440 mse_train: 0.0069708832 acc_train: 0.2120689655 nll_val: 4789.2348632812 kl_val: -6.5145745277 mse_val: 0.0252064988 acc_val: 0.7425287356 time: 2.9381s
Epoch: 0090 nll_train: 1326.4254150391 kl_train: -5.7506713867 mse_train: 0.0069811862 acc_train: 0.2132183908 nll_val: 4791.5219726562 kl_val: -6.5205841064 mse_val: 0.0252185371 acc_val: 0.7436781609 time: 2.8513s
Epoch: 0091 nll_train: 1325.4277954102 kl_train: -5.7524154186 mse_train: 0.0069759351 acc_train: 0.2114942529 nll_val: 4786.1435546875 kl_val: -6.5265445709 mse_val: 0.0251902286 acc_val: 0.7436781609 time: 2.9427s
Best model so far, saving...
Epoch: 0092 nll_train: 1329.1592102051 kl_train: -5.7542231083 mse_train: 0.0069955747 acc_train: 0.2109195402 nll_val: 4785.9492187500 kl_val: -6.5326972008 mse_val: 0.0251892060 acc_val: 0.7436781609 time: 3.7973s
Best model so far, saving...
Epoch: 0093 nll_train: 1322.7413024902 kl_train: -5.7561280727 mse_train: 0.0069617959 acc_train: 0.2109195402 nll_val: 4787.1508789062 kl_val: -6.5390028954 mse_val: 0.0251955315 acc_val: 0.7436781609 time: 3.8527s
Epoch: 0094 nll_train: 1321.3230895996 kl_train: -5.7581532001 mse_train: 0.0069543318 acc_train: 0.2109195402 nll_val: 4788.8974609375 kl_val: -6.5454349518 mse_val: 0.0252047200 acc_val: 0.7448275862 time: 3.8447s
Epoch: 0095 nll_train: 1326.6269836426 kl_train: -5.7602684498 mse_train: 0.0069822475 acc_train: 0.2109195402 nll_val: 4782.6757812500 kl_val: -6.5520691872 mse_val: 0.0251719765 acc_val: 0.7471264368 time: 3.8275s
Best model so far, saving...
Epoch: 0096 nll_train: 1323.8727416992 kl_train: -5.7625153065 mse_train: 0.0069677503 acc_train: 0.2109195402 nll_val: 4783.1835937500 kl_val: -6.5587158203 mse_val: 0.0251746513 acc_val: 0.7459770115 time: 3.8432s
Epoch: 0097 nll_train: 1320.5696716309 kl_train: -5.7648525238 mse_train: 0.0069503665 acc_train: 0.2109195402 nll_val: 4784.3681640625 kl_val: -6.5652346611 mse_val: 0.0251808837 acc_val: 0.7459770115 time: 6.6726s
Epoch: 0098 nll_train: 1323.0800781250 kl_train: -5.7672283649 mse_train: 0.0069635793 acc_train: 0.2109195402 nll_val: 4784.1406250000 kl_val: -6.5716352463 mse_val: 0.0251796842 acc_val: 0.7471264368 time: 3.9377s
Epoch: 0099 nll_train: 1324.9862976074 kl_train: -5.7695956230 mse_train: 0.0069736125 acc_train: 0.2109195402 nll_val: 4780.7553710938 kl_val: -6.5779862404 mse_val: 0.0251618698 acc_val: 0.7459770115 time: 4.1378s
Best model so far, saving...
Epoch: 0100 nll_train: 1320.0066833496 kl_train: -5.7720019817 mse_train: 0.0069474033 acc_train: 0.2114942529 nll_val: 4779.8818359375 kl_val: -6.5844597816 mse_val: 0.0251572672 acc_val: 0.7471264368 time: 3.6323s
Best model so far, saving...
Epoch: 0101 nll_train: 1321.3551330566 kl_train: -5.7744951248 mse_train: 0.0069545007 acc_train: 0.2114942529 nll_val: 4775.6967773438 kl_val: -6.5908536911 mse_val: 0.0251352452 acc_val: 0.7482758621 time: 3.7330s
Best model so far, saving...
Epoch: 0102 nll_train: 1323.3152770996 kl_train: -5.7770113945 mse_train: 0.0069648173 acc_train: 0.2097701149 nll_val: 4773.1713867188 kl_val: -6.5973563194 mse_val: 0.0251219515 acc_val: 0.7482758621 time: 4.1029s
Best model so far, saving...
Epoch: 0103 nll_train: 1321.0536499023 kl_train: -5.7796237469 mse_train: 0.0069529135 acc_train: 0.2103448276 nll_val: 4772.6762695312 kl_val: -6.6038851738 mse_val: 0.0251193475 acc_val: 0.7482758621 time: 3.9066s
Best model so far, saving...
Epoch: 0104 nll_train: 1320.1552734375 kl_train: -5.7822823524 mse_train: 0.0069481849 acc_train: 0.2091954023 nll_val: 4772.5385742188 kl_val: -6.6102490425 mse_val: 0.0251186267 acc_val: 0.7482758621 time: 3.7770s
Best model so far, saving...
Epoch: 0105 nll_train: 1321.8280029297 kl_train: -5.7849605083 mse_train: 0.0069569895 acc_train: 0.2097701149 nll_val: 4768.4008789062 kl_val: -6.6165924072 mse_val: 0.0250968430 acc_val: 0.7482758621 time: 3.7386s
Best model so far, saving...
Epoch: 0106 nll_train: 1321.5491943359 kl_train: -5.7876658440 mse_train: 0.0069555215 acc_train: 0.2091954023 nll_val: 4765.9526367188 kl_val: -6.6228399277 mse_val: 0.0250839610 acc_val: 0.7494252874 time: 3.6465s
Best model so far, saving...
Epoch: 0107 nll_train: 1318.8644409180 kl_train: -5.7903819084 mse_train: 0.0069413914 acc_train: 0.2086206897 nll_val: 4768.7011718750 kl_val: -6.6289939880 mse_val: 0.0250984281 acc_val: 0.7494252874 time: 3.6367s
Epoch: 0108 nll_train: 1320.5539245605 kl_train: -5.7930982113 mse_train: 0.0069502833 acc_train: 0.2080459770 nll_val: 4764.7675781250 kl_val: -6.6352386475 mse_val: 0.0250777211 acc_val: 0.7505747126 time: 4.0627s
Best model so far, saving...
Epoch: 0109 nll_train: 1320.8779296875 kl_train: -5.7958641052 mse_train: 0.0069519879 acc_train: 0.2086206897 nll_val: 4763.8247070312 kl_val: -6.6414422989 mse_val: 0.0250727572 acc_val: 0.7494252874 time: 3.7171s
Best model so far, saving...
Epoch: 0110 nll_train: 1318.9450988770 kl_train: -5.7986900806 mse_train: 0.0069418162 acc_train: 0.2080459770 nll_val: 4765.4824218750 kl_val: -6.6476688385 mse_val: 0.0250814874 acc_val: 0.7505747126 time: 3.9489s
Epoch: 0111 nll_train: 1318.4013977051 kl_train: -5.8015594482 mse_train: 0.0069389543 acc_train: 0.2086206897 nll_val: 4761.2763671875 kl_val: -6.6538715363 mse_val: 0.0250593461 acc_val: 0.7505747126 time: 3.4477s
Best model so far, saving...
Epoch: 0112 nll_train: 1320.0169067383 kl_train: -5.8044576645 mse_train: 0.0069474572 acc_train: 0.2086206897 nll_val: 4761.4687500000 kl_val: -6.6600079536 mse_val: 0.0250603575 acc_val: 0.7505747126 time: 3.6729s
Epoch: 0113 nll_train: 1318.1768798828 kl_train: -5.8073999882 mse_train: 0.0069377730 acc_train: 0.2074712644 nll_val: 4759.5488281250 kl_val: -6.6661801338 mse_val: 0.0250502583 acc_val: 0.7517241379 time: 3.5384s
Best model so far, saving...
Epoch: 0114 nll_train: 1318.3665771484 kl_train: -5.8104209900 mse_train: 0.0069387710 acc_train: 0.2097701149 nll_val: 4759.2993164062 kl_val: -6.6723804474 mse_val: 0.0250489432 acc_val: 0.7505747126 time: 4.8768s
Best model so far, saving...
Epoch: 0115 nll_train: 1319.3165893555 kl_train: -5.8135039806 mse_train: 0.0069437711 acc_train: 0.2103448276 nll_val: 4759.0249023438 kl_val: -6.6786699295 mse_val: 0.0250474978 acc_val: 0.7528735632 time: 3.9910s
Best model so far, saving...
Epoch: 0116 nll_train: 1318.4507751465 kl_train: -5.8166406155 mse_train: 0.0069392140 acc_train: 0.2097701149 nll_val: 4756.0400390625 kl_val: -6.6848950386 mse_val: 0.0250317883 acc_val: 0.7528735632 time: 7.0194s
Best model so far, saving...
Epoch: 0117 nll_train: 1316.9375305176 kl_train: -5.8197977543 mse_train: 0.0069312500 acc_train: 0.2097701149 nll_val: 4758.5625000000 kl_val: -6.6910815239 mse_val: 0.0250450633 acc_val: 0.7540229885 time: 4.4445s
Epoch: 0118 nll_train: 1317.8574218750 kl_train: -5.8229703903 mse_train: 0.0069360916 acc_train: 0.2097701149 nll_val: 4755.2099609375 kl_val: -6.6972708702 mse_val: 0.0250274148 acc_val: 0.7551724138 time: 4.1759s
Best model so far, saving...
Epoch: 0119 nll_train: 1317.2686767578 kl_train: -5.8261687756 mse_train: 0.0069329925 acc_train: 0.2091954023 nll_val: 4752.3730468750 kl_val: -6.7035117149 mse_val: 0.0250124875 acc_val: 0.7574712644 time: 3.5026s
Best model so far, saving...
Epoch: 0120 nll_train: 1316.4463195801 kl_train: -5.8294394016 mse_train: 0.0069286642 acc_train: 0.2091954023 nll_val: 4758.1123046875 kl_val: -6.7097916603 mse_val: 0.0250426959 acc_val: 0.7574712644 time: 3.5892s
Epoch: 0121 nll_train: 1318.0305175781 kl_train: -5.8327355385 mse_train: 0.0069370017 acc_train: 0.2097701149 nll_val: 4760.0097656250 kl_val: -6.7160210609 mse_val: 0.0250526816 acc_val: 0.7586206897 time: 3.8533s
Epoch: 0122 nll_train: 1318.5920104980 kl_train: -5.8360342979 mse_train: 0.0069399578 acc_train: 0.2103448276 nll_val: 4754.7749023438 kl_val: -6.7222676277 mse_val: 0.0250251293 acc_val: 0.7574712644 time: 4.1212s
Epoch: 0123 nll_train: 1322.3182373047 kl_train: -5.8394122124 mse_train: 0.0069595699 acc_train: 0.2103448276 nll_val: 4752.7519531250 kl_val: -6.7288479805 mse_val: 0.0250144824 acc_val: 0.7597701149 time: 3.8245s
Epoch: 0124 nll_train: 1313.4523925781 kl_train: -5.8429973125 mse_train: 0.0069129071 acc_train: 0.2097701149 nll_val: 4758.0703125000 kl_val: -6.7357020378 mse_val: 0.0250424724 acc_val: 0.7609195402 time: 3.9253s
Epoch: 0125 nll_train: 1316.1637573242 kl_train: -5.8467333317 mse_train: 0.0069271774 acc_train: 0.2097701149 nll_val: 4758.4335937500 kl_val: -6.7425355911 mse_val: 0.0250443872 acc_val: 0.7620689655 time: 3.8055s
Epoch: 0126 nll_train: 1320.8955078125 kl_train: -5.8504791260 mse_train: 0.0069520815 acc_train: 0.2091954023 nll_val: 4753.8911132812 kl_val: -6.7493300438 mse_val: 0.0250204764 acc_val: 0.7620689655 time: 4.0054s
Epoch: 0127 nll_train: 1314.4004821777 kl_train: -5.8543062210 mse_train: 0.0069178971 acc_train: 0.2097701149 nll_val: 4761.6240234375 kl_val: -6.7562465668 mse_val: 0.0250611752 acc_val: 0.7632183908 time: 3.9025s
Epoch: 0128 nll_train: 1314.7865600586 kl_train: -5.8582077026 mse_train: 0.0069199289 acc_train: 0.2091954023 nll_val: 4762.3569335938 kl_val: -6.7631955147 mse_val: 0.0250650365 acc_val: 0.7632183908 time: 3.9906s
Epoch: 0129 nll_train: 1318.8972778320 kl_train: -5.8621416092 mse_train: 0.0069415641 acc_train: 0.2091954023 nll_val: 4755.2573242188 kl_val: -6.7701435089 mse_val: 0.0250276644 acc_val: 0.7643678161 time: 4.3121s
Epoch: 0130 nll_train: 1315.8495483398 kl_train: -5.8661236763 mse_train: 0.0069255236 acc_train: 0.2097701149 nll_val: 4756.2543945312 kl_val: -6.7772846222 mse_val: 0.0250329170 acc_val: 0.7666666667 time: 3.5910s
Epoch: 0131 nll_train: 1313.6558837891 kl_train: -5.8701963425 mse_train: 0.0069139785 acc_train: 0.2091954023 nll_val: 4756.3564453125 kl_val: -6.7843856812 mse_val: 0.0250334535 acc_val: 0.7666666667 time: 3.9109s
Epoch: 0132 nll_train: 1317.5882873535 kl_train: -5.8743093014 mse_train: 0.0069346743 acc_train: 0.2097701149 nll_val: 4756.5942382812 kl_val: -6.7913079262 mse_val: 0.0250347033 acc_val: 0.7678160920 time: 4.2575s
Epoch: 0133 nll_train: 1315.4066467285 kl_train: -5.8784415722 mse_train: 0.0069231929 acc_train: 0.2091954023 nll_val: 4754.4536132812 kl_val: -6.7982463837 mse_val: 0.0250234380 acc_val: 0.7678160920 time: 3.8451s
Epoch: 0134 nll_train: 1313.4131164551 kl_train: -5.8826439381 mse_train: 0.0069127001 acc_train: 0.2091954023 nll_val: 4756.4731445312 kl_val: -6.8051280975 mse_val: 0.0250340682 acc_val: 0.7678160920 time: 3.7497s
Epoch: 0135 nll_train: 1316.8229980469 kl_train: -5.8868601322 mse_train: 0.0069306472 acc_train: 0.2097701149 nll_val: 4752.4370117188 kl_val: -6.8120522499 mse_val: 0.0250128228 acc_val: 0.7689655172 time: 3.8177s
Epoch: 0136 nll_train: 1315.3669128418 kl_train: -5.8911504745 mse_train: 0.0069229837 acc_train: 0.2097701149 nll_val: 4752.5332031250 kl_val: -6.8190336227 mse_val: 0.0250133313 acc_val: 0.7689655172 time: 3.6806s
Epoch: 0137 nll_train: 1312.9596862793 kl_train: -5.8955125809 mse_train: 0.0069103137 acc_train: 0.2097701149 nll_val: 4755.9077148438 kl_val: -6.8259582520 mse_val: 0.0250310916 acc_val: 0.7701149425 time: 3.6647s
Epoch: 0138 nll_train: 1315.6199645996 kl_train: -5.8999154568 mse_train: 0.0069243159 acc_train: 0.2103448276 nll_val: 4750.8217773438 kl_val: -6.8329410553 mse_val: 0.0250043217 acc_val: 0.7701149425 time: 3.5707s
Best model so far, saving...
Epoch: 0139 nll_train: 1314.3076477051 kl_train: -5.9043490887 mse_train: 0.0069174089 acc_train: 0.2097701149 nll_val: 4753.6884765625 kl_val: -6.8399286270 mse_val: 0.0250194110 acc_val: 0.7701149425 time: 3.5745s
Epoch: 0140 nll_train: 1312.7467956543 kl_train: -5.9088509083 mse_train: 0.0069091931 acc_train: 0.2103448276 nll_val: 4752.6591796875 kl_val: -6.8469009399 mse_val: 0.0250139926 acc_val: 0.7712643678 time: 3.7596s
Epoch: 0141 nll_train: 1314.5899658203 kl_train: -5.9134020805 mse_train: 0.0069188940 acc_train: 0.2103448276 nll_val: 4751.0024414062 kl_val: -6.8539338112 mse_val: 0.0250052735 acc_val: 0.7724137931 time: 3.4990s
Epoch: 0142 nll_train: 1313.1776428223 kl_train: -5.9179792404 mse_train: 0.0069114611 acc_train: 0.2109195402 nll_val: 4751.8408203125 kl_val: -6.8609361649 mse_val: 0.0250096861 acc_val: 0.7735632184 time: 3.5995s
Epoch: 0143 nll_train: 1313.0343017578 kl_train: -5.9225728512 mse_train: 0.0069107066 acc_train: 0.2120689655 nll_val: 4749.5429687500 kl_val: -6.8679113388 mse_val: 0.0249975920 acc_val: 0.7747126437 time: 3.6097s
Best model so far, saving...
Epoch: 0144 nll_train: 1313.9677429199 kl_train: -5.9271879196 mse_train: 0.0069156190 acc_train: 0.2114942529 nll_val: 4749.5097656250 kl_val: -6.8749737740 mse_val: 0.0249974206 acc_val: 0.7747126437 time: 3.7195s
Best model so far, saving...
Epoch: 0145 nll_train: 1313.0952453613 kl_train: -5.9318912029 mse_train: 0.0069110274 acc_train: 0.2114942529 nll_val: 4748.0791015625 kl_val: -6.8821711540 mse_val: 0.0249898881 acc_val: 0.7747126437 time: 3.6556s
Best model so far, saving...
Epoch: 0146 nll_train: 1311.7088012695 kl_train: -5.9367017746 mse_train: 0.0069037296 acc_train: 0.2109195402 nll_val: 4747.5595703125 kl_val: -6.8894572258 mse_val: 0.0249871518 acc_val: 0.7735632184 time: 3.5955s
Best model so far, saving...
Epoch: 0147 nll_train: 1312.8210144043 kl_train: -5.9415943623 mse_train: 0.0069095839 acc_train: 0.2097701149 nll_val: 4749.7851562500 kl_val: -6.8967757225 mse_val: 0.0249988642 acc_val: 0.7747126437 time: 4.0711s
Epoch: 0148 nll_train: 1312.5555419922 kl_train: -5.9465517998 mse_train: 0.0069081873 acc_train: 0.2097701149 nll_val: 4746.8906250000 kl_val: -6.9041008949 mse_val: 0.0249836333 acc_val: 0.7770114943 time: 4.0555s
Best model so far, saving...
Epoch: 0149 nll_train: 1311.6003112793 kl_train: -5.9515290260 mse_train: 0.0069031595 acc_train: 0.2114942529 nll_val: 4745.3134765625 kl_val: -6.9114522934 mse_val: 0.0249753371 acc_val: 0.7758620690 time: 3.8593s
Best model so far, saving...
Epoch: 0150 nll_train: 1312.4882812500 kl_train: -5.9565875530 mse_train: 0.0069078326 acc_train: 0.2120689655 nll_val: 4745.3818359375 kl_val: -6.9189028740 mse_val: 0.0249756929 acc_val: 0.7770114943 time: 3.6560s
Epoch: 0151 nll_train: 1311.7573852539 kl_train: -5.9617185593 mse_train: 0.0069039860 acc_train: 0.2114942529 nll_val: 4747.4062500000 kl_val: -6.9264421463 mse_val: 0.0249863472 acc_val: 0.7770114943 time: 3.7235s
Epoch: 0152 nll_train: 1311.3985595703 kl_train: -5.9668900967 mse_train: 0.0069020977 acc_train: 0.2103448276 nll_val: 4745.3071289062 kl_val: -6.9339237213 mse_val: 0.0249752980 acc_val: 0.7781609195 time: 3.9908s
Best model so far, saving...
Epoch: 0153 nll_train: 1312.7499389648 kl_train: -5.9721100330 mse_train: 0.0069092105 acc_train: 0.2091954023 nll_val: 4744.5556640625 kl_val: -6.9414029121 mse_val: 0.0249713473 acc_val: 0.7781609195 time: 3.7675s
Best model so far, saving...
Epoch: 0154 nll_train: 1309.7411193848 kl_train: -5.9773931503 mse_train: 0.0068933743 acc_train: 0.2086206897 nll_val: 4746.4165039062 kl_val: -6.9489083290 mse_val: 0.0249811355 acc_val: 0.7827586207 time: 3.2710s
Epoch: 0155 nll_train: 1311.2820739746 kl_train: -5.9827258587 mse_train: 0.0069014846 acc_train: 0.2091954023 nll_val: 4746.4628906250 kl_val: -6.9564008713 mse_val: 0.0249813814 acc_val: 0.7816091954 time: 2.8168s
Epoch: 0156 nll_train: 1311.9801635742 kl_train: -5.9881193638 mse_train: 0.0069051589 acc_train: 0.2091954023 nll_val: 4747.2172851562 kl_val: -6.9640412331 mse_val: 0.0249853544 acc_val: 0.7804597701 time: 2.7848s
Epoch: 0157 nll_train: 1309.7221679688 kl_train: -5.9936401844 mse_train: 0.0068932748 acc_train: 0.2086206897 nll_val: 4746.7431640625 kl_val: -6.9717717171 mse_val: 0.0249828603 acc_val: 0.7816091954 time: 3.2965s
Epoch: 0158 nll_train: 1309.5377197266 kl_train: -5.9992263317 mse_train: 0.0068923035 acc_train: 0.2080459770 nll_val: 4746.8071289062 kl_val: -6.9793896675 mse_val: 0.0249831937 acc_val: 0.7816091954 time: 3.6007s
Epoch: 0159 nll_train: 1309.8614501953 kl_train: -6.0047647953 mse_train: 0.0068940073 acc_train: 0.2086206897 nll_val: 4747.8007812500 kl_val: -6.9869613647 mse_val: 0.0249884240 acc_val: 0.7816091954 time: 3.7512s
Epoch: 0160 nll_train: 1309.1399536133 kl_train: -6.0103199482 mse_train: 0.0068902100 acc_train: 0.2080459770 nll_val: 4747.1928710938 kl_val: -6.9946093559 mse_val: 0.0249852259 acc_val: 0.7827586207 time: 3.6527s
Epoch: 0161 nll_train: 1310.0820007324 kl_train: -6.0159468651 mse_train: 0.0068951683 acc_train: 0.2080459770 nll_val: 4749.7563476562 kl_val: -7.0024776459 mse_val: 0.0249987151 acc_val: 0.7827586207 time: 3.7028s
Epoch: 0162 nll_train: 1310.0799865723 kl_train: -6.0216951370 mse_train: 0.0068951576 acc_train: 0.2080459770 nll_val: 4749.0771484375 kl_val: -7.0103850365 mse_val: 0.0249951407 acc_val: 0.7839080460 time: 4.1299s
Epoch: 0163 nll_train: 1309.9674377441 kl_train: -6.0275006294 mse_train: 0.0068945650 acc_train: 0.2074712644 nll_val: 4749.8603515625 kl_val: -7.0183258057 mse_val: 0.0249992609 acc_val: 0.7850574713 time: 4.0124s
Epoch: 0164 nll_train: 1308.8918151855 kl_train: -6.0333795547 mse_train: 0.0068889043 acc_train: 0.2068965517 nll_val: 4747.6479492188 kl_val: -7.0262594223 mse_val: 0.0249876175 acc_val: 0.7850574713 time: 4.0234s
Epoch: 0165 nll_train: 1310.9007873535 kl_train: -6.0392923355 mse_train: 0.0068994771 acc_train: 0.2057471264 nll_val: 4749.8881835938 kl_val: -7.0341591835 mse_val: 0.0249994099 acc_val: 0.7850574713 time: 4.0554s
Epoch: 0166 nll_train: 1308.3089599609 kl_train: -6.0452055931 mse_train: 0.0068858359 acc_train: 0.2057471264 nll_val: 4756.4775390625 kl_val: -7.0419473648 mse_val: 0.0250340905 acc_val: 0.7816091954 time: 4.0030s
Epoch: 0167 nll_train: 1325.8921508789 kl_train: -6.0510878563 mse_train: 0.0069783794 acc_train: 0.2063218391 nll_val: 4746.4633789062 kl_val: -7.0498676300 mse_val: 0.0249813851 acc_val: 0.7804597701 time: 3.9720s
Epoch: 0168 nll_train: 1300.7230224609 kl_train: -6.0570847988 mse_train: 0.0068459107 acc_train: 0.2057471264 nll_val: 4755.0805664062 kl_val: -7.0578894615 mse_val: 0.0250267442 acc_val: 0.7781609195 time: 3.9625s
Epoch: 0169 nll_train: 1310.8620910645 kl_train: -6.0631449223 mse_train: 0.0068992740 acc_train: 0.2045977011 nll_val: 4750.5561523438 kl_val: -7.0656437874 mse_val: 0.0250029266 acc_val: 0.7793103448 time: 3.7921s
Epoch: 0170 nll_train: 1313.0064392090 kl_train: -6.0691175461 mse_train: 0.0069105603 acc_train: 0.2045977011 nll_val: 4748.4345703125 kl_val: -7.0734324455 mse_val: 0.0249917600 acc_val: 0.7770114943 time: 3.6452s
Epoch: 0171 nll_train: 1301.2572631836 kl_train: -6.0752198696 mse_train: 0.0068487228 acc_train: 0.2051724138 nll_val: 4757.5678710938 kl_val: -7.0813975334 mse_val: 0.0250398293 acc_val: 0.7781609195 time: 3.7287s
Epoch: 0172 nll_train: 1310.5114135742 kl_train: -6.0813386440 mse_train: 0.0068974282 acc_train: 0.2045977011 nll_val: 4751.2602539062 kl_val: -7.0892167091 mse_val: 0.0250066333 acc_val: 0.7781609195 time: 3.9915s
Epoch: 0173 nll_train: 1310.1349487305 kl_train: -6.0874190331 mse_train: 0.0068954471 acc_train: 0.2040229885 nll_val: 4745.2651367188 kl_val: -7.0971469879 mse_val: 0.0249750782 acc_val: 0.7781609195 time: 3.7866s
Epoch: 0174 nll_train: 1302.0957336426 kl_train: -6.0936584473 mse_train: 0.0068531350 acc_train: 0.2034482759 nll_val: 4752.7431640625 kl_val: -7.1052045822 mse_val: 0.0250144359 acc_val: 0.7758620690 time: 3.8311s
Epoch: 0175 nll_train: 1309.8627929688 kl_train: -6.0999557972 mse_train: 0.0068940142 acc_train: 0.2022988506 nll_val: 4748.1323242188 kl_val: -7.1130847931 mse_val: 0.0249901712 acc_val: 0.7770114943 time: 3.7410s
Epoch: 0176 nll_train: 1307.6633911133 kl_train: -6.1062803268 mse_train: 0.0068824386 acc_train: 0.2022988506 nll_val: 4750.8305664062 kl_val: -7.1211805344 mse_val: 0.0250043664 acc_val: 0.7770114943 time: 3.8228s
Epoch: 0177 nll_train: 1304.3891906738 kl_train: -6.1127738953 mse_train: 0.0068652063 acc_train: 0.2022988506 nll_val: 4751.9228515625 kl_val: -7.1293368340 mse_val: 0.0250101201 acc_val: 0.7770114943 time: 3.7886s
Epoch: 0178 nll_train: 1308.9752502441 kl_train: -6.1192607880 mse_train: 0.0068893428 acc_train: 0.2022988506 nll_val: 4749.9296875000 kl_val: -7.1374511719 mse_val: 0.0249996278 acc_val: 0.7770114943 time: 3.6657s
Epoch: 0179 nll_train: 1306.1956481934 kl_train: -6.1257913113 mse_train: 0.0068747137 acc_train: 0.2028735632 nll_val: 4752.9570312500 kl_val: -7.1457753181 mse_val: 0.0250155628 acc_val: 0.7758620690 time: 3.6210s
Epoch: 0180 nll_train: 1305.0462951660 kl_train: -6.1324739456 mse_train: 0.0068686648 acc_train: 0.2034482759 nll_val: 4751.9150390625 kl_val: -7.1542072296 mse_val: 0.0250100791 acc_val: 0.7770114943 time: 3.7917s
Epoch: 0181 nll_train: 1307.8519897461 kl_train: -6.1392168999 mse_train: 0.0068834312 acc_train: 0.2040229885 nll_val: 4759.5332031250 kl_val: -7.1626739502 mse_val: 0.0250501744 acc_val: 0.7781609195 time: 3.6471s
Epoch: 0182 nll_train: 1320.4603271484 kl_train: -6.1460206509 mse_train: 0.0069497905 acc_train: 0.2040229885 nll_val: 4746.9687500000 kl_val: -7.1714787483 mse_val: 0.0249840450 acc_val: 0.7781609195 time: 3.4905s
Epoch: 0183 nll_train: 1295.7427062988 kl_train: -6.1530768871 mse_train: 0.0068196983 acc_train: 0.2045977011 nll_val: 4759.3085937500 kl_val: -7.1804261208 mse_val: 0.0250489898 acc_val: 0.7793103448 time: 3.4813s
Epoch: 0184 nll_train: 1310.3758544922 kl_train: -6.1602065563 mse_train: 0.0068967139 acc_train: 0.2034482759 nll_val: 4748.3828125000 kl_val: -7.1888937950 mse_val: 0.0249914899 acc_val: 0.7793103448 time: 3.6765s
Epoch: 0185 nll_train: 1309.7198486328 kl_train: -6.1672198772 mse_train: 0.0068932610 acc_train: 0.2034482759 nll_val: 4749.9599609375 kl_val: -7.1975636482 mse_val: 0.0249997843 acc_val: 0.7804597701 time: 3.7068s
Epoch: 0186 nll_train: 1297.4988403320 kl_train: -6.1744730473 mse_train: 0.0068289412 acc_train: 0.2034482759 nll_val: 4759.2993164062 kl_val: -7.2062878609 mse_val: 0.0250489395 acc_val: 0.7804597701 time: 3.7933s
Epoch: 0187 nll_train: 1310.3983459473 kl_train: -6.1817197800 mse_train: 0.0068968332 acc_train: 0.2034482759 nll_val: 4745.8247070312 kl_val: -7.2147083282 mse_val: 0.0249780249 acc_val: 0.7816091954 time: 3.8302s
Epoch: 0188 nll_train: 1305.1719665527 kl_train: -6.1889240742 mse_train: 0.0068693259 acc_train: 0.2034482759 nll_val: 4750.5424804688 kl_val: -7.2233777046 mse_val: 0.0250028539 acc_val: 0.7816091954 time: 3.8743s
Epoch: 0189 nll_train: 1300.4553527832 kl_train: -6.1962952614 mse_train: 0.0068445021 acc_train: 0.2028735632 nll_val: 4754.6401367188 kl_val: -7.2319869995 mse_val: 0.0250244197 acc_val: 0.7827586207 time: 3.6952s
Epoch: 0190 nll_train: 1310.6134948730 kl_train: -6.2036354542 mse_train: 0.0068979654 acc_train: 0.2028735632 nll_val: 4751.6586914062 kl_val: -7.2404780388 mse_val: 0.0250087287 acc_val: 0.7827586207 time: 3.6646s
Epoch: 0191 nll_train: 1302.4592895508 kl_train: -6.2110362053 mse_train: 0.0068550485 acc_train: 0.2028735632 nll_val: 4753.2661132812 kl_val: -7.2490758896 mse_val: 0.0250171889 acc_val: 0.7827586207 time: 3.8125s
Epoch: 0192 nll_train: 1302.0721435547 kl_train: -6.2185392380 mse_train: 0.0068530113 acc_train: 0.2028735632 nll_val: 4751.6860351562 kl_val: -7.2574915886 mse_val: 0.0250088722 acc_val: 0.7827586207 time: 3.5932s
Epoch: 0193 nll_train: 1307.9266357422 kl_train: -6.2259290218 mse_train: 0.0068838244 acc_train: 0.2022988506 nll_val: 4749.7534179688 kl_val: -7.2657260895 mse_val: 0.0249987002 acc_val: 0.7827586207 time: 3.5886s
Epoch: 0194 nll_train: 1301.8022460938 kl_train: -6.2333123684 mse_train: 0.0068515909 acc_train: 0.2017241379 nll_val: 4755.8085937500 kl_val: -7.2740683556 mse_val: 0.0250305682 acc_val: 0.7804597701 time: 3.9555s
Epoch: 0195 nll_train: 1304.0745544434 kl_train: -6.2407484055 mse_train: 0.0068635492 acc_train: 0.2017241379 nll_val: 4753.8491210938 kl_val: -7.2823982239 mse_val: 0.0250202548 acc_val: 0.7793103448 time: 4.0782s
Epoch: 0196 nll_train: 1306.0558776855 kl_train: -6.2481808662 mse_train: 0.0068739788 acc_train: 0.2011494253 nll_val: 4752.5302734375 kl_val: -7.2908282280 mse_val: 0.0250133201 acc_val: 0.7793103448 time: 3.6263s
Epoch: 0197 nll_train: 1300.5226745605 kl_train: -6.2557201385 mse_train: 0.0068448558 acc_train: 0.2005747126 nll_val: 4754.0332031250 kl_val: -7.2994742393 mse_val: 0.0250212289 acc_val: 0.7793103448 time: 3.0290s
Epoch: 0198 nll_train: 1304.5288391113 kl_train: -6.2633311749 mse_train: 0.0068659412 acc_train: 0.1994252874 nll_val: 4749.5834960938 kl_val: -7.3079547882 mse_val: 0.0249978043 acc_val: 0.7793103448 time: 3.0805s
Epoch: 0199 nll_train: 1280.1403808594 kl_train: -6.2700078487 mse_train: 0.0067375802 acc_train: 0.1994252874 nll_val: 4748.3212890625 kl_val: -7.3091902733 mse_val: 0.0249911621 acc_val: 0.7781609195 time: 3.0831s
Optimization finished
Best epoch 153
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 324.8900451660 kl_test: -6.4182796478 mse_test: 0.0017099477 acc_test: 0.1402298851
MSE: [ 0.000795719854 , 0.000782056421 , 0.000776232802 , 0.000755005167 , 0.000799199217 , 0.000727118109 , 0.001252307557 , 0.001311525819 , 0.001283567399 , 0.001528418506 , 0.001495119766 , 0.001466143527 , 0.001688222052 , 0.001710532466 , 0.001950640348 , 0.001998504158 , 0.002064592438 , 0.002162518213 , 0.002210132778 ]
Accuracy for experiment id 13 is 0.13555555555555557
Using MLP encoder.
Using learned interaction net decoder.
Epoch: 0000 nll_train: 96770.6303710938 kl_train: -13.1398096085 mse_train: 0.5093190260 acc_train: 0.5051724138 nll_val: 16640.2675781250 kl_val: -13.7452974319 mse_val: 0.0875803456 acc_val: 0.0000000000 time: 3.1652s
Best model so far, saving...
Epoch: 0001 nll_train: 35181.7890625000 kl_train: -7.4375398159 mse_train: 0.1851672977 acc_train: 0.2034482759 nll_val: 7895.8081054688 kl_val: -14.7837486267 mse_val: 0.0415568799 acc_val: 0.0000000000 time: 3.1499s
Best model so far, saving...
Epoch: 0002 nll_train: 20034.7656250000 kl_train: -5.3613827229 mse_train: 0.1054461300 acc_train: 0.0706896552 nll_val: 7898.0239257812 kl_val: -19.5034904480 mse_val: 0.0415685475 acc_val: 0.0000000000 time: 3.1208s
Epoch: 0003 nll_train: 3525.8356323242 kl_train: -5.6448936462 mse_train: 0.0185570300 acc_train: 0.0206896552 nll_val: 11110.6660156250 kl_val: -18.6502914429 mse_val: 0.0584771894 acc_val: 1.0000000000 time: 3.1756s
Epoch: 0004 nll_train: 5178.4423828125 kl_train: -6.1476869583 mse_train: 0.0272549586 acc_train: 0.0172413793 nll_val: 11475.9951171875 kl_val: -19.3414211273 mse_val: 0.0603999719 acc_val: 0.0000000000 time: 3.1522s
Epoch: 0005 nll_train: 4815.4935302734 kl_train: -4.9296860695 mse_train: 0.0253447033 acc_train: 0.0155172414 nll_val: 7564.0283203125 kl_val: -5.1154050827 mse_val: 0.0398106724 acc_val: 0.0000000000 time: 3.2441s
Best model so far, saving...
Epoch: 0006 nll_train: 1696.8021240234 kl_train: -3.1377323866 mse_train: 0.0089305373 acc_train: 0.0379310345 nll_val: 8369.0546875000 kl_val: -0.8861188889 mse_val: 0.0440476574 acc_val: 0.0000000000 time: 3.2579s
Epoch: 0007 nll_train: 2465.3836059570 kl_train: -2.6576120853 mse_train: 0.0129757025 acc_train: 0.0666666667 nll_val: 8547.0488281250 kl_val: -0.3660934269 mse_val: 0.0449844636 acc_val: 0.0000000000 time: 3.1449s
Epoch: 0008 nll_train: 2512.3376770020 kl_train: -2.4971677065 mse_train: 0.0132228297 acc_train: 0.0873563218 nll_val: 8390.4345703125 kl_val: -0.2899169028 mse_val: 0.0441601798 acc_val: 0.0000000000 time: 3.1044s
Epoch: 0009 nll_train: 1728.8955078125 kl_train: -2.4143804312 mse_train: 0.0090994502 acc_train: 0.0971264368 nll_val: 8267.1201171875 kl_val: -0.2793404162 mse_val: 0.0435111560 acc_val: 0.0000000000 time: 3.0370s
Epoch: 0010 nll_train: 2041.0697631836 kl_train: -2.3595737219 mse_train: 0.0107424716 acc_train: 0.1080459770 nll_val: 8120.5297851562 kl_val: -0.2603775859 mse_val: 0.0427396260 acc_val: 0.0000000000 time: 3.0773s
Epoch: 0011 nll_train: 1546.1129150391 kl_train: -2.3171972632 mse_train: 0.0081374375 acc_train: 0.1097701149 nll_val: 8131.0722656250 kl_val: -0.2445103228 mse_val: 0.0427951179 acc_val: 0.0000000000 time: 3.0077s
Epoch: 0012 nll_train: 1405.4156799316 kl_train: -2.2848849297 mse_train: 0.0073969250 acc_train: 0.1137931034 nll_val: 7956.3613281250 kl_val: -0.3508384228 mse_val: 0.0418755896 acc_val: 0.0000000000 time: 3.0843s
Epoch: 0013 nll_train: 1628.1954956055 kl_train: -2.2612390518 mse_train: 0.0085694498 acc_train: 0.1155172414 nll_val: 7584.7861328125 kl_val: -0.5465227365 mse_val: 0.0399199240 acc_val: 0.0011494253 time: 3.8843s
Epoch: 0014 nll_train: 1474.2741088867 kl_train: -2.2440394759 mse_train: 0.0077593373 acc_train: 0.1172413793 nll_val: 7251.3339843750 kl_val: -0.7439512014 mse_val: 0.0381649137 acc_val: 0.0057471264 time: 3.7539s
Best model so far, saving...
Epoch: 0015 nll_train: 1467.3700561523 kl_train: -2.2317100167 mse_train: 0.0077229997 acc_train: 0.1166666667 nll_val: 6977.4995117188 kl_val: -0.8149206042 mse_val: 0.0367236771 acc_val: 0.0333333333 time: 3.7559s
Best model so far, saving...
Epoch: 0016 nll_train: 1294.2893981934 kl_train: -2.2228016853 mse_train: 0.0068120493 acc_train: 0.1172413793 nll_val: 6899.5703125000 kl_val: -0.8112297058 mse_val: 0.0363135263 acc_val: 0.0655172414 time: 3.7841s
Best model so far, saving...
Epoch: 0017 nll_train: 1399.6473388672 kl_train: -2.2174658179 mse_train: 0.0073665648 acc_train: 0.1149425287 nll_val: 6746.2470703125 kl_val: -0.8701582551 mse_val: 0.0355065577 acc_val: 0.0666666667 time: 3.8838s
Best model so far, saving...
Epoch: 0018 nll_train: 1401.4080505371 kl_train: -2.2151179314 mse_train: 0.0073758318 acc_train: 0.1137931034 nll_val: 6422.1093750000 kl_val: -1.0478657484 mse_val: 0.0338005722 acc_val: 0.0666666667 time: 4.0494s
Best model so far, saving...
Epoch: 0019 nll_train: 1389.2174377441 kl_train: -2.2151392698 mse_train: 0.0073116706 acc_train: 0.1155172414 nll_val: 6154.2001953125 kl_val: -1.2655626535 mse_val: 0.0323905237 acc_val: 0.0712643678 time: 4.0701s
Best model so far, saving...
Epoch: 0020 nll_train: 1347.5305480957 kl_train: -2.2165142894 mse_train: 0.0070922659 acc_train: 0.1132183908 nll_val: 6028.7666015625 kl_val: -1.4336618185 mse_val: 0.0317303501 acc_val: 0.0735632184 time: 3.9271s
Best model so far, saving...
Epoch: 0021 nll_train: 1299.7976684570 kl_train: -2.2190271020 mse_train: 0.0068410400 acc_train: 0.1120689655 nll_val: 5870.6562500000 kl_val: -1.5701279640 mse_val: 0.0308981873 acc_val: 0.0954022989 time: 3.9289s
Best model so far, saving...
Epoch: 0022 nll_train: 1355.4009704590 kl_train: -2.2230305672 mse_train: 0.0071336889 acc_train: 0.1103448276 nll_val: 5626.1328125000 kl_val: -1.6978256702 mse_val: 0.0296112243 acc_val: 0.1344827586 time: 3.8884s
Best model so far, saving...
Epoch: 0023 nll_train: 1354.3089904785 kl_train: -2.2282413840 mse_train: 0.0071279416 acc_train: 0.1091954023 nll_val: 5442.2109375000 kl_val: -1.8642745018 mse_val: 0.0286432169 acc_val: 0.1402298851 time: 3.8645s
Best model so far, saving...
Epoch: 0024 nll_train: 1345.3871765137 kl_train: -2.2342760563 mse_train: 0.0070809851 acc_train: 0.1109195402 nll_val: 5331.6181640625 kl_val: -2.1392476559 mse_val: 0.0280611459 acc_val: 0.1494252874 time: 3.9272s
Best model so far, saving...
Epoch: 0025 nll_train: 1307.8171997070 kl_train: -2.2405812740 mse_train: 0.0068832482 acc_train: 0.1097701149 nll_val: 5327.8105468750 kl_val: -2.4426672459 mse_val: 0.0280411057 acc_val: 0.1816091954 time: 3.8555s
Best model so far, saving...
Epoch: 0026 nll_train: 1311.1842346191 kl_train: -2.2473937869 mse_train: 0.0069009697 acc_train: 0.1068965517 nll_val: 5275.4731445312 kl_val: -2.6660802364 mse_val: 0.0277656466 acc_val: 0.2229885057 time: 3.8444s
Best model so far, saving...
Epoch: 0027 nll_train: 1336.9836120605 kl_train: -2.2547228336 mse_train: 0.0070367554 acc_train: 0.1051724138 nll_val: 5123.6611328125 kl_val: -2.8050272465 mse_val: 0.0269666351 acc_val: 0.2597701149 time: 3.9285s
Best model so far, saving...
Epoch: 0028 nll_train: 1334.7025451660 kl_train: -2.2623825073 mse_train: 0.0070247500 acc_train: 0.1045977011 nll_val: 5036.5009765625 kl_val: -2.9288198948 mse_val: 0.0265078992 acc_val: 0.2712643678 time: 4.1227s
Best model so far, saving...
Epoch: 0029 nll_train: 1316.7418212891 kl_train: -2.2700718045 mse_train: 0.0069302198 acc_train: 0.1034482759 nll_val: 5014.2080078125 kl_val: -3.0816605091 mse_val: 0.0263905637 acc_val: 0.2816091954 time: 4.0953s
Best model so far, saving...
Epoch: 0030 nll_train: 1295.8991394043 kl_train: -2.2777027488 mse_train: 0.0068205217 acc_train: 0.1028735632 nll_val: 5005.1342773438 kl_val: -3.2550244331 mse_val: 0.0263428129 acc_val: 0.3034482759 time: 3.9133s
Best model so far, saving...
Epoch: 0031 nll_train: 1315.8973388672 kl_train: -2.2855657339 mse_train: 0.0069257752 acc_train: 0.1034482759 nll_val: 4954.6552734375 kl_val: -3.4319334030 mse_val: 0.0260771308 acc_val: 0.3206896552 time: 3.9576s
Best model so far, saving...
Epoch: 0032 nll_train: 1327.5554199219 kl_train: -2.2937415838 mse_train: 0.0069871331 acc_train: 0.1034482759 nll_val: 4896.2753906250 kl_val: -3.6086528301 mse_val: 0.0257698707 acc_val: 0.3505747126 time: 3.9104s
Best model so far, saving...
Epoch: 0033 nll_train: 1312.5420532227 kl_train: -2.3020330071 mse_train: 0.0069081157 acc_train: 0.1028735632 nll_val: 4892.8540039062 kl_val: -3.7795429230 mse_val: 0.0257518608 acc_val: 0.3816091954 time: 3.8774s
Best model so far, saving...
Epoch: 0034 nll_train: 1296.5781860352 kl_train: -2.3103030324 mse_train: 0.0068240963 acc_train: 0.1005747126 nll_val: 4899.6337890625 kl_val: -3.9259877205 mse_val: 0.0257875472 acc_val: 0.4183908046 time: 3.8783s
Epoch: 0035 nll_train: 1311.8187866211 kl_train: -2.3186824322 mse_train: 0.0069043088 acc_train: 0.1000000000 nll_val: 4868.0117187500 kl_val: -4.0340094566 mse_val: 0.0256211143 acc_val: 0.4471264368 time: 3.9008s
Best model so far, saving...
Epoch: 0036 nll_train: 1316.6127319336 kl_train: -2.3271499276 mse_train: 0.0069295407 acc_train: 0.1005747126 nll_val: 4845.9501953125 kl_val: -4.1086235046 mse_val: 0.0255050026 acc_val: 0.4724137931 time: 3.8305s
Best model so far, saving...
Epoch: 0037 nll_train: 1303.4061279297 kl_train: -2.3354502320 mse_train: 0.0068600322 acc_train: 0.1000000000 nll_val: 4839.2041015625 kl_val: -4.1594204903 mse_val: 0.0254694931 acc_val: 0.4977011494 time: 3.8709s
Best model so far, saving...
Epoch: 0038 nll_train: 1298.8121643066 kl_train: -2.3436840773 mse_train: 0.0068358530 acc_train: 0.0994252874 nll_val: 4841.1411132812 kl_val: -4.1863622665 mse_val: 0.0254796911 acc_val: 0.5218390805 time: 4.0361s
Epoch: 0039 nll_train: 1308.7284240723 kl_train: -2.3519636393 mse_train: 0.0068880441 acc_train: 0.0994252874 nll_val: 4824.6430664062 kl_val: -4.1954293251 mse_val: 0.0253928564 acc_val: 0.5494252874 time: 4.0413s
Best model so far, saving...
Epoch: 0040 nll_train: 1307.7956237793 kl_train: -2.3602132797 mse_train: 0.0068831347 acc_train: 0.1000000000 nll_val: 4814.0800781250 kl_val: -4.1970491409 mse_val: 0.0253372621 acc_val: 0.5643678161 time: 3.9210s
Best model so far, saving...
Epoch: 0041 nll_train: 1298.7474060059 kl_train: -2.3684462309 mse_train: 0.0068355125 acc_train: 0.1000000000 nll_val: 4808.5161132812 kl_val: -4.1973557472 mse_val: 0.0253079794 acc_val: 0.5862068966 time: 3.8463s
Best model so far, saving...
Epoch: 0042 nll_train: 1297.6914672852 kl_train: -2.3767265081 mse_train: 0.0068299548 acc_train: 0.0994252874 nll_val: 4806.9853515625 kl_val: -4.1926183701 mse_val: 0.0252999216 acc_val: 0.5977011494 time: 3.8532s
Best model so far, saving...
Epoch: 0043 nll_train: 1305.2397766113 kl_train: -2.3851198554 mse_train: 0.0068696827 acc_train: 0.1000000000 nll_val: 4801.3002929688 kl_val: -4.1848096848 mse_val: 0.0252700001 acc_val: 0.6103448276 time: 3.8401s
Best model so far, saving...
Epoch: 0044 nll_train: 1301.9522705078 kl_train: -2.3936392665 mse_train: 0.0068523801 acc_train: 0.1005747126 nll_val: 4802.0600585938 kl_val: -4.1792945862 mse_val: 0.0252739973 acc_val: 0.6160919540 time: 3.8216s
Epoch: 0045 nll_train: 1294.5219116211 kl_train: -2.4019733071 mse_train: 0.0068132732 acc_train: 0.1011494253 nll_val: 4805.0166015625 kl_val: -4.1768617630 mse_val: 0.0252895579 acc_val: 0.6218390805 time: 3.8125s
Epoch: 0046 nll_train: 1300.3781127930 kl_train: -2.4103600383 mse_train: 0.0068440951 acc_train: 0.1011494253 nll_val: 4808.0800781250 kl_val: -4.1726922989 mse_val: 0.0253056828 acc_val: 0.6252873563 time: 3.8411s
Epoch: 0047 nll_train: 1301.2280273438 kl_train: -2.4187806845 mse_train: 0.0068485689 acc_train: 0.1017241379 nll_val: 4812.9306640625 kl_val: -4.1679701805 mse_val: 0.0253312122 acc_val: 0.6310344828 time: 3.8203s
Epoch: 0048 nll_train: 1299.1169738770 kl_train: -2.4272100925 mse_train: 0.0068374581 acc_train: 0.1005747126 nll_val: 4805.7387695312 kl_val: -4.1680617332 mse_val: 0.0252933595 acc_val: 0.6321839080 time: 3.9943s
Epoch: 0049 nll_train: 1296.1176757812 kl_train: -2.4354529381 mse_train: 0.0068216719 acc_train: 0.1005747126 nll_val: 4797.5336914062 kl_val: -4.1679096222 mse_val: 0.0252501760 acc_val: 0.6367816092 time: 4.0182s
Best model so far, saving...
Epoch: 0050 nll_train: 1301.6563110352 kl_train: -2.4437508583 mse_train: 0.0068508229 acc_train: 0.1005747126 nll_val: 4792.4897460938 kl_val: -4.1687474251 mse_val: 0.0252236277 acc_val: 0.6390804598 time: 4.1172s
Best model so far, saving...
Epoch: 0051 nll_train: 1295.6653137207 kl_train: -2.4520386457 mse_train: 0.0068192910 acc_train: 0.1011494253 nll_val: 4797.2519531250 kl_val: -4.1742897034 mse_val: 0.0252486933 acc_val: 0.6425287356 time: 3.8557s
Epoch: 0052 nll_train: 1291.4451293945 kl_train: -2.4602313042 mse_train: 0.0067970790 acc_train: 0.1017241379 nll_val: 4803.0805664062 kl_val: -4.1788625717 mse_val: 0.0252793711 acc_val: 0.6459770115 time: 3.8214s
Epoch: 0053 nll_train: 1298.4992980957 kl_train: -2.4685004950 mse_train: 0.0068342064 acc_train: 0.1017241379 nll_val: 4797.4306640625 kl_val: -4.1857666969 mse_val: 0.0252496339 acc_val: 0.6517241379 time: 3.7912s
Epoch: 0054 nll_train: 1296.1858215332 kl_train: -2.4768881798 mse_train: 0.0068220304 acc_train: 0.1022988506 nll_val: 4805.3588867188 kl_val: -4.1955361366 mse_val: 0.0252913609 acc_val: 0.6540229885 time: 3.7428s
Epoch: 0055 nll_train: 1291.4422607422 kl_train: -2.4853103161 mse_train: 0.0067970644 acc_train: 0.1000000000 nll_val: 4839.7211914062 kl_val: -4.2087936401 mse_val: 0.0254722126 acc_val: 0.6551724138 time: 3.7692s
Epoch: 0056 nll_train: 1336.6174621582 kl_train: -2.4938892126 mse_train: 0.0070348285 acc_train: 0.1000000000 nll_val: 4806.3183593750 kl_val: -4.2309746742 mse_val: 0.0252964143 acc_val: 0.6609195402 time: 5.0915s
Epoch: 0057 nll_train: 1286.1329345703 kl_train: -2.5023723841 mse_train: 0.0067691207 acc_train: 0.1000000000 nll_val: 4809.1562500000 kl_val: -4.2620282173 mse_val: 0.0253113490 acc_val: 0.6632183908 time: 4.2605s
Epoch: 0058 nll_train: 1289.4358520508 kl_train: -2.5109467506 mse_train: 0.0067865041 acc_train: 0.1005747126 nll_val: 4807.3178710938 kl_val: -4.2794909477 mse_val: 0.0253016688 acc_val: 0.6666666667 time: 4.0721s
Epoch: 0059 nll_train: 1304.8034362793 kl_train: -2.5197340250 mse_train: 0.0068673865 acc_train: 0.1011494253 nll_val: 4804.3242187500 kl_val: -4.2906651497 mse_val: 0.0252859127 acc_val: 0.6678160920 time: 4.1746s
Epoch: 0060 nll_train: 1310.9025573730 kl_train: -2.5284841061 mse_train: 0.0068994866 acc_train: 0.1011494253 nll_val: 4803.7885742188 kl_val: -4.3114080429 mse_val: 0.0252831001 acc_val: 0.6689655172 time: 4.2064s
Epoch: 0061 nll_train: 1279.9704895020 kl_train: -2.5370107889 mse_train: 0.0067366866 acc_train: 0.1011494253 nll_val: 4802.7573242188 kl_val: -4.3365631104 mse_val: 0.0252776667 acc_val: 0.6701149425 time: 4.1001s
Epoch: 0062 nll_train: 1302.5210266113 kl_train: -2.5458465815 mse_train: 0.0068553737 acc_train: 0.1028735632 nll_val: 4796.2890625000 kl_val: -4.3459744453 mse_val: 0.0252436269 acc_val: 0.6689655172 time: 3.9627s
Epoch: 0063 nll_train: 1300.8428039551 kl_train: -2.5549880266 mse_train: 0.0068465410 acc_train: 0.1034482759 nll_val: 4802.1342773438 kl_val: -4.3553566933 mse_val: 0.0252743904 acc_val: 0.6701149425 time: 3.9381s
Epoch: 0064 nll_train: 1284.1945800781 kl_train: -2.5638357401 mse_train: 0.0067589185 acc_train: 0.1022988506 nll_val: 4803.9248046875 kl_val: -4.3677501678 mse_val: 0.0252838098 acc_val: 0.6701149425 time: 3.9610s
Epoch: 0065 nll_train: 1290.6735839844 kl_train: -2.5727052689 mse_train: 0.0067930188 acc_train: 0.1022988506 nll_val: 4808.6357421875 kl_val: -4.3711156845 mse_val: 0.0253086034 acc_val: 0.6689655172 time: 4.1931s
Epoch: 0066 nll_train: 1299.4254760742 kl_train: -2.5819309950 mse_train: 0.0068390813 acc_train: 0.1011494253 nll_val: 4814.4316406250 kl_val: -4.3727931976 mse_val: 0.0253391154 acc_val: 0.6678160920 time: 3.9560s
Epoch: 0067 nll_train: 1288.1469421387 kl_train: -2.5913002491 mse_train: 0.0067797210 acc_train: 0.1022988506 nll_val: 4811.3994140625 kl_val: -4.3776817322 mse_val: 0.0253231525 acc_val: 0.6678160920 time: 3.8839s
Epoch: 0068 nll_train: 1290.4901428223 kl_train: -2.6008801460 mse_train: 0.0067920532 acc_train: 0.1017241379 nll_val: 4809.1977539062 kl_val: -4.3780136108 mse_val: 0.0253115650 acc_val: 0.6701149425 time: 5.4980s
Epoch: 0069 nll_train: 1292.3002624512 kl_train: -2.6107141972 mse_train: 0.0068015801 acc_train: 0.1028735632 nll_val: 4799.7001953125 kl_val: -4.3791255951 mse_val: 0.0252615772 acc_val: 0.6701149425 time: 3.7974s
Epoch: 0070 nll_train: 1288.9333496094 kl_train: -2.6206101179 mse_train: 0.0067838595 acc_train: 0.1034482759 nll_val: 4796.6738281250 kl_val: -4.3833937645 mse_val: 0.0252456497 acc_val: 0.6689655172 time: 3.6055s
Epoch: 0071 nll_train: 1289.2489929199 kl_train: -2.6305389404 mse_train: 0.0067855208 acc_train: 0.1034482759 nll_val: 4792.7944335938 kl_val: -4.3834714890 mse_val: 0.0252252314 acc_val: 0.6689655172 time: 3.6631s
Epoch: 0072 nll_train: 1291.2586669922 kl_train: -2.6403642893 mse_train: 0.0067960980 acc_train: 0.1040229885 nll_val: 4794.5712890625 kl_val: -4.3805365562 mse_val: 0.0252345838 acc_val: 0.6655172414 time: 3.0984s
Epoch: 0073 nll_train: 1289.8281250000 kl_train: -2.6502072811 mse_train: 0.0067885686 acc_train: 0.1045977011 nll_val: 4798.8007812500 kl_val: -4.3780617714 mse_val: 0.0252568480 acc_val: 0.6643678161 time: 3.2219s
Epoch: 0074 nll_train: 1288.4802856445 kl_train: -2.6601235867 mse_train: 0.0067814745 acc_train: 0.1040229885 nll_val: 4800.7006835938 kl_val: -4.3786416054 mse_val: 0.0252668466 acc_val: 0.6655172414 time: 3.1579s
Epoch: 0075 nll_train: 1289.8613891602 kl_train: -2.6702433825 mse_train: 0.0067887433 acc_train: 0.1045977011 nll_val: 4802.6220703125 kl_val: -4.3805294037 mse_val: 0.0252769571 acc_val: 0.6632183908 time: 3.1198s
Epoch: 0076 nll_train: 1289.7851257324 kl_train: -2.6806148291 mse_train: 0.0067883430 acc_train: 0.1057471264 nll_val: 4803.0439453125 kl_val: -4.3850870132 mse_val: 0.0252791736 acc_val: 0.6620689655 time: 3.0377s
Epoch: 0077 nll_train: 1287.6907348633 kl_train: -2.6909576654 mse_train: 0.0067773187 acc_train: 0.1063218391 nll_val: 4803.9985351562 kl_val: -4.3898029327 mse_val: 0.0252842009 acc_val: 0.6620689655 time: 3.0812s
Epoch: 0078 nll_train: 1289.0452575684 kl_train: -2.7013564110 mse_train: 0.0067844482 acc_train: 0.1074712644 nll_val: 4802.7026367188 kl_val: -4.3922328949 mse_val: 0.0252773799 acc_val: 0.6643678161 time: 3.0883s
Epoch: 0079 nll_train: 1288.7412414551 kl_train: -2.7116529942 mse_train: 0.0067828486 acc_train: 0.1068965517 nll_val: 4803.8642578125 kl_val: -4.3985943794 mse_val: 0.0252834950 acc_val: 0.6620689655 time: 3.1002s
Epoch: 0080 nll_train: 1287.2477416992 kl_train: -2.7219465971 mse_train: 0.0067749880 acc_train: 0.1057471264 nll_val: 4808.0771484375 kl_val: -4.4092507362 mse_val: 0.0253056716 acc_val: 0.6620689655 time: 3.0604s
Epoch: 0081 nll_train: 1288.4899902344 kl_train: -2.7324652672 mse_train: 0.0067815263 acc_train: 0.1051724138 nll_val: 4809.0678710938 kl_val: -4.4186940193 mse_val: 0.0253108833 acc_val: 0.6620689655 time: 3.0973s
Epoch: 0082 nll_train: 1287.7404785156 kl_train: -2.7433154583 mse_train: 0.0067775811 acc_train: 0.1063218391 nll_val: 4814.2607421875 kl_val: -4.4298171997 mse_val: 0.0253382139 acc_val: 0.6620689655 time: 3.1643s
Epoch: 0083 nll_train: 1285.9257202148 kl_train: -2.7542669773 mse_train: 0.0067680299 acc_train: 0.1057471264 nll_val: 4819.7055664062 kl_val: -4.4413371086 mse_val: 0.0253668688 acc_val: 0.6620689655 time: 3.2146s
Epoch: 0084 nll_train: 1286.8262939453 kl_train: -2.7654488087 mse_train: 0.0067727698 acc_train: 0.1057471264 nll_val: 4823.5937500000 kl_val: -4.4552078247 mse_val: 0.0253873356 acc_val: 0.6632183908 time: 3.1476s
Epoch: 0085 nll_train: 1285.5153198242 kl_train: -2.7769796848 mse_train: 0.0067658700 acc_train: 0.1057471264 nll_val: 4823.2617187500 kl_val: -4.4683771133 mse_val: 0.0253855865 acc_val: 0.6620689655 time: 3.1347s
Epoch: 0086 nll_train: 1285.0712280273 kl_train: -2.7886173725 mse_train: 0.0067635321 acc_train: 0.1057471264 nll_val: 4826.1787109375 kl_val: -4.4812803268 mse_val: 0.0254009385 acc_val: 0.6632183908 time: 3.0640s
Epoch: 0087 nll_train: 1286.2124938965 kl_train: -2.8002675772 mse_train: 0.0067695394 acc_train: 0.1057471264 nll_val: 4825.0205078125 kl_val: -4.4939203262 mse_val: 0.0253948458 acc_val: 0.6632183908 time: 3.1163s
Epoch: 0088 nll_train: 1285.6120910645 kl_train: -2.8121238947 mse_train: 0.0067663789 acc_train: 0.1063218391 nll_val: 4828.8056640625 kl_val: -4.5056605339 mse_val: 0.0254147686 acc_val: 0.6643678161 time: 3.1639s
Epoch: 0089 nll_train: 1284.0084838867 kl_train: -2.8240284920 mse_train: 0.0067579392 acc_train: 0.1068965517 nll_val: 4829.8901367188 kl_val: -4.5200157166 mse_val: 0.0254204739 acc_val: 0.6643678161 time: 3.0895s
Epoch: 0090 nll_train: 1285.2122802734 kl_train: -2.8360501528 mse_train: 0.0067642748 acc_train: 0.1068965517 nll_val: 4826.7705078125 kl_val: -4.5400505066 mse_val: 0.0254040547 acc_val: 0.6655172414 time: 3.1233s
Epoch: 0091 nll_train: 1284.4508666992 kl_train: -2.8482242823 mse_train: 0.0067602674 acc_train: 0.1057471264 nll_val: 4829.9965820312 kl_val: -4.5583848953 mse_val: 0.0254210327 acc_val: 0.6689655172 time: 3.1111s
Epoch: 0092 nll_train: 1283.1462097168 kl_train: -2.8603761196 mse_train: 0.0067534002 acc_train: 0.1045977011 nll_val: 4828.2485351562 kl_val: -4.5733380318 mse_val: 0.0254118349 acc_val: 0.6712643678 time: 3.1813s
Epoch: 0093 nll_train: 1284.2644042969 kl_train: -2.8727260828 mse_train: 0.0067592864 acc_train: 0.1045977011 nll_val: 4833.7583007812 kl_val: -4.5895600319 mse_val: 0.0254408326 acc_val: 0.6724137931 time: 3.1554s
Epoch: 0094 nll_train: 1284.2674255371 kl_train: -2.8851217031 mse_train: 0.0067593016 acc_train: 0.1057471264 nll_val: 4835.4658203125 kl_val: -4.6082668304 mse_val: 0.0254498199 acc_val: 0.6735632184 time: 3.1907s
Epoch: 0095 nll_train: 1281.6340026855 kl_train: -2.8972014189 mse_train: 0.0067454425 acc_train: 0.1063218391 nll_val: 4839.9072265625 kl_val: -4.6239552498 mse_val: 0.0254731942 acc_val: 0.6712643678 time: 3.1945s
Epoch: 0096 nll_train: 1286.0252685547 kl_train: -2.9091945887 mse_train: 0.0067685543 acc_train: 0.1063218391 nll_val: 4841.1606445312 kl_val: -4.6447873116 mse_val: 0.0254797880 acc_val: 0.6747126437 time: 3.1774s
Epoch: 0097 nll_train: 1282.9382324219 kl_train: -2.9216663837 mse_train: 0.0067523067 acc_train: 0.1063218391 nll_val: 4838.5249023438 kl_val: -4.6600461006 mse_val: 0.0254659206 acc_val: 0.6770114943 time: 3.0753s
Epoch: 0098 nll_train: 1285.6607055664 kl_train: -2.9344303608 mse_train: 0.0067666346 acc_train: 0.1057471264 nll_val: 4836.9628906250 kl_val: -4.6745343208 mse_val: 0.0254576989 acc_val: 0.6758620690 time: 3.0748s
Epoch: 0099 nll_train: 1277.8400573730 kl_train: -2.9467887878 mse_train: 0.0067254734 acc_train: 0.1068965517 nll_val: 4840.0932617188 kl_val: -4.6884183884 mse_val: 0.0254741721 acc_val: 0.6781609195 time: 3.0005s
Epoch: 0100 nll_train: 1283.4519653320 kl_train: -2.9594045877 mse_train: 0.0067550103 acc_train: 0.1074712644 nll_val: 4840.8579101562 kl_val: -4.6954727173 mse_val: 0.0254781973 acc_val: 0.6781609195 time: 2.9026s
Epoch: 0101 nll_train: 1282.3240356445 kl_train: -2.9722421169 mse_train: 0.0067490737 acc_train: 0.1091954023 nll_val: 4842.2055664062 kl_val: -4.7058954239 mse_val: 0.0254852884 acc_val: 0.6804597701 time: 2.9818s
Epoch: 0102 nll_train: 1277.5796203613 kl_train: -2.9854512215 mse_train: 0.0067241029 acc_train: 0.1097701149 nll_val: 4840.6298828125 kl_val: -4.7140898705 mse_val: 0.0254769940 acc_val: 0.6816091954 time: 4.0811s
Epoch: 0103 nll_train: 1283.9193725586 kl_train: -2.9990847111 mse_train: 0.0067574701 acc_train: 0.1114942529 nll_val: 4839.2700195312 kl_val: -4.7158231735 mse_val: 0.0254698396 acc_val: 0.6793103448 time: 3.9032s
Epoch: 0104 nll_train: 1279.3278503418 kl_train: -3.0126364231 mse_train: 0.0067333039 acc_train: 0.1114942529 nll_val: 4839.3183593750 kl_val: -4.7234911919 mse_val: 0.0254700966 acc_val: 0.6804597701 time: 3.8846s
Epoch: 0105 nll_train: 1277.8382873535 kl_train: -3.0264565945 mse_train: 0.0067254644 acc_train: 0.1109195402 nll_val: 4844.0463867188 kl_val: -4.7280230522 mse_val: 0.0254949816 acc_val: 0.6827586207 time: 3.8685s
Epoch: 0106 nll_train: 1281.6685791016 kl_train: -3.0402147770 mse_train: 0.0067456235 acc_train: 0.1120689655 nll_val: 4843.2358398438 kl_val: -4.7346544266 mse_val: 0.0254907142 acc_val: 0.6816091954 time: 15.6542s
Epoch: 0107 nll_train: 1277.0205688477 kl_train: -3.0538434982 mse_train: 0.0067211607 acc_train: 0.1114942529 nll_val: 4844.9350585938 kl_val: -4.7498488426 mse_val: 0.0254996587 acc_val: 0.6827586207 time: 4.4913s
Epoch: 0108 nll_train: 1278.4876098633 kl_train: -3.0674743652 mse_train: 0.0067288819 acc_train: 0.1114942529 nll_val: 4844.9633789062 kl_val: -4.7606801987 mse_val: 0.0254998077 acc_val: 0.6839080460 time: 3.9147s
Epoch: 0109 nll_train: 1280.0151367188 kl_train: -3.0810565948 mse_train: 0.0067369210 acc_train: 0.1120689655 nll_val: 4843.3588867188 kl_val: -4.7693696022 mse_val: 0.0254913606 acc_val: 0.6862068966 time: 3.7038s
Epoch: 0110 nll_train: 1276.9469299316 kl_train: -3.0942666531 mse_train: 0.0067207734 acc_train: 0.1109195402 nll_val: 4844.6860351562 kl_val: -4.7820792198 mse_val: 0.0254983492 acc_val: 0.6839080460 time: 3.6855s
Epoch: 0111 nll_train: 1277.3063354492 kl_train: -3.1071456671 mse_train: 0.0067226646 acc_train: 0.1114942529 nll_val: 4846.3554687500 kl_val: -4.7972984314 mse_val: 0.0255071316 acc_val: 0.6827586207 time: 3.8692s
Epoch: 0112 nll_train: 1277.2128295898 kl_train: -3.1200656891 mse_train: 0.0067221716 acc_train: 0.1114942529 nll_val: 4849.1879882812 kl_val: -4.8196496964 mse_val: 0.0255220421 acc_val: 0.6816091954 time: 4.3669s
Epoch: 0113 nll_train: 1275.9018859863 kl_train: -3.1329965591 mse_train: 0.0067152726 acc_train: 0.1103448276 nll_val: 4850.4052734375 kl_val: -4.8435840607 mse_val: 0.0255284496 acc_val: 0.6816091954 time: 3.7127s
Epoch: 0114 nll_train: 1277.9623718262 kl_train: -3.1461547613 mse_train: 0.0067261176 acc_train: 0.1097701149 nll_val: 4853.2509765625 kl_val: -4.8637790680 mse_val: 0.0255434234 acc_val: 0.6816091954 time: 5.0688s
Epoch: 0115 nll_train: 1275.7173156738 kl_train: -3.1594139338 mse_train: 0.0067143011 acc_train: 0.1103448276 nll_val: 4854.9824218750 kl_val: -4.8791723251 mse_val: 0.0255525392 acc_val: 0.6827586207 time: 3.5829s
Epoch: 0116 nll_train: 1276.2086791992 kl_train: -3.1724156141 mse_train: 0.0067168871 acc_train: 0.1109195402 nll_val: 4856.3085937500 kl_val: -4.8922252655 mse_val: 0.0255595166 acc_val: 0.6827586207 time: 3.8912s
Epoch: 0117 nll_train: 1275.5041503906 kl_train: -3.1852103472 mse_train: 0.0067131794 acc_train: 0.1109195402 nll_val: 4860.6406250000 kl_val: -4.9038953781 mse_val: 0.0255823154 acc_val: 0.6816091954 time: 3.8034s
Epoch: 0118 nll_train: 1274.9748840332 kl_train: -3.1978111267 mse_train: 0.0067103932 acc_train: 0.1097701149 nll_val: 4860.6562500000 kl_val: -4.9107050896 mse_val: 0.0255823974 acc_val: 0.6816091954 time: 3.9206s
Epoch: 0119 nll_train: 1275.7165527344 kl_train: -3.2100925446 mse_train: 0.0067142977 acc_train: 0.1086206897 nll_val: 4859.1684570312 kl_val: -4.9206080437 mse_val: 0.0255745687 acc_val: 0.6758620690 time: 4.9427s
Epoch: 0120 nll_train: 1273.3575744629 kl_train: -3.2220351696 mse_train: 0.0067018816 acc_train: 0.1080459770 nll_val: 4859.9052734375 kl_val: -4.9336743355 mse_val: 0.0255784467 acc_val: 0.6747126437 time: 3.9950s
Epoch: 0121 nll_train: 1274.5507202148 kl_train: -3.2346163988 mse_train: 0.0067081614 acc_train: 0.1074712644 nll_val: 4862.1308593750 kl_val: -4.9485883713 mse_val: 0.0255901571 acc_val: 0.6758620690 time: 4.0647s
Epoch: 0122 nll_train: 1274.4237670898 kl_train: -3.2477122545 mse_train: 0.0067074934 acc_train: 0.1068965517 nll_val: 4863.1660156250 kl_val: -4.9612603188 mse_val: 0.0255956110 acc_val: 0.6758620690 time: 3.8237s
Epoch: 0123 nll_train: 1272.0378417969 kl_train: -3.2609061003 mse_train: 0.0066949363 acc_train: 0.1074712644 nll_val: 4863.5590820312 kl_val: -4.9712576866 mse_val: 0.0255976785 acc_val: 0.6747126437 time: 3.7273s
Epoch: 0124 nll_train: 1273.3587951660 kl_train: -3.2738165855 mse_train: 0.0067018882 acc_train: 0.1068965517 nll_val: 4864.5522460938 kl_val: -4.9796533585 mse_val: 0.0256029051 acc_val: 0.6758620690 time: 3.8545s
Epoch: 0125 nll_train: 1272.0430297852 kl_train: -3.2867224216 mse_train: 0.0066949626 acc_train: 0.1063218391 nll_val: 4866.0659179688 kl_val: -4.9953012466 mse_val: 0.0256108716 acc_val: 0.6735632184 time: 3.6742s
Epoch: 0126 nll_train: 1274.2283020020 kl_train: -3.3000161648 mse_train: 0.0067064642 acc_train: 0.1063218391 nll_val: 4865.4106445312 kl_val: -5.0107588768 mse_val: 0.0256074257 acc_val: 0.6724137931 time: 4.0193s
Epoch: 0127 nll_train: 1271.3274536133 kl_train: -3.3136487007 mse_train: 0.0066911975 acc_train: 0.1068965517 nll_val: 4864.8608398438 kl_val: -5.0225777626 mse_val: 0.0256045293 acc_val: 0.6712643678 time: 3.9202s
Epoch: 0128 nll_train: 1270.8601379395 kl_train: -3.3276191950 mse_train: 0.0066887377 acc_train: 0.1063218391 nll_val: 4868.1484375000 kl_val: -5.0370922089 mse_val: 0.0256218333 acc_val: 0.6724137931 time: 3.9767s
Epoch: 0129 nll_train: 1269.2355957031 kl_train: -3.3415760994 mse_train: 0.0066801871 acc_train: 0.1063218391 nll_val: 4865.9370117188 kl_val: -5.0489754677 mse_val: 0.0256101936 acc_val: 0.6712643678 time: 3.5555s
Epoch: 0130 nll_train: 1271.6300048828 kl_train: -3.3547703028 mse_train: 0.0066927886 acc_train: 0.1051724138 nll_val: 4864.2099609375 kl_val: -5.0577054024 mse_val: 0.0256011002 acc_val: 0.6701149425 time: 5.2908s
Epoch: 0131 nll_train: 1272.0923461914 kl_train: -3.3675241470 mse_train: 0.0066952227 acc_train: 0.1045977011 nll_val: 4853.8774414062 kl_val: -5.0752100945 mse_val: 0.0255467240 acc_val: 0.6712643678 time: 3.8165s
Epoch: 0132 nll_train: 1267.8008117676 kl_train: -3.3810559511 mse_train: 0.0066726360 acc_train: 0.1045977011 nll_val: 4856.9990234375 kl_val: -5.0864677429 mse_val: 0.0255631506 acc_val: 0.6712643678 time: 4.1348s
Epoch: 0133 nll_train: 1271.3913574219 kl_train: -3.3951835632 mse_train: 0.0066915333 acc_train: 0.1040229885 nll_val: 4865.1884765625 kl_val: -5.1049423218 mse_val: 0.0256062541 acc_val: 0.6724137931 time: 3.8253s
Epoch: 0134 nll_train: 1266.9278259277 kl_train: -3.4097267389 mse_train: 0.0066680416 acc_train: 0.1040229885 nll_val: 4873.9458007812 kl_val: -5.1239523888 mse_val: 0.0256523434 acc_val: 0.6724137931 time: 3.6232s
Epoch: 0135 nll_train: 1269.8577880859 kl_train: -3.4248820543 mse_train: 0.0066834616 acc_train: 0.1045977011 nll_val: 4866.4096679688 kl_val: -5.1378741264 mse_val: 0.0256126840 acc_val: 0.6701149425 time: 5.6118s
Epoch: 0136 nll_train: 1268.8148498535 kl_train: -3.4399282932 mse_train: 0.0066779722 acc_train: 0.1034482759 nll_val: 4855.6552734375 kl_val: -5.1543703079 mse_val: 0.0255560782 acc_val: 0.6678160920 time: 3.8233s
Epoch: 0137 nll_train: 1265.6027221680 kl_train: -3.4542269707 mse_train: 0.0066610669 acc_train: 0.1028735632 nll_val: 4852.8442382812 kl_val: -5.1727852821 mse_val: 0.0255412851 acc_val: 0.6689655172 time: 3.7294s
Epoch: 0138 nll_train: 1268.7014160156 kl_train: -3.4689117670 mse_train: 0.0066773754 acc_train: 0.1028735632 nll_val: 4861.6933593750 kl_val: -5.1976428032 mse_val: 0.0255878586 acc_val: 0.6701149425 time: 3.6913s
Epoch: 0139 nll_train: 1272.9253845215 kl_train: -3.4851421118 mse_train: 0.0066996069 acc_train: 0.1028735632 nll_val: 4878.2475585938 kl_val: -5.2238717079 mse_val: 0.0256749876 acc_val: 0.6724137931 time: 3.6851s
Epoch: 0140 nll_train: 1264.2785034180 kl_train: -3.5021398067 mse_train: 0.0066540968 acc_train: 0.1028735632 nll_val: 4890.4169921875 kl_val: -5.2521357536 mse_val: 0.0257390346 acc_val: 0.6701149425 time: 3.7605s
Epoch: 0141 nll_train: 1267.8770141602 kl_train: -3.5198986530 mse_train: 0.0066730366 acc_train: 0.1011494253 nll_val: 4896.0849609375 kl_val: -5.2717781067 mse_val: 0.0257688649 acc_val: 0.6712643678 time: 3.8844s
Epoch: 0142 nll_train: 1265.1777648926 kl_train: -3.5380403996 mse_train: 0.0066588304 acc_train: 0.1011494253 nll_val: 4895.7602539062 kl_val: -5.2849912643 mse_val: 0.0257671569 acc_val: 0.6712643678 time: 3.7321s
Epoch: 0143 nll_train: 1265.6874694824 kl_train: -3.5567418337 mse_train: 0.0066615134 acc_train: 0.1000000000 nll_val: 4895.8261718750 kl_val: -5.2956004143 mse_val: 0.0257675070 acc_val: 0.6689655172 time: 3.7171s
Epoch: 0144 nll_train: 1263.7456054688 kl_train: -3.5764930248 mse_train: 0.0066512921 acc_train: 0.0994252874 nll_val: 4903.7763671875 kl_val: -5.3128933907 mse_val: 0.0258093495 acc_val: 0.6678160920 time: 3.5879s
Epoch: 0145 nll_train: 1266.7725219727 kl_train: -3.5964614153 mse_train: 0.0066672233 acc_train: 0.0988505747 nll_val: 4915.2993164062 kl_val: -5.3485474586 mse_val: 0.0258699954 acc_val: 0.6678160920 time: 3.6663s
Epoch: 0146 nll_train: 1273.6994934082 kl_train: -3.6166189909 mse_train: 0.0067036817 acc_train: 0.0954022989 nll_val: 4890.7734375000 kl_val: -5.3587298393 mse_val: 0.0257409085 acc_val: 0.6678160920 time: 3.9524s
Epoch: 0147 nll_train: 1264.8886718750 kl_train: -3.6373869181 mse_train: 0.0066573084 acc_train: 0.0954022989 nll_val: 4875.9130859375 kl_val: -5.3867735863 mse_val: 0.0256626979 acc_val: 0.6666666667 time: 3.9313s
Epoch: 0148 nll_train: 1257.3614807129 kl_train: -3.6573919058 mse_train: 0.0066176921 acc_train: 0.0954022989 nll_val: 4856.8999023438 kl_val: -5.3955860138 mse_val: 0.0255626272 acc_val: 0.6643678161 time: 3.7044s
Epoch: 0149 nll_train: 1272.6911621094 kl_train: -3.6781105995 mse_train: 0.0066983743 acc_train: 0.0954022989 nll_val: 4841.9062500000 kl_val: -5.4078307152 mse_val: 0.0254837181 acc_val: 0.6609195402 time: 3.7515s
Epoch: 0150 nll_train: 1253.8343200684 kl_train: -3.6984935999 mse_train: 0.0065991278 acc_train: 0.0942528736 nll_val: 4852.4780273438 kl_val: -5.4301018715 mse_val: 0.0255393572 acc_val: 0.6609195402 time: 3.6533s
Epoch: 0151 nll_train: 1269.7560119629 kl_train: -3.7200024128 mse_train: 0.0066829260 acc_train: 0.0942528736 nll_val: 4849.4711914062 kl_val: -5.4349694252 mse_val: 0.0255235285 acc_val: 0.6551724138 time: 4.3476s
Epoch: 0152 nll_train: 1260.4241943359 kl_train: -3.7427500486 mse_train: 0.0066338113 acc_train: 0.0931034483 nll_val: 4858.3320312500 kl_val: -5.4747447968 mse_val: 0.0255701672 acc_val: 0.6540229885 time: 3.9232s
Epoch: 0153 nll_train: 1259.2271728516 kl_train: -3.7654341459 mse_train: 0.0066275109 acc_train: 0.0931034483 nll_val: 4857.9824218750 kl_val: -5.4986453056 mse_val: 0.0255683269 acc_val: 0.6540229885 time: 3.9061s
Epoch: 0154 nll_train: 1265.3448181152 kl_train: -3.7886106968 mse_train: 0.0066597090 acc_train: 0.0936781609 nll_val: 4853.0307617188 kl_val: -5.5162925720 mse_val: 0.0255422648 acc_val: 0.6494252874 time: 3.9627s
Epoch: 0155 nll_train: 1254.1902465820 kl_train: -3.8116855621 mse_train: 0.0066010007 acc_train: 0.0925287356 nll_val: 4855.5512695312 kl_val: -5.5413951874 mse_val: 0.0255555324 acc_val: 0.6459770115 time: 4.0104s
Epoch: 0156 nll_train: 1264.6873474121 kl_train: -3.8354681730 mse_train: 0.0066562482 acc_train: 0.0919540230 nll_val: 4848.5000000000 kl_val: -5.5509400368 mse_val: 0.0255184211 acc_val: 0.6459770115 time: 3.8629s
Epoch: 0157 nll_train: 1260.7770385742 kl_train: -3.8592754602 mse_train: 0.0066356685 acc_train: 0.0931034483 nll_val: 4858.1088867188 kl_val: -5.5763478279 mse_val: 0.0255689900 acc_val: 0.6448275862 time: 3.7167s
Epoch: 0158 nll_train: 1255.7961120605 kl_train: -3.8825272322 mse_train: 0.0066094531 acc_train: 0.0925287356 nll_val: 4863.1967773438 kl_val: -5.5864849091 mse_val: 0.0255957711 acc_val: 0.6390804598 time: 3.3628s
Epoch: 0159 nll_train: 1264.0767211914 kl_train: -3.9067299366 mse_train: 0.0066530349 acc_train: 0.0919540230 nll_val: 4867.2412109375 kl_val: -5.6072025299 mse_val: 0.0256170575 acc_val: 0.6390804598 time: 3.4587s
Epoch: 0160 nll_train: 1250.6159667969 kl_train: -3.9304001331 mse_train: 0.0065821896 acc_train: 0.0908045977 nll_val: 4879.5073242188 kl_val: -5.6480374336 mse_val: 0.0256816167 acc_val: 0.6390804598 time: 3.5521s
Epoch: 0161 nll_train: 1264.1629333496 kl_train: -3.9556863308 mse_train: 0.0066534890 acc_train: 0.0890804598 nll_val: 4872.5166015625 kl_val: -5.6587996483 mse_val: 0.0256448239 acc_val: 0.6379310345 time: 3.4851s
Epoch: 0162 nll_train: 1253.8886413574 kl_train: -3.9805178642 mse_train: 0.0065994131 acc_train: 0.0879310345 nll_val: 4877.6728515625 kl_val: -5.6917281151 mse_val: 0.0256719608 acc_val: 0.6367816092 time: 3.2926s
Epoch: 0163 nll_train: 1256.8353576660 kl_train: -4.0063637495 mse_train: 0.0066149232 acc_train: 0.0879310345 nll_val: 4877.8110351562 kl_val: -5.7163209915 mse_val: 0.0256726872 acc_val: 0.6356321839 time: 3.9291s
Epoch: 0164 nll_train: 1254.8485412598 kl_train: -4.0332647562 mse_train: 0.0066044660 acc_train: 0.0885057471 nll_val: 4886.7270507812 kl_val: -5.7529788017 mse_val: 0.0257196147 acc_val: 0.6356321839 time: 3.4994s
Epoch: 0165 nll_train: 1253.1401977539 kl_train: -4.0594674349 mse_train: 0.0065954744 acc_train: 0.0873563218 nll_val: 4892.8525390625 kl_val: -5.7815775871 mse_val: 0.0257518552 acc_val: 0.6356321839 time: 3.6333s
Epoch: 0166 nll_train: 1256.6715393066 kl_train: -4.0857316256 mse_train: 0.0066140603 acc_train: 0.0867816092 nll_val: 4897.0615234375 kl_val: -5.8349571228 mse_val: 0.0257740077 acc_val: 0.6367816092 time: 3.4307s
Epoch: 0167 nll_train: 1249.9226379395 kl_train: -4.1127734184 mse_train: 0.0065785397 acc_train: 0.0844827586 nll_val: 4900.0795898438 kl_val: -5.9000163078 mse_val: 0.0257898923 acc_val: 0.6413793103 time: 3.4969s
Epoch: 0168 nll_train: 1256.6950988770 kl_train: -4.1411581039 mse_train: 0.0066141847 acc_train: 0.0833333333 nll_val: 4897.3554687500 kl_val: -5.9361104965 mse_val: 0.0257755518 acc_val: 0.6413793103 time: 3.4988s
Epoch: 0169 nll_train: 1249.7521667480 kl_train: -4.1697193384 mse_train: 0.0065776434 acc_train: 0.0798850575 nll_val: 4900.2954101562 kl_val: -5.9776892662 mse_val: 0.0257910248 acc_val: 0.6425287356 time: 3.4631s
Epoch: 0170 nll_train: 1253.3262023926 kl_train: -4.2004914284 mse_train: 0.0065964538 acc_train: 0.0775862069 nll_val: 4896.8305664062 kl_val: -5.9923744202 mse_val: 0.0257727895 acc_val: 0.6379310345 time: 3.5213s
Epoch: 0171 nll_train: 1249.8532409668 kl_train: -4.2322398424 mse_train: 0.0065781750 acc_train: 0.0741379310 nll_val: 4897.6010742188 kl_val: -6.0143384933 mse_val: 0.0257768482 acc_val: 0.6367816092 time: 3.5620s
Epoch: 0172 nll_train: 1250.4496765137 kl_train: -4.2640827894 mse_train: 0.0065813140 acc_train: 0.0724137931 nll_val: 4897.8173828125 kl_val: -6.0378332138 mse_val: 0.0257779863 acc_val: 0.6379310345 time: 3.4491s
Epoch: 0173 nll_train: 1249.9408264160 kl_train: -4.2967132330 mse_train: 0.0065786359 acc_train: 0.0706896552 nll_val: 4899.7563476562 kl_val: -6.0624256134 mse_val: 0.0257881898 acc_val: 0.6367816092 time: 3.5669s
Epoch: 0174 nll_train: 1247.7474670410 kl_train: -4.3300130367 mse_train: 0.0065670917 acc_train: 0.0695402299 nll_val: 4901.3422851562 kl_val: -6.0858945847 mse_val: 0.0257965364 acc_val: 0.6356321839 time: 3.7844s
Epoch: 0175 nll_train: 1249.0522766113 kl_train: -4.3636572361 mse_train: 0.0065739587 acc_train: 0.0678160920 nll_val: 4900.6464843750 kl_val: -6.1147723198 mse_val: 0.0257928763 acc_val: 0.6356321839 time: 3.3872s
Epoch: 0176 nll_train: 1244.7320556641 kl_train: -4.3975085020 mse_train: 0.0065512208 acc_train: 0.0678160920 nll_val: 4905.2783203125 kl_val: -6.1452536583 mse_val: 0.0258172527 acc_val: 0.6321839080 time: 3.3711s
Epoch: 0177 nll_train: 1248.3502197266 kl_train: -4.4329290390 mse_train: 0.0065702641 acc_train: 0.0655172414 nll_val: 4903.8447265625 kl_val: -6.1698265076 mse_val: 0.0258097090 acc_val: 0.6310344828 time: 3.4318s
Epoch: 0178 nll_train: 1242.2619628906 kl_train: -4.4697359800 mse_train: 0.0065382213 acc_train: 0.0626436782 nll_val: 4909.9477539062 kl_val: -6.2078337669 mse_val: 0.0258418284 acc_val: 0.6321839080 time: 3.3838s
Epoch: 0179 nll_train: 1248.0752563477 kl_train: -4.5066603422 mse_train: 0.0065688167 acc_train: 0.0591954023 nll_val: 4910.8520507812 kl_val: -6.2246661186 mse_val: 0.0258465838 acc_val: 0.6275862069 time: 3.8901s
Epoch: 0180 nll_train: 1242.1118164062 kl_train: -4.5409561396 mse_train: 0.0065374300 acc_train: 0.0586206897 nll_val: 4909.6645507812 kl_val: -6.2511324883 mse_val: 0.0258403365 acc_val: 0.6275862069 time: 4.1759s
Epoch: 0181 nll_train: 1246.7256774902 kl_train: -4.5750128031 mse_train: 0.0065617141 acc_train: 0.0568965517 nll_val: 4909.5776367188 kl_val: -6.2674069405 mse_val: 0.0258398801 acc_val: 0.6229885057 time: 3.5227s
Epoch: 0182 nll_train: 1237.8168334961 kl_train: -4.6094261408 mse_train: 0.0065148253 acc_train: 0.0563218391 nll_val: 4918.0473632812 kl_val: -6.2870326042 mse_val: 0.0258844588 acc_val: 0.6195402299 time: 3.6096s
Epoch: 0183 nll_train: 1246.3332519531 kl_train: -4.6449099779 mse_train: 0.0065596487 acc_train: 0.0545977011 nll_val: 4912.3354492188 kl_val: -6.2819008827 mse_val: 0.0258543957 acc_val: 0.6149425287 time: 2.8019s
Epoch: 0184 nll_train: 1236.3421020508 kl_train: -4.6797721386 mse_train: 0.0065070629 acc_train: 0.0545977011 nll_val: 4921.0693359375 kl_val: -6.2715034485 mse_val: 0.0259003602 acc_val: 0.6091954023 time: 2.8061s
Epoch: 0185 nll_train: 1245.5720520020 kl_train: -4.7152855396 mse_train: 0.0065556424 acc_train: 0.0545977011 nll_val: 4913.9755859375 kl_val: -6.2644157410 mse_val: 0.0258630291 acc_val: 0.6034482759 time: 2.8065s
Epoch: 0186 nll_train: 1235.7768859863 kl_train: -4.7504055500 mse_train: 0.0065040884 acc_train: 0.0540229885 nll_val: 4919.8432617188 kl_val: -6.2931995392 mse_val: 0.0258939080 acc_val: 0.6011494253 time: 2.8545s
Epoch: 0187 nll_train: 1245.2985534668 kl_train: -4.7841780186 mse_train: 0.0065542028 acc_train: 0.0534482759 nll_val: 4931.0410156250 kl_val: -6.3377971649 mse_val: 0.0259528458 acc_val: 0.5965517241 time: 2.7223s
Epoch: 0188 nll_train: 1262.6687316895 kl_train: -4.8165192604 mse_train: 0.0066456246 acc_train: 0.0528735632 nll_val: 4915.2211914062 kl_val: -6.5075745583 mse_val: 0.0258695837 acc_val: 0.6126436782 time: 2.6744s
Epoch: 0189 nll_train: 1222.6932678223 kl_train: -4.8468403816 mse_train: 0.0064352276 acc_train: 0.0511494253 nll_val: 4932.1562500000 kl_val: -6.6683816910 mse_val: 0.0259587169 acc_val: 0.6126436782 time: 2.6567s
Epoch: 0190 nll_train: 1261.9869384766 kl_train: -4.8834774494 mse_train: 0.0066420360 acc_train: 0.0494252874 nll_val: 4916.2578125000 kl_val: -6.7293262482 mse_val: 0.0258750394 acc_val: 0.6126436782 time: 2.6257s
Epoch: 0191 nll_train: 1215.4982604980 kl_train: -4.9145517349 mse_train: 0.0063973590 acc_train: 0.0488505747 nll_val: 4932.3603515625 kl_val: -6.8359241486 mse_val: 0.0259597898 acc_val: 0.6160919540 time: 2.6158s
Epoch: 0192 nll_train: 1264.5123291016 kl_train: -4.9540624619 mse_train: 0.0066553277 acc_train: 0.0482758621 nll_val: 4909.4067382812 kl_val: -6.8761272430 mse_val: 0.0258389823 acc_val: 0.6183908046 time: 3.1185s
Epoch: 0193 nll_train: 1209.0375061035 kl_train: -4.9909360409 mse_train: 0.0063633558 acc_train: 0.0477011494 nll_val: 4953.3779296875 kl_val: -7.0728158951 mse_val: 0.0260704085 acc_val: 0.6298850575 time: 7.0826s
Epoch: 0194 nll_train: 1284.4629821777 kl_train: -5.0387978554 mse_train: 0.0067603313 acc_train: 0.0471264368 nll_val: 4917.0097656250 kl_val: -7.0701074600 mse_val: 0.0258789957 acc_val: 0.6287356322 time: 4.3511s
Epoch: 0195 nll_train: 1202.5079650879 kl_train: -5.0740127563 mse_train: 0.0063289893 acc_train: 0.0442528736 nll_val: 4961.7314453125 kl_val: -7.3734788895 mse_val: 0.0261143781 acc_val: 0.6482758621 time: 3.4059s
Epoch: 0196 nll_train: 1301.8965759277 kl_train: -5.1248917580 mse_train: 0.0068520872 acc_train: 0.0448275862 nll_val: 4923.9218750000 kl_val: -7.3535571098 mse_val: 0.0259153768 acc_val: 0.6413793103 time: 3.3259s
Epoch: 0197 nll_train: 1196.7521972656 kl_train: -5.1596424580 mse_train: 0.0062986956 acc_train: 0.0413793103 nll_val: 4976.2666015625 kl_val: -7.7330064774 mse_val: 0.0261908751 acc_val: 0.6632183908 time: 3.3160s
Epoch: 0198 nll_train: 1315.3873901367 kl_train: -5.2134578228 mse_train: 0.0069230916 acc_train: 0.0390804598 nll_val: 4947.0957031250 kl_val: -7.7088818550 mse_val: 0.0260373428 acc_val: 0.6540229885 time: 3.0630s
Epoch: 0199 nll_train: 1214.5033874512 kl_train: -5.2442319393 mse_train: 0.0063921229 acc_train: 0.0379310345 nll_val: 4921.2944335938 kl_val: -8.1058788300 mse_val: 0.0259015467 acc_val: 0.6793103448 time: 3.0687s
Optimization finished
Best epoch 50
--------------------------------
--------Testing-----------------
--------------------------------
nll_test: 406.7773742676 kl_test: -1.9798175097 mse_test: 0.0021409334 acc_test: 0.0494252874
MSE: [ 0.001092709485 , 0.000952842704 , 0.000964733306 , 0.000966260501 , 0.001005577156 , 0.000946188753 , 0.001466072979 , 0.001627661870 , 0.001574965660 , 0.001903521246 , 0.001882297802 , 0.001841134392 , 0.002048286609 , 0.002106932923 , 0.002269322518 , 0.002222494222 , 0.002319290303 , 0.002370205941 , 0.002393657109 ]
Accuracy for experiment id 14 is 0.04777777777777778
Accuracy for experiment id 15 is 0.0
Accuracy for experiment id 16 is 0.0
Accuracy for experiment id 17 is 0.0
Accuracy for experiment id 18 is 0.0
Accuracy for experiment id 19 is 0.0


###############################################################################
Science Cluster
Job 2541272 for user 'bjerkovic'
Finished at: Fri Dec  2 04:15:58 CET 2022

Job details:
============

Name                : exp8
User                : bjerkovic
Partition           : csedu
Nodes               : cn48
Cores               : 2
State               : COMPLETED
Submit              : 2022-12-02T02:07:52
Start               : 2022-12-02T02:07:53
End                 : 2022-12-02T04:15:58
Reserved walltime   : 1-06:00:00
Used walltime       :   02:08:05
Used CPU time       :   02:36:26 (efficiency: 61.07%)
% User (Computation): 61.56%
% System (I/O)      : 38.44%
Mem reserved        : 2G/core
Max Mem used        : 3.30G (cn48)
Max Disk Write      : 0.00  (cn48)
Max Disk Read       : 849.92K (cn48)

